{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>PYthon Neural Analysis Package.</p> <p>pynapple is a light-weight python library for neurophysiological data analysis. The goal is to offer a versatile set of tools to study typical data in the field, i.e. time series (spike times, behavioral events, etc.) and time intervals (trials, brain states, etc.). It also provides users with generic functions for neuroscience such as tuning curves and cross-correlograms.</p> <ul> <li>Free software: MIT License</li> <li>Documentation: https://pynapple-org.github.io/pynapple</li> <li>Notebooks and tutorials : https://pynapple-org.github.io/pynapple/generated/gallery/</li> </ul> <p>Note If you are using pynapple, please cite the following biorxiv paper</p>"},{"location":"#new-release","title":"New release","text":"<p>The version 0.4 of pynapple will rely on the numpy array container approach instead of Pandas. Pynapple builtin functions will remain the same except for functions inherited from Pandas. Typically this line of code in <code>pynapple&lt;=0.3.6</code> : <pre><code>meantsd = tsdframe.mean(1)\n</code></pre> will now be : <pre><code>meantsd = np.mean(tsdframe, 1)\n</code></pre> in <code>pynapple&gt;=0.4.0</code>. This allows for a better handling of returned objects.</p> <p>Additionaly, it is now possible to define time series objects with more than 2 dimensions with <code>TsdTensor</code>. You can also look at this notebook for a demonstration of numpy compatibilities.</p>"},{"location":"#_1","title":"Overview","text":""},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<p>The best way to install pynapple is with pip within a new conda environment :</p> <pre><code>$ conda create --name pynapple pip python=3.8\n$ conda activate pynapple\n$ pip install pynapple\n</code></pre> <p>or directly from the source code:</p> <pre><code>$ conda create --name pynapple pip python=3.8\n$ conda activate pynapple\n$ # clone the repository\n$ git clone https://github.com/pynapple-org/pynapple.git\n$ cd pynapple\n$ # Install in editable mode with `-e` or, equivalently, `--editable`\n$ pip install -e .\n</code></pre> <p>Note The package is now using a pyproject.toml file for installation and dependencies management. If you want to run the tests, use pip install -e .[dev]</p> <p>This procedure will install all the dependencies including </p> <ul> <li>pandas</li> <li>numpy</li> <li>scipy</li> <li>numba</li> <li>pynwb 2.0</li> <li>tabulate</li> <li>h5py</li> </ul> <p>For spyder users, it is recommended to install spyder after installing pynapple with :</p> <pre><code>$ conda create --name pynapple pip python=3.8\n$ conda activate pynapple\n$ pip install pynapple\n$ pip install spyder\n$ spyder\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>After installation, you can now import the package: </p> <pre><code>$ python\n&gt;&gt;&gt; import pynapple as nap\n</code></pre> <p>You'll find an example of the package below. Click here to download the example dataset. The folder includes a NWB file containing the data.</p> <p><pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pynapple as nap\n# LOADING DATA FROM NWB\ndata = nap.load_file(\"A2929-200711.nwb\")\nspikes = data[\"units\"]\nhead_direction = data[\"ry\"]\nwake_ep = data[\"position_time_support\"]\n# COMPUTING TUNING CURVES\ntuning_curves = nap.compute_1d_tuning_curves(\nspikes, head_direction, 120, minmax=(0, 2 * np.pi)\n)\n# PLOT\nplt.figure()\nfor i in spikes:\nplt.subplot(3, 5, i + 1, projection=\"polar\")\nplt.plot(tuning_curves[i])\nplt.xticks([0, np.pi / 2, np.pi, 3 * np.pi / 2])\nplt.show()\n</code></pre> Shown below, the final figure from the example code displays the firing rate of 15 neurons as a function of the direction of the head of the animal in the horizontal plane.</p> <p> </p>"},{"location":"#credits","title":"Credits","text":"<p>Special thanks to Francesco P. Battaglia (https://github.com/fpbattaglia) for the development of the original TSToolbox (https://github.com/PeyracheLab/TStoolbox) and neuroseries (https://github.com/NeuroNetMem/neuroseries) packages, the latter constituting the core of pynapple.</p> <p>This package was developped by Guillaume Viejo (https://github.com/gviejo) and other members of the Peyrache Lab.</p> <p>Logo: Sofia Skromne Carrasco, 2021.</p>"},{"location":"AUTHORS/","title":"Credits","text":""},{"location":"AUTHORS/#development-lead","title":"Development Lead","text":"<ul> <li>Guillaume Viejo guillaume.viejo@gmail.com</li> </ul>"},{"location":"AUTHORS/#contributors","title":"Contributors","text":"<ul> <li>Adrien Peyrache</li> <li>Dan Levenstein</li> <li>Sofia Skromne Carrasco</li> <li>Sara Mahallati</li> <li>Gilberto Vite</li> <li>Davide Spalla</li> <li>Luigi Petrucco</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/pynapple-org/pynapple/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in     troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \\\"bug\\\" and \\\"help wanted\\\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \\\"enhancement\\\" and \\\"help wanted\\\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>pynapple could always use more documentation, whether as part of the official pynapple docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/pynapple-org/pynapple/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to     implement.</li> <li>Remember that this is a volunteer-driven project, and that     contributions are welcome :)</li> </ul>"},{"location":"CONTRIBUTING/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up [pynapple]{https://github.com/pynapple-org/pynapple} for local development.</p> <ol> <li>Fork the [pynapple]{https://github.com/pynapple-org/pynapple} repo on GitHub.</li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/pynapple.git\n</code></pre> </li> <li> <p>Install your local copy with pip. </p> <pre><code>$ cd pynapple/\n$ pip install -e .\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> </ol> <ol> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.nd.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy.      </li> </ol>"},{"location":"HISTORY/","title":"History","text":"<p>This package somehow started about 20 years ago in Bruce McNaughton's lab. Dave Redish started the TSToolbox package in Matlab.  Another postdoc in the lab, Francesco Battaglia, then made major contributions to the package. Francesco passed it on to Adrien Peyrache and other trainees in Paris and The Netherlands. Around 2016-2017, Luke Sjulson started TSToolbox2, still in Matlab and which includes some important changes.</p> <p>In 2018, Francesco started neuroseries, a Python package built on Pandas. It was quickly adopted in Adrien's lab, especially by Guillaume Viejo, a postdoc in the lab. Gradually, the majority of the lab was using it and new functions were constantly added. In 2021, Guillaume and other trainees in Adrien's lab decided to fork from neuroseries and started pynapple. The core of pynapple is largely built upon neuroseries. Some of the original changes to TSToolbox made by Luke were included in this package, especially the time_support property of all ts/tsd objects.</p>"},{"location":"HISTORY/#036-2023-09-11","title":"0.3.6 (2023-09-11)","text":"<ul> <li>Fix issue in NWB reader class with units</li> <li>Implement a linear interpolation function.</li> </ul>"},{"location":"HISTORY/#035-2023-08-08","title":"0.3.5 (2023-08-08)","text":"<ul> <li>NWB reader class</li> <li>NPZ reader class</li> <li>Folder class for navigating a dataset.</li> <li>Cross-correlograms function can take tuple</li> <li>New doc with mkdocs-gallery</li> </ul>"},{"location":"HISTORY/#034-2023-06-29","title":"0.3.4 (2023-06-29)","text":"<ul> <li>TsGroup.to_tsd and Tsd.to_tsgroup transformations</li> <li>Count can take IntervalSet</li> <li>Saving to npz functions for all objects.</li> <li>tsd.value_from can take TsdFrame</li> <li>Warning message for deprecating current IO. </li> </ul>"},{"location":"HISTORY/#033-2023-04-17","title":"0.3.3 (2023-04-17)","text":"<ul> <li>Fixed minor bug with tkinter</li> </ul>"},{"location":"HISTORY/#032-2023-04-12","title":"0.3.2 (2023-04-12)","text":"<ul> <li>PyQt removed from the list of dependencies</li> </ul>"},{"location":"HISTORY/#031-2022-12-08","title":"0.3.1 (2022-12-08)","text":"<ul> <li>Core functions rewritten with Numba</li> </ul>"},{"location":"HISTORY/#024-2022-05-02","title":"0.2.4 (2022-05-02)","text":""},{"location":"HISTORY/#023-2022-04-05","title":"0.2.3 (2022-04-05)","text":"<ul> <li>Fixed minor bug when saving DLC in NWB.</li> </ul>"},{"location":"HISTORY/#023-2022-04-05_1","title":"0.2.3 (2022-04-05)","text":"<ul> <li>Alpha release</li> </ul>"},{"location":"HISTORY/#022-2022-04-05","title":"0.2.2 (2022-04-05)","text":"<ul> <li>Beta testing version for public</li> </ul>"},{"location":"HISTORY/#021-2022-02-07","title":"0.2.1 (2022-02-07)","text":"<ul> <li>Beta testing version for Peyrache Lab.</li> </ul>"},{"location":"HISTORY/#020-2022-01-10","title":"0.2.0 (2022-01-10)","text":"<ul> <li>First version for pynapple with main features in core, process and IO.</li> </ul>"},{"location":"HISTORY/#020-pre-release-2022-01-06","title":"0.2.0 Pre-release (2022-01-06)","text":"<ul> <li>Pre-release version for pynapple with main features in core and process.</li> </ul>"},{"location":"HISTORY/#011-2021-10-25","title":"0.1.1 (2021-10-25)","text":"<ul> <li>First release on PyPI.</li> <li>Firt minimal version</li> </ul>"},{"location":"pynacollada/","title":"Pynacollada","text":""},{"location":"pynacollada/#python-neural-analysis-collaborative-repository","title":"Python neural analysis collaborative repository","text":""},{"location":"pynacollada/#pynacollada","title":"pynacollada","text":"<p>Collaborative platform for high-level analysis with pynapple. Check it out!</p>"},{"location":"generated/gallery/","title":"Usage","text":""},{"location":"generated/gallery/#examples","title":"Examples","text":"<p>Tutorials and examples for the pynapple package.</p> <p> IO Tutorial </p> <p> Numpy tutorial </p> <p> Streaming data from DANDI </p> <p> Core Tutorial </p> <p> Peyrache et al (2015) Dataset Tutorial </p> <p> Quick start </p> <p> Zheng et al (2022) Dataset Tutorial </p> <p> Advanced processing </p> <p> Download all examples in Python source code: gallery_python.zip</p> <p> Download all examples in Jupyter notebooks: gallery_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/mg_execution_times/","title":"Computation times","text":"<p>00:02.268 total execution time for generated_gallery files:</p> <p>+----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_HD_dataset (docs/examples/tutorial_HD_dataset.py)                               | 00:02.268 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_human_dataset (docs/examples/tutorial_human_dataset.py)                      | 00:00.000 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_pynapple_core (docs/examples/tutorial_pynapple_core.py)                      | 00:00.000 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_pynapple_dandi (docs/examples/tutorial_pynapple_dandi.py)                   | 00:00.000 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_pynapple_io (docs/examples/tutorial_pynapple_io.py)                            | 00:00.000 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_pynapple_numpy (docs/examples/tutorial_pynapple_numpy.py)                   | 00:00.000 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_pynapple_process (docs/examples/tutorial_pynapple_process.py)             | 00:00.000 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_pynapple_quick_start (docs/examples/tutorial_pynapple_quick_start.py) | 00:00.000 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/gallery/tutorial_HD_dataset/","title":"Peyrache et al (2015) Dataset Tutorial","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_HD_dataset/#peyrache-et-al-2015-dataset-tutorial","title":"Peyrache et al (2015) Dataset Tutorial","text":"<p>This tutorial demonstrates how we use Pynapple to generate Figure 4a in the publication. The NWB file for the example is hosted on OSF. We show below how to stream it. The entire dataset can be downloaded here.</p> <p>See the documentation of Pynapple for instructions on installing the package.</p> <p>This tutorial was made by Dhruv Mehrotra and Guillaume Viejo.</p> <p>Warning</p> <p>This tutorial uses seaborn and matplotlib for displaying the figure</p> <p>You can install all with <code>pip install matplotlib seaborn tqdm</code></p> <p>mkdocs_gallery_thumbnail_number = 2</p> <p>Now, import the necessary libraries:</p> <pre><code>import numpy as np\nimport pandas as pd\nimport pynapple as nap\nimport scipy.ndimage\nimport matplotlib.pyplot as plt\nimport requests, math, os\nimport tqdm\n</code></pre>"},{"location":"generated/gallery/tutorial_HD_dataset/#downloading-the-data","title":"Downloading the data","text":"<p>It's a small NWB file.</p> <pre><code>path = \"Mouse32-140822.nwb\"\nif path not in os.listdir(\".\"):\nr = requests.get(f\"https://osf.io/jb2gd/download\", stream=True)\nblock_size = 1024*1024\nwith open(path, 'wb') as f:\nfor data in tqdm.tqdm(r.iter_content(block_size), unit='MB', unit_scale=True,\ntotal=math.ceil(int(r.headers.get('content-length', 0))//block_size)):\nf.write(data)\n</code></pre>"},{"location":"generated/gallery/tutorial_HD_dataset/#parsing-the-data","title":"Parsing the data","text":"<p>The first step is to load the data and other relevant variables of interest</p> <pre><code>data = nap.load_file(path)  # Load the NWB file for this dataset\n</code></pre> <p>What does this look like ?</p> <pre><code>print(data)\n</code></pre> <p>Out:</p> <pre><code>Mouse32-140822\n\u250d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2511\n\u2502 Keys                  \u2502 Type        \u2502\n\u251d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2525\n\u2502 units                 \u2502 TsGroup     \u2502\n\u2502 sws                   \u2502 IntervalSet \u2502\n\u2502 rem                   \u2502 IntervalSet \u2502\n\u2502 position_time_support \u2502 IntervalSet \u2502\n\u2502 epochs                \u2502 IntervalSet \u2502\n\u2502 ry                    \u2502 Tsd         \u2502\n\u2515\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2519\n</code></pre>"},{"location":"generated/gallery/tutorial_HD_dataset/#head-direction-tuning-curves","title":"Head-Direction Tuning Curves","text":"<p>To plot Head-Direction Tuning curves, we need the spike timings and the orientation of the animal. These quantities are stored in the variables 'units' and 'ry'.</p> <pre><code>spikes = data[\"units\"]  # Get spike timings\nepochs = data[\"epochs\"]  # Get the behavioural epochs (in this case, sleep and wakefulness)\nangle = data[\"ry\"]  # Get the tracked orientation of the animal\n</code></pre> <p>What does this look like ?</p> <pre><code>print(spikes)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate  location      group\n-------  ------  ----------  -------\n      0    2.97  thalamus          1\n1    2.43  thalamus          1\n2    5.93  thalamus          1\n3    5.04  thalamus          1\n4    0.3   adn               2\n5    0.87  adn               2\n6    0.36  adn               2\n7   10.52  adn               2\n8    2.62  adn               2\n9    2.56  adn               2\n10    7.07  adn               2\n11    0.38  adn               2\n12    1.58  adn               2\n13    4.88  adn               2\n14    8.47  adn               2\n15    0.24  adn               3\n16    0.27  adn               3\n17    6.13  adn               3\n18   11.01  adn               3\n19    5.23  adn               3\n20    6.2   adn               3\n21    2.85  adn               3\n22    9.71  adn               3\n23    1.71  adn               3\n24   19.65  adn               3\n25    3.88  adn               3\n26    4.02  adn               3\n27    0.69  adn               3\n28    1.78  adn               4\n29    4.23  adn               4\n30    2.15  adn               4\n31    0.59  adn               4\n32    1.13  adn               4\n33    5.26  adn               4\n34    1.57  adn               4\n35    4.75  thalamus          5\n36    1.31  thalamus          5\n37    0.77  thalamus          5\n38    2.02  thalamus          5\n39   27.21  thalamus          5\n40    7.28  thalamus          5\n41    0.88  thalamus          5\n42    1.02  thalamus          5\n43    6.85  thalamus          6\n44    0.94  thalamus          6\n45    0.56  thalamus          6\n46    1.15  thalamus          6\n47    0.46  thalamus          6\n48    0.19  thalamus          7\n</code></pre> <p>Here, rate is the mean firing rate of the unit. Location indicates the brain region the unit was recorded from, and group refers to the shank number on which the cell was located.</p> <p>This dataset contains units recorded from the anterior thalamus. Head-direction (HD) cells are found in the anterodorsal nucleus of the thalamus (henceforth referred to as ADn). Units were also recorded from nearby thalamic nuclei in this animal. For the purposes of our tutorial, we are interested in the units recorded in ADn. We can restrict ourselves to analysis of these units rather easily, using Pynapple.</p> <pre><code>spikes_adn = spikes.getby_category(\"location\")[\"adn\"]  # Select only those units that are in ADn\n</code></pre> <p>What does this look like ?</p> <pre><code>print(spikes_adn)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate  location      group\n-------  ------  ----------  -------\n      4    0.3   adn               2\n5    0.87  adn               2\n6    0.36  adn               2\n7   10.52  adn               2\n8    2.62  adn               2\n9    2.56  adn               2\n10    7.07  adn               2\n11    0.38  adn               2\n12    1.58  adn               2\n13    4.88  adn               2\n14    8.47  adn               2\n15    0.24  adn               3\n16    0.27  adn               3\n17    6.13  adn               3\n18   11.01  adn               3\n19    5.23  adn               3\n20    6.2   adn               3\n21    2.85  adn               3\n22    9.71  adn               3\n23    1.71  adn               3\n24   19.65  adn               3\n25    3.88  adn               3\n26    4.02  adn               3\n27    0.69  adn               3\n28    1.78  adn               4\n29    4.23  adn               4\n30    2.15  adn               4\n31    0.59  adn               4\n32    1.13  adn               4\n33    5.26  adn               4\n34    1.57  adn               4\n</code></pre> <p>Let's compute some head-direction tuning curves. To do this in Pynapple, all you need is a single line of code!</p> <p>Plot firing rate of ADn units as a function of heading direction, i.e. a head-direction tuning curve</p> <pre><code>tuning_curves = nap.compute_1d_tuning_curves(\ngroup=spikes_adn, \nfeature=angle, \nnb_bins=61, \nep = epochs['wake'],\nminmax=(0, 2 * np.pi)\n)\n</code></pre> <p>What does this look like ?</p> <pre><code>print(tuning_curves)\n</code></pre> <p>Out:</p> <pre><code>                4         5         6          7         8         9   ...        29        30        31        32         33        34\n0.051502  0.255172  0.127586  0.170115   5.316080  0.000000  0.722987  ...  0.297700  0.850573  0.552872  0.467815   5.273551  1.786203\n0.154505  0.300635  0.000000  0.187897   8.305042  0.037579  0.338214  ...  0.375794  0.751587  1.014643  0.375794   8.380200  1.954127\n0.257508  0.189885  0.094943  0.094943  11.867814  0.158238  0.094943  ...  0.348123  1.076015  1.677318  0.158238  10.506971  3.892643\n0.360511  0.498062  0.052428  0.078641  16.724387  0.052428  0.314565  ...  0.366993  0.629131  1.782537  0.235924  12.556397  5.321396\n0.463514  0.362941  0.111674  0.139593  20.631824  0.055837  0.335023  ...  0.418779  0.642127  2.456834  0.307104  15.271458  8.431408\n...            ...       ...       ...        ...       ...       ...  ...       ...       ...       ...       ...        ...       ...\n5.819672  0.063460  0.158650  0.190380   1.491313  0.793252  0.507681  ...  0.095190  2.221104  0.095190  0.253841   0.285571  0.507681\n5.922675  0.024772  0.123861  0.123861   1.337703  0.569762  0.445901  ...  0.099089  1.535882  0.049545  0.396357   0.767941  0.272495\n6.025678  0.000000  0.112276  0.028069   1.936754  0.533309  0.224551  ...  0.112276  1.038549  0.084207  0.364896   0.954343  0.196482\n6.128681  0.000000  0.138009  0.165611   2.152940  0.220814  0.386425  ...  0.027602  0.938461  0.055204  0.331222   1.021266  0.441629\n6.231684  0.067699  0.101548  0.135397   2.504850  0.000000  0.304644  ...  0.101548  0.812384  0.304644  0.541589   2.234055  0.609288\n\n[61 rows x 31 columns]\n</code></pre> <p>Each row indicates an angular bin (in radians), and each column corresponds to a single unit. Let's compute the preferred angle quickly as follows:</p> <pre><code>pref_ang = tuning_curves.idxmax()\n</code></pre> <p>For easier visualization, we will colour our plots according to the preferred angle of the cell. To do so, we will normalize the range of angles we have, over a colourmap.</p> <pre><code>norm = plt.Normalize()  # Normalizes data into the range [0,1]\ncolor = plt.cm.hsv(norm([i / (2 * np.pi) for i in pref_ang.values]))  # Assigns a colour in the HSV colourmap for each value of preferred angle\ncolor = pd.DataFrame(index=pref_ang.index, data = color, columns = ['r', 'g', 'b', 'a'])\n</code></pre> <p>To make the tuning curves look nice, we will smooth them before plotting, using this custom function:</p> <pre><code>from scipy.ndimage import gaussian_filter1d\ndef smoothAngularTuningCurves(tuning_curves, sigma=2):\ntmp = np.concatenate((tuning_curves.values, tuning_curves.values, tuning_curves.values))\ntmp = gaussian_filter1d(tmp, sigma=sigma, axis=0)\nreturn pd.DataFrame(index = tuning_curves.index,\ndata = tmp[tuning_curves.shape[0]:tuning_curves.shape[0]*2], \ncolumns = tuning_curves.columns\n)\n</code></pre> <p>Therefore, we have:</p> <pre><code>smoothcurves = smoothAngularTuningCurves(tuning_curves, sigma=3)\n</code></pre> <p>What does this look like? Let's plot the tuning curves!</p> <pre><code>plt.figure(figsize=(12, 9))\nfor i, n in enumerate(pref_ang.sort_values().index.values):\nplt.subplot(8, 4, i + 1, projection='polar')  # Plot the curves in 8 rows and 4 columns\nplt.plot(\nsmoothcurves[n], color=color.loc[n]\n)  # Colour of the curves determined by preferred angle    \nplt.xlabel(\"Angle (rad)\")  # Angle in radian, on the X-axis\nplt.ylabel(\"Firing Rate (Hz)\")  # Firing rate in Hz, on the Y-axis\nplt.xticks([])\nplt.show()\n</code></pre> <p></p> <p>Awesome!</p>"},{"location":"generated/gallery/tutorial_HD_dataset/#decoding","title":"Decoding","text":"<p>Now that we have HD tuning curves, we can go one step further. Using only the population activity of ADn units, we can decode the direction the animal is looking in. We will then compare this to the real head direction of the animal, and discover that population activity in the ADn indeed codes for HD.</p> <p>To decode the population activity, we will be using a Bayesian Decoder as implemented in Pynapple. Just a single line of code!</p> <pre><code>decoded, proba_feature = nap.decode_1d(\ntuning_curves=tuning_curves,\ngroup=spikes_adn,\nep=epochs[\"wake\"],\nbin_size=0.1,  # second\nfeature=angle,\n)\n</code></pre> <p>What does this look like ?</p> <pre><code>print(decoded)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n---------------  --------\n8812.35          0.772523\n8812.45          0.66952\n8812.55          0.463514\n8812.65          0.66952\n8812.75          0.66952\n...\n10770.850000007  0.66952\n10770.950000007  0.66952\n10771.050000007  0.66952\n10771.150000007  0.66952\n10771.250000007  0.66952\ndtype: float64, shape: (19590,)\n</code></pre> <p>The variable 'decoded' indicates the most probable angle in which the animal was looking. There is another variable, 'proba_feature' that denotes the probability of a given angular bin at a given time point. We can look at it below:</p> <pre><code>print(proba_feature.as_dataframe())\n</code></pre> <p>Out:</p> <pre><code>              0.051502  0.154505      0.257508      0.360511  ...      5.922675      6.025678      6.128681      6.231684\n8812.35   2.199077e-06  0.000223  3.717901e-03  1.769861e-02  ...  6.116697e-18  2.510509e-15  1.483408e-14  2.212419e-11\n8812.45   8.561129e-08  0.000013  1.858017e-03  9.312326e-03  ...  2.920013e-20  1.609456e-18  1.628816e-16  2.464448e-13\n8812.55   4.168300e-04  0.022715  1.916560e-01  1.919596e-01  ...  9.577405e-12  2.723566e-10  2.698994e-09  4.014847e-07\n8812.65   1.082000e-05  0.000156  5.663501e-03  1.973657e-02  ...  1.160040e-15  2.430940e-14  6.175847e-13  4.426551e-10\n8812.75   4.128198e-05  0.001369  2.088684e-02  3.927845e-02  ...  7.808235e-14  1.152593e-12  5.077673e-12  5.290852e-09\n...                ...       ...           ...           ...  ...           ...           ...           ...           ...\n10770.85  6.695624e-05  0.000003  1.968111e-07  1.057744e-07  ...  1.980321e-01  3.692355e-02  3.310523e-02  4.842028e-03\n10770.95  2.924858e-04  0.000005  1.392026e-07  4.800910e-08  ...  2.808207e-01  3.751799e-02  5.668237e-02  8.429181e-03\n10771.05  1.093979e-03  0.000115  1.300380e-05  3.998319e-06  ...  2.313362e-01  9.806256e-02  6.114471e-02  2.121581e-02\n10771.15  5.537065e-03  0.001235  1.198087e-04  2.325507e-05  ...  2.470057e-01  1.578827e-01  1.547459e-01  1.115693e-01\n10771.25  5.969857e-04  0.000058  2.970409e-06  2.588196e-06  ...  2.164274e-01  7.182830e-02  6.686085e-02  1.501718e-02\n\n[19590 rows x 61 columns]\n</code></pre> <p>Each row of this pandas DataFrame is a time bin, and each column is an angular bin. The sum of all values in a row add up to 1.</p> <p>Now, let's plot the raster plot for a given period of time, and overlay the actual and decoded HD on the population activity.</p> <pre><code>ep = nap.IntervalSet(\nstart=10717, end=10730\n)  # Select an arbitrary interval for plotting\nplt.figure()\nplt.rc(\"font\", size=12)\nfor i, n in enumerate(spikes_adn.keys()):\nplt.plot(\nspikes[n].restrict(ep).fillna(pref_ang[n]), \"|\", color=color.loc[n]\n)  # raster plot for each cell\nplt.plot(\ndecoded.restrict(ep), \"--\", color=\"grey\", linewidth=2, label=\"decoded HD\"\n)  # decoded HD\nplt.legend(loc=\"upper left\")\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;matplotlib.legend.Legend object at 0x7f742e76f850&gt;\n</code></pre> <p>From this plot, we can see that the decoder is able to estimate the head-direction based on the population activity in ADn. Amazing!</p> <p>What does the probability distribution in this example event look like? Ideally, the bins with the highest probability will correspond to the bins having the most spikes. Let's plot the probability matrix to visualize this.</p> <pre><code>smoothed = scipy.ndimage.gaussian_filter(\nproba_feature, 1\n)  # Smoothening the probability distribution\n# Create a DataFrame with the smoothed distribution\np_feature = pd.DataFrame(\nindex=proba_feature.index.values,\ncolumns=proba_feature.columns.values,\ndata=smoothed,\n)\np_feature = nap.TsdFrame(p_feature)  # Make it a Pynapple TsdFrame\nplt.figure()\nplt.plot(\nangle.restrict(ep), \"w\", linewidth=2, label=\"actual HD\", zorder=1\n)  # Actual HD, in white\nplt.plot(\ndecoded.restrict(ep), \"--\", color=\"grey\", linewidth=2, label=\"decoded HD\", zorder=1\n)  # Decoded HD, in grey\n# Plot the smoothed probability distribution\nplt.imshow(\nnp.transpose(p_feature.restrict(ep).values),\naspect=\"auto\",\ninterpolation=\"bilinear\",\nextent=[ep[\"start\"].values[0], ep[\"end\"].values[0], 0, 2 * np.pi],\norigin=\"lower\",\ncmap=\"viridis\",\n)\nplt.xlabel(\"Time (s)\")  # X-axis is time in seconds\nplt.ylabel(\"Angle (rad)\")  # Y-axis is the angle in radian\nplt.colorbar(label=\"probability\")\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;matplotlib.colorbar.Colorbar object at 0x7f742c56b370&gt;\n</code></pre> <p>From this probability distribution, we observe that the decoded HD very closely matches the actual HD. Therefore, the population activity in ADn is a reliable estimate of the heading direction of the animal.</p> <p>I hope this tutorial was helpful. If you have any questions, comments or suggestions, please feel free to reach out to the Pynapple Team!</p> <p>Total running time of the script: ( 0 minutes  2.268 seconds)</p> <p> Download Python source code: tutorial_HD_dataset.py</p> <p> Download Jupyter notebook: tutorial_HD_dataset.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/tutorial_human_dataset/","title":"Zheng et al (2022) Dataset Tutorial","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_human_dataset/#zheng-et-al-2022-dataset-tutorial","title":"Zheng et al (2022) Dataset Tutorial","text":"<p>This tutorial demonstrates how we use Pynapple on various publicly available datasets in systems neuroscience to streamline analysis. In this tutorial, we will examine the dataset from Zheng et al (2022), which was used to generate Figure 4c in the publication.</p> <p>The NWB file for the example used here is provided in this repository. The entire dataset can be downloaded here.</p> <p>See the documentation of Pynapple for instructions on installing the package.</p> <p>This tutorial was made by Dhruv Mehrotra.</p> <p>First, import the necessary libraries:</p> <p>Warning</p> <p>This tutorial uses seaborn and matplotlib for displaying the figure as well as the dandi package</p> <p>You can install all with <code>pip install matplotlib seaborn dandi dandischema</code></p> <p>Now, import the necessary libraries:</p> <pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pynapple as nap\nimport seaborn as sns\n</code></pre>"},{"location":"generated/gallery/tutorial_human_dataset/#stream-the-data-from-dandi","title":"Stream the data from DANDI","text":"<pre><code>from pynwb import NWBHDF5IO\nfrom dandi.dandiapi import DandiAPIClient\nimport fsspec\nfrom fsspec.implementations.cached import CachingFileSystem\nimport h5py\n# Enter the session ID and path to the file\ndandiset_id, filepath = (\"000207\", \"sub-4/sub-4_ses-4_ecephys.nwb\")\nwith DandiAPIClient() as client:\nasset = client.get_dandiset(dandiset_id, \"draft\").get_asset_by_path(filepath)\ns3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n# first, create a virtual filesystem based on the http protocol\nfs = fsspec.filesystem(\"http\")\n# create a cache to save downloaded data to disk (optional)\nfs = CachingFileSystem(\nfs=fs,\ncache_storage=\"nwb-cache\",  # Local folder for the cache\n)\n# next, open the file\nfile = h5py.File(fs.open(s3_url, \"rb\"))\nio = NWBHDF5IO(file=file, load_namespaces=True)\n</code></pre> <p>Out:</p> <pre><code>/mnt/home/gviejo/mambaforge/envs/pynapple/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.5.0 because version 1.7.0 is already loaded.\n  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n/mnt/home/gviejo/mambaforge/envs/pynapple/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'core' version 2.5.0 because version 2.6.0-alpha is already loaded.\n  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n/mnt/home/gviejo/mambaforge/envs/pynapple/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.1.0 because version 0.4.0 is already loaded.\n  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n</code></pre>"},{"location":"generated/gallery/tutorial_human_dataset/#parsing-the-data","title":"Parsing the data","text":"<p>The first step is to load the data from the Neurodata Without Borders (NWB) file. This is done as follows:</p> <pre><code>custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", palette=\"colorblind\", font_scale=1.5, rc=custom_params)\ndata = nap.NWBFile(io.read())  # Load the NWB file for this dataset\n# What does this look like?\nprint(data)\n</code></pre> <p>Out:</p> <pre><code>4\n\u250d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2511\n\u2502 Keys                     \u2502 Type        \u2502\n\u251d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2525\n\u2502 units                    \u2502 TsGroup     \u2502\n\u2502 timediscrimination_table \u2502 IntervalSet \u2502\n\u2502 recognition_table        \u2502 IntervalSet \u2502\n\u2502 encoding_table           \u2502 IntervalSet \u2502\n\u2502 experiment_ids           \u2502 Tsd         \u2502\n\u2502 events                   \u2502 Tsd         \u2502\n\u2515\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2519\n</code></pre> <p>Get spike timings</p> <pre><code>spikes = data[\"units\"]\n</code></pre> <p>What does this look like?</p> <pre><code>print(spikes)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate      x       y       z    imp  location           filtering    group_name      origChannel\n-------  ------  -----  ------  ------  -----  -----------------  -----------  ------------  -------------\n      0    7     26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  139\n1    7.24  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  139\n2    6.09  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  140\n3    6.92  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  142\n4    0.4   26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  142\n5    0.46  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  143\n6    1.81  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  143\n7    4.79  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  144\n8    1.31  21.36   -0.59  -21.55    nan  amygdala_right     300-3000Hz   micros                  154\n9    0.62  21.36   -0.59  -21.55    nan  amygdala_right     300-3000Hz   micros                  156\n10    1.51  21.36   -0.59  -21.55    nan  amygdala_right     300-3000Hz   micros                  157\n11    0.77  21.36   -0.59  -21.55    nan  amygdala_right     300-3000Hz   micros                  157\n12    1.46  21.36   -0.59  -21.55    nan  amygdala_right     300-3000Hz   micros                  158\n13    0.4   21.36   -0.59  -21.55    nan  amygdala_right     300-3000Hz   micros                  158\n14    6.27  21.36   -0.59  -21.55    nan  amygdala_right     300-3000Hz   micros                  158\n15    1.15  21.36   -0.59  -21.55    nan  amygdala_right     300-3000Hz   micros                  159\n16    2.15  21.36   -0.59  -21.55    nan  amygdala_right     300-3000Hz   micros                  159\n17    0.63  21.36   -0.59  -21.55    nan  amygdala_right     300-3000Hz   micros                  159\n18    2.88  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  184\n19    0.35  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  185\n20    1.75  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  185\n21    0.58  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  186\n22    2.13  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  186\n23    1.51  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  187\n24    0.27  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  188\n25    7.35  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  188\n26    0.37  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  188\n27    1.89  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  188\n28    1.13  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  189\n29    0.6   26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  190\n30    0.18  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  190\n31    2.33  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  191\n32    0.31  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  191\n33    0.69  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  191\n34    0.66  26.63  -15.83  -16.49    nan  hippocampus_right  300-3000Hz   micros                  192\n</code></pre> <p>This TsGroup has, among other information, the mean firing rate of the unit, the X, Y and Z coordinates, the brain region the unit was recorded from, and the channel number on which the unit was located.</p> <p>Next, let's get the encoding table of all stimulus times, as shown below:</p> <pre><code>encoding_table = data[\"encoding_table\"]\n# What does this look like?\nprint(encoding_table)\n</code></pre> <p>Out:</p> <pre><code>/mnt/home/gviejo/pynapple/pynapple/io/interface_nwb.py:444: UserWarning: Too many metadata. Returning pandas.DataFrame, not IntervalSet\n  data = self._f_eval[self.data[key][\"type\"]](obj)\nstart_time   stop_time  fixcross_time  ExperimentID  boundary1_time  boundary2_time  boundary3_time  stimCategory  Clip_name\nid                                                                                                                              0     1.068463    9.101330       0.056555          70.0        5.068463             NaN             NaN             0  NB_24.mp4\n1    10.182342   18.114558       9.116686          70.0       12.880601       14.434447             NaN             1  SB_23.mp4\n2    19.071072   27.137388      18.130498          70.0       23.071072             NaN             NaN             2  HB_20.mp4\n3    28.160476   36.390234      27.154926          70.0       32.160475             NaN             NaN             0  NB_26.mp4\n4    37.470909   44.584497      36.409347          70.0       41.470909             NaN             NaN             0  NB_25.mp4\n..         ...         ...            ...           ...             ...             ...             ...           ...        ...\n85  892.755233  900.921092     891.747973          70.0      898.284193             NaN             NaN             1   SB_2.mp4\n86  901.906850  910.171979     900.940121          70.0      905.906850             NaN             NaN             2  HB_27.mp4\n87  911.138396  919.335950     910.190938          70.0      915.138396      912.697926      917.067151             2  HB_10.mp4\n88  920.363950  928.329845     919.354902          70.0      921.906193      923.478531      924.924117             1  SB_27.mp4\n89  930.639635  939.073029     929.718098          70.0      932.997106      934.191369             NaN             1  SB_25.mp4\n\n[90 rows x 9 columns]\n</code></pre> <p>This table has, among other things, the scene boundary times for which we will plot the peri-event time histogram (PETH).</p> <p>There are 3 types of scene boundaries in this data. For the purposes of demonstration, we will use only the \"No boundary\" (NB) and the \"Hard boundary\" (HB conditions). The encoding table has a stimCategory field, which tells us the type of boundary corresponding to a given trial.</p> <pre><code>stimCategory = np.array(\nencoding_table.stimCategory\n)  # Get the scene boundary type for all trials\n# What does this look like?\nprint(stimCategory)\n</code></pre> <p>Out:</p> <pre><code>[0 1 2 0 0 0 1 0 2 1 2 0 2 0 1 1 1 1 0 0 2 0 0 2 0 1 0 2 0 0 2 0 0 0 0 2 2\n2 0 0 1 1 1 1 0 2 1 1 0 2 1 0 2 2 2 0 1 0 1 1 2 2 0 2 2 2 1 1 2 1 0 2 2 1\n0 1 2 0 2 2 1 1 1 1 2 1 2 2 1 1]\n</code></pre> <p>Trials marked 0 correspond to NB, while trials marked 2 correspond to HB. Let's extract the trial numbers for NB and HB trials, as shown below:</p> <pre><code>indxNB = np.where(stimCategory == 0)  # NB trial indices\nindxHB = np.where(stimCategory == 2)  # HB trial indices\n</code></pre> <p>The encoding table also has 3 types of boundary times. For the purposes of our demonstration, we will focus on boundary1 times, and extract them as shown below:</p> <pre><code>boundary1_time = np.array(encoding_table.boundary1_time)  # Get timings of Boundary1\n# What does this look like?\nprint(boundary1_time)\n</code></pre> <p>Out:</p> <pre><code>[  5.06846275  12.88060075  23.071072    32.1604755   41.470909\n  49.5747065   56.07442325  72.803867    82.1299925   92.77667275\n  99.9925845  109.0787315  118.0778575  133.12435825 140.94827125\n 147.8236585  160.726736   170.441769   183.29262575 191.11327175\n 199.63382425 208.5142425  217.6186295  232.62687825 241.7270365\n 250.58327775 259.5545145  268.6661045  278.026902   287.04517875\n 301.2426685  310.21093375 319.1822215  328.13037175 337.16751875\n 363.737004   372.785663   381.86749625 391.2704085  400.5077995\n 407.28660475 416.0272855  431.792285   441.75272875 448.88401175\n 456.97766275 463.88864475 472.13035175 490.53584775 499.7064625\n 507.6917495  518.78793775 528.140675   537.1412335  552.5116415\n 562.4287995  569.44012625 580.99649325 591.974217   597.74463025\n 608.3638655  624.44389125 633.4175285  642.3969695  651.776966\n 660.699774   676.9439885  681.3602465  692.799878   699.53861175\n 711.341688   720.4819275  729.43782175 772.99961425 780.19490625\n 788.1175045  798.34264525 807.54519    817.25781375 835.1736475\n 842.54760125 852.294991   861.40924725 868.63873425 887.93252325\n 898.284193   905.9068505  915.1383965  921.90619325 932.99710575]\n</code></pre> <p>This contains the timings of all boundaries in this block of trials. Note that we also have the type of boundary for each trial. Let's store the NB and HB boundary timings in separate variables, as Pynapple Ts objects:</p> <pre><code>NB = nap.Ts(boundary1_time[indxNB])  # NB timings\nHB = nap.Ts(boundary1_time[indxHB])  # HB timings\n</code></pre>"},{"location":"generated/gallery/tutorial_human_dataset/#peri-event-time-histogram-peth","title":"Peri-Event Time Histogram (PETH)","text":"<p>A PETH is a plot where we align a variable of interest (for example, spikes) to an external event (in this case, to boundary times). This visualization helps us infer relationships between the two.</p> <p>For our demonstration, we will align the spikes of the first unit, which is located in the hippocampus, to the times of NB and HB. You can do a quick check to verify that the first unit is indeed located in the hippocampus, we leave it to you.</p> <p>With Pynapple, PETHs can be computed with a single line of code!</p> <pre><code>NB_peth = nap.compute_perievent(\nspikes[0], NB, minmax=(-0.5, 1)\n)  # Compute PETH of unit aligned to NB, for -0.5 to 1s windows\nHB_peth = nap.compute_perievent(\nspikes[0], HB, minmax=(-0.5, 1)\n)  # Compute PETH of unit aligned to HB, for -0.5 to 1s windows\n</code></pre> <p>Let's plot the PETH</p> <pre><code>plt.figure(figsize =(15,8))\nplt.subplot(211)  # Plot the figures in 2 rows\nfor i, n in enumerate(NB_peth):\nplt.plot(\nNB_peth[n].as_units(\"s\").fillna(i),\n\"o\",\ncolor=[102 / 255, 204 / 255, 0 / 255],\nmarkersize=4,\n)  # Plot PETH\nplt.axvline(0, linewidth=2, color=\"k\", linestyle=\"--\")  # Plot a line at t = 0\nplt.yticks([0, 30])  # Set ticks on Y-axis\nplt.gca().set_yticklabels([\"1\", \"30\"])  # Label the ticks\nplt.xlabel(\"Time from NB (s)\")  # Time from boundary in seconds, on X-axis\nplt.ylabel(\"Trial Number\")  # Trial number on Y-axis\nplt.subplot(212)\nfor i, n in enumerate(HB_peth):\nplt.plot(\nHB_peth[n].as_units(\"s\").fillna(i),\n\"o\",\ncolor=[255 / 255, 99 / 255, 71 / 255],\nmarkersize=4,\n)  # Plot PETH\nplt.axvline(0, linewidth=2, color=\"k\", linestyle=\"--\")  # Plot a line at t = 0\nplt.yticks([0, 30])  # Set ticks on Y-axis\nplt.gca().set_yticklabels([\"1\", \"30\"])  # Label the ticks\nplt.xlabel(\"Time from HB (s)\")  # Time from boundary in seconds, on X-axis\nplt.ylabel(\"Trial Number\")  # Trial number on Y-axis\nplt.subplots_adjust(wspace=0.2, hspace=0.5, top=0.85)\n</code></pre> <p></p> <p>Awesome! From the PETH, we can see that this neuron fires after boundary onset in HB trials. This is an example of what the authors describe here as a boundary cell.</p>"},{"location":"generated/gallery/tutorial_human_dataset/#peth-of-firing-rate-for-nb-and-hb-cells","title":"PETH of firing rate for NB and HB cells","text":"<p>Now that we have the PETH of spiking, we can go one step further. We will plot the mean firing rate of this cell aligned to the boundary for each trial type. Doing this in Pynapple is very simple!</p> <pre><code>bin_size = 0.2  # 200ms bin size\nstep_size = 0.01  # 10ms step size, to make overlapping bins\nwinsize = int(bin_size / step_size)  # Window size\n</code></pre> <p>Use Pynapple to compute binned spike counts</p> <pre><code>counts_NB = NB_peth.count(step_size)  # Spike counts binned in 10ms steps, for NB trials\ncounts_HB = HB_peth.count(step_size)  # Spike counts binned in 10ms steps, for HB trials\n</code></pre> <p>Smooth the binned spike counts using a window of size 20, for both trial types</p> <pre><code>counts_NB = (\ncounts_NB.as_dataframe()\n.rolling(winsize, win_type=\"gaussian\", min_periods=1, center=True, axis=0)\n.mean(std=0.2 * winsize)\n)\ncounts_HB = (\ncounts_HB.as_dataframe()\n.rolling(winsize, win_type=\"gaussian\", min_periods=1, center=True, axis=0)\n.mean(std=0.2 * winsize)\n)\n</code></pre> <p>Out:</p> <pre><code>/mnt/home/gviejo/mambaforge/envs/pynapple/lib/python3.10/site-packages/pandas/core/window/rolling.py:1218: DeprecationWarning: Importing gaussian from 'scipy.signal' is deprecated and will raise an error in SciPy 1.13.0. Please use 'scipy.signal.windows.gaussian' or the convenience function 'scipy.signal.get_window' instead.\n  window = self._scipy_weight_generator(  # type: ignore[misc]\n/mnt/home/gviejo/mambaforge/envs/pynapple/lib/python3.10/site-packages/pandas/core/window/rolling.py:1218: DeprecationWarning: Importing gaussian from 'scipy.signal' is deprecated and will raise an error in SciPy 1.13.0. Please use 'scipy.signal.windows.gaussian' or the convenience function 'scipy.signal.get_window' instead.\n  window = self._scipy_weight_generator(  # type: ignore[misc]\n</code></pre> <p>Compute firing rate for both trial types</p> <pre><code>fr_NB = counts_NB * winsize\nfr_HB = counts_HB * winsize\n</code></pre> <p>Compute the mean firing rate for both trial types</p> <pre><code>meanfr_NB = fr_NB.mean(axis=1)\nmeanfr_HB = fr_HB.mean(axis=1)\n</code></pre> <p>Compute standard error of mean (SEM) of the firing rate for both trial types</p> <pre><code>error_NB = fr_NB.sem(axis=1)\nerror_HB = fr_HB.sem(axis=1)\n</code></pre> <p>Plot the mean +/- SEM of firing rate for both trial types</p> <pre><code>plt.figure(figsize =(15,8))\nplt.plot(\nmeanfr_NB, color=[102 / 255, 204 / 255, 0 / 255], label=\"NB\"\n)  # Plot mean firing rate for NB trials\n# Plot SEM for NB trials\nplt.fill_between(\nmeanfr_NB.index.values,\nmeanfr_NB.values - error_NB,\nmeanfr_NB.values + error_NB,\ncolor=[102 / 255, 204 / 255, 0 / 255],\nalpha=0.2,\n)\nplt.plot(\nmeanfr_HB, color=[255 / 255, 99 / 255, 71 / 255], label=\"HB\"\n)  # Plot mean firing rate for HB trials\n# Plot SEM for NB trials\nplt.fill_between(\nmeanfr_HB.index.values,\nmeanfr_HB.values - error_HB,\nmeanfr_HB.values + error_HB,\ncolor=[255 / 255, 99 / 255, 71 / 255],\nalpha=0.2,\n)\nplt.axvline(0, linewidth=2, color=\"k\", linestyle=\"--\")  # Plot a line at t = 0\nplt.xlabel(\"Time from boundary (s)\")  # Time from boundary in seconds, on X-axis\nplt.ylabel(\"Firing rate (Hz)\")  # Firing rate in Hz on Y-axis\nplt.legend(loc=\"upper right\")\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;matplotlib.legend.Legend object at 0x7f51598d0910&gt;\n</code></pre> <p>This plot verifies what we visualized in the PETH rasters above, that this cell responds to a hard boundary. Hence, it is a boundary cell. To learn more about these cells, please check out the original study here.</p> <p>I hope this tutorial was helpful. If you have any questions, comments or suggestions, please feel free to reach out to the Pynapple Team!</p> <p>Total running time of the script: ( 0 minutes  1.142 seconds)</p> <p> Download Python source code: tutorial_human_dataset.py</p> <p> Download Jupyter notebook: tutorial_human_dataset.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/tutorial_pynapple_core/","title":"Core Tutorial","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_pynapple_core/#core-tutorial","title":"Core Tutorial","text":"<p>This script will introduce the basics of handling time series data with pynapple.</p> <p>Warning</p> <p>This tutorial uses seaborn and matplotlib for displaying the figure.</p> <p>You can install both with <code>pip install matplotlib seaborn</code></p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pynapple as nap\nimport pandas as pd\nimport seaborn as sns\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", palette=\"colorblind\", font_scale=1.5, rc=custom_params)\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_core/#time-series-object","title":"Time series object","text":"<p>Let's create a Tsd object with artificial data. In this example, every time point is 1 second apart.</p> <pre><code>tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100), time_units=\"s\")\nprint(tsd)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  ---------\n0.0         0.837886\n1.0         0.984464\n2.0         0.132698\n3.0         0.160919\n4.0         0.0565268\n...\n95.0        0.0565268\n96.0        0.0565268\n97.0        0.0565268\n98.0        0.0565268\n99.0        0.0565268\ndtype: float64, shape: (100,)\n</code></pre> <p>It is possible to toggle between seconds, milliseconds and microseconds. Note that when using as_units, the returned object is a simple pandas series.</p> <pre><code>print(tsd.as_units(\"ms\"), \"\\n\")\nprint(tsd.as_units(\"us\"))\n</code></pre> <p>Out:</p> <pre><code>Time (ms)\n0.0        0.837886\n1000.0     0.984464\n2000.0     0.132698\n3000.0     0.160919\n4000.0     0.056527\n             ...   95000.0    0.265375\n96000.0    0.787128\n97000.0    0.779701\n98000.0    0.965388\n99000.0    0.750425\nLength: 100, dtype: float64 Time (us)\n0           0.837886\n1000000     0.984464\n2000000     0.132698\n3000000     0.160919\n4000000     0.056527\n              ...   95000000    0.265375\n96000000    0.787128\n97000000    0.779701\n98000000    0.965388\n99000000    0.750425\nLength: 100, dtype: float64\n</code></pre> <p>Pynapple is able to handle data that only contains timestamps, such as an object containing only spike times. To do so, we construct a Ts object which holds only times. In this case, we generate 10 random spike times between 0 and 100 ms.</p> <pre><code>ts = nap.Ts(t=np.sort(np.random.uniform(0, 100, 10)), time_units=\"ms\")\nprint(ts)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n0.015338233\n0.036392086\n0.038976501\n0.050082372\n0.053644448\n0.054794703\n0.054879278\n0.070577021\n0.076230739\n0.083081248\nshape: 10\n</code></pre> <p>If the time series contains multiple columns, we use a TsdFrame.</p> <pre><code>tsdframe = nap.TsdFrame(\nt=np.arange(100), d=np.random.rand(100, 3), time_units=\"s\", columns=[\"a\", \"b\", \"c\"]\n)\nprint(tsdframe)\n</code></pre> <p>Out:</p> <pre><code>Time (s)            a          b         c\n----------  ---------  ---------  --------\n0.0         0.661739   0.87173    0.967485\n1.0         0.825824   0.0165395  0.291632\n2.0         0.884573   0.955788   0.567704\n3.0         0.158759   0.642384   0.667699\n4.0         0.0899414  0.790992   0.723888\n...\n95.0        0.246979   0.313108   0.127022\n96.0        0.524553   0.335116   0.207804\n97.0        0.177493   0.343051   0.267484\n98.0        0.284349   0.303969   0.521931\n99.0        0.212372   0.170611   0.273874\ndtype: float64, shape: (100, 3)\n</code></pre> <p>And if the number of dimension is even larger, we can use the TsdTensor (typically movies).</p> <pre><code>tsdframe = nap.TsdTensor(\nt=np.arange(100), d=np.random.rand(100, 3, 4)\n)\nprint(tsdframe)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  ---------------------------------------------------\n0.0         [[0.7013092650689476 ... 0.7013092650689476] ...]\n1.0         [[0.7989437177447981 ... 0.7989437177447981] ...]\n2.0         [[0.3961248295106903 ... 0.3961248295106903] ...]\n3.0         [[0.530207084427306 ... 0.530207084427306] ...]\n4.0         [[0.7860950718820281 ... 0.7860950718820281] ...]\n...\n95.0        [[0.1341367004474281 ... 0.1341367004474281] ...]\n96.0        [[0.11013639735110914 ... 0.11013639735110914] ...]\n97.0        [[0.6978348384693492 ... 0.6978348384693492] ...]\n98.0        [[0.7451943354175604 ... 0.7451943354175604] ...]\n99.0        [[0.950343127889017 ... 0.950343127889017] ...]\ndtype: float64, shape: (100, 3, 4)\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_core/#interval-sets-object","title":"Interval Sets object","text":"<p>The IntervalSet object stores multiple epochs with a common time unit. It can then be used to restrict time series to this particular set of epochs.</p> <pre><code>epochs = nap.IntervalSet(start=[0, 10], end=[5, 15], time_units=\"s\")\nnew_tsd = tsd.restrict(epochs)\nprint(epochs)\nprint(\"\\n\")\nprint(new_tsd)\n</code></pre> <p>Out:</p> <pre><code>   start   end\n0    0.0   5.0\n1   10.0  15.0\n\nTime (s)\n----------  ---------\n0           0.837886\n1           0.984464\n2           0.132698\n3           0.160919\n4           0.0565268\n5           0.448241\n10          0.310943\n11          0.779746\n12          0.784978\n13          0.847672\n14          0.693251\n15          0.352199\ndtype: float64, shape: (12,)\n</code></pre> <p>Multiple operations are available for IntervalSet. For example, IntervalSet can be merged. See the full documentation of the class here for a list of all the functions that can be used to manipulate IntervalSets.</p> <pre><code>epoch1 = nap.IntervalSet(start=[0], end=[10])  # no time units passed. Default is us.\nepoch2 = nap.IntervalSet(start=[5, 30], end=[20, 45])\nepoch = epoch1.union(epoch2)\nprint(epoch1, \"\\n\")\nprint(epoch2, \"\\n\")\nprint(epoch)\n</code></pre> <p>Out:</p> <pre><code>   start   end\n0    0.0  10.0 start   end\n0    5.0  20.0\n1   30.0  45.0 start   end\n0    0.0  20.0\n1   30.0  45.0\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_core/#tsgroup-object","title":"TsGroup object","text":"<p>Multiple time series with different time stamps (.i.e. a group of neurons with different spike times from one session) can be grouped with the TsGroup object. The TsGroup behaves like a dictionary but it is also possible to slice with a list of indexes</p> <pre><code>my_ts = {\n0: nap.Ts(\nt=np.sort(np.random.uniform(0, 100, 1000)), time_units=\"s\"\n),  # here a simple dictionary\n1: nap.Ts(t=np.sort(np.random.uniform(0, 100, 2000)), time_units=\"s\"),\n2: nap.Ts(t=np.sort(np.random.uniform(0, 100, 3000)), time_units=\"s\"),\n}\ntsgroup = nap.TsGroup(my_ts)\nprint(tsgroup, \"\\n\")\nprint(tsgroup[0], \"\\n\")  # dictionary like indexing returns directly the Ts object\nprint(tsgroup[[0, 2]])  # list like indexing\n</code></pre> <p>Out:</p> <pre><code>  Index    rate\n-------  ------\n      0   10\n1   20.01\n      2   30.01 Time (s)\n0.02311518\n0.051072582\n0.144789637\n0.250865188\n0.324662723\n...\n99.687970622\n99.709137622\n99.795416057\n99.867229363\n99.991162872\nshape: 1000 Index    rate\n-------  ------\n      0   10\n2   30.01\n</code></pre> <p>Operations such as restrict can thus be directly applied to the TsGroup as well as other operations.</p> <pre><code>newtsgroup = tsgroup.restrict(epochs)\ncount = tsgroup.count(\n1, epochs, time_units=\"s\"\n)  # Here counting the elements within bins of 1 seconds\nprint(count)\n</code></pre> <p>Out:</p> <pre><code>  Time (s)    0    1    2\n----------  ---  ---  ---\n       0.5   11   19   29\n1.5   12   20   31\n2.5   12   24   24\n3.5    9   13   34\n4.5   14   26   32\n10.5    5   21   40\n11.5   12   17   22\n12.5   11   12   24\n13.5   10   21   28\n14.5    8   18   42\ndtype: int64, shape: (10, 3)\n</code></pre> <p>One advantage of grouping time series is that metainformation can be appended directly on an element-wise basis. In this case, we add labels to each Ts object when instantiating the group and after. We can then use this label to split the group. See the TsGroup documentation for a complete methodology for splitting TsGroup objects.</p> <pre><code>label1 = pd.Series(index=list(my_ts.keys()), data=[0, 1, 0])\ntsgroup = nap.TsGroup(my_ts, time_units=\"s\", label1=label1)\ntsgroup.set_info(label2=np.array([\"a\", \"a\", \"b\"]))\nprint(tsgroup, \"\\n\")\nnewtsgroup = tsgroup.getby_category(\"label1\")\nprint(newtsgroup[0], \"\\n\")\nprint(newtsgroup[1])\n</code></pre> <p>Out:</p> <pre><code>  Index    rate    label1  label2\n-------  ------  --------  --------\n      0   10            0  a\n      1   20.01         1  a\n      2   30.01         0  b Index    rate    label1  label2\n-------  ------  --------  --------\n      0   10            0  a\n      2   30.01         0  b Index    rate    label1  label2\n-------  ------  --------  --------\n      1   20.01         1  a\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_core/#time-support","title":"Time support","text":"<p>A key feature of how pynapple manipulates time series is an inherent time support object defined for Ts, Tsd, TsdFrame and TsGroup objects. The time support object is defined as an IntervalSet that provides the time serie with a context. For example, the restrict operation will automatically update the time support object for the new time series. Ideally, the time support object should be defined for all time series when instantiating them. If no time series is given, the time support is inferred from the start and end of the time series.</p> <p>In this example, a TsGroup is instantiated with and without a time support. Notice how the frequency of each Ts element is changed when the time support is defined explicitly.</p> <pre><code>time_support = nap.IntervalSet(start=0, end=200, time_units=\"s\")\nmy_ts = {\n0: nap.Ts(\nt=np.sort(np.random.uniform(0, 100, 10)), time_units=\"s\"\n),  # here a simple dictionnary\n1: nap.Ts(t=np.sort(np.random.uniform(0, 100, 20)), time_units=\"s\"),\n2: nap.Ts(t=np.sort(np.random.uniform(0, 100, 30)), time_units=\"s\"),\n}\ntsgroup = nap.TsGroup(my_ts)\ntsgroup_with_time_support = nap.TsGroup(my_ts, time_support=time_support)\nprint(tsgroup, \"\\n\")\nprint(tsgroup_with_time_support, \"\\n\")\nprint(tsgroup_with_time_support.time_support)  # acceding the time support\n</code></pre> <p>Out:</p> <pre><code>  Index    rate\n-------  ------\n      0    0.11\n      1    0.21\n      2    0.32 Index    rate\n-------  ------\n      0    0.05\n      1    0.1\n      2    0.15 start    end\n0    0.0  200.0\n</code></pre> <p>We can use value_from which as it indicates assign to every timestamps the closed value in time from another time series. Let's define the time series we want to assign values from.</p> <pre><code>tsd_sin = nap.Tsd(t=np.arange(0, 100, 1), d=np.sin(np.arange(0, 10, 0.1)))\ntsgroup_sin = tsgroup.value_from(tsd_sin)\nplt.figure(figsize=(12, 6))\nplt.plot(tsgroup[0].fillna(0), \"|\", markersize=20, mew=3)\nplt.plot(tsd_sin, linewidth=2)\nplt.plot(tsgroup_sin[0], \"o\", markersize=20)\nplt.title(\"ts.value_from(tsd)\")\nplt.xlabel(\"Time (s)\")\nplt.yticks([-1, 0, 1])\nplt.show()\n</code></pre> <p></p> <p>Total running time of the script: ( 0 minutes  1.853 seconds)</p> <p> Download Python source code: tutorial_pynapple_core.py</p> <p> Download Jupyter notebook: tutorial_pynapple_core.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/tutorial_pynapple_dandi/","title":"Streaming data from DANDI","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_pynapple_dandi/#streaming-data-from-dandi","title":"Streaming data from DANDI","text":"<p>This script shows how to stream data from the DANDI Archive all the way to pynapple.</p> <p>Warning</p> <p>This tutorial uses seaborn and matplotlib for displaying the figure as well as the DANDI package</p> <p>You can install all with <code>pip install matplotlib seaborn dandi</code></p>"},{"location":"generated/gallery/tutorial_pynapple_dandi/#prelude","title":"Prelude","text":"<p>The data used in this tutorial were used in this publication: Sargolini, Francesca, et al. \"Conjunctive representation of position, direction, and velocity in entorhinal cortex.\" Science 312.5774 (2006): 758-762. The data can be found on the DANDI Archive in Dandiset 000582.</p> <p>mkdocs_gallery_thumbnail_number = 2</p>"},{"location":"generated/gallery/tutorial_pynapple_dandi/#dandi","title":"DANDI","text":"<p>DANDI allows you to stream data without downloading all the files. In this case the data extracted from the NWB file are stored in the nwb-cache folder.</p> <pre><code>from pynwb import NWBHDF5IO\nfrom dandi.dandiapi import DandiAPIClient\nimport fsspec\nfrom fsspec.implementations.cached import CachingFileSystem\nimport h5py\n# ecephys\ndandiset_id, filepath = (\n\"000582\",\n\"sub-10073/sub-10073_ses-17010302_behavior+ecephys.nwb\",\n)\nwith DandiAPIClient() as client:\nasset = client.get_dandiset(dandiset_id, \"draft\").get_asset_by_path(filepath)\ns3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n# first, create a virtual filesystem based on the http protocol\nfs = fsspec.filesystem(\"http\")\n# create a cache to save downloaded data to disk (optional)\nfs = CachingFileSystem(\nfs=fs,\ncache_storage=\"nwb-cache\",  # Local folder for the cache\n)\n# next, open the file\nfile = h5py.File(fs.open(s3_url, \"rb\"))\nio = NWBHDF5IO(file=file, load_namespaces=True)\nprint(io)\n</code></pre> <p>Out:</p> <pre><code>/mnt/home/gviejo/mambaforge/envs/pynapple/lib/python3.10/site-packages/etelemetry/client.py:95: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n  from pkg_resources import parse_version\n/mnt/home/gviejo/mambaforge/envs/pynapple/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('mpl_toolkits')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/mnt/home/gviejo/mambaforge/envs/pynapple/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('ruamel')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/mnt/home/gviejo/mambaforge/envs/pynapple/lib/python3.10/site-packages/pkg_resources/__init__.py:2871: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('ruamel.yaml')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(pkg)\n/mnt/home/gviejo/mambaforge/envs/pynapple/lib/python3.10/site-packages/pkg_resources/__init__.py:2350: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('ruamel')`.\nImplementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n  declare_namespace(parent)\nA newer version (0.56.2) of dandi/dandi-cli is available. You are using 0.56.1\n/mnt/home/gviejo/mambaforge/envs/pynapple/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.8.0 because version 1.7.0 is already loaded.\n  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n/mnt/home/gviejo/mambaforge/envs/pynapple/lib/python3.10/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-experimental' version 0.5.0 because version 0.4.0 is already loaded.\n  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n&lt;pynwb.NWBHDF5IO object at 0x7f5162f2b9a0&gt;\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_dandi/#pynapple","title":"Pynapple","text":"<p>If opening the NWB works, you can start streaming data straight into pynapple with the <code>NWBFile</code> class.</p> <pre><code>import pynapple as nap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", palette=\"colorblind\", font_scale=1.5, rc=custom_params)\nnwb = nap.NWBFile(io.read())\nprint(nwb)\n</code></pre> <p>Out:</p> <pre><code>17010302\n\u250d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2511\n\u2502 Keys                \u2502 Type     \u2502\n\u251d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2525\n\u2502 units               \u2502 TsGroup  \u2502\n\u2502 ElectricalSeriesLFP \u2502 Tsd      \u2502\n\u2502 SpatialSeriesLED1   \u2502 TsdFrame \u2502\n\u2502 ElectricalSeries    \u2502 Tsd      \u2502\n\u2515\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2519\n</code></pre> <p>We can load the spikes as a TsGroup for inspection.</p> <pre><code>units = nwb[\"units\"]\nprint(units)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate  unit_name    histology    hemisphere      depth\n-------  ------  -----------  -----------  ------------  -------\n      0    2.93  t1c1         MEC LII                     0.0024\n      1    1.5   t2c1         MEC LII                     0.0024\n      2    2.58  t2c3         MEC LII                     0.0024\n      3    1.13  t3c1         MEC LII                     0.0024\n      4    1.29  t3c2         MEC LII                     0.0024\n      5    1.36  t3c3         MEC LII                     0.0024\n      6    2.89  t3c4         MEC LII                     0.0024\n      7    1.47  t4c1         MEC LII                     0.0024\n</code></pre> <p>As well as the position</p> <pre><code>position = nwb[\"SpatialSeriesLED1\"]\n</code></pre> <p>Here we compute the 2d tuning curves</p> <pre><code>tc, binsxy = nap.compute_2d_tuning_curves(units, position, 20)\n</code></pre> <p>Out:</p> <pre><code>/mnt/home/gviejo/pynapple/pynapple/process/tuning_curves.py:204: RuntimeWarning: invalid value encountered in divide\n  count = count / occupancy\n</code></pre> <p>Let's plot the tuning curves</p> <pre><code>plt.figure(figsize=(15, 7))\nfor i in tc.keys():\nplt.subplot(2, 4, i + 1)\nplt.imshow(tc[i], origin=\"lower\", aspect=\"auto\")\nplt.title(\"Unit {}\".format(i))\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Let's plot the spikes of unit 1 who has a nice grid Here I use the function <code>value_from</code> to assign to each spike the closest position in time.</p> <pre><code>plt.figure(figsize=(15, 6))\nplt.subplot(121)\nextent = (\nnp.min(position[\"x\"]),\nnp.max(position[\"x\"]),\nnp.min(position[\"y\"]),\nnp.max(position[\"y\"]),\n)\nplt.imshow(tc[1], origin=\"lower\", extent=extent, aspect=\"auto\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.subplot(122)\nplt.plot(position[\"y\"], position[\"x\"], color=\"grey\")\nspk_pos = units[1].value_from(position)\nplt.plot(spk_pos[\"y\"], spk_pos[\"x\"], \"o\", color=\"red\", markersize=5, alpha=0.5)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>Total running time of the script: ( 0 minutes  7.266 seconds)</p> <p> Download Python source code: tutorial_pynapple_dandi.py</p> <p> Download Jupyter notebook: tutorial_pynapple_dandi.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/tutorial_pynapple_io/","title":"IO Tutorial","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_pynapple_io/#io-tutorial","title":"IO Tutorial","text":"<p>This notebook is designed to demonstrate the pynapple IO. It is build around the specifications of the BIDS standard for sharing datasets. The key ideas are summarized as follow :</p> <ul> <li> <p>Hierarchy of folders</p> <p></p> </li> <li> <p>Filename template</p> <p></p> </li> <li> <p>Metadata files</p> <p></p> </li> </ul>"},{"location":"generated/gallery/tutorial_pynapple_io/#navigating-a-structured-dataset","title":"Navigating a structured dataset","text":"<p>The dataset in this example can be found here.</p> <pre><code>import numpy as np\nimport pynapple as nap\n# mkdocs_gallery_thumbnail_path = '_static/treeview.png'\nproject_path = \"../../your/path/to/MyProject\"\nproject = nap.load_folder(project_path)\nprint(project)\n</code></pre> <p>Out:</p> <pre><code>\ud83d\udcc2 MyProject\n\u2514\u2500\u2500 \ud83d\udcc2 sub-A2929\n</code></pre> <p>The pynapple IO offers a convenient way of visualizing and navigating a folder based dataset. To visualize the whole hierarchy of Folders, you can call the view property or the expand function.</p> <pre><code>project.view\n</code></pre> <p>Out:</p> <pre><code>\ud83d\udcc2 MyProject\n\u2514\u2500\u2500 \ud83d\udcc2 sub-A2929\n    \u2514\u2500\u2500 \ud83d\udcc2 ses-A2929-200711\n        \u251c\u2500\u2500 \ud83d\udcc2 derivatives\n        \u2502   \u251c\u2500\u2500 spikes.npz      |        TsGroup\n        \u2502   \u251c\u2500\u2500 sleep_ep.npz    |        IntervalSet\n        \u2502   \u251c\u2500\u2500 position.npz    |        TsdFrame\n        \u2502   \u2514\u2500\u2500 wake_ep.npz     |        IntervalSet\n        \u251c\u2500\u2500 \ud83d\udcc2 pynapplenwb\n        \u2502   \u2514\u2500\u2500 A2929-200711    |        NWB file\n        \u251c\u2500\u2500 x_plus_y.npz    |        Tsd\n        \u2514\u2500\u2500 stimulus-fish.npz       |        IntervalSet\n</code></pre> <p>Here it shows all the subjects (in this case only A2929), all the sessions and all of the derivatives folders. It shows as well all the NPZ files that contains a pynapple object and the NWB files.</p> <p>The object project behaves like a nested dictionnary. It is then easy to loop and navigate through a hierarchy of folders when doing analyses. In this case, we are gonna take only the session A2929-200711.</p> <pre><code>session = project[\"sub-A2929\"][\"ses-A2929-200711\"]\nprint(session)\n</code></pre> <p>Out:</p> <pre><code>\ud83d\udcc2 ses-A2929-200711\n\u251c\u2500\u2500 \ud83d\udcc2 derivatives\n\u251c\u2500\u2500 \ud83d\udcc2 pynapplenwb\n\u251c\u2500\u2500 x_plus_y.npz    |        Tsd\n\u2514\u2500\u2500 stimulus-fish.npz       |        IntervalSet\n</code></pre> <p>I can expand to see what the folders contains.</p> <pre><code>print(session.expand())\n</code></pre> <p>Out:</p> <pre><code>\ud83d\udcc2 ses-A2929-200711\n\u251c\u2500\u2500 \ud83d\udcc2 derivatives\n\u2502   \u251c\u2500\u2500 spikes.npz      |        TsGroup\n\u2502   \u251c\u2500\u2500 sleep_ep.npz    |        IntervalSet\n\u2502   \u251c\u2500\u2500 position.npz    |        TsdFrame\n\u2502   \u2514\u2500\u2500 wake_ep.npz     |        IntervalSet\n\u251c\u2500\u2500 \ud83d\udcc2 pynapplenwb\n\u2502   \u2514\u2500\u2500 A2929-200711    |        NWB file\n\u251c\u2500\u2500 x_plus_y.npz    |        Tsd\n\u2514\u2500\u2500 stimulus-fish.npz       |        IntervalSet\nNone\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_io/#loading-files","title":"Loading files","text":"<p>By default, pynapple save objects as NPZ. It is a convenient way to save all the properties of an object such as the time support. The pynapple IO offers an easy way to load any NPZ files that matches the structures defined for a pynapple object.</p> <pre><code>spikes = session[\"derivatives\"][\"spikes\"]\nposition = session[\"derivatives\"][\"position\"]\nwake_ep = session[\"derivatives\"][\"wake_ep\"]\nsleep_ep = session[\"derivatives\"][\"sleep_ep\"]\n</code></pre> <p>Objects are only loaded when they are called.</p> <pre><code>print(session[\"derivatives\"][\"spikes\"])\n</code></pre> <p>Out:</p> <pre><code>  Index    rate    group  location\n-------  ------  -------  ----------\n      0    7.3         0  adn\n      1    5.73        0  adn\n      2    8.12        0  adn\n      3    6.68        0  adn\n      4   10.77        0  adn\n      5   11           0  adn\n      6   16.52        0  adn\n      7    2.2         1  ca1\n      8    2.02        1  ca1\n      9    1.07        1  ca1\n     10    3.92        1  ca1\n     11    3.31        1  ca1\n     12    1.09        1  ca1\n     13    1.28        1  ca1\n     14    1.32        1  ca1\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_io/#metadata","title":"Metadata","text":"<p>A good practice for sharing datasets is to write as many metainformation as possible. Following BIDS specifications, any data files should be accompagned by a JSON sidecar file.</p> <pre><code>import os\nfor f in os.listdir(session[\"derivatives\"].path):\nprint(f)\n</code></pre> <p>Out:</p> <pre><code>wake_ep.json\nposition.json\nsleep_ep.json\nspikes.npz\nsleep_ep.npz\nspikes.json\nposition.npz\nwake_ep.npz\n</code></pre> <p>To read the metainformation associated with a file, you can use the functions <code>doc</code>, <code>info</code> or <code>metadata</code> :</p> <pre><code>session[\"derivatives\"].doc(\"spikes\")\nsession[\"derivatives\"].doc(\"position\")\n</code></pre> <p>Out:</p> <pre><code>\u256d\u2500 ../../your/path/to/MyProject/sub-A2929/ses-A2929-200711/derivatives/spikes.npz \u2500\u256e\n\u2502 time : 2023-07-11 12:40:10.338066                                                \u2502\n\u2502 info : Neurons recorded simultaneously in ADN and CA1                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 ../../your/path/to/MyProject/sub-A2929/ses-A2929-200711/derivatives/position.npz \u2500\u256e\n\u2502 time : 2023-07-11 12:40:10.364061                                                  \u2502\n\u2502 info : Position and head-direction of the mouse recorded with Optitrack            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_io/#saving-a-pynapple-object","title":"Saving a pynapple object","text":"<p>In this case, we define a new Tsd and a new IntervalSet that we would like to save in the session folder.</p> <pre><code>tsd = position[\"x\"] + 1\nepoch = nap.IntervalSet(start=np.array([0, 3]), end=np.array([1, 6]))\nsession.save(\"x_plus_1\", tsd, description=\"Random position\")\nsession.save(\"stimulus-fish\", epoch, description=\"Fish pictures to V1\")\n</code></pre> <p>We can visualize the newly saved objects.</p> <pre><code>session.expand()\n</code></pre> <p>Out:</p> <pre><code>\ud83d\udcc2 ses-A2929-200711\n\u251c\u2500\u2500 \ud83d\udcc2 derivatives\n\u2502   \u251c\u2500\u2500 spikes.npz      |        TsGroup\n\u2502   \u251c\u2500\u2500 sleep_ep.npz    |        IntervalSet\n\u2502   \u251c\u2500\u2500 position.npz    |        TsdFrame\n\u2502   \u2514\u2500\u2500 wake_ep.npz     |        IntervalSet\n\u251c\u2500\u2500 \ud83d\udcc2 pynapplenwb\n\u2502   \u2514\u2500\u2500 A2929-200711    |        NWB file\n\u251c\u2500\u2500 x_plus_y.npz    |        Tsd\n\u251c\u2500\u2500 stimulus-fish.npz       |        IntervalSet\n\u2514\u2500\u2500 x_plus_1.npz    |        Tsd\n</code></pre> <pre><code>session.doc(\"stimulus-fish\")\n</code></pre> <p>Out:</p> <pre><code>\u256d\u2500 ../../your/path/to/MyProject/sub-A2929/ses-A2929-200711/stimulus-fish.npz \u2500\u256e\n\u2502 time : 2023-10-03 17:48:57.817177                                           \u2502\n\u2502 info : Fish pictures to V1                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>session[\"x_plus_1\"]\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  --------\n670.6407    0.957143\n670.649     0.956137\n670.65735   0.955147\n670.66565   0.954213\n670.674     0.953244\n...\n1199.9616   0.953244\n1199.96995  0.953244\n1199.97825  0.953244\n1199.9866   0.953244\n1199.99495  0.953244\ndtype: float64, shape: (63527,)\n</code></pre> <p>Total running time of the script: ( 0 minutes  5.894 seconds)</p> <p> Download Python source code: tutorial_pynapple_io.py</p> <p> Download Jupyter notebook: tutorial_pynapple_io.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/tutorial_pynapple_numpy/","title":"Numpy tutorial","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_pynapple_numpy/#numpy-tutorial","title":"Numpy tutorial","text":"<p>This tutorial shows how pynapple interact with numpy.</p> <pre><code>import numpy as np\nimport pynapple as nap\nimport pandas as pd\n</code></pre> <p>Multiple time series object are avaible depending on the shape of the data.</p> <ul> <li><code>TsdTensor</code> : for data with of more than 2 dimensions, typically movies.</li> <li><code>TsdFrame</code> : for column-based data. It can be easily converted to a pandas.DataFrame. Columns can be labelled and selected similar to pandas.</li> <li><code>Tsd</code> : One-dimensional time series. It can be converted to a pandas.Series.</li> <li><code>Ts</code> : For timestamps data only.</li> </ul>"},{"location":"generated/gallery/tutorial_pynapple_numpy/#initialization","title":"Initialization","text":"<pre><code>tsdtensor = nap.TsdTensor(t=np.arange(100), d=np.random.rand(100, 5, 5), time_units=\"s\")\ntsdframe = nap.TsdFrame(t=np.arange(100), d=np.random.rand(100, 3), columns = ['a', 'b', 'c'])\ntsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\nts = nap.Ts(t=np.arange(100))\nprint(tsdtensor)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  -----------------------------------------------------\n0.0         [[0.6679716889640175 ... 0.6679716889640175] ...]\n1.0         [[0.5275118854651649 ... 0.5275118854651649] ...]\n2.0         [[0.048474108077754874 ... 0.048474108077754874] ...]\n3.0         [[0.352159351812118 ... 0.352159351812118] ...]\n4.0         [[0.5002324080595832 ... 0.5002324080595832] ...]\n...\n95.0        [[0.7623541074282261 ... 0.7623541074282261] ...]\n96.0        [[0.9013025982225515 ... 0.9013025982225515] ...]\n97.0        [[0.7363854672499466 ... 0.7363854672499466] ...]\n98.0        [[0.6759911796883422 ... 0.6759911796883422] ...]\n99.0        [[0.2299953295750694 ... 0.2299953295750694] ...]\ndtype: float64, shape: (100, 5, 5)\n</code></pre> <p>tsd and ts can be converted to a pandas.Series</p> <pre><code>print(tsd.as_series())\n</code></pre> <p>Out:</p> <pre><code>0.0     0.357955\n1.0     0.018227\n2.0     0.734151\n3.0     0.582217\n4.0     0.625421\n          ...   95.0    0.673633\n96.0    0.537181\n97.0    0.808001\n98.0    0.101543\n99.0    0.020365\nLength: 100, dtype: float64\n</code></pre> <p>tsdframe to a pandas.DataFrame</p> <pre><code>print(tsdframe.as_dataframe())\n</code></pre> <p>Out:</p> <pre><code>             a         b         c\n0.0   0.459267  0.402103  0.622282\n1.0   0.136493  0.936746  0.563627\n2.0   0.967718  0.436909  0.823681\n3.0   0.268041  0.550958  0.499102\n4.0   0.384593  0.282024  0.811827\n...        ...       ...       ...\n95.0  0.712022  0.652925  0.704547\n96.0  0.066707  0.264471  0.593065\n97.0  0.513316  0.927389  0.370928\n98.0  0.258396  0.981057  0.815519\n99.0  0.848732  0.248553  0.393930\n\n[100 rows x 3 columns]\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_numpy/#attributes","title":"Attributes","text":"<p>The numpy array is accesible with the attributes <code>.values</code>, <code>.d</code> and functions <code>.as_array()</code>, <code>to_numpy()</code>. The time index array is a <code>TsIndex</code> object accessible with <code>.index</code> or <code>.t</code>. <code>.shape</code> and <code>.ndim</code> are also accessible.</p> <pre><code>print(tsdtensor.ndim)\nprint(tsdframe.shape)\nprint(len(tsd))\n</code></pre> <p>Out:</p> <pre><code>3\n(100, 3)\n100\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_numpy/#slicing","title":"Slicing","text":"<p>Slicing is very similar to numpy array. The first dimension is always time and time support is always passed on if a pynapple object is returned.</p> <p>First 10 elements. Return a TsdTensor</p> <pre><code>print(tsdtensor[0:10]) \n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  -----------------------------------------------------\n0           [[0.6679716889640175 ... 0.6679716889640175] ...]\n1           [[0.5275118854651649 ... 0.5275118854651649] ...]\n2           [[0.048474108077754874 ... 0.048474108077754874] ...]\n3           [[0.352159351812118 ... 0.352159351812118] ...]\n4           [[0.5002324080595832 ... 0.5002324080595832] ...]\n5           [[0.728652283503968 ... 0.728652283503968] ...]\n6           [[0.27752782860399505 ... 0.27752782860399505] ...]\n7           [[0.7767273009429135 ... 0.7767273009429135] ...]\n8           [[0.5075065597408408 ... 0.5075065597408408] ...]\n9           [[0.6306568904847472 ... 0.6306568904847472] ...]\ndtype: float64, shape: (10, 5, 5)\n</code></pre> <p>First column. Return a Tsd</p> <pre><code>print(tsdframe[:,0])\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  --------\n0.0         0.459267\n1.0         0.136493\n2.0         0.967718\n3.0         0.268041\n4.0         0.384593\n...\n95.0        0.384593\n96.0        0.384593\n97.0        0.384593\n98.0        0.384593\n99.0        0.384593\ndtype: float64, shape: (100,)\n</code></pre> <p>First element. Return a numpy ndarray</p> <pre><code>print(tsdtensor[0])\n</code></pre> <p>Out:</p> <pre><code>[[0.66797169 0.12707683 0.68205635 0.76496272 0.68497356]\n[0.47876519 0.65315106 0.82514157 0.83210838 0.20706615]\n[0.85286255 0.32311728 0.68644883 0.21918788 0.8099498 ]\n[0.90646119 0.4007193  0.92502858 0.73672795 0.29586105]\n[0.92490856 0.71227867 0.6011257  0.97335743 0.19006455]]\n</code></pre> <p>The time support is never changing when slicing time down.</p> <pre><code>print(tsd.time_support)\nprint(tsd[0:20].time_support)\n</code></pre> <p>Out:</p> <pre><code>   start   end\n0    0.0  99.0\n   start   end\n0    0.0  99.0\n</code></pre> <p>TsdFrame offers special slicing similar to pandas.DataFrame.</p> <p>Only TsdFrame can have columns labelling and indexing.</p> <pre><code>print(tsdframe.loc['a'])\nprint(tsdframe.loc[['a', 'c']])\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  --------\n0.0         0.459267\n1.0         0.136493\n2.0         0.967718\n3.0         0.268041\n4.0         0.384593\n...\n95.0        0.384593\n96.0        0.384593\n97.0        0.384593\n98.0        0.384593\n99.0        0.384593\ndtype: float64, shape: (100,)\nTime (s)            a         c\n----------  ---------  --------\n0.0         0.459267   0.622282\n1.0         0.136493   0.563627\n2.0         0.967718   0.823681\n3.0         0.268041   0.499102\n4.0         0.384593   0.811827\n...\n95.0        0.712022   0.704547\n96.0        0.0667072  0.593065\n97.0        0.513316   0.370928\n98.0        0.258396   0.815519\n99.0        0.848732   0.39393\ndtype: float64, shape: (100, 2)\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_numpy/#arithmetic","title":"Arithmetic","text":"<p>Arithmetical operations works similar to numpy</p> <pre><code>tsd = nap.Tsd(t=np.arange(5), d=np.ones(5))\nprint(tsd + 1)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  --\n0            2\n1            2\n2            2\n3            2\n4            2\ndtype: float64, shape: (5,)\n</code></pre> <p>It is possible to do array operations on the time series provided that the dimensions matches. The output will still be a time series object.</p> <pre><code>print(tsd - np.ones(5))\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  --\n0            0\n1            0\n2            0\n3            0\n4            0\ndtype: float64, shape: (5,)\n</code></pre> <p>Nevertheless operations like this are not permitted :</p> <pre><code>try:\ntsd + tsd\nexcept Exception as error:\nprint(error)\n</code></pre> <p>Out:</p> <pre><code>operand type(s) all returned NotImplemented from __array_ufunc__(&lt;ufunc 'add'&gt;, '__call__', Time (s)\n----------  --\n0            1\n1            1\n2            1\n3            1\n4            1\ndtype: float64, shape: (5,), Time (s)\n----------  --\n0            1\n1            1\n2            1\n3            1\n4            1\ndtype: float64, shape: (5,)): 'Tsd', 'Tsd'\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_numpy/#array-operations","title":"Array operations","text":"<p>The most common numpy functions will return a time series if the output first dimension matches the shape of the time index.</p> <p>Here i average along the time axis and get a numpy array.</p> <pre><code>print(np.mean(tsdtensor, 0))\n</code></pre> <p>Out:</p> <pre><code>[[0.53989598 0.46693001 0.55904247 0.5010459  0.55898987]\n[0.48734047 0.48102587 0.53695667 0.52594123 0.49629738]\n[0.47293619 0.47610233 0.49849669 0.45082319 0.47720145]\n[0.47790069 0.49505447 0.49640728 0.51418605 0.48386155]\n[0.4746642  0.48501922 0.50571252 0.55373443 0.52821584]]\n</code></pre> <p>Here I average across the second dimension and get a TsdFrame</p> <pre><code>print(np.mean(tsdtensor, 1))\n</code></pre> <p>Out:</p> <pre><code>Time (s)           0         1         2         3         4\n----------  --------  --------  --------  --------  --------\n0.0         0.766194  0.443269  0.74396   0.705269  0.437583\n1.0         0.580335  0.302325  0.541451  0.569046  0.528532\n2.0         0.393452  0.417835  0.719888  0.614542  0.5917\n3.0         0.246327  0.326752  0.494667  0.51683   0.501655\n4.0         0.565123  0.461321  0.617374  0.679687  0.433112\n...\n95.0        0.713203  0.354376  0.585371  0.360126  0.518726\n96.0        0.355684  0.58468   0.586803  0.617898  0.494554\n97.0        0.583016  0.296817  0.73607   0.413082  0.621698\n98.0        0.36898   0.277896  0.393621  0.548997  0.377446\n99.0        0.473297  0.448439  0.457955  0.449895  0.64016\ndtype: float64, shape: (100, 5)\n</code></pre> <p>This is not true for fft functions though.</p> <pre><code>try:\nnp.fft.fft(tsd)\nexcept Exception as error:\nprint(error)\n</code></pre> <p>Out:</p> <pre><code>no implementation found for 'numpy.fft.fft' on types that implement __array_function__: [&lt;class 'pynapple.core.time_series.Tsd'&gt;]\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_numpy/#concatenating","title":"Concatenating","text":"<p>It is possible to concatenate time series providing than they don't overlap meaning time indexe should be already sorted through all time series to concatenate</p> <pre><code>tsd1 = nap.Tsd(t=np.arange(5), d=np.ones(5))\ntsd2 = nap.Tsd(t=np.arange(5)+10, d=np.ones(5)*2)\ntsd3 = nap.Tsd(t=np.arange(5)+20, d=np.ones(5)*3)\nprint(np.concatenate((tsd1, tsd2, tsd3)))\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  --\n0            1\n1            1\n2            1\n3            1\n4            1\n10           2\n11           2\n12           2\n13           2\n14           2\n20           3\n21           3\n22           3\n23           3\n24           3\ndtype: float64, shape: (15,)\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_numpy/#spliting","title":"Spliting","text":"<p>Array split functions are also implemented</p> <pre><code>print(np.array_split(tsdtensor[0:10], 2))\n</code></pre> <p>Out:</p> <pre><code>[Time (s)\n----------  -----------------------------------------------------\n0           [[0.6679716889640175 ... 0.6679716889640175] ...]\n1           [[0.5275118854651649 ... 0.5275118854651649] ...]\n2           [[0.048474108077754874 ... 0.048474108077754874] ...]\n3           [[0.352159351812118 ... 0.352159351812118] ...]\n4           [[0.5002324080595832 ... 0.5002324080595832] ...]\ndtype: float64, shape: (5, 5, 5), Time (s)\n----------  ---------------------------------------------------\n5           [[0.728652283503968 ... 0.728652283503968] ...]\n6           [[0.27752782860399505 ... 0.27752782860399505] ...]\n7           [[0.7767273009429135 ... 0.7767273009429135] ...]\n8           [[0.5075065597408408 ... 0.5075065597408408] ...]\n9           [[0.6306568904847472 ... 0.6306568904847472] ...]\ndtype: float64, shape: (5, 5, 5)]\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_numpy/#modifying","title":"Modifying","text":"<p>It is possible to modify a time series element wise</p> <pre><code>print(tsd1)\ntsd1[0] = np.pi\nprint(tsd1)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  --\n0            1\n1            1\n2            1\n3            1\n4            1\ndtype: float64, shape: (5,)\nTime (s)\n----------  -------\n0           3.14159\n1           1\n2           1\n3           1\n4           1\ndtype: float64, shape: (5,)\n</code></pre> <p>It is also possible to modify a time series with logical operations</p> <pre><code>tsd[tsd.values&gt;0.5] = 0.0\nprint(tsd)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  --\n0            0\n1            0\n2            0\n3            0\n4            0\ndtype: float64, shape: (5,)\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_numpy/#sorting","title":"Sorting","text":"<p>It is not possible to sort along the first dimension as it would break the sorting of the time index</p> <pre><code>tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\ntry:\nnp.sort(tsd)\nexcept Exception as error:\nprint(error)\n</code></pre> <p>Out:</p> <pre><code>no implementation found for 'numpy.sort' on types that implement __array_function__: [&lt;class 'pynapple.core.time_series.Tsd'&gt;]\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.031 seconds)</p> <p> Download Python source code: tutorial_pynapple_numpy.py</p> <p> Download Jupyter notebook: tutorial_pynapple_numpy.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/tutorial_pynapple_process/","title":"Advanced processing","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_pynapple_process/#advanced-processing","title":"Advanced processing","text":"<p>The pynapple package provides a small set of high-level functions that are widely used in systems neuroscience.</p> <ul> <li>Discrete correlograms</li> <li>Tuning curves</li> <li>Decoding</li> <li>PETH</li> <li>Randomization</li> </ul> <p>This notebook provides few examples with artificial data.</p> <p>Warning</p> <p>This tutorial uses seaborn and matplotlib for displaying the figure.</p> <p>You can install both with <code>pip install matplotlib seaborn</code></p> <pre><code>import numpy as np\nimport pandas as pd\nimport pynapple as nap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", palette=\"colorblind\", font_scale=1.5, rc=custom_params)\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_process/#discrete-correlograms","title":"Discrete correlograms","text":"<p>The function to compute cross-correlogram is cross_correlogram.</p> <p>The function is compiled with numba to improve performances. This means it only accepts pure numpy arrays as input arguments.</p> <pre><code>ts1 = nap.Ts(t=np.sort(np.random.uniform(0, 1000, 1000)), time_units=\"s\")\nts2 = nap.Ts(t=np.sort(np.random.uniform(0, 1000, 10)), time_units=\"s\")\nts1_time_array = ts1.as_units(\"s\").index.values\nts2_time_array = ts2.as_units(\"s\").index.values\nbinsize = 0.1  # second\ncc12, xt = nap.cross_correlogram(\nt1=ts1_time_array, t2=ts2_time_array, binsize=binsize, windowsize=1  # second\n)\nplt.figure(figsize=(10, 6))\nplt.bar(xt, cc12, binsize)\nplt.xlabel(\"Time t1 (us)\")\nplt.ylabel(\"CC\")\n</code></pre> <p></p> <p>Out:</p> <pre><code>Text(40.625, 0.5, 'CC')\n</code></pre> <p>To simplify converting to a numpy.ndarray, pynapple provides wrappers for computing autocorrelogram and crosscorrelogram for TsGroup. The function is then called for each unit or each pairs of units. It returns directly a pandas.DataFrame holding all the correlograms. In this example, autocorrelograms and cross-correlograms are computed for the same TsGroup.</p> <pre><code>epoch = nap.IntervalSet(start=0, end=1000, time_units=\"s\")\nts_group = nap.TsGroup({0: ts1, 1: ts2}, time_support=epoch)\nautocorrs = nap.compute_autocorrelogram(\ngroup=ts_group, binsize=100, windowsize=1000, time_units=\"ms\", ep=epoch  # ms  # ms\n)\ncrosscorrs = nap.compute_crosscorrelogram(\ngroup=ts_group, binsize=100, windowsize=1000, time_units=\"ms\"  # ms  # ms\n)\nprint(autocorrs, \"\\n\")\nprint(crosscorrs, \"\\n\")\n</code></pre> <p>Out:</p> <pre><code>         0    1\n-0.9  0.96  0.0\n-0.8  0.92  0.0\n-0.7  1.17  0.0\n-0.6  1.11  0.0\n-0.5  0.86  0.0\n-0.4  0.99  0.0\n-0.3  1.09  0.0\n-0.2  0.95  0.0\n-0.1  0.96  0.0\n 0.0  0.00  0.0\n 0.1  0.96  0.0\n 0.2  0.95  0.0\n 0.3  1.09  0.0\n 0.4  0.99  0.0\n 0.5  0.86  0.0\n 0.6  1.11  0.0\n 0.7  1.17  0.0\n 0.8  0.92  0.0\n 0.9  0.96  0.0 0\n1\n-0.9  0.0\n-0.8  1.0\n-0.7  0.0\n-0.6  2.0\n-0.5  4.0\n-0.4  1.0\n-0.3  0.0\n-0.2  0.0\n-0.1  3.0\n 0.0  2.0\n 0.1  0.0\n 0.2  1.0\n 0.3  0.0\n 0.4  1.0\n 0.5  2.0\n 0.6  0.0\n 0.7  1.0\n 0.8  2.0\n 0.9  0.0 </code></pre>"},{"location":"generated/gallery/tutorial_pynapple_process/#peri-event-time-histogram-peth","title":"Peri-Event Time Histogram (PETH)","text":"<p>A second way to examine the relationship between spiking and an event (i.e. stimulus) is to compute a PETH. pynapple uses the function <code>compute_perievent</code> to center spike time around the timestamps of an event within a given window.</p> <pre><code>stim = nap.Tsd(\nt=np.sort(np.random.uniform(0, 1000, 50)), d=np.random.rand(50), time_units=\"s\"\n)\npeth0 = nap.compute_perievent(ts1, stim, minmax=(-0.1, 0.2), time_unit=\"s\")\nprint(peth0)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate    ref_times\n-------  ------  -----------\n      0  nan         78.513\n      1    6.67      78.8417\n      2  nan         85.0961\n      3  nan         85.1987\n      4    3.33     105.857\n      5  nan        136.427\n      6  nan        145.879\n      7    3.33     205.517\n      8    3.33     207.54\n      9  nan        211.637\n     10  nan        228.275\n     11  nan        258.534\n     12  nan        296.897\n     13  nan        347.208\n     14    3.33     361.181\n     15    3.33     413.129\n     16    3.33     418.614\n     17  nan        419.974\n     18  nan        428.919\n     19  nan        479.724\n     20    3.33     500.469\n     21  nan        505.009\n     22  nan        520.488\n     23  nan        548.482\n     24  nan        575.504\n     25    3.33     579.486\n     26  nan        591.174\n     27  nan        592.477\n     28    3.33     613.891\n     29  nan        633.46\n     30    3.33     656.839\n     31  nan        660.136\n     32  nan        663.065\n     33  nan        665.236\n     34  nan        673.042\n     35  nan        684.376\n     36  nan        691.76\n     37  nan        697.944\n     38    3.33     819.187\n     39  nan        821.761\n     40  nan        821.768\n     41  nan        823.108\n     42  nan        826.045\n     43  nan        840.194\n     44  nan        848.399\n     45  nan        895.544\n     46  nan        902.414\n     47  nan        945.194\n     48  nan        985.547\n     49  nan        987.166\n</code></pre> <p>It is then easy to create a raster plot around the times of the stimulation event by calling the <code>to_tsd</code> function of pynapple to \"flatten\" the TsGroup peth0.</p> <p>mkdocs_gallery_thumbnail_number = 2</p> <pre><code>plt.figure(figsize=(10, 6))\nplt.subplot(211)\nplt.plot(np.sum(peth0.count(0.01), 1), linewidth=3, color=\"red\")\nplt.xlim(-0.1, 0.2)\nplt.ylabel(\"Count\")\nplt.axvline(0.0)\nplt.subplot(212)\nplt.plot(peth0.to_tsd(), \"|\", markersize=20, color=\"red\", mew=4)\nplt.xlabel(\"Time from stim (s)\")\nplt.ylabel(\"Stimulus\")\nplt.xlim(-0.1, 0.2)\nplt.axvline(0.0)\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;matplotlib.lines.Line2D object at 0x7f5159eb7a60&gt;\n</code></pre> <p>The same function can be applied to a group of neurons. In this case, it returns a dict of TsGroup</p> <pre><code>pethall = nap.compute_perievent(ts_group, stim, minmax=(-0.1, 0.2), time_unit=\"s\")\nprint(pethall[1])\n</code></pre> <p>Out:</p> <pre><code>  Index    rate    ref_times\n-------  ------  -----------\n      0  nan         78.513\n      1  nan         78.8417\n      2  nan         85.0961\n      3  nan         85.1987\n      4  nan        105.857\n      5  nan        136.427\n      6  nan        145.879\n      7  nan        205.517\n      8  nan        207.54\n      9  nan        211.637\n     10  nan        228.275\n     11  nan        258.534\n     12  nan        296.897\n     13  nan        347.208\n     14  nan        361.181\n     15  nan        413.129\n     16  nan        418.614\n     17  nan        419.974\n     18  nan        428.919\n     19  nan        479.724\n     20  nan        500.469\n     21  nan        505.009\n     22  nan        520.488\n     23  nan        548.482\n     24  nan        575.504\n     25  nan        579.486\n     26  nan        591.174\n     27  nan        592.477\n     28  nan        613.891\n     29  nan        633.46\n     30  nan        656.839\n     31  nan        660.136\n     32  nan        663.065\n     33  nan        665.236\n     34    3.33     673.042\n     35  nan        684.376\n     36  nan        691.76\n     37  nan        697.944\n     38  nan        819.187\n     39  nan        821.761\n     40  nan        821.768\n     41  nan        823.108\n     42  nan        826.045\n     43  nan        840.194\n     44  nan        848.399\n     45  nan        895.544\n     46  nan        902.414\n     47  nan        945.194\n     48  nan        985.547\n     49  nan        987.166\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_process/#tuning-curves","title":"Tuning curves","text":"<p>pynapple can compute 1 dimension tuning curves (for example firing rate as a function of angular direction) and 2 dimension tuning curves ( for example firing rate as a function of position). In both cases, a TsGroup object can be directly passed to the function.</p> <p>First we will create the 2D features:</p> <pre><code>dt = 0.1\nfeatures = np.vstack((np.cos(np.arange(0, 1000, dt)), np.sin(np.arange(0, 1000, dt)))).T\n# features += np.random.randn(features.shape[0], features.shape[1])*0.05\nfeatures = nap.TsdFrame(\nt=np.arange(0, 1000, dt),\nd=features,\ntime_units=\"s\",\ntime_support=epoch,\ncolumns=[\"a\", \"b\"],\n)\nprint(features)\nplt.figure(figsize=(15, 7))\nplt.subplot(121)\nplt.plot(features[0:100])\nplt.title(\"Features\")\nplt.xlabel(\"Time(s)\")\nplt.subplot(122)\nplt.title(\"Features\")\nplt.plot(features[\"a\"][0:100], features[\"b\"][0:100])\nplt.xlabel(\"Feature a\")\nplt.ylabel(\"Feature b\")\n</code></pre> <p></p> <p>Out:</p> <pre><code>Time (s)           a          b\n----------  --------  ---------\n0.0         1         0\n0.1         0.995004  0.0998334\n0.2         0.980067  0.198669\n0.3         0.955336  0.29552\n0.4         0.921061  0.389418\n...\n999.5       0.889961  0.456036\n999.6       0.839987  0.542606\n999.7       0.781621  0.623754\n999.8       0.715445  0.69867\n999.9       0.64212   0.766604\ndtype: float64, shape: (10000, 2)\nText(732.5909090909089, 0.5, 'Feature b')\n</code></pre> <p>Here we call the function <code>compute_2d_tuning_curves</code>. To check the accuracy of the tuning curves, we will display the spikes aligned to the features with the function <code>value_from</code> which assign to each spikes the corresponding feature value for neuron 0.</p> <pre><code>tcurves2d, binsxy = nap.compute_2d_tuning_curves(\ngroup=ts_group, feature=features, nb_bins=10\n)\nts_to_features = ts_group[1].value_from(features)\nplt.figure()\nplt.plot(ts_to_features[\"a\"], ts_to_features[\"b\"], \"o\", color=\"red\", markersize=4)\nextents = (\nnp.min(features[\"b\"]),\nnp.max(features[\"b\"]),\nnp.min(features[\"a\"]),\nnp.max(features[\"a\"]),\n)\nplt.imshow(tcurves2d[1].T, origin=\"lower\", extent=extents, cmap=\"viridis\")\nplt.title(\"Tuning curve unit 0 2d\")\nplt.xlabel(\"feature a\")\nplt.ylabel(\"feature b\")\nplt.grid(False)\nplt.show()\n</code></pre> <p></p> <p>Out:</p> <pre><code>/mnt/home/gviejo/pynapple/pynapple/process/tuning_curves.py:204: RuntimeWarning: invalid value encountered in divide\n  count = count / occupancy\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_process/#decoding","title":"Decoding","text":"<p>Pynapple supports 1 dimensional and 2 dimensional bayesian decoding. The function returns the decoded feature as well as the probabilities for each timestamps.</p> <p>First we generate some artificial \"place fields\" in 2 dimensions based on the features.</p> <p>This part is just to generate units with a relationship to the features (i.e. \"place fields\")</p> <pre><code>times = features.as_units(\"us\").index.values\nft = features.values\nalpha = np.arctan2(ft[:, 1], ft[:, 0])\nbins = np.repeat(np.linspace(-np.pi, np.pi, 13)[::, np.newaxis], 2, 1)\nbins += np.array([-2 * np.pi / 24, 2 * np.pi / 24])\nts_group = {}\nfor i in range(12):\nts = times[(alpha &gt;= bins[i, 0]) &amp; (alpha &lt;= bins[i + 1, 1])]\nts_group[i] = nap.Ts(ts, time_units=\"us\")\nts_group = nap.TsGroup(ts_group, time_support=epoch)\nprint(ts_group)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate\n-------  ------\n      0    1.25\n      1    1.67\n      2    1.67\n      3    1.66\n      4    1.67\n      5    1.67\n      6    1.67\n      7    1.67\n      8    1.67\n      9    1.67\n     10    1.67\n     11    1.25\n</code></pre> <p>To decode we need to compute tuning curves in 2D.</p> <pre><code>import warnings\nwarnings.filterwarnings(\"ignore\")\ntcurves2d, binsxy = nap.compute_2d_tuning_curves(\ngroup=ts_group,\nfeature=features,\nnb_bins=10,\nep=epoch,\nminmax=(-1.0, 1.0, -1.0, 1.0),\n)\n</code></pre> <p>Then we plot the \"place fields\".</p> <pre><code>plt.figure(figsize=(20, 9))\nfor i in ts_group.keys():\nplt.subplot(2, 6, i + 1)\nplt.imshow(\ntcurves2d[i], extent=(binsxy[1][0], binsxy[1][-1], binsxy[0][0], binsxy[0][-1])\n)\nplt.xticks()\nplt.show()\n</code></pre> <p></p> <p>Then we call the actual decoding function in 2d.</p> <pre><code>decoded, proba_feature = nap.decode_2d(\ntuning_curves=tcurves2d,\ngroup=ts_group,\nep=epoch,\nbin_size=0.1,  # second\nxy=binsxy,\nfeatures=features,\n)\nplt.figure(figsize=(15, 5))\nplt.subplot(131)\nplt.plot(features[\"a\"].as_units(\"s\").loc[0:20], label=\"True\")\nplt.plot(decoded[\"a\"].as_units(\"s\").loc[0:20], label=\"Decoded\")\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Feature a\")\nplt.subplot(132)\nplt.plot(features[\"b\"].as_units(\"s\").loc[0:20], label=\"True\")\nplt.plot(decoded[\"b\"].as_units(\"s\").loc[0:20], label=\"Decoded\")\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Feature b\")\nplt.subplot(133)\nplt.plot(\nfeatures[\"a\"].as_units(\"s\").loc[0:20],\nfeatures[\"b\"].as_units(\"s\").loc[0:20],\nlabel=\"True\",\n)\nplt.plot(\ndecoded[\"a\"].as_units(\"s\").loc[0:20],\ndecoded[\"b\"].as_units(\"s\").loc[0:20],\nlabel=\"Decoded\",\n)\nplt.xlabel(\"Feature a\")\nplt.ylabel(\"Feature b\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"generated/gallery/tutorial_pynapple_process/#randomization","title":"Randomization","text":"<p>Pynapple provides some ready-to-use randomization methods to compute null distributions for statistical testing. Different methods preserve or destroy different features of the data, here's a brief overview.</p> <p><code>shift_timestamps</code> shifts all the timestamps in a <code>Ts</code> object by the same random amount, wrapping the end of the time support to its beginning. This randomization preserves the temporal structure in the data but destroys the temporal relationships with other quantities (e.g. behavioural data). When applied on a <code>TsGroup</code> object, each series in the group is shifted independently.</p> <pre><code>ts = nap.Ts(t=np.sort(np.random.uniform(0, 100, 10)), time_units=\"ms\")\nrand_ts = nap.shift_timestamps(ts, min_shift=1, max_shift=20)\n</code></pre> <p><code>shuffle_ts_intervals</code> computes the intervals between consecutive timestamps, permutes them, and generates a new set of timestamps with the permuted intervals. This procedure preserve the distribution of intervals, but not their sequence.</p> <pre><code>ts = nap.Ts(t=np.sort(np.random.uniform(0, 100, 10)), time_units=\"s\")\nrand_ts = nap.shuffle_ts_intervals(ts)\n</code></pre> <p><code>jitter_timestamps</code> shifts each timestamp in the data of an independent random amount. When applied with a small <code>max_jitter</code>, this procedure destroys the fine temporal structure of the data, while preserving structure on longer timescales.</p> <pre><code>ts = nap.Ts(t=np.sort(np.random.uniform(0, 100, 10)), time_units=\"s\")\nrand_ts = nap.jitter_timestamps(ts, max_jitter=1)\n</code></pre> <p><code>resample_timestamps</code> uniformly re-draws the same number of timestamps in <code>ts</code>, in the same time support. This procedures preserve the total number of timestamps, but destroys any other feature of the original data.</p> <pre><code>ts = nap.Ts(t=np.sort(np.random.uniform(0, 100, 10)), time_units=\"s\")\nrand_ts = nap.resample_timestamps(ts)\n</code></pre> <p>Total running time of the script: ( 0 minutes  1.835 seconds)</p> <p> Download Python source code: tutorial_pynapple_process.py</p> <p> Download Jupyter notebook: tutorial_pynapple_process.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/","title":"Quick start","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#quick-start","title":"Quick start","text":"<p>The examplar data to replicate the figure in the jupyter notebook can be found here. </p> <p>The data contains a sample recordings taken simultaneously from the anterodorsal thalamus and the hippocampus and contains both a sleep and wake session. It contains both head-direction cells (i.e. cells that fire for a particular head direction in the horizontal plane) and place cells (i.e. cells that fire for a particular position in the environment).</p> <p>Preprocessing of the data was made with Kilosort 2.0 and spike sorting was made with Klusters.</p> <p>Instructions for installing pynapple can be found here.</p> <p>This notebook is meant to provide an overview of pynapple by going through:</p> <ul> <li>Input output (IO). In this case, pynapple will load a NWB file using the NWBFile object within a project Folder that represent a dataset. </li> <li>Core functions that handle time series, interval sets and groups of time series. See this notebook for a detailled usage of the core functions.</li> <li>Process functions. A small collection of high-level functions widely used in system neuroscience. This notebook details those functions.</li> </ul> <p>Warning</p> <p>This tutorial uses seaborn and matplotlib for displaying the figure.</p> <p>You can install both with <code>pip install matplotlib seaborn</code></p> <pre><code>import numpy as np\nimport pandas as pd\nimport pynapple as nap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", palette=\"colorblind\", font_scale=1.5, rc=custom_params)\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#io","title":"IO","text":"<p>The first step is to give the path to the data folder.</p> <pre><code>DATA_DIRECTORY = \"../../your/path/to/MyProject/\"\n</code></pre> <p>We can load the session with the function load_folder. Pynapple will walks throught the folder and collects every subfolders. We can use the attribute <code>view</code> or the function <code>expand</code> to display a tree view of the dataset. The treeview shows all the compatible data format (i.e npz files or NWBs files) and their equivalent pynapple type.</p> <pre><code>data = nap.load_folder(DATA_DIRECTORY)\ndata.view\n</code></pre> <p>Out:</p> <pre><code>\ud83d\udcc2 MyProject\n\u2514\u2500\u2500 \ud83d\udcc2 sub-A2929\n    \u2514\u2500\u2500 \ud83d\udcc2 ses-A2929-200711\n        \u251c\u2500\u2500 \ud83d\udcc2 derivatives\n        \u2502   \u251c\u2500\u2500 spikes.npz      |        TsGroup\n        \u2502   \u251c\u2500\u2500 sleep_ep.npz    |        IntervalSet\n        \u2502   \u251c\u2500\u2500 position.npz    |        TsdFrame\n        \u2502   \u2514\u2500\u2500 wake_ep.npz     |        IntervalSet\n        \u251c\u2500\u2500 \ud83d\udcc2 pynapplenwb\n        \u2502   \u2514\u2500\u2500 A2929-200711    |        NWB file\n        \u251c\u2500\u2500 x_plus_y.npz    |        Tsd\n        \u2514\u2500\u2500 stimulus-fish.npz       |        IntervalSet\n</code></pre> <p>The object <code>data</code> is a <code>Folder</code> object that allows easy navigation and interaction with a dataset. In this case, we want to load the NWB file in the folder <code>/pynapplenwb</code>. Data are always lazy loaded. No time series is loaded until it's actually called. When calling the NWB file, the object <code>nwb</code> is an interface to the NWB file. All the data inside the NWB file that are compatible with one of the pynapple objects are shown with their corresponding keys.</p> <pre><code>nwb = data[\"sub-A2929\"][\"ses-A2929-200711\"][\"pynapplenwb\"][\"A2929-200711\"]\nprint(nwb)\n</code></pre> <p>Out:</p> <pre><code>A2929-200711\n\u250d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u252f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2511\n\u2502 Keys                  \u2502 Type        \u2502\n\u251d\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u253f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2525\n\u2502 units                 \u2502 TsGroup     \u2502\n\u2502 position_time_support \u2502 IntervalSet \u2502\n\u2502 epochs                \u2502 IntervalSet \u2502\n\u2502 z                     \u2502 Tsd         \u2502\n\u2502 y                     \u2502 Tsd         \u2502\n\u2502 x                     \u2502 Tsd         \u2502\n\u2502 rz                    \u2502 Tsd         \u2502\n\u2502 ry                    \u2502 Tsd         \u2502\n\u2502 rx                    \u2502 Tsd         \u2502\n\u2515\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2537\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2519\n</code></pre> <p>We can individually call each object and they are actually loaded.</p> <p><code>units</code> is a TsGroup object. It allows to group together time series with different timestamps and couple metainformation to each neuron. In this case, the location of where the neuron was recorded has been added when loading the session for the first time. We load <code>units</code> as <code>spikes</code></p> <pre><code>spikes = nwb[\"units\"]\nprint(spikes)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate  location      group\n-------  ------  ----------  -------\n      0    7.3   adn               0\n1    5.73  adn               0\n2    8.12  adn               0\n3    6.68  adn               0\n4   10.77  adn               0\n5   11     adn               0\n6   16.52  adn               0\n7    2.2   ca1               1\n8    2.02  ca1               1\n9    1.07  ca1               1\n10    3.92  ca1               1\n11    3.31  ca1               1\n12    1.09  ca1               1\n13    1.28  ca1               1\n14    1.32  ca1               1\n</code></pre> <p>In this case, the TsGroup holds 15 neurons and it is possible to access, similar to a dictionnary, the spike times of a single neuron:</p> <pre><code>neuron_0 = spikes[0]\nprint(neuron_0)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n0.00845\n0.03265\n0.1323\n0.3034\n0.329\n...\n1186.12755\n1189.384\n1194.13475\n1196.2075\n1196.67675\nshape: 8764\n</code></pre> <p><code>neuron_0</code> is a Ts object containing the times of the spikes.</p> <p>The other information about the session is contained in <code>nwb[\"epochs\"]</code>. In this case, the start and end of the sleep and wake epochs. If the NWB time intervals contains tags of the epochs, pynapple will try to group them together and return a dictionnary of IntervalSet instead of IntervalSet.</p> <pre><code>epochs = nwb[\"epochs\"]\nprint(epochs)\n</code></pre> <p>Out:</p> <pre><code>{'sleep':    start    end\n0    0.0  600.0, 'wake':    start     end\n0  600.0  1200.0}\n</code></pre> <p>Finally this dataset contains tracking of the animal in the environment. <code>rx</code>, <code>ry</code>, <code>rz</code> represent respectively the roll, the yaw and the pitch of the head of the animal. <code>x</code> and <code>z</code> represent the position of the animal in the horizontal plane while <code>y</code> represents the elevation. Here we load only the head-direction as a Tsd object.</p> <pre><code>head_direction = nwb[\"ry\"]\nprint(head_direction)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  -------\n670.6407    5.20715\n670.649     5.18103\n670.65735   5.15551\n670.66565   5.13654\n670.674     5.12085\n...\n1199.9616   5.12085\n1199.96995  5.12085\n1199.97825  5.12085\n1199.9866   5.12085\n1199.99495  5.12085\ndtype: float64, shape: (63527,)\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#core","title":"Core","text":"<p>The core functions of pynapple provides many ways to manipulate time series. In this example, spike times are restricted to the wake epoch. Notice how the frequencies change from the original object.</p> <pre><code>wake_ep = epochs[\"wake\"]\nspikes_wake = spikes.restrict(wake_ep)\nprint(spikes_wake)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate  location      group\n-------  ------  ----------  -------\n      0    4.85  adn               0\n1    8.06  adn               0\n2    7.11  adn               0\n3    7.66  adn               0\n4    7.97  adn               0\n5   11.29  adn               0\n6   22.08  adn               0\n7    1.82  ca1               1\n8    2.84  ca1               1\n9    0.7   ca1               1\n10    4.78  ca1               1\n11    4.93  ca1               1\n12    1.71  ca1               1\n13    0.97  ca1               1\n14    0.26  ca1               1\n</code></pre> <p>The same operation can be applied to all time series.</p> <pre><code># In this example, we want all the epochs for which position in `x` is above a certain threhsold. For this we use the function `threshold`.\nposx = nwb[\"x\"]\nthreshold = 0.08\nposxpositive = posx.threshold(threshold)\nplt.figure()\nplt.plot(posx)\nplt.plot(posxpositive, \".\")\nplt.axhline(threshold)\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"x\")\nplt.title(\"x &gt; {}\".format(threshold))\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>The epochs above the threshold can be accessed through the time support of the Tsd object. The time support is an important concept in the pynapple package. It helps the user to define the epochs for which the time serie should be defined. By default, Ts, Tsd and TsGroup objects possess a time support (defined as an IntervalSet). It is recommended to pass the time support when instantiating one of those objects.</p> <pre><code>epochs_above_thr = posxpositive.time_support\nprint(epochs_above_thr)\n</code></pre> <p>Out:</p> <pre><code>        start         end\n0  682.660850  745.565725\n1  752.240350  752.440325\n2  752.582000  752.673650\n3  757.498375  758.998300\n4  789.863275  790.271575\n5  875.225250  876.066875\n6  878.158425  878.641725\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#tuning-curves","title":"Tuning curves","text":"<p>Let's do a more advanced analysis. Neurons from ADn (group 0 in the <code>spikes</code> group object) are know to fire for a particular direction. Therefore, we can compute their tuning curves, i.e. their firing rates as a function of the head-direction of the animal in the horizontal plane (ry). To do this, we can use the function <code>compute_1d_tuning_curves</code>. In this case, the tuning curves are computed over 120 bins and between 0 and 2$\\pi$.</p> <pre><code>tuning_curves = nap.compute_1d_tuning_curves(\ngroup=spikes, feature=head_direction, nb_bins=121, minmax=(0, 2 * np.pi)\n)\nprint(tuning_curves)\n</code></pre> <p>Out:</p> <pre><code>                 0    1         2          3         4          5   ...        9          10         11         12        13        14\n0.025964  45.520459  0.0  0.000000   6.207335  6.207335   6.207335  ...  2.069112   6.207335  10.345559  14.483782  0.000000  2.069112\n0.077891  55.049762  0.0  0.000000   5.504976  3.302986   3.302986  ...  2.201990   8.807962  16.514929   1.100995  0.000000  0.000000\n0.129818  76.369034  0.0  0.000000  17.144069  4.675655   1.558552  ...  1.558552   3.117103  12.468414   9.351310  1.558552  0.000000\n0.181745  82.179721  0.0  0.000000   6.522200  1.304440   2.608880  ...  0.000000   6.522200  19.566600   9.131080  2.608880  0.000000\n0.233672  73.851374  0.0  0.000000  13.187745  5.275098   2.637549  ...  1.318775  15.825294  30.331814   7.912647  2.637549  0.000000\n...             ...  ...       ...        ...       ...        ...  ...       ...        ...        ...        ...       ...       ...\n6.049513  15.001060  0.0  0.000000  12.273595  1.363733  15.001060  ...  0.000000   8.182397   2.727466   0.000000  0.000000  1.363733\n6.101440  22.327159  0.0  0.000000  13.954475  0.000000  11.163580  ...  0.000000   2.790895  11.163580   2.790895  0.000000  0.000000\n6.153367  47.062150  0.0  0.000000  21.177967  0.000000   7.059322  ...  2.353107   7.059322  11.765537   0.000000  2.353107  0.000000\n6.205295  56.003958  0.0  2.000141   8.000565  2.000141  10.000707  ...  0.000000  14.000990  24.001696   6.000424  0.000000  0.000000\n6.257222  38.712414  0.0  0.000000   7.742483  0.000000   7.742483  ...  0.000000   7.742483   7.742483   7.742483  0.000000  0.000000\n\n[121 rows x 15 columns]\n</code></pre> <p>We can plot tuning curves in polar plots.</p> <pre><code>neuron_location = spikes.get_info(\"location\")  # to know where the neuron was recorded\nplt.figure(figsize=(12, 9))\nfor i, n in enumerate(tuning_curves.columns):\nplt.subplot(3, 5, i + 1, projection=\"polar\")\nplt.plot(tuning_curves[n])\nplt.title(neuron_location[n] + \"-\" + str(n), fontsize=18)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>While ADN neurons show obvious modulation for head-direction, it is not obvious for all CA1 cells. Therefore we want to restrict the remaining of the analyses to only ADN neurons. We can split the <code>spikes</code> group with the function <code>getby_category</code>.</p> <pre><code>spikes_by_location = spikes.getby_category(\"location\")\nprint(spikes_by_location[\"adn\"])\nprint(spikes_by_location[\"ca1\"])\nspikes_adn = spikes_by_location[\"adn\"]\n</code></pre> <p>Out:</p> <pre><code>  Index    rate  location      group\n-------  ------  ----------  -------\n      0    7.3   adn               0\n1    5.73  adn               0\n2    8.12  adn               0\n3    6.68  adn               0\n4   10.77  adn               0\n5   11     adn               0\n6   16.52  adn               0\nIndex    rate  location      group\n-------  ------  ----------  -------\n      7    2.2   ca1               1\n8    2.02  ca1               1\n9    1.07  ca1               1\n10    3.92  ca1               1\n11    3.31  ca1               1\n12    1.09  ca1               1\n13    1.28  ca1               1\n14    1.32  ca1               1\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#correlograms","title":"Correlograms","text":"<p>A classical question with head-direction cells is how pairs stay coordinated across brain states i.e. wake vs sleep (see Peyrache, A., Lacroix, M. M., Petersen, P. C., &amp; Buzs\u00e1ki, G. (2015). Internally organized mechanisms of the head direction sense. Nature neuroscience, 18(4), 569-575.)</p> <p>In this example, this coordination across brain states will be evaluated with cross-correlograms of pairs of neurons. We can call the function <code>compute_crosscorrelogram</code> during both sleep and wake epochs.</p> <pre><code>cc_wake = nap.compute_crosscorrelogram(\ngroup=spikes_adn,\nbinsize=20,  # ms\nwindowsize=4000,  # ms\nep=epochs[\"wake\"],\nnorm=True,\ntime_units=\"ms\",\n)\ncc_sleep = nap.compute_crosscorrelogram(\ngroup=spikes_adn,\nbinsize=5,  # ms\nwindowsize=400,  # ms\nep=epochs[\"sleep\"],\nnorm=True,\ntime_units=\"ms\",\n)\n</code></pre> <p>From the previous figure, we can see that neurons 0 and 1 fires for opposite directions during wake. Therefore we expect their cross-correlograms to show a trough around 0 time lag, meaning those two neurons do not fire spikes together. A similar trough during sleep for the same pair thus indicates a persistence of their coordination even if the animal is not moving its head. mkdocs_gallery_thumbnail_number = 3</p> <pre><code>xtwake = cc_wake.index.values\nxtsleep = cc_sleep.index.values\nplt.figure(figsize=(15, 5))\nplt.subplot(131, projection=\"polar\")\nplt.plot(tuning_curves[[0, 1]])  # The tuning curves of the pair [0,1]\nplt.subplot(132)\nplt.fill_between(\nxtwake, np.zeros_like(xtwake), cc_wake[(0, 1)].values, color=\"darkgray\"\n)\nplt.title(\"wake\")\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"CC\")\nplt.subplot(133)\nplt.fill_between(\nxtsleep, np.zeros_like(xtsleep), cc_sleep[(0, 1)].values, color=\"lightgrey\"\n)\nplt.title(\"sleep\")\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"CC\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#decoding","title":"Decoding","text":"<p>This last analysis shows how to use the pynapple's decoding function.</p> <p>The previous result indicates a persistent coordination of head-direction cells during sleep. Therefore it is possible to decode a virtual head-direction signal even if the animal is not moving its head. This example uses the function <code>decode_1d</code> which implements bayesian decoding (see : Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.)</p> <p>First we can validate the decoding function with the real position of the head of the animal during wake.</p> <pre><code>tuning_curves_adn = nap.compute_1d_tuning_curves(\nspikes_adn, head_direction, nb_bins=61, minmax=(0, 2 * np.pi)\n)\ndecoded, proba_angle = nap.decode_1d(\ntuning_curves=tuning_curves_adn,\ngroup=spikes_adn,\nep=epochs[\"wake\"],\nbin_size=0.3,  # second\nfeature=head_direction,\n)\nprint(decoded)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  -------\n600.15      2.11156\n600.45      2.11156\n600.75      2.31757\n601.05      2.00856\n601.35      2.11156\n...\n1198.65     2.11156\n1198.95     2.11156\n1199.25     2.11156\n1199.55     2.11156\n1199.85     2.11156\ndtype: float64, shape: (2000,)\n</code></pre> <p>We can plot the decoded head-direction along with the true head-direction.</p> <pre><code>plt.figure(figsize=(20, 5))\nplt.plot(head_direction.as_units(\"s\"), label=\"True\")\nplt.plot(decoded.as_units(\"s\"), label=\"Decoded\")\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Head-direction (rad)\")\nplt.show()\n</code></pre> <p></p>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#raster","title":"Raster","text":"<p>Finally we can decode activity during sleep and overlay spiking activity of ADN neurons as a raster plot (in this case only during the first 4 seconds). Pynapple return as well the probability of being in a particular state. We can display it next to the spike train.</p> <p>First let's decode during sleep with a bin size of 40 ms.</p> <pre><code>decoded_sleep, proba_angle_Sleep = nap.decode_1d(\ntuning_curves=tuning_curves_adn,\ngroup=spikes_adn,\nep=epochs[\"sleep\"],\nbin_size=0.04,  # second\nfeature=head_direction,\n)\n</code></pre> <p>Here we are gonna chain the TsGroup function <code>set_info</code> and the function <code>to_tsd</code> to flatten the TsGroup and quickly assign to each spikes a corresponding value found in the metadata table. Any columns of the metadata table can be assigned to timestamps in a TsGroup.</p> <p>Here the value assign to the spikes comes from the preferred firing direction of the neurons. The following line is a quick way to sort the neurons based on their preferred firing direction</p> <pre><code>order = np.argsort(np.argmax(tuning_curves_adn.values, 0))\nprint(order)\n</code></pre> <p>Out:</p> <pre><code>[0 4 2 6 1 3 5]\n</code></pre> <p>Assigning order as a metadata of TsGroup</p> <pre><code>spikes_adn.set_info(order=order)\nprint(spikes_adn)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate  location      group    order\n-------  ------  ----------  -------  -------\n      0    7.3   adn               0        0\n1    5.73  adn               0        4\n2    8.12  adn               0        2\n3    6.68  adn               0        6\n4   10.77  adn               0        1\n5   11     adn               0        3\n6   16.52  adn               0        5\n</code></pre> <p>\"Flattening\" the TsGroup to a Tsd based on <code>order</code>. It's then very easy to call plot on <code>tsd_adn</code> to display the raster</p> <pre><code>tsd_adn = spikes_adn.to_tsd(\"order\")\nprint(tsd_adn)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n----------  --\n0.00845      0\n0.03265      0\n0.07745      6\n0.1323       0\n0.14045      5\n...\n1199.9065    5\n1199.91745   5\n1199.94065   5\n1199.95035   5\n1199.96795   5\ndtype: float64, shape: (79349,)\n</code></pre> <p>Plotting everything</p> <pre><code>subep = nap.IntervalSet(start=0, end=10, time_units=\"s\")\nplt.figure(figsize=(19, 10))\nplt.subplot(211)\nplt.plot(tsd_adn.restrict(subep), \"|\", markersize=20)\nplt.xlim(subep.start[0], subep.end[0])\nplt.ylabel(\"Order\")\nplt.title(\"Decoding during sleep\")\nplt.subplot(212)\np = proba_angle_Sleep.restrict(subep)\nplt.imshow(p.values.T, aspect=\"auto\", origin=\"lower\", cmap=\"viridis\")\nplt.title(\"Probability\")\nplt.xticks([0, p.shape[0] - 1], subep.values[0])\nplt.yticks([0, p.shape[1]], [\"0\", \"360\"])\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Head-direction (deg)\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Out:</p> <pre><code>No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n</code></pre> <p>Total running time of the script: ( 0 minutes  5.766 seconds)</p> <p> Download Python source code: tutorial_pynapple_quick_start.py</p> <p> Download Jupyter notebook: tutorial_pynapple_quick_start.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"old_pages/core.interval_set/","title":"Core.interval set","text":""},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set","title":"pynapple.core.interval_set","text":""},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet","title":"IntervalSet","text":"<p>             Bases: <code>DataFrame</code></p> <p>A subclass of pandas.DataFrame representing a (irregular) set of time intervals in elapsed time, with relative operations</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>class IntervalSet(pd.DataFrame):\n# class IntervalSet():\n\"\"\"\n    A subclass of pandas.DataFrame representing a (irregular) set of time intervals in elapsed time, with relative operations\n    \"\"\"\ndef __init__(self, start, end=None, time_units=\"s\", **kwargs):\n\"\"\"\n        IntervalSet initializer\n        If start and end and not aligned, meaning that \\n\n        1. len(start) != len(end)\n        2. end[i] &gt; start[i]\n        3. start[i+1] &gt; end[i]\n        4. start and end are not sorted,\n        IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point\n        Parameters\n        ----------\n        start : numpy.ndarray or number or pandas.DataFrame\n            Beginning of intervals\n        end : numpy.ndarray or number, optional\n            Ends of intervals\n        time_units : str, optional\n            Time unit of the intervals ('us', 'ms', 's' [default])\n        **kwargs\n            Additional parameters passed ot pandas.DataFrame\n        Returns\n        -------\n        IntervalSet\n            _\n        Raises\n        ------\n        RuntimeError\n            Description\n        ValueError\n            If a pandas.DataFrame is passed, it should contains\n            a column 'start' and a column 'end'.\n        \"\"\"\nif end is None:\ndf = pd.DataFrame(start)\nif \"start\" not in df.columns or \"end\" not in df.columns:\nraise ValueError(\"wrong columns name\")\nstart = df[\"start\"].values.astype(np.float64)\nend = df[\"end\"].values.astype(np.float64)\nstart = TsIndex.sort_timestamps(\nTsIndex.format_timestamps(start.ravel(), time_units)\n)\nend = TsIndex.sort_timestamps(\nTsIndex.format_timestamps(end.ravel(), time_units)\n)\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\nself._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\nreturn\nstart = np.array(start).astype(np.float64)\nend = np.array(end).astype(np.float64)\nstart = TsIndex.format_timestamps(np.array(start).ravel(), time_units)\nend = TsIndex.format_timestamps(np.array(end).ravel(), time_units)\nif len(start) != len(end):\nraise RuntimeError(\"Starts end ends are not of the same length\")\nif not (np.diff(start) &gt; 0).all():\nwarnings.warn(\"start is not sorted.\", stacklevel=2)\nstart = np.sort(start)\nif not (np.diff(end) &gt; 0).all():\nwarnings.warn(\"end is not sorted.\", stacklevel=2)\nend = np.sort(end)\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\n# self._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\ndef __repr__(self):\nreturn self.as_units(\"s\").__repr__()\ndef __str__(self):\nreturn self.__repr__()\ndef time_span(self):\n\"\"\"\n        Time span of the interval set.\n        Returns\n        -------\n        out: IntervalSet\n            an IntervalSet with a single interval encompassing the whole IntervalSet\n        \"\"\"\ns = self[\"start\"][0]\ne = self[\"end\"].iloc[-1]\nreturn IntervalSet(s, e)\ndef tot_length(self, time_units=\"s\"):\n\"\"\"\n        Total elapsed time in the set.\n        Parameters\n        ----------\n        time_units : None, optional\n            The time units to return the result in ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: float\n            _\n        \"\"\"\ntot_l = (self[\"end\"] - self[\"start\"]).sum()\nreturn TsIndex.return_timestamps(np.array([tot_l]), time_units)[0]\ndef intersect(self, a):\n\"\"\"\n        set intersection of IntervalSet\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to intersect self with\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitintersect(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\ndef union(self, a):\n\"\"\"\n        set union of IntervalSet\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to union self with\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitunion(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\ndef set_diff(self, a):\n\"\"\"\n        set difference of IntervalSet\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to set-substract from self\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitdiff(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\ndef in_interval(self, tsd):\n\"\"\"\n        finds out in which element of the interval set each point in a time series fits.\n        NaNs for those that don't fit an interval\n        Parameters\n        ----------\n        tsd : Tsd\n            The tsd to be binned\n        Returns\n        -------\n        out: numpy.ndarray\n            an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet\n        \"\"\"\ntimes = tsd.index\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nreturn jitin_interval(times, starts, ends)\ndef drop_short_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Drops the short intervals in the interval set.\n        Parameters\n        ----------\n        threshold : numeric\n            Time threshold for \"short\" intervals\n        time_units : None, optional\n            The time units for the treshold ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: IntervalSet\n            A copied IntervalSet with the dropped intervals\n        \"\"\"\nthreshold = TsIndex.format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &gt; threshold].reset_index(\ndrop=True\n)\ndef drop_long_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Drops the long intervals in the interval set.\n        Parameters\n        ----------\n        threshold : numeric\n            Time threshold for \"long\" intervals\n        time_units : None, optional\n            The time units for the treshold ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: IntervalSet\n            A copied IntervalSet with the dropped intervals\n        \"\"\"\nthreshold = TsIndex.format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &lt; threshold].reset_index(\ndrop=True\n)\ndef as_units(self, units=\"s\"):\n\"\"\"\n        returns a DataFrame with time expressed in the desired unit\n        Parameters\n        ----------\n        units : None, optional\n            'us', 'ms', or 's' [default]\n        Returns\n        -------\n        out: pandas.DataFrame\n            DataFrame with adjusted times\n        \"\"\"\ndata = self.values.copy()\ndata = TsIndex.return_timestamps(data, units)\nif units == \"us\":\ndata = data.astype(np.int64)\ndf = pd.DataFrame(index=self.index.values, data=data, columns=self.columns)\nreturn df\ndef merge_close_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Merges intervals that are very close.\n        Parameters\n        ----------\n        threshold : numeric\n            time threshold for the closeness of the intervals\n        time_units : None, optional\n            time units for the threshold ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: IntervalSet\n            a copied IntervalSet with merged intervals\n        \"\"\"\nif len(self) == 0:\nreturn IntervalSet(start=[], end=[])\nthreshold = TsIndex.format_timestamps(\nnp.array((threshold,), dtype=np.float64).ravel(), time_units\n)[0]\nstart = self[\"start\"].values\nend = self[\"end\"].values\ntojoin = (start[1:] - end[0:-1]) &gt; threshold\nstart = np.hstack((start[0], start[1:][tojoin]))\nend = np.hstack((end[0:-1][tojoin], end[-1]))\nreturn IntervalSet(start=start, end=end)\ndef get_intervals_center(self, alpha=0.5):\n\"\"\"\n        Returns by default the centers of each intervals.\n        It is possible to bias the midpoint by changing the alpha parameter between [0, 1]\n        For each epoch:\n        t = start + (end-start)*alpha\n        Parameters\n        ----------\n        alpha : float, optional\n            The midpoint within each interval.\n        Returns\n        -------\n        Ts\n            Timestamps object\n        \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nif not isinstance(alpha, float):\nraise RuntimeError(\"Parameter alpha should be float type\")\nalpha = np.clip(alpha, 0, 1)\nt = starts + (ends - starts) * alpha\nreturn time_series.Ts(t=t, time_support=self)\ndef save(self, filename):\n\"\"\"\n        Save IntervalSet object in npz format. The file will contain the starts and ends.\n        The main purpose of this function is to save small/medium sized IntervalSet\n        objects. For example, you determined some epochs for one session that you want to save\n        to avoid recomputing them.\n        You can load the object with numpy.load. Keys are 'start', 'end' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n        &gt;&gt;&gt; ep.save(\"my_ep.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['start', 'end', 'type']\n        &gt;&gt;&gt; print(file['start'])\n        [0. 10. 20.]\n        It is then easy to recreate the IntervalSet object.\n        &gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n           start   end\n        0    0.0   5.0\n        1   10.0  12.0\n        2   20.0  33.0\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nstart=self.start.values,\nend=self.end.values,\ntype=np.array([\"IntervalSet\"], dtype=np.str_),\n)\nreturn\n@property\ndef _constructor(self):\nreturn IntervalSet\n@property\ndef starts(self):\n\"\"\"Return the starts of the IntervalSet as a Ts object\n        Returns\n        -------\n        Ts\n            The starts of the IntervalSet\n        \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nreturn time_series.Ts(t=self.values[:, 0], time_support=self)\n@property\ndef ends(self):\n\"\"\"Return the ends of the IntervalSet as a Ts object\n        Returns\n        -------\n        Ts\n            The ends of the IntervalSet\n        \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nreturn time_series.Ts(t=self.values[:, 1], time_support=self)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.starts","title":"starts  <code>property</code>","text":"<pre><code>starts\n</code></pre> <p>Return the starts of the IntervalSet as a Ts object</p> <p>Returns:</p> Type Description <code>Ts</code> <p>The starts of the IntervalSet</p>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.ends","title":"ends  <code>property</code>","text":"<pre><code>ends\n</code></pre> <p>Return the ends of the IntervalSet as a Ts object</p> <p>Returns:</p> Type Description <code>Ts</code> <p>The ends of the IntervalSet</p>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.__init__","title":"__init__","text":"<pre><code>__init__(start, end=None, time_units='s', **kwargs)\n</code></pre> <p>IntervalSet initializer</p> <p>If start and end and not aligned, meaning that </p> <ol> <li>len(start) != len(end)</li> <li>end[i] &gt; start[i]</li> <li>start[i+1] &gt; end[i]</li> <li>start and end are not sorted,</li> </ol> <p>IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>ndarray or number or DataFrame</code> <p>Beginning of intervals</p> required <code>end</code> <code>ndarray or number</code> <p>Ends of intervals</p> <code>None</code> <code>time_units</code> <code>str</code> <p>Time unit of the intervals ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>**kwargs</code> <p>Additional parameters passed ot pandas.DataFrame</p> <code>{}</code> <p>Returns:</p> Type Description <code>IntervalSet</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Description</p> <code>ValueError</code> <p>If a pandas.DataFrame is passed, it should contains a column 'start' and a column 'end'.</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def __init__(self, start, end=None, time_units=\"s\", **kwargs):\n\"\"\"\n    IntervalSet initializer\n    If start and end and not aligned, meaning that \\n\n    1. len(start) != len(end)\n    2. end[i] &gt; start[i]\n    3. start[i+1] &gt; end[i]\n    4. start and end are not sorted,\n    IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point\n    Parameters\n    ----------\n    start : numpy.ndarray or number or pandas.DataFrame\n        Beginning of intervals\n    end : numpy.ndarray or number, optional\n        Ends of intervals\n    time_units : str, optional\n        Time unit of the intervals ('us', 'ms', 's' [default])\n    **kwargs\n        Additional parameters passed ot pandas.DataFrame\n    Returns\n    -------\n    IntervalSet\n        _\n    Raises\n    ------\n    RuntimeError\n        Description\n    ValueError\n        If a pandas.DataFrame is passed, it should contains\n        a column 'start' and a column 'end'.\n    \"\"\"\nif end is None:\ndf = pd.DataFrame(start)\nif \"start\" not in df.columns or \"end\" not in df.columns:\nraise ValueError(\"wrong columns name\")\nstart = df[\"start\"].values.astype(np.float64)\nend = df[\"end\"].values.astype(np.float64)\nstart = TsIndex.sort_timestamps(\nTsIndex.format_timestamps(start.ravel(), time_units)\n)\nend = TsIndex.sort_timestamps(\nTsIndex.format_timestamps(end.ravel(), time_units)\n)\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\nself._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\nreturn\nstart = np.array(start).astype(np.float64)\nend = np.array(end).astype(np.float64)\nstart = TsIndex.format_timestamps(np.array(start).ravel(), time_units)\nend = TsIndex.format_timestamps(np.array(end).ravel(), time_units)\nif len(start) != len(end):\nraise RuntimeError(\"Starts end ends are not of the same length\")\nif not (np.diff(start) &gt; 0).all():\nwarnings.warn(\"start is not sorted.\", stacklevel=2)\nstart = np.sort(start)\nif not (np.diff(end) &gt; 0).all():\nwarnings.warn(\"end is not sorted.\", stacklevel=2)\nend = np.sort(end)\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\n# self._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.time_span","title":"time_span","text":"<pre><code>time_span()\n</code></pre> <p>Time span of the interval set.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>an IntervalSet with a single interval encompassing the whole IntervalSet</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def time_span(self):\n\"\"\"\n    Time span of the interval set.\n    Returns\n    -------\n    out: IntervalSet\n        an IntervalSet with a single interval encompassing the whole IntervalSet\n    \"\"\"\ns = self[\"start\"][0]\ne = self[\"end\"].iloc[-1]\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.tot_length","title":"tot_length","text":"<pre><code>tot_length(time_units='s')\n</code></pre> <p>Total elapsed time in the set.</p> <p>Parameters:</p> Name Type Description Default <code>time_units</code> <code>None</code> <p>The time units to return the result in ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def tot_length(self, time_units=\"s\"):\n\"\"\"\n    Total elapsed time in the set.\n    Parameters\n    ----------\n    time_units : None, optional\n        The time units to return the result in ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: float\n        _\n    \"\"\"\ntot_l = (self[\"end\"] - self[\"start\"]).sum()\nreturn TsIndex.return_timestamps(np.array([tot_l]), time_units)[0]\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.intersect","title":"intersect","text":"<pre><code>intersect(a)\n</code></pre> <p>set intersection of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to intersect self with</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def intersect(self, a):\n\"\"\"\n    set intersection of IntervalSet\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to intersect self with\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitintersect(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.union","title":"union","text":"<pre><code>union(a)\n</code></pre> <p>set union of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to union self with</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def union(self, a):\n\"\"\"\n    set union of IntervalSet\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to union self with\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitunion(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.set_diff","title":"set_diff","text":"<pre><code>set_diff(a)\n</code></pre> <p>set difference of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to set-substract from self</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def set_diff(self, a):\n\"\"\"\n    set difference of IntervalSet\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to set-substract from self\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitdiff(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.in_interval","title":"in_interval","text":"<pre><code>in_interval(tsd)\n</code></pre> <p>finds out in which element of the interval set each point in a time series fits.</p> <p>NaNs for those that don't fit an interval</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>Tsd</code> <p>The tsd to be binned</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def in_interval(self, tsd):\n\"\"\"\n    finds out in which element of the interval set each point in a time series fits.\n    NaNs for those that don't fit an interval\n    Parameters\n    ----------\n    tsd : Tsd\n        The tsd to be binned\n    Returns\n    -------\n    out: numpy.ndarray\n        an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet\n    \"\"\"\ntimes = tsd.index\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nreturn jitin_interval(times, starts, ends)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.drop_short_intervals","title":"drop_short_intervals","text":"<pre><code>drop_short_intervals(threshold, time_units='s')\n</code></pre> <p>Drops the short intervals in the interval set.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>Time threshold for \"short\" intervals</p> required <code>time_units</code> <code>None</code> <p>The time units for the treshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>A copied IntervalSet with the dropped intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def drop_short_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Drops the short intervals in the interval set.\n    Parameters\n    ----------\n    threshold : numeric\n        Time threshold for \"short\" intervals\n    time_units : None, optional\n        The time units for the treshold ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: IntervalSet\n        A copied IntervalSet with the dropped intervals\n    \"\"\"\nthreshold = TsIndex.format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &gt; threshold].reset_index(\ndrop=True\n)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.drop_long_intervals","title":"drop_long_intervals","text":"<pre><code>drop_long_intervals(threshold, time_units='s')\n</code></pre> <p>Drops the long intervals in the interval set.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>Time threshold for \"long\" intervals</p> required <code>time_units</code> <code>None</code> <p>The time units for the treshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>A copied IntervalSet with the dropped intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def drop_long_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Drops the long intervals in the interval set.\n    Parameters\n    ----------\n    threshold : numeric\n        Time threshold for \"long\" intervals\n    time_units : None, optional\n        The time units for the treshold ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: IntervalSet\n        A copied IntervalSet with the dropped intervals\n    \"\"\"\nthreshold = TsIndex.format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &lt; threshold].reset_index(\ndrop=True\n)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.as_units","title":"as_units","text":"<pre><code>as_units(units='s')\n</code></pre> <p>returns a DataFrame with time expressed in the desired unit</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>None</code> <p>'us', 'ms', or 's' [default]</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>DataFrame</code> <p>DataFrame with adjusted times</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    returns a DataFrame with time expressed in the desired unit\n    Parameters\n    ----------\n    units : None, optional\n        'us', 'ms', or 's' [default]\n    Returns\n    -------\n    out: pandas.DataFrame\n        DataFrame with adjusted times\n    \"\"\"\ndata = self.values.copy()\ndata = TsIndex.return_timestamps(data, units)\nif units == \"us\":\ndata = data.astype(np.int64)\ndf = pd.DataFrame(index=self.index.values, data=data, columns=self.columns)\nreturn df\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.merge_close_intervals","title":"merge_close_intervals","text":"<pre><code>merge_close_intervals(threshold, time_units='s')\n</code></pre> <p>Merges intervals that are very close.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>time threshold for the closeness of the intervals</p> required <code>time_units</code> <code>None</code> <p>time units for the threshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>a copied IntervalSet with merged intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def merge_close_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Merges intervals that are very close.\n    Parameters\n    ----------\n    threshold : numeric\n        time threshold for the closeness of the intervals\n    time_units : None, optional\n        time units for the threshold ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: IntervalSet\n        a copied IntervalSet with merged intervals\n    \"\"\"\nif len(self) == 0:\nreturn IntervalSet(start=[], end=[])\nthreshold = TsIndex.format_timestamps(\nnp.array((threshold,), dtype=np.float64).ravel(), time_units\n)[0]\nstart = self[\"start\"].values\nend = self[\"end\"].values\ntojoin = (start[1:] - end[0:-1]) &gt; threshold\nstart = np.hstack((start[0], start[1:][tojoin]))\nend = np.hstack((end[0:-1][tojoin], end[-1]))\nreturn IntervalSet(start=start, end=end)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.get_intervals_center","title":"get_intervals_center","text":"<pre><code>get_intervals_center(alpha=0.5)\n</code></pre> <p>Returns by default the centers of each intervals.</p> <p>It is possible to bias the midpoint by changing the alpha parameter between [0, 1] For each epoch: t = start + (end-start)*alpha</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>The midpoint within each interval.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Ts</code> <p>Timestamps object</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def get_intervals_center(self, alpha=0.5):\n\"\"\"\n    Returns by default the centers of each intervals.\n    It is possible to bias the midpoint by changing the alpha parameter between [0, 1]\n    For each epoch:\n    t = start + (end-start)*alpha\n    Parameters\n    ----------\n    alpha : float, optional\n        The midpoint within each interval.\n    Returns\n    -------\n    Ts\n        Timestamps object\n    \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nif not isinstance(alpha, float):\nraise RuntimeError(\"Parameter alpha should be float type\")\nalpha = np.clip(alpha, 0, 1)\nt = starts + (ends - starts) * alpha\nreturn time_series.Ts(t=t, time_support=self)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.save","title":"save","text":"<pre><code>save(filename)\n</code></pre> <p>Save IntervalSet object in npz format. The file will contain the starts and ends.</p> <p>The main purpose of this function is to save small/medium sized IntervalSet objects. For example, you determined some epochs for one session that you want to save to avoid recomputing them.</p> <p>You can load the object with numpy.load. Keys are 'start', 'end' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n&gt;&gt;&gt; ep.save(\"my_ep.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['start', 'end', 'type']\n&gt;&gt;&gt; print(file['start'])\n[0. 10. 20.]\n</code></pre> <p>It is then easy to recreate the IntervalSet object.</p> <pre><code>&gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n   start   end\n0    0.0   5.0\n1   10.0  12.0\n2   20.0  33.0\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save IntervalSet object in npz format. The file will contain the starts and ends.\n    The main purpose of this function is to save small/medium sized IntervalSet\n    objects. For example, you determined some epochs for one session that you want to save\n    to avoid recomputing them.\n    You can load the object with numpy.load. Keys are 'start', 'end' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n    &gt;&gt;&gt; ep.save(\"my_ep.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['start', 'end', 'type']\n    &gt;&gt;&gt; print(file['start'])\n    [0. 10. 20.]\n    It is then easy to recreate the IntervalSet object.\n    &gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n       start   end\n    0    0.0   5.0\n    1   10.0  12.0\n    2   20.0  33.0\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nstart=self.start.values,\nend=self.end.values,\ntype=np.array([\"IntervalSet\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.jitfix_iset","title":"jitfix_iset","text":"<pre><code>jitfix_iset(start, end)\n</code></pre> <p>0 - &gt; \"Some starts and ends are equal. Removing 1 microsecond!\", 1 - &gt; \"Some ends precede the relative start. Dropping them!\", 2 - &gt; \"Some starts precede the previous end. Joining them!\", 3 - &gt; \"Some epochs have no duration\"</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>ndarray</code> <p>Description</p> required <code>end</code> <code>ndarray</code> <p>Description</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>@jit(nopython=True)\ndef jitfix_iset(start, end):\n\"\"\"\n    0 - &gt; \"Some starts and ends are equal. Removing 1 microsecond!\",\n    1 - &gt; \"Some ends precede the relative start. Dropping them!\",\n    2 - &gt; \"Some starts precede the previous end. Joining them!\",\n    3 - &gt; \"Some epochs have no duration\"\n    Parameters\n    ----------\n    start : numpy.ndarray\n        Description\n    end : numpy.ndarray\n        Description\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nto_warn = np.zeros(4, dtype=np.bool_)\nm = start.shape[0]\ndata = np.zeros((m, 2), dtype=np.float64)\ni = 0\nct = 0\nwhile i &lt; m:\nnewstart = start[i]\nnewend = end[i]\nwhile i &lt; m:\nif end[i] == start[i]:\nto_warn[3] = True\ni += 1\nelse:\nnewstart = start[i]\nnewend = end[i]\nbreak\nwhile i &lt; m:\nif end[i] &lt; start[i]:\nto_warn[1] = True\ni += 1\nelse:\nnewstart = start[i]\nnewend = end[i]\nbreak\nwhile i &lt; m - 1:\nif start[i + 1] &lt; end[i]:\nto_warn[2] = True\ni += 1\nnewend = max(end[i - 1], end[i])\nelse:\nbreak\nif i &lt; m - 1:\nif newend == start[i + 1]:\nto_warn[0] = True\nnewend -= 1.0e-6\ndata[ct, 0] = newstart\ndata[ct, 1] = newend\nct += 1\ni += 1\ndata = data[0:ct]\nreturn (data, to_warn)\n</code></pre>"},{"location":"old_pages/core.time_series/","title":"Core.time series","text":""},{"location":"old_pages/core.time_series/#pynapple.core.time_series","title":"pynapple.core.time_series","text":""},{"location":"old_pages/core.time_series/#pynapple.core.time_series--pynapple-time-series","title":"Pynapple time series","text":"<p>Pynapple time series are containers specialized for neurophysiological time series.</p> <p>They provides standardized time representation, plus various functions for manipulating times series with identical sampling frequency.</p> <p>Multiple time series object are avaible depending on the shape of the data.</p> <ul> <li><code>TsdTensor</code> : for data with of more than 2 dimensions, typically movies.</li> <li><code>TsdFrame</code> : for column-based data. It can be easily converted to a pandas.DataFrame. Columns can be labelled and selected similar to pandas.</li> <li><code>Tsd</code> : One-dimensional time series. It can be converted to a pandas.Series.</li> <li><code>Ts</code> : For timestamps data only.</li> </ul> <p>Most of the same functions are available through all classes. Objects behaves like numpy.ndarray. Slicing can be done the same way for example  <code>tsd[0:10]</code> returns the first 10 rows. Similarly, you can call any numpy functions like <code>np.mean(tsd, 1)</code>.</p>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor","title":"TsdTensor","text":"<p>             Bases: <code>NDArrayOperatorsMixin</code>, <code>_AbstractTsd</code></p> <p>TsdTensor</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class TsdTensor(NDArrayOperatorsMixin, _AbstractTsd):\n\"\"\"\n    TsdTensor\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, d, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n        TsdTensor initializer\n        Parameters\n        ----------\n        t : numpy.ndarray\n            the time index t\n        d : numpy.ndarray\n            The data\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default]).\n        time_support : IntervalSet, optional\n            The time support of the TsdFrame object\n        \"\"\"\nif isinstance(t, np.ndarray) and d is None:\nraise RuntimeError(\"Missing argument d when initializing TsdTensor\")\nif isinstance(t, (list, tuple)):\nt = np.array(t)\nif isinstance(d, (list, tuple)):\nd = np.array(d)\nassert (\nd.ndim &gt;= 3\n), \"Data should have more than 2 dimensions. If ndim &lt; 3, use TsdFrame or Tsd object\"\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index) != len(d):\nraise ValueError(\n\"Length of values \"\nf\"({len(d)}) \"\n\"does not match length of index \"\nf\"({len(self.index)})\"\n)\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt, d = jitrestrict(self.index.values, d, starts, ends)\nself.index = TsIndex(t)\nself.values = d\nelse:\ntime_support = IntervalSet(start=self.index[0], end=self.index[-1])\nself.values = d\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.values = np.empty(0)\nself.time_support = IntervalSet(start=[], end=[])\nself.nap_class = self.__class__.__name__\nself.dtype = self.values.dtype\ndef __repr__(self):\nheaders = [\"Time (s)\", \"\"]\nbottom = \"dtype: {}\".format(self.dtype) + \", shape: {}\".format(self.shape)\nif len(self):\ndef create_str(array):\nif array.ndim == 1:\nreturn (\n\"[\" + array[0].__repr__() + \" ... \" + array[0].__repr__() + \"]\"\n)\nelse:\nreturn \"[\" + create_str(array[0]) + \" ...]\"\n_str_ = []\nif self.shape[0] &lt; 100:\nfor i, array in zip(self.index, self.values):\n_str_.append([i.__repr__(), create_str(array)])\nelse:\nfor i, array in zip(self.index[0:5], self.values[0:5]):\n_str_.append([i.__repr__(), create_str(array)])\n_str_.append([\"...\", \"\"])\nfor i, array in zip(self.index[-5:], self.values[-5:]):\n_str_.append([i.__repr__(), create_str(array)])\nreturn tabulate(_str_, headers=headers, colalign=(\"left\",)) + \"\\n\" + bottom\nelse:\nreturn tabulate([], headers=headers) + \"\\n\" + bottom\ndef save(self, filename):\n\"\"\"\n        Save TsdTensor object in npz format. The file will contain the timestamps, the\n        data and the time support.\n        The main purpose of this function is to save small/medium sized time series\n        objects. For example, you extracted several channels from your recording and\n        filtered them. You can save the filtered channels as a npz to avoid\n        reprocessing it.\n        You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type'\n        and 'columns' for columns names.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsdtensor = nap.TsdTensor(t=np.array([0., 1.]), d = np.zeros((2,3,4)))\n        &gt;&gt;&gt; tsdtensor.save(\"my_path/my_tsdtensor.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_tsdtensor.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'd', 'start', 'end', ''type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1.]\n        It is then easy to recreate the TsdTensor object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.TsdTensor(t=file['t'], d=file['d'], time_support=time_support)\n        Time (s)\n        0.0       [[[0.0 ...]]]\n        1.0       [[[0.0 ...]]]\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([self.nap_class], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor.times","title":"times","text":"<pre><code>times(units='s')\n</code></pre> <p>The time index of the object, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>the time indexes</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the object, returned as np.double in the desired time units.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.ndarray\n        the time indexes\n    \"\"\"\nreturn self.index.in_units(units)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor.as_array","title":"as_array","text":"<pre><code>as_array()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_array(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor.data","title":"data","text":"<pre><code>data()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy()\n</code></pre> <p>Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling <code>plot(tsd)</code></p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def to_numpy(self):\n\"\"\"\n    Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling `plot(tsd)`\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor.start_time","title":"start_time","text":"<pre><code>start_time(units='s')\n</code></pre> <p>The first time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def start_time(self, units=\"s\"):\n\"\"\"\n    The first time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[0]\nelse:\nreturn None\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor.end_time","title":"end_time","text":"<pre><code>end_time(units='s')\n</code></pre> <p>The last time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def end_time(self, units=\"s\"):\n\"\"\"\n    The last time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[-1]\nelse:\nreturn None\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor.value_from","title":"value_from","text":"<pre><code>value_from(data, ep=None)\n</code></pre> <p>Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>The object holding the values to replace.</p> required <code>ep</code> <code>IntervalSet(optional)</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>Object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, data, ep=None):\n\"\"\"\n    Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument\n    Parameters\n    ----------\n    data : Tsd/TsdFrame/TsdTensor\n        The object holding the values to replace.\n    ep : IntervalSet (optional)\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    out : Tsd/TsdFrame/TsdTensor\n        Object with the new values\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n    newts is the same size as ts restrict to ep.\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\nif not isinstance(data, (TsdTensor, TsdFrame, Tsd)):\nraise RuntimeError(\n\"The time series to align to should be Tsd/TsdFrame/TsdTensor.\"\n)\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nif data_target_array.ndim == 1:\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\nelse:\nt, d, ns, ne = jitvaluefromtensor(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nif isinstance(data, TsdFrame):\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nreturn data.__class__(t, d, time_support=time_support)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor.count","title":"count","text":"<pre><code>count(*args, **kwargs)\n</code></pre> <p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsd.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsd.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsd.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsd.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; bincount = ts.count(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n&gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n</code></pre> <p>And bincount automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bincount.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  100.0  800.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsd.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsd.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsd.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsd.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; bincount = ts.count(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n    &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n    And bincount automatically inherit ep as time support:\n    &gt;&gt;&gt; bincount.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  100.0  800.0\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\ntime_array = self.index.values\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\nt, d = jitcount(time_array, starts, ends, bin_size)\nelse:\n_, d = jittsrestrict_with_count(time_array, starts, ends)\nt = starts + (ends - starts) / 2\nreturn Tsd(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor.restrict","title":"restrict","text":"<pre><code>restrict(iset)\n</code></pre> <p>Restricts a time series object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>(Ts, Tsd, TsdFrame or TsdTensor)</code> <p>Tsd object restricted to ep</p> <p>Examples:</p> <p>The Ts object is restrict to the intervals defined by ep.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n&gt;&gt;&gt; newts = ts.restrict(ep)\n</code></pre> <p>The time support of newts automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newts.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0    0.0  500.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, iset):\n\"\"\"\n    Restricts a time series object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    iset : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    out: Ts, Tsd, TsdFrame or TsdTensor\n        Tsd object restricted to ep\n    Examples\n    --------\n    The Ts object is restrict to the intervals defined by ep.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n    &gt;&gt;&gt; newts = ts.restrict(ep)\n    The time support of newts automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newts.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0    0.0  500.0\n    \"\"\"\nassert isinstance(iset, IntervalSet), \"Argument should be IntervalSet\"\ntime_array = self.index.values\nstarts = iset.start.values\nends = iset.end.values\nif isinstance(self.values, np.ndarray):\ndata_array = self.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=iset, columns=self.columns)\nelse:\nreturn self.__class__(t=t, d=d, time_support=iset)\nelse:\nt = jittsrestrict(time_array, starts, ends)\nreturn Ts(t, time_support=iset)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor.bin_average","title":"bin_average","text":"<pre><code>bin_average(bin_size, ep=None, time_units='s')\n</code></pre> <p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>(Tsd, TsdFrame, TsdTensor)</code> <p>A Tsd object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsd automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsd.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd, TsdFrame, TsdTensor\n        A Tsd object indexed by the center of the bins and holding the averaged data points.\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n    And bintsd automatically inherit ep as time support:\n    &gt;&gt;&gt; bintsd.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nif data_array.ndim &gt; 1:\nt, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\nelse:\nt, d = jitbin(time_array, data_array, starts, ends, bin_size)\nif d.ndim == 1:\nreturn Tsd(t=t, d=d, time_support=ep)\nelif d.ndim == 2:\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=ep, columns=self.columns)\nelse:\nreturn TsdFrame(t=t, d=d, time_support=ep)\nelse:\nreturn TsdTensor(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy the data, index and time support</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def copy(self):\n\"\"\"Copy the data, index and time support\"\"\"\nreturn self.__class__(\nt=self.index.copy(), d=self.values.copy(), time_support=self.time_support\n)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor.__init__","title":"__init__","text":"<pre><code>__init__(t, d, time_units='s', time_support=None, **kwargs)\n</code></pre> <p>TsdTensor initializer</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray</code> <p>the time index t</p> required <code>d</code> <code>ndarray</code> <p>The data</p> required <code>time_units</code> <code>str</code> <p>The time units in which times are specified ('us', 'ms', 's' [default]).</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the TsdFrame object</p> <code>None</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, d, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n    TsdTensor initializer\n    Parameters\n    ----------\n    t : numpy.ndarray\n        the time index t\n    d : numpy.ndarray\n        The data\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default]).\n    time_support : IntervalSet, optional\n        The time support of the TsdFrame object\n    \"\"\"\nif isinstance(t, np.ndarray) and d is None:\nraise RuntimeError(\"Missing argument d when initializing TsdTensor\")\nif isinstance(t, (list, tuple)):\nt = np.array(t)\nif isinstance(d, (list, tuple)):\nd = np.array(d)\nassert (\nd.ndim &gt;= 3\n), \"Data should have more than 2 dimensions. If ndim &lt; 3, use TsdFrame or Tsd object\"\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index) != len(d):\nraise ValueError(\n\"Length of values \"\nf\"({len(d)}) \"\n\"does not match length of index \"\nf\"({len(self.index)})\"\n)\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt, d = jitrestrict(self.index.values, d, starts, ends)\nself.index = TsIndex(t)\nself.values = d\nelse:\ntime_support = IntervalSet(start=self.index[0], end=self.index[-1])\nself.values = d\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.values = np.empty(0)\nself.time_support = IntervalSet(start=[], end=[])\nself.nap_class = self.__class__.__name__\nself.dtype = self.values.dtype\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdTensor.save","title":"save","text":"<pre><code>save(filename)\n</code></pre> <p>Save TsdTensor object in npz format. The file will contain the timestamps, the data and the time support.</p> <p>The main purpose of this function is to save small/medium sized time series objects. For example, you extracted several channels from your recording and filtered them. You can save the filtered channels as a npz to avoid reprocessing it.</p> <p>You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type' and 'columns' for columns names.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsdtensor = nap.TsdTensor(t=np.array([0., 1.]), d = np.zeros((2,3,4)))\n&gt;&gt;&gt; tsdtensor.save(\"my_path/my_tsdtensor.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_tsdtensor.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'd', 'start', 'end', ''type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1.]\n</code></pre> <p>It is then easy to recreate the TsdTensor object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.TsdTensor(t=file['t'], d=file['d'], time_support=time_support)\nTime (s)\n0.0       [[[0.0 ...]]]\n1.0       [[[0.0 ...]]]\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save TsdTensor object in npz format. The file will contain the timestamps, the\n    data and the time support.\n    The main purpose of this function is to save small/medium sized time series\n    objects. For example, you extracted several channels from your recording and\n    filtered them. You can save the filtered channels as a npz to avoid\n    reprocessing it.\n    You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type'\n    and 'columns' for columns names.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsdtensor = nap.TsdTensor(t=np.array([0., 1.]), d = np.zeros((2,3,4)))\n    &gt;&gt;&gt; tsdtensor.save(\"my_path/my_tsdtensor.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_tsdtensor.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'd', 'start', 'end', ''type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1.]\n    It is then easy to recreate the TsdTensor object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.TsdTensor(t=file['t'], d=file['d'], time_support=time_support)\n    Time (s)\n    0.0       [[[0.0 ...]]]\n    1.0       [[[0.0 ...]]]\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([self.nap_class], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame","title":"TsdFrame","text":"<p>             Bases: <code>NDArrayOperatorsMixin</code>, <code>_AbstractTsd</code></p> <p>TsdFrame</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class TsdFrame(NDArrayOperatorsMixin, _AbstractTsd):\n\"\"\"\n    TsdFrame\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, d=None, time_units=\"s\", time_support=None, columns=None):\n\"\"\"\n        TsdFrame initializer\n        A pandas.DataFrame can be passed directly\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.DataFrame\n            the time index t,  or a pandas.DataFrame (if d is None)\n        d : numpy.ndarray\n            The data\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default]).\n        time_support : IntervalSet, optional\n            The time support of the TsdFrame object\n        columns : iterables\n            Column names\n        \"\"\"\nif isinstance(t, np.ndarray) and d is None:\nraise RuntimeError(\"Missing argument d when initializing TsdFrame\")\nc = columns\nif isinstance(t, pd.DataFrame):\nd = t.values\nc = t.columns.values\nt = t.index.values\nif isinstance(t, (list, tuple)):\nt = np.array(t)\nif isinstance(d, (list, tuple)):\nd = np.array(d)\nassert d.ndim &lt;= 2, \"Data should be 1 or 2 dimensional\"\nif d.ndim == 1:\nd = d[:, np.newaxis]\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index) != len(d):\nraise ValueError(\n\"Length of values \"\nf\"({len(d)}) \"\n\"does not match length of index \"\nf\"({len(self.index)})\"\n)\nif c is None or len(c) != d.shape[1]:\nc = np.arange(d.shape[1], dtype=\"int\")\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt, d = jitrestrict(self.index.values, d, starts, ends)\nself.index = TsIndex(t)\nself.values = d\nelse:\ntime_support = IntervalSet(start=self.index[0], end=self.index[-1])\nself.values = d\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.values = np.empty(0)\nself.time_support = IntervalSet(start=[], end=[])\nself.columns = pd.Index(c)\nself.nap_class = self.__class__.__name__\nself.dtype = self.values.dtype\n@property\ndef loc(self):\nreturn _TsdFrameSliceHelper(self)\ndef __repr__(self):\nheaders = [\"Time (s)\"] + [str(k) for k in self.columns]\nbottom = \"dtype: {}\".format(self.dtype) + \", shape: {}\".format(self.shape)\nmax_cols = 5\ntry:\nmax_cols = os.get_terminal_size()[0] // 16\nexcept Exception:\nimport shutil\nmax_cols = shutil.get_terminal_size().columns // 16\nelse:\npass\nif self.shape[1] &gt; max_cols:\nheaders = headers[0 : max_cols + 1] + [\"...\"]\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nif len(self):\ntable = []\nend = [\"...\"] if self.shape[1] &gt; max_cols else []\nif len(self) &gt; 51:\nfor i, array in zip(self.index[0:5], self.values[0:5, 0:max_cols]):\ntable.append([i] + [k for k in array] + end)\ntable.append([\"...\"])\nfor i, array in zip(self.index[-5:], self.values[-5:, 0:max_cols]):\ntable.append([i] + [k for k in array] + end)\nreturn tabulate(table, headers=headers) + \"\\n\" + bottom\nelse:\nfor i, array in zip(self.index, self.values[:, 0:max_cols]):\ntable.append([i] + [k for k in array] + end)\nreturn tabulate(table, headers=headers) + \"\\n\" + bottom\nelse:\nreturn tabulate([], headers=headers) + \"\\n\" + bottom\ndef __getitem__(self, key, *args, **kwargs):\nif (\nisinstance(key, str)\nor hasattr(key, \"__iter__\")\nand all([isinstance(k, str) for k in key])\n):\nreturn self.loc[key]\nelse:\nreturn super().__getitem__(key, *args, **kwargs)\ndef __setitem__(self, key, value):\ntry:\nif isinstance(key, str):\nnew_key = self.columns.get_indexer([key])\nself.values.__setitem__((slice(None, None, None), new_key[0]), value)\nelif hasattr(key, \"__iter__\") and all([isinstance(k, str) for k in key]):\nnew_key = self.columns.get_indexer(key)\nself.values.__setitem__((slice(None, None, None), new_key), value)\nelse:\nself.values.__setitem__(key, value)\nexcept IndexError:\nraise IndexError\ndef as_dataframe(self):\n\"\"\"\n        Convert the TsdFrame object to a pandas.DataFrame object.\n        Returns\n        -------\n        out: pandas.DataFrame\n            _\n        \"\"\"\nreturn pd.DataFrame(\nindex=self.index.values, data=self.values, columns=self.columns\n)\ndef as_units(self, units=\"s\"):\n\"\"\"\n        Returns a DataFrame with time expressed in the desired unit.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        pandas.DataFrame\n            the series object with adjusted times\n        \"\"\"\nt = self.index.in_units(units)\nif units == \"us\":\nt = t.astype(np.int64)\ndf = pd.DataFrame(index=t, data=self.values)\ndf.index.name = \"Time (\" + str(units) + \")\"\ndf.columns = self.columns.copy()\nreturn df\ndef save(self, filename):\n\"\"\"\n        Save TsdFrame object in npz format. The file will contain the timestamps, the\n        data and the time support.\n        The main purpose of this function is to save small/medium sized time series\n        objects. For example, you extracted several channels from your recording and\n        filtered them. You can save the filtered channels as a npz to avoid\n        reprocessing it.\n        You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type'\n        and 'columns' for columns names.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n        &gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'd', 'start', 'end', 'columns', 'type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1.]\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n                  a  b\n        Time (s)\n        0.0       2  3\n        1.0       4  5\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ncols_name = self.columns\nif cols_name.dtype == np.dtype(\"O\"):\ncols_name = cols_name.astype(str)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ncolumns=cols_name,\ntype=np.array([\"TsdFrame\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.times","title":"times","text":"<pre><code>times(units='s')\n</code></pre> <p>The time index of the object, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>the time indexes</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the object, returned as np.double in the desired time units.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.ndarray\n        the time indexes\n    \"\"\"\nreturn self.index.in_units(units)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.as_array","title":"as_array","text":"<pre><code>as_array()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_array(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.data","title":"data","text":"<pre><code>data()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy()\n</code></pre> <p>Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling <code>plot(tsd)</code></p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def to_numpy(self):\n\"\"\"\n    Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling `plot(tsd)`\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.start_time","title":"start_time","text":"<pre><code>start_time(units='s')\n</code></pre> <p>The first time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def start_time(self, units=\"s\"):\n\"\"\"\n    The first time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[0]\nelse:\nreturn None\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.end_time","title":"end_time","text":"<pre><code>end_time(units='s')\n</code></pre> <p>The last time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def end_time(self, units=\"s\"):\n\"\"\"\n    The last time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[-1]\nelse:\nreturn None\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.value_from","title":"value_from","text":"<pre><code>value_from(data, ep=None)\n</code></pre> <p>Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>The object holding the values to replace.</p> required <code>ep</code> <code>IntervalSet(optional)</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>Object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, data, ep=None):\n\"\"\"\n    Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument\n    Parameters\n    ----------\n    data : Tsd/TsdFrame/TsdTensor\n        The object holding the values to replace.\n    ep : IntervalSet (optional)\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    out : Tsd/TsdFrame/TsdTensor\n        Object with the new values\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n    newts is the same size as ts restrict to ep.\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\nif not isinstance(data, (TsdTensor, TsdFrame, Tsd)):\nraise RuntimeError(\n\"The time series to align to should be Tsd/TsdFrame/TsdTensor.\"\n)\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nif data_target_array.ndim == 1:\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\nelse:\nt, d, ns, ne = jitvaluefromtensor(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nif isinstance(data, TsdFrame):\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nreturn data.__class__(t, d, time_support=time_support)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.count","title":"count","text":"<pre><code>count(*args, **kwargs)\n</code></pre> <p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsd.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsd.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsd.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsd.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; bincount = ts.count(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n&gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n</code></pre> <p>And bincount automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bincount.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  100.0  800.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsd.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsd.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsd.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsd.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; bincount = ts.count(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n    &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n    And bincount automatically inherit ep as time support:\n    &gt;&gt;&gt; bincount.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  100.0  800.0\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\ntime_array = self.index.values\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\nt, d = jitcount(time_array, starts, ends, bin_size)\nelse:\n_, d = jittsrestrict_with_count(time_array, starts, ends)\nt = starts + (ends - starts) / 2\nreturn Tsd(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.restrict","title":"restrict","text":"<pre><code>restrict(iset)\n</code></pre> <p>Restricts a time series object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>(Ts, Tsd, TsdFrame or TsdTensor)</code> <p>Tsd object restricted to ep</p> <p>Examples:</p> <p>The Ts object is restrict to the intervals defined by ep.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n&gt;&gt;&gt; newts = ts.restrict(ep)\n</code></pre> <p>The time support of newts automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newts.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0    0.0  500.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, iset):\n\"\"\"\n    Restricts a time series object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    iset : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    out: Ts, Tsd, TsdFrame or TsdTensor\n        Tsd object restricted to ep\n    Examples\n    --------\n    The Ts object is restrict to the intervals defined by ep.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n    &gt;&gt;&gt; newts = ts.restrict(ep)\n    The time support of newts automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newts.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0    0.0  500.0\n    \"\"\"\nassert isinstance(iset, IntervalSet), \"Argument should be IntervalSet\"\ntime_array = self.index.values\nstarts = iset.start.values\nends = iset.end.values\nif isinstance(self.values, np.ndarray):\ndata_array = self.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=iset, columns=self.columns)\nelse:\nreturn self.__class__(t=t, d=d, time_support=iset)\nelse:\nt = jittsrestrict(time_array, starts, ends)\nreturn Ts(t, time_support=iset)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.bin_average","title":"bin_average","text":"<pre><code>bin_average(bin_size, ep=None, time_units='s')\n</code></pre> <p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>(Tsd, TsdFrame, TsdTensor)</code> <p>A Tsd object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsd automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsd.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd, TsdFrame, TsdTensor\n        A Tsd object indexed by the center of the bins and holding the averaged data points.\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n    And bintsd automatically inherit ep as time support:\n    &gt;&gt;&gt; bintsd.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nif data_array.ndim &gt; 1:\nt, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\nelse:\nt, d = jitbin(time_array, data_array, starts, ends, bin_size)\nif d.ndim == 1:\nreturn Tsd(t=t, d=d, time_support=ep)\nelif d.ndim == 2:\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=ep, columns=self.columns)\nelse:\nreturn TsdFrame(t=t, d=d, time_support=ep)\nelse:\nreturn TsdTensor(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy the data, index and time support</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def copy(self):\n\"\"\"Copy the data, index and time support\"\"\"\nreturn self.__class__(\nt=self.index.copy(), d=self.values.copy(), time_support=self.time_support\n)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.__init__","title":"__init__","text":"<pre><code>__init__(\nt,\nd=None,\ntime_units=\"s\",\ntime_support=None,\ncolumns=None,\n)\n</code></pre> <p>TsdFrame initializer A pandas.DataFrame can be passed directly</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray or DataFrame</code> <p>the time index t,  or a pandas.DataFrame (if d is None)</p> required <code>d</code> <code>ndarray</code> <p>The data</p> <code>None</code> <code>time_units</code> <code>str</code> <p>The time units in which times are specified ('us', 'ms', 's' [default]).</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the TsdFrame object</p> <code>None</code> <code>columns</code> <code>iterables</code> <p>Column names</p> <code>None</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, d=None, time_units=\"s\", time_support=None, columns=None):\n\"\"\"\n    TsdFrame initializer\n    A pandas.DataFrame can be passed directly\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.DataFrame\n        the time index t,  or a pandas.DataFrame (if d is None)\n    d : numpy.ndarray\n        The data\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default]).\n    time_support : IntervalSet, optional\n        The time support of the TsdFrame object\n    columns : iterables\n        Column names\n    \"\"\"\nif isinstance(t, np.ndarray) and d is None:\nraise RuntimeError(\"Missing argument d when initializing TsdFrame\")\nc = columns\nif isinstance(t, pd.DataFrame):\nd = t.values\nc = t.columns.values\nt = t.index.values\nif isinstance(t, (list, tuple)):\nt = np.array(t)\nif isinstance(d, (list, tuple)):\nd = np.array(d)\nassert d.ndim &lt;= 2, \"Data should be 1 or 2 dimensional\"\nif d.ndim == 1:\nd = d[:, np.newaxis]\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index) != len(d):\nraise ValueError(\n\"Length of values \"\nf\"({len(d)}) \"\n\"does not match length of index \"\nf\"({len(self.index)})\"\n)\nif c is None or len(c) != d.shape[1]:\nc = np.arange(d.shape[1], dtype=\"int\")\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt, d = jitrestrict(self.index.values, d, starts, ends)\nself.index = TsIndex(t)\nself.values = d\nelse:\ntime_support = IntervalSet(start=self.index[0], end=self.index[-1])\nself.values = d\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.values = np.empty(0)\nself.time_support = IntervalSet(start=[], end=[])\nself.columns = pd.Index(c)\nself.nap_class = self.__class__.__name__\nself.dtype = self.values.dtype\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.as_dataframe","title":"as_dataframe","text":"<pre><code>as_dataframe()\n</code></pre> <p>Convert the TsdFrame object to a pandas.DataFrame object.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_dataframe(self):\n\"\"\"\n    Convert the TsdFrame object to a pandas.DataFrame object.\n    Returns\n    -------\n    out: pandas.DataFrame\n        _\n    \"\"\"\nreturn pd.DataFrame(\nindex=self.index.values, data=self.values, columns=self.columns\n)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.as_units","title":"as_units","text":"<pre><code>as_units(units='s')\n</code></pre> <p>Returns a DataFrame with time expressed in the desired unit.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>the series object with adjusted times</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    Returns a DataFrame with time expressed in the desired unit.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    pandas.DataFrame\n        the series object with adjusted times\n    \"\"\"\nt = self.index.in_units(units)\nif units == \"us\":\nt = t.astype(np.int64)\ndf = pd.DataFrame(index=t, data=self.values)\ndf.index.name = \"Time (\" + str(units) + \")\"\ndf.columns = self.columns.copy()\nreturn df\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.save","title":"save","text":"<pre><code>save(filename)\n</code></pre> <p>Save TsdFrame object in npz format. The file will contain the timestamps, the data and the time support.</p> <p>The main purpose of this function is to save small/medium sized time series objects. For example, you extracted several channels from your recording and filtered them. You can save the filtered channels as a npz to avoid reprocessing it.</p> <p>You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type' and 'columns' for columns names.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n&gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'd', 'start', 'end', 'columns', 'type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1.]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n          a  b\nTime (s)\n0.0       2  3\n1.0       4  5\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save TsdFrame object in npz format. The file will contain the timestamps, the\n    data and the time support.\n    The main purpose of this function is to save small/medium sized time series\n    objects. For example, you extracted several channels from your recording and\n    filtered them. You can save the filtered channels as a npz to avoid\n    reprocessing it.\n    You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type'\n    and 'columns' for columns names.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n    &gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'd', 'start', 'end', 'columns', 'type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1.]\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n              a  b\n    Time (s)\n    0.0       2  3\n    1.0       4  5\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ncols_name = self.columns\nif cols_name.dtype == np.dtype(\"O\"):\ncols_name = cols_name.astype(str)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ncolumns=cols_name,\ntype=np.array([\"TsdFrame\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd","title":"Tsd","text":"<p>             Bases: <code>NDArrayOperatorsMixin</code>, <code>_AbstractTsd</code></p> <p>A container around numpy.ndarray specialized for neurophysiology time series.</p> <p>Tsd provides standardized time representation, plus various functions for manipulating times series.</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class Tsd(NDArrayOperatorsMixin, _AbstractTsd):\n\"\"\"\n    A container around numpy.ndarray specialized for neurophysiology time series.\n    Tsd provides standardized time representation, plus various functions for manipulating times series.\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, d=None, time_units=\"s\", time_support=None):\n\"\"\"\n        Tsd Initializer.\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.Series\n            An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n        d : numpy.ndarray, optional\n            The data of the time series\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default])\n        time_support : IntervalSet, optional\n            The time support of the tsd object\n        \"\"\"\nif isinstance(t, np.ndarray) and d is None:\nraise RuntimeError(\"Missing argument d when initializing Tsd\")\nif isinstance(t, pd.Series):\nd = t.values\nt = t.index.values\nif isinstance(t, (list, tuple)):\nt = np.array(t)\nif isinstance(d, (list, tuple)):\nd = np.array(d)\nassert d.ndim == 1, \"Data should be 1 dimension\"\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index) != len(d):\nraise ValueError(\n\"Length of values \"\nf\"({len(d)}) \"\n\"does not match length of index \"\nf\"({len(self.index)})\"\n)\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt, d = jitrestrict(self.index.values, d, starts, ends)\nself.index = TsIndex(t)\nself.values = d\nelse:\ntime_support = IntervalSet(start=self.index[0], end=self.index[-1])\nself.values = d\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.values = np.empty(0)\nself.time_support = IntervalSet(start=[], end=[])\nself.nap_class = self.__class__.__name__\nself.dtype = self.values.dtype\ndef __repr__(self):\nheaders = [\"Time (s)\", \"\"]\nbottom = \"dtype: {}\".format(self.dtype) + \", shape: {}\".format(self.shape)\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nif len(self):\nif len(self) &lt; 51:\nreturn (\ntabulate(\nnp.vstack((self.index, self.values)).T,\nheaders=headers,\ncolalign=(\"left\",),\n)\n+ \"\\n\"\n+ bottom\n)\nelse:\ntable = []\nfor i, v in zip(self.index[0:5], self.values[0:5]):\ntable.append([i, v])\ntable.append([\"...\"])\nfor i, array in zip(self.index[-5:], self.values[-5:]):\ntable.append([i, v])\nreturn (\ntabulate(table, headers=headers, colalign=(\"left\",))\n+ \"\\n\"\n+ bottom\n)\nelse:\nreturn tabulate([], headers=headers) + \"\\n\" + bottom\ndef as_series(self):\n\"\"\"\n        Convert the Ts/Tsd object to a pandas.Series object.\n        Returns\n        -------\n        out: pandas.Series\n            _\n        \"\"\"\nreturn pd.Series(\nindex=self.index.values, data=self.values, copy=True, dtype=\"float64\"\n)\ndef as_units(self, units=\"s\"):\n\"\"\"\n        Returns a pandas Series with time expressed in the desired unit.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        pandas.Series\n            the series object with adjusted times\n        \"\"\"\nss = self.as_series()\nt = self.index.in_units(units)\nif units == \"us\":\nt = t.astype(np.int64)\nss.index = t\nss.index.name = \"Time (\" + str(units) + \")\"\nreturn ss\ndef threshold(self, thr, method=\"above\"):\n\"\"\"\n        Apply a threshold function to the tsd to return a new tsd\n        with the time support being the epochs above/below/&gt;=/&lt;= the threshold\n        Parameters\n        ----------\n        thr : float\n            The threshold value\n        method : str, optional\n            The threshold method (above/below/aboveequal/belowequal)\n        Returns\n        -------\n        out: Tsd\n            All the time points below/ above/greater than equal to/less than equal to the threshold\n        Raises\n        ------\n        ValueError\n            Raise an error if method is not 'below' or 'above'\n        RuntimeError\n            Raise an error if thr is too high/low and no epochs is found.\n        Examples\n        --------\n        This example finds all epoch above 0.5 within the tsd object.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n        &gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n        The epochs with the times above/below the threshold can be accessed through the time support:\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n        &gt;&gt;&gt; tsd.threshold(50).time_support\n        &gt;&gt;&gt;    start   end\n        &gt;&gt;&gt; 0   50.5  99.0\n        \"\"\"\ntime_array = self.index.values\ndata_array = self.values\nstarts = self.time_support.start.values\nends = self.time_support.end.values\nif method not in [\"above\", \"below\", \"aboveequal\", \"belowequal\"]:\nraise ValueError(\n\"Method {} for thresholding is not accepted.\".format(method)\n)\nt, d, ns, ne = jitthreshold(time_array, data_array, starts, ends, thr, method)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\ndef to_tsgroup(self):\n\"\"\"\n        Convert Tsd to a TsGroup by grouping timestamps with the same values.\n        By default, the values are converted to integers.\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\n        Time (s)\n        0.0    0\n        1.0    2\n        2.0    0\n        3.0    1\n        dtype: int64\n        &gt;&gt;&gt; tsd.to_tsgroup()\n        Index    rate\n        -------  ------\n            0    0.67\n            1    0.33\n            2    0.33\n        The reverse operation can be done with the TsGroup.to_tsd function :\n        &gt;&gt;&gt; tsgroup.to_tsd()\n        Time (s)\n        0.0    0.0\n        1.0    2.0\n        2.0    0.0\n        3.0    1.0\n        dtype: float64\n        Returns\n        -------\n        TsGroup\n            Grouped timestamps\n        \"\"\"\nts_group = importlib.import_module(\".ts_group\", \"pynapple.core\")\nt = self.index.values\nd = self.values.astype(\"int\")\nidx = np.unique(d)\ngroup = {}\nfor k in idx:\ngroup[k] = Ts(t=t[d == k], time_support=self.time_support)\nreturn ts_group.TsGroup(group, time_support=self.time_support)\ndef save(self, filename):\n\"\"\"\n        Save Tsd object in npz format. The file will contain the timestamps, the\n        data and the time support.\n        The main purpose of this function is to save small/medium sized time series\n        objects. For example, you extracted one channel from your recording and\n        filtered it. You can save the filtered channel as a npz to avoid\n        reprocessing it.\n        You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n        &gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'd', 'start', 'end', 'type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1.]\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\n        Time (s)\n        0.0    2\n        1.0    3\n        dtype: int64\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([self.nap_class], dtype=np.str_),\n)\nreturn\ndef interpolate(self, ts, ep=None, left=None, right=None):\n\"\"\"Wrapper of the numpy linear interpolation method. See https://numpy.org/doc/stable/reference/generated/numpy.interp.html for an explanation of the parameters.\n        The argument ts should be Ts, Tsd, TsdFrame, TsdTensor to ensure interpolating from sorted timestamps in the right unit,\n        Parameters\n        ----------\n        ts : Ts, Tsd or TsdFrame\n            The object holding the timestamps\n        ep : IntervalSet, optional\n            The epochs to use to interpolate. If None, the time support of Tsd is used.\n        left : None, optional\n            Value to return for ts &lt; tsd[0], default is tsd[0].\n        right : None, optional\n            Value to return for ts &gt; tsd[-1], default is tsd[-1].\n        \"\"\"\nif not isinstance(ts, (Ts, Tsd, TsdFrame)):\nraise RuntimeError(\n\"First argument should be an instance of Ts, Tsd or TsdFrame\"\n)\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nnew_t = ts.restrict(ep).index\nnew_d = np.empty(len(new_t))\nnew_d.fill(np.nan)\nstart = 0\nfor i in range(len(ep)):\nt = ts.restrict(ep.loc[[i]])\ntmp = self.restrict(ep.loc[[i]])\nif len(t) and len(tmp):\nnew_d[start : start + len(t)] = np.interp(\nt.index.values, tmp.index.values, tmp.values, left=left, right=right\n)\nstart += len(t)\nreturn Tsd(t=new_t, d=new_d, time_support=ep)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.times","title":"times","text":"<pre><code>times(units='s')\n</code></pre> <p>The time index of the object, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>the time indexes</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the object, returned as np.double in the desired time units.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.ndarray\n        the time indexes\n    \"\"\"\nreturn self.index.in_units(units)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.as_array","title":"as_array","text":"<pre><code>as_array()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_array(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.data","title":"data","text":"<pre><code>data()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy()\n</code></pre> <p>Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling <code>plot(tsd)</code></p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def to_numpy(self):\n\"\"\"\n    Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling `plot(tsd)`\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.start_time","title":"start_time","text":"<pre><code>start_time(units='s')\n</code></pre> <p>The first time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def start_time(self, units=\"s\"):\n\"\"\"\n    The first time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[0]\nelse:\nreturn None\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.end_time","title":"end_time","text":"<pre><code>end_time(units='s')\n</code></pre> <p>The last time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def end_time(self, units=\"s\"):\n\"\"\"\n    The last time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[-1]\nelse:\nreturn None\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.value_from","title":"value_from","text":"<pre><code>value_from(data, ep=None)\n</code></pre> <p>Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>The object holding the values to replace.</p> required <code>ep</code> <code>IntervalSet(optional)</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>Object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, data, ep=None):\n\"\"\"\n    Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument\n    Parameters\n    ----------\n    data : Tsd/TsdFrame/TsdTensor\n        The object holding the values to replace.\n    ep : IntervalSet (optional)\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    out : Tsd/TsdFrame/TsdTensor\n        Object with the new values\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n    newts is the same size as ts restrict to ep.\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\nif not isinstance(data, (TsdTensor, TsdFrame, Tsd)):\nraise RuntimeError(\n\"The time series to align to should be Tsd/TsdFrame/TsdTensor.\"\n)\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nif data_target_array.ndim == 1:\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\nelse:\nt, d, ns, ne = jitvaluefromtensor(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nif isinstance(data, TsdFrame):\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nreturn data.__class__(t, d, time_support=time_support)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.count","title":"count","text":"<pre><code>count(*args, **kwargs)\n</code></pre> <p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsd.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsd.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsd.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsd.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; bincount = ts.count(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n&gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n</code></pre> <p>And bincount automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bincount.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  100.0  800.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsd.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsd.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsd.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsd.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; bincount = ts.count(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n    &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n    And bincount automatically inherit ep as time support:\n    &gt;&gt;&gt; bincount.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  100.0  800.0\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\ntime_array = self.index.values\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\nt, d = jitcount(time_array, starts, ends, bin_size)\nelse:\n_, d = jittsrestrict_with_count(time_array, starts, ends)\nt = starts + (ends - starts) / 2\nreturn Tsd(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.restrict","title":"restrict","text":"<pre><code>restrict(iset)\n</code></pre> <p>Restricts a time series object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>(Ts, Tsd, TsdFrame or TsdTensor)</code> <p>Tsd object restricted to ep</p> <p>Examples:</p> <p>The Ts object is restrict to the intervals defined by ep.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n&gt;&gt;&gt; newts = ts.restrict(ep)\n</code></pre> <p>The time support of newts automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newts.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0    0.0  500.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, iset):\n\"\"\"\n    Restricts a time series object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    iset : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    out: Ts, Tsd, TsdFrame or TsdTensor\n        Tsd object restricted to ep\n    Examples\n    --------\n    The Ts object is restrict to the intervals defined by ep.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n    &gt;&gt;&gt; newts = ts.restrict(ep)\n    The time support of newts automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newts.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0    0.0  500.0\n    \"\"\"\nassert isinstance(iset, IntervalSet), \"Argument should be IntervalSet\"\ntime_array = self.index.values\nstarts = iset.start.values\nends = iset.end.values\nif isinstance(self.values, np.ndarray):\ndata_array = self.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=iset, columns=self.columns)\nelse:\nreturn self.__class__(t=t, d=d, time_support=iset)\nelse:\nt = jittsrestrict(time_array, starts, ends)\nreturn Ts(t, time_support=iset)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.bin_average","title":"bin_average","text":"<pre><code>bin_average(bin_size, ep=None, time_units='s')\n</code></pre> <p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>(Tsd, TsdFrame, TsdTensor)</code> <p>A Tsd object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsd automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsd.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd, TsdFrame, TsdTensor\n        A Tsd object indexed by the center of the bins and holding the averaged data points.\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n    And bintsd automatically inherit ep as time support:\n    &gt;&gt;&gt; bintsd.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nif data_array.ndim &gt; 1:\nt, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\nelse:\nt, d = jitbin(time_array, data_array, starts, ends, bin_size)\nif d.ndim == 1:\nreturn Tsd(t=t, d=d, time_support=ep)\nelif d.ndim == 2:\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=ep, columns=self.columns)\nelse:\nreturn TsdFrame(t=t, d=d, time_support=ep)\nelse:\nreturn TsdTensor(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy the data, index and time support</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def copy(self):\n\"\"\"Copy the data, index and time support\"\"\"\nreturn self.__class__(\nt=self.index.copy(), d=self.values.copy(), time_support=self.time_support\n)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.__init__","title":"__init__","text":"<pre><code>__init__(t, d=None, time_units='s', time_support=None)\n</code></pre> <p>Tsd Initializer.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray or Series</code> <p>An object transformable in a time series, or a pandas.Series equivalent (if d is None)</p> required <code>d</code> <code>ndarray</code> <p>The data of the time series</p> <code>None</code> <code>time_units</code> <code>str</code> <p>The time units in which times are specified ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the tsd object</p> <code>None</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, d=None, time_units=\"s\", time_support=None):\n\"\"\"\n    Tsd Initializer.\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.Series\n        An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n    d : numpy.ndarray, optional\n        The data of the time series\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default])\n    time_support : IntervalSet, optional\n        The time support of the tsd object\n    \"\"\"\nif isinstance(t, np.ndarray) and d is None:\nraise RuntimeError(\"Missing argument d when initializing Tsd\")\nif isinstance(t, pd.Series):\nd = t.values\nt = t.index.values\nif isinstance(t, (list, tuple)):\nt = np.array(t)\nif isinstance(d, (list, tuple)):\nd = np.array(d)\nassert d.ndim == 1, \"Data should be 1 dimension\"\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index) != len(d):\nraise ValueError(\n\"Length of values \"\nf\"({len(d)}) \"\n\"does not match length of index \"\nf\"({len(self.index)})\"\n)\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt, d = jitrestrict(self.index.values, d, starts, ends)\nself.index = TsIndex(t)\nself.values = d\nelse:\ntime_support = IntervalSet(start=self.index[0], end=self.index[-1])\nself.values = d\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.values = np.empty(0)\nself.time_support = IntervalSet(start=[], end=[])\nself.nap_class = self.__class__.__name__\nself.dtype = self.values.dtype\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.as_series","title":"as_series","text":"<pre><code>as_series()\n</code></pre> <p>Convert the Ts/Tsd object to a pandas.Series object.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>Series</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_series(self):\n\"\"\"\n    Convert the Ts/Tsd object to a pandas.Series object.\n    Returns\n    -------\n    out: pandas.Series\n        _\n    \"\"\"\nreturn pd.Series(\nindex=self.index.values, data=self.values, copy=True, dtype=\"float64\"\n)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.as_units","title":"as_units","text":"<pre><code>as_units(units='s')\n</code></pre> <p>Returns a pandas Series with time expressed in the desired unit.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Type Description <code>Series</code> <p>the series object with adjusted times</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    Returns a pandas Series with time expressed in the desired unit.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    pandas.Series\n        the series object with adjusted times\n    \"\"\"\nss = self.as_series()\nt = self.index.in_units(units)\nif units == \"us\":\nt = t.astype(np.int64)\nss.index = t\nss.index.name = \"Time (\" + str(units) + \")\"\nreturn ss\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.threshold","title":"threshold","text":"<pre><code>threshold(thr, method='above')\n</code></pre> <p>Apply a threshold function to the tsd to return a new tsd with the time support being the epochs above/below/&gt;=/&lt;= the threshold</p> <p>Parameters:</p> Name Type Description Default <code>thr</code> <code>float</code> <p>The threshold value</p> required <code>method</code> <code>str</code> <p>The threshold method (above/below/aboveequal/belowequal)</p> <code>'above'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>All the time points below/ above/greater than equal to/less than equal to the threshold</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raise an error if method is not 'below' or 'above'</p> <code>RuntimeError</code> <p>Raise an error if thr is too high/low and no epochs is found.</p> <p>Examples:</p> <p>This example finds all epoch above 0.5 within the tsd object.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n</code></pre> <p>The epochs with the times above/below the threshold can be accessed through the time support:</p> <pre><code>&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n&gt;&gt;&gt; tsd.threshold(50).time_support\n&gt;&gt;&gt;    start   end\n&gt;&gt;&gt; 0   50.5  99.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def threshold(self, thr, method=\"above\"):\n\"\"\"\n    Apply a threshold function to the tsd to return a new tsd\n    with the time support being the epochs above/below/&gt;=/&lt;= the threshold\n    Parameters\n    ----------\n    thr : float\n        The threshold value\n    method : str, optional\n        The threshold method (above/below/aboveequal/belowequal)\n    Returns\n    -------\n    out: Tsd\n        All the time points below/ above/greater than equal to/less than equal to the threshold\n    Raises\n    ------\n    ValueError\n        Raise an error if method is not 'below' or 'above'\n    RuntimeError\n        Raise an error if thr is too high/low and no epochs is found.\n    Examples\n    --------\n    This example finds all epoch above 0.5 within the tsd object.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n    The epochs with the times above/below the threshold can be accessed through the time support:\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n    &gt;&gt;&gt; tsd.threshold(50).time_support\n    &gt;&gt;&gt;    start   end\n    &gt;&gt;&gt; 0   50.5  99.0\n    \"\"\"\ntime_array = self.index.values\ndata_array = self.values\nstarts = self.time_support.start.values\nends = self.time_support.end.values\nif method not in [\"above\", \"below\", \"aboveequal\", \"belowequal\"]:\nraise ValueError(\n\"Method {} for thresholding is not accepted.\".format(method)\n)\nt, d, ns, ne = jitthreshold(time_array, data_array, starts, ends, thr, method)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.to_tsgroup","title":"to_tsgroup","text":"<pre><code>to_tsgroup()\n</code></pre> <p>Convert Tsd to a TsGroup by grouping timestamps with the same values. By default, the values are converted to integers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\nTime (s)\n0.0    0\n1.0    2\n2.0    0\n3.0    1\ndtype: int64\n</code></pre> <pre><code>&gt;&gt;&gt; tsd.to_tsgroup()\nIndex    rate\n-------  ------\n    0    0.67\n    1    0.33\n    2    0.33\n</code></pre> <p>The reverse operation can be done with the TsGroup.to_tsd function :</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd()\nTime (s)\n0.0    0.0\n1.0    2.0\n2.0    0.0\n3.0    1.0\ndtype: float64\n</code></pre> <p>Returns:</p> Type Description <code>TsGroup</code> <p>Grouped timestamps</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def to_tsgroup(self):\n\"\"\"\n    Convert Tsd to a TsGroup by grouping timestamps with the same values.\n    By default, the values are converted to integers.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\n    Time (s)\n    0.0    0\n    1.0    2\n    2.0    0\n    3.0    1\n    dtype: int64\n    &gt;&gt;&gt; tsd.to_tsgroup()\n    Index    rate\n    -------  ------\n        0    0.67\n        1    0.33\n        2    0.33\n    The reverse operation can be done with the TsGroup.to_tsd function :\n    &gt;&gt;&gt; tsgroup.to_tsd()\n    Time (s)\n    0.0    0.0\n    1.0    2.0\n    2.0    0.0\n    3.0    1.0\n    dtype: float64\n    Returns\n    -------\n    TsGroup\n        Grouped timestamps\n    \"\"\"\nts_group = importlib.import_module(\".ts_group\", \"pynapple.core\")\nt = self.index.values\nd = self.values.astype(\"int\")\nidx = np.unique(d)\ngroup = {}\nfor k in idx:\ngroup[k] = Ts(t=t[d == k], time_support=self.time_support)\nreturn ts_group.TsGroup(group, time_support=self.time_support)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.save","title":"save","text":"<pre><code>save(filename)\n</code></pre> <p>Save Tsd object in npz format. The file will contain the timestamps, the data and the time support.</p> <p>The main purpose of this function is to save small/medium sized time series objects. For example, you extracted one channel from your recording and filtered it. You can save the filtered channel as a npz to avoid reprocessing it.</p> <p>You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n&gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'd', 'start', 'end', 'type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1.]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\nTime (s)\n0.0    2\n1.0    3\ndtype: int64\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save Tsd object in npz format. The file will contain the timestamps, the\n    data and the time support.\n    The main purpose of this function is to save small/medium sized time series\n    objects. For example, you extracted one channel from your recording and\n    filtered it. You can save the filtered channel as a npz to avoid\n    reprocessing it.\n    You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n    &gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'd', 'start', 'end', 'type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1.]\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\n    Time (s)\n    0.0    2\n    1.0    3\n    dtype: int64\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([self.nap_class], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.interpolate","title":"interpolate","text":"<pre><code>interpolate(ts, ep=None, left=None, right=None)\n</code></pre> <p>Wrapper of the numpy linear interpolation method. See https://numpy.org/doc/stable/reference/generated/numpy.interp.html for an explanation of the parameters. The argument ts should be Ts, Tsd, TsdFrame, TsdTensor to ensure interpolating from sorted timestamps in the right unit,</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>(Ts, Tsd or TsdFrame)</code> <p>The object holding the timestamps</p> required <code>ep</code> <code>IntervalSet</code> <p>The epochs to use to interpolate. If None, the time support of Tsd is used.</p> <code>None</code> <code>left</code> <code>None</code> <p>Value to return for ts &lt; tsd[0], default is tsd[0].</p> <code>None</code> <code>right</code> <code>None</code> <p>Value to return for ts &gt; tsd[-1], default is tsd[-1].</p> <code>None</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def interpolate(self, ts, ep=None, left=None, right=None):\n\"\"\"Wrapper of the numpy linear interpolation method. See https://numpy.org/doc/stable/reference/generated/numpy.interp.html for an explanation of the parameters.\n    The argument ts should be Ts, Tsd, TsdFrame, TsdTensor to ensure interpolating from sorted timestamps in the right unit,\n    Parameters\n    ----------\n    ts : Ts, Tsd or TsdFrame\n        The object holding the timestamps\n    ep : IntervalSet, optional\n        The epochs to use to interpolate. If None, the time support of Tsd is used.\n    left : None, optional\n        Value to return for ts &lt; tsd[0], default is tsd[0].\n    right : None, optional\n        Value to return for ts &gt; tsd[-1], default is tsd[-1].\n    \"\"\"\nif not isinstance(ts, (Ts, Tsd, TsdFrame)):\nraise RuntimeError(\n\"First argument should be an instance of Ts, Tsd or TsdFrame\"\n)\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nnew_t = ts.restrict(ep).index\nnew_d = np.empty(len(new_t))\nnew_d.fill(np.nan)\nstart = 0\nfor i in range(len(ep)):\nt = ts.restrict(ep.loc[[i]])\ntmp = self.restrict(ep.loc[[i]])\nif len(t) and len(tmp):\nnew_d[start : start + len(t)] = np.interp(\nt.index.values, tmp.index.values, tmp.values, left=left, right=right\n)\nstart += len(t)\nreturn Tsd(t=new_t, d=new_d, time_support=ep)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts","title":"Ts","text":"<p>             Bases: <code>_AbstractTsd</code></p> <p>Timestamps only object for a time series with only time index,</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class Ts(_AbstractTsd):\n\"\"\"\n    Timestamps only object for a time series with only time index,\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, time_units=\"s\", time_support=None):\n\"\"\"\n        Ts Initializer\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.Series\n            An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default])\n        time_support : IntervalSet, optional\n            The time support of the Ts object\n        \"\"\"\nif isinstance(t, Number):\nt = np.array([t])\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt = jittsrestrict(self.index.values, starts, ends)\nself.index = TsIndex(t)\nelse:\ntime_support = IntervalSet(start=t[0], end=t[-1])\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.time_support = IntervalSet(start=[], end=[])\nself.values = None\nself.nap_class = self.__class__.__name__\ndef __repr__(self):\nupper = \"Time (s)\"\nif len(self) &lt; 100:\n_str_ = \"\\n\".join([i.__repr__() for i in self.index])\nelse:\n_str_ = \"\\n\".join(\n[i.__repr__() for i in self.index[0:5]]\n+ [\"...\"]\n+ [i.__repr__() for i in self.index[-5:]]\n)\nbottom = \"shape: {}\".format(len(self.index))\nreturn \"\\n\".join((upper, _str_, bottom))\ndef __getitem__(self, key):\ntry:\nif isinstance(key, tuple):\nindex = self.index.__getitem__(key[0])\nelse:\nindex = self.index.__getitem__(key)\nif isinstance(index, Number):\nindex = np.array([index])\nif isinstance(index, np.ndarray):\nreturn Ts(t=index, time_support=self.time_support)\nelse:\nreturn None\nexcept RuntimeError:\nraise IndexError\ndef __setitem__(self, key, value):\npass\ndef as_series(self):\n\"\"\"\n        Convert the Ts/Tsd object to a pandas.Series object.\n        Returns\n        -------\n        out: pandas.Series\n            _\n        \"\"\"\nreturn pd.Series(index=self.index.values, dtype=\"object\")\ndef as_units(self, units=\"s\"):\n\"\"\"\n        Returns a pandas Series with time expressed in the desired unit.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        pandas.Series\n            the series object with adjusted times\n        \"\"\"\nt = self.index.in_units(units)\nif units == \"us\":\nt = t.astype(np.int64)\nss = pd.Series(index=t, dtype=\"object\")\nss.index.name = \"Time (\" + str(units) + \")\"\nreturn ss\ndef fillna(self, value):\n\"\"\"\n        Similar to pandas fillna function.\n        Parameters\n        ----------\n        value : Number\n            Value for filling\n        Returns\n        -------\n        Tsd\n        \"\"\"\nassert isinstance(value, Number), \"Only a scalar can be passed to fillna\"\nd = np.empty(len(self))\nd.fill(value)\nreturn Tsd(t=self.index, d=d, time_support=self.time_support)\ndef save(self, filename):\n\"\"\"\n        Save Ts object in npz format. The file will contain the timestamps and\n        the time support.\n        The main purpose of this function is to save small/medium sized timestamps\n        object.\n        You can load the object with numpy.load. Keys are 't', 'start' and 'end' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n        &gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'start', 'end', 'type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1. 1.5]\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\n        Time (s)\n        0.0\n        1.0\n        1.5\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([\"Ts\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.times","title":"times","text":"<pre><code>times(units='s')\n</code></pre> <p>The time index of the object, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>the time indexes</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the object, returned as np.double in the desired time units.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.ndarray\n        the time indexes\n    \"\"\"\nreturn self.index.in_units(units)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.as_array","title":"as_array","text":"<pre><code>as_array()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_array(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.data","title":"data","text":"<pre><code>data()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy()\n</code></pre> <p>Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling <code>plot(tsd)</code></p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def to_numpy(self):\n\"\"\"\n    Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling `plot(tsd)`\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.start_time","title":"start_time","text":"<pre><code>start_time(units='s')\n</code></pre> <p>The first time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def start_time(self, units=\"s\"):\n\"\"\"\n    The first time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[0]\nelse:\nreturn None\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.end_time","title":"end_time","text":"<pre><code>end_time(units='s')\n</code></pre> <p>The last time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def end_time(self, units=\"s\"):\n\"\"\"\n    The last time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[-1]\nelse:\nreturn None\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.value_from","title":"value_from","text":"<pre><code>value_from(data, ep=None)\n</code></pre> <p>Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>The object holding the values to replace.</p> required <code>ep</code> <code>IntervalSet(optional)</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>Object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, data, ep=None):\n\"\"\"\n    Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument\n    Parameters\n    ----------\n    data : Tsd/TsdFrame/TsdTensor\n        The object holding the values to replace.\n    ep : IntervalSet (optional)\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    out : Tsd/TsdFrame/TsdTensor\n        Object with the new values\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n    newts is the same size as ts restrict to ep.\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\nif not isinstance(data, (TsdTensor, TsdFrame, Tsd)):\nraise RuntimeError(\n\"The time series to align to should be Tsd/TsdFrame/TsdTensor.\"\n)\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nif data_target_array.ndim == 1:\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\nelse:\nt, d, ns, ne = jitvaluefromtensor(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nif isinstance(data, TsdFrame):\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nreturn data.__class__(t, d, time_support=time_support)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.count","title":"count","text":"<pre><code>count(*args, **kwargs)\n</code></pre> <p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsd.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsd.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsd.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsd.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; bincount = ts.count(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n&gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n</code></pre> <p>And bincount automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bincount.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  100.0  800.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsd.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsd.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsd.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsd.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; bincount = ts.count(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n    &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n    And bincount automatically inherit ep as time support:\n    &gt;&gt;&gt; bincount.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  100.0  800.0\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\ntime_array = self.index.values\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\nt, d = jitcount(time_array, starts, ends, bin_size)\nelse:\n_, d = jittsrestrict_with_count(time_array, starts, ends)\nt = starts + (ends - starts) / 2\nreturn Tsd(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.restrict","title":"restrict","text":"<pre><code>restrict(iset)\n</code></pre> <p>Restricts a time series object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>(Ts, Tsd, TsdFrame or TsdTensor)</code> <p>Tsd object restricted to ep</p> <p>Examples:</p> <p>The Ts object is restrict to the intervals defined by ep.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n&gt;&gt;&gt; newts = ts.restrict(ep)\n</code></pre> <p>The time support of newts automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newts.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0    0.0  500.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, iset):\n\"\"\"\n    Restricts a time series object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    iset : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    out: Ts, Tsd, TsdFrame or TsdTensor\n        Tsd object restricted to ep\n    Examples\n    --------\n    The Ts object is restrict to the intervals defined by ep.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n    &gt;&gt;&gt; newts = ts.restrict(ep)\n    The time support of newts automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newts.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0    0.0  500.0\n    \"\"\"\nassert isinstance(iset, IntervalSet), \"Argument should be IntervalSet\"\ntime_array = self.index.values\nstarts = iset.start.values\nends = iset.end.values\nif isinstance(self.values, np.ndarray):\ndata_array = self.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=iset, columns=self.columns)\nelse:\nreturn self.__class__(t=t, d=d, time_support=iset)\nelse:\nt = jittsrestrict(time_array, starts, ends)\nreturn Ts(t, time_support=iset)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.bin_average","title":"bin_average","text":"<pre><code>bin_average(bin_size, ep=None, time_units='s')\n</code></pre> <p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>(Tsd, TsdFrame, TsdTensor)</code> <p>A Tsd object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsd automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsd.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd, TsdFrame, TsdTensor\n        A Tsd object indexed by the center of the bins and holding the averaged data points.\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n    And bintsd automatically inherit ep as time support:\n    &gt;&gt;&gt; bintsd.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nif data_array.ndim &gt; 1:\nt, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\nelse:\nt, d = jitbin(time_array, data_array, starts, ends, bin_size)\nif d.ndim == 1:\nreturn Tsd(t=t, d=d, time_support=ep)\nelif d.ndim == 2:\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=ep, columns=self.columns)\nelse:\nreturn TsdFrame(t=t, d=d, time_support=ep)\nelse:\nreturn TsdTensor(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy the data, index and time support</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def copy(self):\n\"\"\"Copy the data, index and time support\"\"\"\nreturn self.__class__(\nt=self.index.copy(), d=self.values.copy(), time_support=self.time_support\n)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.__init__","title":"__init__","text":"<pre><code>__init__(t, time_units='s', time_support=None)\n</code></pre> <p>Ts Initializer</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray or Series</code> <p>An object transformable in a time series, or a pandas.Series equivalent (if d is None)</p> required <code>time_units</code> <code>str</code> <p>The time units in which times are specified ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the Ts object</p> <code>None</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, time_units=\"s\", time_support=None):\n\"\"\"\n    Ts Initializer\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.Series\n        An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default])\n    time_support : IntervalSet, optional\n        The time support of the Ts object\n    \"\"\"\nif isinstance(t, Number):\nt = np.array([t])\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt = jittsrestrict(self.index.values, starts, ends)\nself.index = TsIndex(t)\nelse:\ntime_support = IntervalSet(start=t[0], end=t[-1])\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.time_support = IntervalSet(start=[], end=[])\nself.values = None\nself.nap_class = self.__class__.__name__\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.as_series","title":"as_series","text":"<pre><code>as_series()\n</code></pre> <p>Convert the Ts/Tsd object to a pandas.Series object.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>Series</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_series(self):\n\"\"\"\n    Convert the Ts/Tsd object to a pandas.Series object.\n    Returns\n    -------\n    out: pandas.Series\n        _\n    \"\"\"\nreturn pd.Series(index=self.index.values, dtype=\"object\")\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.as_units","title":"as_units","text":"<pre><code>as_units(units='s')\n</code></pre> <p>Returns a pandas Series with time expressed in the desired unit.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Type Description <code>Series</code> <p>the series object with adjusted times</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    Returns a pandas Series with time expressed in the desired unit.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    pandas.Series\n        the series object with adjusted times\n    \"\"\"\nt = self.index.in_units(units)\nif units == \"us\":\nt = t.astype(np.int64)\nss = pd.Series(index=t, dtype=\"object\")\nss.index.name = \"Time (\" + str(units) + \")\"\nreturn ss\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.fillna","title":"fillna","text":"<pre><code>fillna(value)\n</code></pre> <p>Similar to pandas fillna function.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Number</code> <p>Value for filling</p> required <p>Returns:</p> Type Description <code>Tsd</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def fillna(self, value):\n\"\"\"\n    Similar to pandas fillna function.\n    Parameters\n    ----------\n    value : Number\n        Value for filling\n    Returns\n    -------\n    Tsd\n    \"\"\"\nassert isinstance(value, Number), \"Only a scalar can be passed to fillna\"\nd = np.empty(len(self))\nd.fill(value)\nreturn Tsd(t=self.index, d=d, time_support=self.time_support)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.save","title":"save","text":"<pre><code>save(filename)\n</code></pre> <p>Save Ts object in npz format. The file will contain the timestamps and the time support.</p> <p>The main purpose of this function is to save small/medium sized timestamps object.</p> <p>You can load the object with numpy.load. Keys are 't', 'start' and 'end' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n&gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'start', 'end', 'type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1. 1.5]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\nTime (s)\n0.0\n1.0\n1.5\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save Ts object in npz format. The file will contain the timestamps and\n    the time support.\n    The main purpose of this function is to save small/medium sized timestamps\n    object.\n    You can load the object with numpy.load. Keys are 't', 'start' and 'end' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n    &gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'start', 'end', 'type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1. 1.5]\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\n    Time (s)\n    0.0\n    1.0\n    1.5\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([\"Ts\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"old_pages/core.ts_group/","title":"Core.ts group","text":""},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group","title":"pynapple.core.ts_group","text":""},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup","title":"TsGroup","text":"<p>             Bases: <code>UserDict</code></p> <p>The TsGroup is a dictionnary-like object to hold multiple <code>Ts</code> or <code>Tsd</code> objects with different time index.</p> <p>Attributes:</p> Name Type Description <code>time_support</code> <code>IntervalSet</code> <p>The time support of the TsGroup</p> <code>rates</code> <code>Series</code> <p>The rate of each element of the TsGroup</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>class TsGroup(UserDict):\n\"\"\"\n    The TsGroup is a dictionnary-like object to hold multiple [`Ts`][pynapple.core.time_series.Ts] or [`Tsd`][pynapple.core.time_series.Tsd] objects with different time index.\n    Attributes\n    ----------\n    time_support: IntervalSet\n        The time support of the TsGroup\n    rates : pandas.Series\n        The rate of each element of the TsGroup\n    \"\"\"\ndef __init__(\nself, data, time_support=None, time_units=\"s\", bypass_check=False, **kwargs\n):\n\"\"\"\n        TsGroup Initializer\n        Parameters\n        ----------\n        data : dict\n            Dictionnary containing Ts/Tsd objects\n        time_support : IntervalSet, optional\n            The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed.\n            If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.\n        time_units : str, optional\n            Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).\n        bypass_check: bool, optional\n            To avoid checking that each element is within time_support.\n            Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand\n        **kwargs\n            Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray.\n            Note that the index should match the index of the input dictionnary.\n        Raises\n        ------\n        RuntimeError\n            Raise error if the union of time support of Ts/Tsd object is empty.\n        \"\"\"\nself._initialized = False\nself.index = np.sort(list(data.keys()))\nself._metadata = pd.DataFrame(index=self.index, columns=[\"rate\"], dtype=\"float\")\n# Transform elements to Ts/Tsd objects\nfor k in self.index:\nif isinstance(data[k], (np.ndarray, list)):\nwarnings.warn(\n\"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\nstacklevel=2,\n)\ndata[k] = Ts(\nt=data[k], time_support=time_support, time_units=time_units\n)\n# If time_support is passed, all elements of data are restricted prior to init\nif isinstance(time_support, IntervalSet):\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nelse:\n# Otherwise do the union of all time supports\ntime_support = union_intervals([data[k].time_support for k in self.index])\nif len(time_support) == 0:\nraise RuntimeError(\n\"Union of time supports is empty. Consider passing a time support as argument.\"\n)\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nUserDict.__init__(self, data)\n# Making the TsGroup non mutable\nself._initialized = True\n# Trying to add argument as metainfo\nself.set_info(**kwargs)\n\"\"\"\n    Base functions\n    \"\"\"\ndef __setitem__(self, key, value):\nif self._initialized:\nraise RuntimeError(\"TsGroup object is not mutable.\")\nself._metadata.loc[int(key), \"rate\"] = float(value.rate)\nsuper().__setitem__(int(key), value)\n# if self.__contains__(key):\n#     raise KeyError(\"Key {} already in group index.\".format(key))\n# else:\n# if isinstance(value, (Ts, Tsd)):\n#     self._metadata.loc[int(key), \"rate\"] = value.rate\n#     super().__setitem__(int(key), value)\n# elif isinstance(value, (np.ndarray, list)):\n#     warnings.warn(\n#         \"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\n#         stacklevel=2,\n#     )\n#     tmp = Ts(t=value, time_units=\"s\")\n#     self._metadata.loc[int(key), \"rate\"] = tmp.rate\n#     super().__setitem__(int(key), tmp)\n# else:\n#     raise ValueError(\"Value with key {} is not an iterable.\".format(key))\ndef __getitem__(self, key):\nif key.__hash__:\nif self.__contains__(key):\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\nelse:\nmetadata = self._metadata.loc[key, self._metadata.columns.drop(\"rate\")]\nreturn TsGroup(\n{k: self[k] for k in key}, time_support=self.time_support, **metadata\n)\ndef __repr__(self):\ncols = self._metadata.columns.drop(\"rate\")\nheaders = [\"Index\", \"rate\"] + [c for c in cols]\nlines = []\nfor i in self.data.keys():\nlines.append(\n[str(i), \"%.2f\" % self._metadata.loc[i, \"rate\"]]\n+ [self._metadata.loc[i, c] for c in cols]\n)\nreturn tabulate(lines, headers=headers)\ndef __str__(self):\nreturn self.__repr__()\ndef keys(self):\n\"\"\"\n        Return index/keys of TsGroup\n        Returns\n        -------\n        list\n            List of keys\n        \"\"\"\nreturn list(self.data.keys())\ndef items(self):\n\"\"\"\n        Return a list of key/object.\n        Returns\n        -------\n        list\n            List of tuples\n        \"\"\"\nreturn list(self.data.items())\ndef values(self):\n\"\"\"\n        Return a list of all the Ts/Tsd objects in the TsGroup\n        Returns\n        -------\n        list\n            List of Ts/Tsd objects\n        \"\"\"\nreturn list(self.data.values())\n@property\ndef rates(self):\n\"\"\"\n        Return the rates of each element of the group in Hz\n        \"\"\"\nreturn self._metadata[\"rate\"]\n#######################\n# Metadata\n#######################\n@property\ndef metadata_columns(self):\n\"\"\"\n        Returns list of metadata columns\n        -------\n        \"\"\"\nreturn list(self._metadata.columns)\ndef set_info(self, *args, **kwargs):\n\"\"\"\n        Add metadata informations about the TsGroup.\n        Metadata are saved as a DataFrame.\n        Parameters\n        ----------\n        *args\n            pandas.Dataframe or list of pandas.DataFrame\n        **kwargs\n            Can be either pandas.Series or numpy.ndarray\n        Raises\n        ------\n        RuntimeError\n            Raise an error if\n                no column labels are found when passing simple arguments,\n                indexes are not equals for a pandas series,\n                not the same length when passing numpy array.\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        To add metadata with a pandas.DataFrame:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n        &gt;&gt;&gt; tsgroup.set_info(structs)\n        &gt;&gt;&gt; tsgroup\n          Index    Freq. (Hz)  struct\n        -------  ------------  --------\n              0             1  pfc\n              1             2  pfc\n              2             4  ca1\n        To add metadata with a pd.Series or numpy.ndarray:\n        &gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n        &gt;&gt;&gt; tsgroup.set_info(hd=hd)\n        &gt;&gt;&gt; tsgroup\n          Index    Freq. (Hz)  struct      hd\n        -------  ------------  --------  ----\n              0             1  pfc          0\n              1             2  pfc          1\n              2             4  ca1          1\n        \"\"\"\nif len(args):\nfor arg in args:\nif isinstance(arg, pd.DataFrame):\nif pd.Index.equals(self._metadata.index, arg.index):\nself._metadata = self._metadata.join(arg)\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(arg, (pd.Series, np.ndarray)):\nraise RuntimeError(\"Columns needs to be labelled for metadata\")\nif len(kwargs):\nfor k, v in kwargs.items():\nif isinstance(v, pd.Series):\nif pd.Index.equals(self._metadata.index, v.index):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(v, np.ndarray):\nif len(self._metadata) == len(v):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Array is not the same length.\")\nreturn\ndef get_info(self, key):\n\"\"\"\n        Returns the metainfo located in one column.\n        The key for the column frequency is \"rate\".\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        Returns\n        -------\n        pandas.Series\n            The metainfo\n        \"\"\"\nif key in [\"freq\", \"frequency\"]:\nkey = \"rate\"\nreturn self._metadata[key]\n#################################\n# Generic functions of Tsd objects\n#################################\ndef restrict(self, ep):\n\"\"\"\n        Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object\n        Parameters\n        ----------\n        ep : IntervalSet\n            the IntervalSet object\n        Returns\n        -------\n        TsGroup\n            TsGroup object restricted to ep\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        &gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n        All objects within the TsGroup automatically inherit the epochs defined by ep.\n        &gt;&gt;&gt; newtsgroup.time_support\n           start    end\n        0    0.0  100.0\n        &gt;&gt;&gt; newtsgroup[0].time_support\n           start    end\n        0    0.0  100.0\n        \"\"\"\nnewgr = {}\nfor k in self.index:\nnewgr[k] = self.data[k].restrict(ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(\nnewgr, time_support=ep, bypass_check=True, **self._metadata[cols]\n)\ndef value_from(self, tsd, ep=None):\n\"\"\"\n        Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument\n        Parameters\n        ----------\n        tsd : Tsd\n            The Tsd object holding the values to replace\n        ep : IntervalSet\n            The IntervalSet object to restrict the operation.\n            If None, the time support of the tsd input object is used.\n        Returns\n        -------\n        TsGroup\n            TsGroup object with the new values\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        The variable tsd is a time series object containing the values to assign, for example the tracking data:\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n        &gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n        \"\"\"\nif ep is None:\nep = tsd.time_support\nnewgr = {}\nfor k in self.data:\nnewgr[k] = self.data[k].value_from(tsd, ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(newgr, time_support=ep, **self._metadata[cols])\ndef count(self, *args, **kwargs):\n\"\"\"\n        Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n        You can call this function in multiple ways :\n        1. *tsgroup.count(bin_size=1, time_units = 'ms')*\n        -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n        2. *tsgroup.count(1, ep=my_epochs)*\n        -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n        3. *tsgroup.count(ep=my_bins)*\n        -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n        4. *tsgroup.count()*\n        -&gt; Count occurent of events within each epoch of the time support.\n        bin_size should be seconds unless specified.\n        If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n        Parameters\n        ----------\n        bin_size : None or float, optional\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: TsdFrame\n            A TsdFrame with the columns being the index of each item in the TsGroup.\n        Examples\n        --------\n        This example shows how to count events within bins of 0.1 second for the first 100 seconds.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        &gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n        &gt;&gt;&gt; bincount\n                  0  1  2\n        Time (s)\n        0.05      0  0  0\n        0.15      0  0  0\n        0.25      0  0  1\n        0.35      0  0  0\n        0.45      0  0  0\n        ...      .. .. ..\n        99.55     0  1  1\n        99.65     0  0  0\n        99.75     0  0  1\n        99.85     0  0  0\n        99.95     1  1  1\n        [1000 rows x 3 columns]\n        \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = float(bin_size)\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\ntime_index, _ = jitcount(np.array([]), starts, ends, bin_size)\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jitcount(\nself.data[self.index[i]].index, starts, ends, bin_size\n)[1]\nelse:\ntime_index = starts + (ends - starts) / 2\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jittsrestrict_with_count(\nself.data[self.index[i]].index, starts, ends\n)[1]\ntoreturn = TsdFrame(t=time_index, d=count, time_support=ep, columns=self.index)\nreturn toreturn\ndef to_tsd(self, *args):\n\"\"\"\n        Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.\n        Parameters\n        ----------\n        *args\n            string, list, numpy.ndarray or pandas.Series\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\n        Index    rate\n        -------  ------\n        0       1\n        5       1\n        By default, the values of the Tsd is the index of the timestamp in the TsGroup:\n        &gt;&gt;&gt; tsgroup.to_tsd()\n        Time (s)\n        0.0    0.0\n        1.0    0.0\n        2.0    5.0\n        3.0    5.0\n        dtype: float64\n        Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.\n        &gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n        &gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\n        Time (s)\n        0.0    3.141593\n        1.0    3.141593\n        2.0    6.283185\n        3.0    6.283185\n        dtype: float64\n        Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :\n        &gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\n        Time (s)\n        0.0   -1.0\n        1.0   -1.0\n        2.0    1.0\n        3.0    1.0\n        dtype: float64\n        The reverse operation can be done with the Tsd.to_tsgroup function :\n        &gt;&gt;&gt; my_tsd\n        Time (s)\n        0.0    0.0\n        1.0    0.0\n        2.0    5.0\n        3.0    5.0\n        dtype: float64\n        &gt;&gt;&gt; my_tsd.to_tsgroup()\n          Index    rate\n        -------  ------\n              0       1\n              5       1\n        Returns\n        -------\n        Tsd\n        Raises\n        ------\n        RuntimeError\n            \"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes\n            \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object\n            \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata,\n            \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series\n        \"\"\"\nif len(args):\nif isinstance(args[0], pd.Series):\nif pd.Index.equals(self._metadata.index, args[0].index):\n_values = args[0].values.flatten()\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(args[0], (np.ndarray, list)):\nif len(self._metadata) == len(args[0]):\n_values = np.array(args[0])\nelse:\nraise RuntimeError(\"Values is not the same length.\")\nelif isinstance(args[0], str):\nif args[0] in self._metadata.columns:\n_values = self._metadata[args[0]].values\nelse:\nraise RuntimeError(\n\"Key {} not in metadata of TsGroup\".format(args[0])\n)\nelse:\npossible_keys = []\nfor k, d in self._metadata.dtypes.items():\nif \"int\" in str(d) or \"float\" in str(d):\npossible_keys.append(k)\nraise RuntimeError(\n\"Unknown argument format. Must be pandas.Series, numpy.ndarray or a string from one of the following values : [{}]\".format(\n\", \".join(possible_keys)\n)\n)\nelse:\n_values = self.index\nnt = 0\nfor n in self.index:\nnt += len(self[n])\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nk = 0\nfor n, v in zip(self.index, _values):\nkl = len(self[n])\ntimes[k : k + kl] = self[n].index\ndata[k : k + kl] = v\nk += kl\nidx = np.argsort(times)\ntoreturn = Tsd(t=times[idx], d=data[idx], time_support=self.time_support)\nreturn toreturn\n\"\"\"\n    Special slicing of metadata\n    \"\"\"\ndef getby_threshold(self, key, thr, op=\"&gt;\"):\n\"\"\"\n        Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        thr : float\n            THe value for thresholding\n        op : str, optional\n            The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.\n        Returns\n        -------\n        TsGroup\n            The new TsGroup\n        Raises\n        ------\n        RuntimeError\n            Raise eror is operation is not recognized.\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n          Index    Freq. (Hz)\n        -------  ------------\n              0             1\n              1             2\n              2             4\n        This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.\n        &gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n          Index    Freq. (Hz)\n        -------  ------------\n              1             2\n              2             4\n        \"\"\"\nif op == \"&gt;\":\nix = list(self._metadata.index[self._metadata[key] &gt; thr])\nreturn self[ix]\nelif op == \"&lt;\":\nix = list(self._metadata.index[self._metadata[key] &lt; thr])\nreturn self[ix]\nelif op == \"&gt;=\":\nix = list(self._metadata.index[self._metadata[key] &gt;= thr])\nreturn self[ix]\nelif op == \"&lt;=\":\nix = list(self._metadata.index[self._metadata[key] &lt;= thr])\nreturn self[ix]\nelse:\nraise RuntimeError(\"Operation {} not recognized.\".format(op))\ndef getby_intervals(self, key, bins):\n\"\"\"\n        Return a list of TsGroup binned.\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        bins : numpy.ndarray or list\n            The bin intervals\n        Returns\n        -------\n        list\n            A list of TsGroup\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n          Index    Freq. (Hz)    alpha\n        -------  ------------  -------\n              0             1        0\n              1             2        1\n              2             4        2\n        This exemple shows how to bin the TsGroup according to one metainfo key.\n        &gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n        &gt;&gt;&gt; newtsgroup\n        [  Index    Freq. (Hz)    alpha\n         -------  ------------  -------\n               0             1        0,\n           Index    Freq. (Hz)    alpha\n         -------  ------------  -------\n               1             2        1]\n        By default, the function returns the center of the bins.\n        &gt;&gt;&gt; bincenter\n        array([0.5, 1.5])\n        \"\"\"\nidx = np.digitize(self._metadata[key], bins) - 1\ngroups = self._metadata.index.groupby(idx)\nix = np.unique(list(groups.keys()))\nix = ix[ix &gt;= 0]\nix = ix[ix &lt; len(bins) - 1]\nxb = bins[0:-1] + np.diff(bins) / 2\nsliced = [self[list(groups[i])] for i in ix]\nreturn sliced, xb[ix]\ndef getby_category(self, key):\n\"\"\"\n        Return a list of TsGroup grouped by category.\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        Returns\n        -------\n        dict\n            A dictionnary of TsGroup\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n          Index    Freq. (Hz)    group\n        -------  ------------  -------\n              0             1        0\n              1             2        1\n              2             4        1\n        This exemple shows how to group the TsGroup according to one metainfo key.\n        &gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n        &gt;&gt;&gt; newtsgroup\n        {0:   Index    Freq. (Hz)    group\n         -------  ------------  -------\n               0             1        0,\n         1:   Index    Freq. (Hz)    group\n         -------  ------------  -------\n               1             2        1\n               2             4        1}\n        \"\"\"\ngroups = self._metadata.groupby(key).groups\nsliced = {k: self[list(groups[k])] for k in groups.keys()}\nreturn sliced\ndef save(self, filename):\n\"\"\"\n        Save TsGroup object in npz format. The file will contain the timestamps,\n        the data (if group of Tsd), group index, the time support and the metadata\n        The main purpose of this function is to save small/medium sized TsGroup\n        objects.\n        The function will \"flatten\" the TsGroup by sorting all the timestamps\n        and assigning to each the corresponding index. Typically, a TsGroup like\n        this :\n            TsGroup({\n                0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n                1 : Tsd(t=[1, 5], d=[5, 6])\n            })\n        will be saved as npz with the following keys:\n            {\n                't' : [0, 1, 2, 4, 5],\n                'd' : [1, 5, 2, 3, 5],\n                'index' : [0, 1, 0, 0, 1],\n                'start' : [0],\n                'end' : [5],\n                'type' : 'TsGroup'\n            }\n        Metadata are saved by columns with the column name as the npz key. To avoid\n        potential conflicts, make sure the columns name of the metadata are different\n        from ['t', 'd', 'start', 'end', 'index']\n        You can load the object with numpy.load. Default keys are 't', 'd'(optional),\n        'start', 'end', 'index' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsgroup = nap.TsGroup({\n            0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n            6 : nap.Ts(t=np.array([1.0, 5.0]))\n            },\n            group = np.array([0, 1]),\n            location = np.array(['right foot', 'left foot'])\n            )\n        &gt;&gt;&gt; tsgroup\n          Index    rate    group  location\n        -------  ------  -------  ----------\n              0     0.6        0  right foot\n              6     0.4        1  left foot\n        &gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['rate', 'group', 'location', 't', 'index', 'start', 'end', 'type']\n        &gt;&gt;&gt; print(file['index'])\n        [0 6 0 0 6]\n        In the case where TsGroup is a set of Ts objects, it is very direct to\n        recreate the TsGroup by using the function to_tsgroup :\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n        &gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n        &gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n        &gt;&gt;&gt; tsgroup\n          Index    rate    group  location\n        -------  ------  -------  ----------\n              0     0.6        0  right foot\n              6     0.4        1  left foot\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ndicttosave = {\"type\": np.array([\"TsGroup\"], dtype=np.str_)}\nfor k in self._metadata.columns:\nif k not in [\"t\", \"d\", \"start\", \"end\", \"index\"]:\ntmp = self._metadata[k].values\nif tmp.dtype == np.dtype(\"O\"):\ntmp = tmp.astype(np.str_)\ndicttosave[k] = tmp\n# We can't use to_tsd here in case tsgroup contains Tsd and not only Ts.\nnt = 0\nfor n in self.index:\nnt += len(self[n])\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nindex = np.zeros(nt, dtype=np.int64)\nk = 0\nfor n in self.index:\nkl = len(self[n])\ntimes[k : k + kl] = self[n].index\ndata[k : k + kl] = self[n].values\nindex[k : k + kl] = int(n)\nk += kl\nidx = np.argsort(times)\ntimes = times[idx]\nindex = index[idx]\ndicttosave[\"t\"] = times\ndicttosave[\"index\"] = index\nif not np.all(np.isnan(data)):\ndicttosave[\"d\"] = data[idx]\ndicttosave[\"start\"] = self.time_support.start.values\ndicttosave[\"end\"] = self.time_support.end.values\nnp.savez(filename, **dicttosave)\nreturn\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.rates","title":"rates  <code>property</code>","text":"<pre><code>rates\n</code></pre> <p>Return the rates of each element of the group in Hz</p>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.metadata_columns","title":"metadata_columns  <code>property</code>","text":"<pre><code>metadata_columns\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.metadata_columns--returns-list-of-metadata-columns","title":"Returns list of metadata columns","text":""},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.__init__","title":"__init__","text":"<pre><code>__init__(\ndata,\ntime_support=None,\ntime_units=\"s\",\nbypass_check=False,\n**kwargs\n)\n</code></pre> <p>TsGroup Initializer</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionnary containing Ts/Tsd objects</p> required <code>time_support</code> <code>IntervalSet</code> <p>The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed. If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.</p> <code>None</code> <code>time_units</code> <code>str</code> <p>Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).</p> <code>'s'</code> <code>bypass_check</code> <p>To avoid checking that each element is within time_support. Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand</p> <code>False</code> <code>**kwargs</code> <p>Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray. Note that the index should match the index of the input dictionnary.</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise error if the union of time support of Ts/Tsd object is empty.</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def __init__(\nself, data, time_support=None, time_units=\"s\", bypass_check=False, **kwargs\n):\n\"\"\"\n    TsGroup Initializer\n    Parameters\n    ----------\n    data : dict\n        Dictionnary containing Ts/Tsd objects\n    time_support : IntervalSet, optional\n        The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed.\n        If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.\n    time_units : str, optional\n        Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).\n    bypass_check: bool, optional\n        To avoid checking that each element is within time_support.\n        Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand\n    **kwargs\n        Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray.\n        Note that the index should match the index of the input dictionnary.\n    Raises\n    ------\n    RuntimeError\n        Raise error if the union of time support of Ts/Tsd object is empty.\n    \"\"\"\nself._initialized = False\nself.index = np.sort(list(data.keys()))\nself._metadata = pd.DataFrame(index=self.index, columns=[\"rate\"], dtype=\"float\")\n# Transform elements to Ts/Tsd objects\nfor k in self.index:\nif isinstance(data[k], (np.ndarray, list)):\nwarnings.warn(\n\"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\nstacklevel=2,\n)\ndata[k] = Ts(\nt=data[k], time_support=time_support, time_units=time_units\n)\n# If time_support is passed, all elements of data are restricted prior to init\nif isinstance(time_support, IntervalSet):\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nelse:\n# Otherwise do the union of all time supports\ntime_support = union_intervals([data[k].time_support for k in self.index])\nif len(time_support) == 0:\nraise RuntimeError(\n\"Union of time supports is empty. Consider passing a time support as argument.\"\n)\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nUserDict.__init__(self, data)\n# Making the TsGroup non mutable\nself._initialized = True\n# Trying to add argument as metainfo\nself.set_info(**kwargs)\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.keys","title":"keys","text":"<pre><code>keys()\n</code></pre> <p>Return index/keys of TsGroup</p> <p>Returns:</p> Type Description <code>list</code> <p>List of keys</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def keys(self):\n\"\"\"\n    Return index/keys of TsGroup\n    Returns\n    -------\n    list\n        List of keys\n    \"\"\"\nreturn list(self.data.keys())\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.items","title":"items","text":"<pre><code>items()\n</code></pre> <p>Return a list of key/object.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of tuples</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def items(self):\n\"\"\"\n    Return a list of key/object.\n    Returns\n    -------\n    list\n        List of tuples\n    \"\"\"\nreturn list(self.data.items())\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.values","title":"values","text":"<pre><code>values()\n</code></pre> <p>Return a list of all the Ts/Tsd objects in the TsGroup</p> <p>Returns:</p> Type Description <code>list</code> <p>List of Ts/Tsd objects</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def values(self):\n\"\"\"\n    Return a list of all the Ts/Tsd objects in the TsGroup\n    Returns\n    -------\n    list\n        List of Ts/Tsd objects\n    \"\"\"\nreturn list(self.data.values())\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.set_info","title":"set_info","text":"<pre><code>set_info(*args, **kwargs)\n</code></pre> <p>Add metadata informations about the TsGroup. Metadata are saved as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>pandas.Dataframe or list of pandas.DataFrame</p> <code>()</code> <code>**kwargs</code> <p>Can be either pandas.Series or numpy.ndarray</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise an error if     no column labels are found when passing simple arguments,     indexes are not equals for a pandas series,     not the same length when passing numpy array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n</code></pre> <p>To add metadata with a pandas.DataFrame:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n&gt;&gt;&gt; tsgroup.set_info(structs)\n&gt;&gt;&gt; tsgroup\n  Index    Freq. (Hz)  struct\n-------  ------------  --------\n      0             1  pfc\n      1             2  pfc\n      2             4  ca1\n</code></pre> <p>To add metadata with a pd.Series or numpy.ndarray:</p> <pre><code>&gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n&gt;&gt;&gt; tsgroup.set_info(hd=hd)\n&gt;&gt;&gt; tsgroup\n  Index    Freq. (Hz)  struct      hd\n-------  ------------  --------  ----\n      0             1  pfc          0\n      1             2  pfc          1\n      2             4  ca1          1\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def set_info(self, *args, **kwargs):\n\"\"\"\n    Add metadata informations about the TsGroup.\n    Metadata are saved as a DataFrame.\n    Parameters\n    ----------\n    *args\n        pandas.Dataframe or list of pandas.DataFrame\n    **kwargs\n        Can be either pandas.Series or numpy.ndarray\n    Raises\n    ------\n    RuntimeError\n        Raise an error if\n            no column labels are found when passing simple arguments,\n            indexes are not equals for a pandas series,\n            not the same length when passing numpy array.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    To add metadata with a pandas.DataFrame:\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n    &gt;&gt;&gt; tsgroup.set_info(structs)\n    &gt;&gt;&gt; tsgroup\n      Index    Freq. (Hz)  struct\n    -------  ------------  --------\n          0             1  pfc\n          1             2  pfc\n          2             4  ca1\n    To add metadata with a pd.Series or numpy.ndarray:\n    &gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n    &gt;&gt;&gt; tsgroup.set_info(hd=hd)\n    &gt;&gt;&gt; tsgroup\n      Index    Freq. (Hz)  struct      hd\n    -------  ------------  --------  ----\n          0             1  pfc          0\n          1             2  pfc          1\n          2             4  ca1          1\n    \"\"\"\nif len(args):\nfor arg in args:\nif isinstance(arg, pd.DataFrame):\nif pd.Index.equals(self._metadata.index, arg.index):\nself._metadata = self._metadata.join(arg)\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(arg, (pd.Series, np.ndarray)):\nraise RuntimeError(\"Columns needs to be labelled for metadata\")\nif len(kwargs):\nfor k, v in kwargs.items():\nif isinstance(v, pd.Series):\nif pd.Index.equals(self._metadata.index, v.index):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(v, np.ndarray):\nif len(self._metadata) == len(v):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Array is not the same length.\")\nreturn\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.get_info","title":"get_info","text":"<pre><code>get_info(key)\n</code></pre> <p>Returns the metainfo located in one column. The key for the column frequency is \"rate\".</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The metainfo</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def get_info(self, key):\n\"\"\"\n    Returns the metainfo located in one column.\n    The key for the column frequency is \"rate\".\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    Returns\n    -------\n    pandas.Series\n        The metainfo\n    \"\"\"\nif key in [\"freq\", \"frequency\"]:\nkey = \"rate\"\nreturn self._metadata[key]\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.restrict","title":"restrict","text":"<pre><code>restrict(ep)\n</code></pre> <p>Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>ep</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Type Description <code>TsGroup</code> <p>TsGroup object restricted to ep</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n&gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n</code></pre> <p>All objects within the TsGroup automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newtsgroup.time_support\n   start    end\n0    0.0  100.0\n&gt;&gt;&gt; newtsgroup[0].time_support\n   start    end\n0    0.0  100.0\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def restrict(self, ep):\n\"\"\"\n    Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    ep : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    TsGroup\n        TsGroup object restricted to ep\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    &gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n    All objects within the TsGroup automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newtsgroup.time_support\n       start    end\n    0    0.0  100.0\n    &gt;&gt;&gt; newtsgroup[0].time_support\n       start    end\n    0    0.0  100.0\n    \"\"\"\nnewgr = {}\nfor k in self.index:\nnewgr[k] = self.data[k].restrict(ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(\nnewgr, time_support=ep, bypass_check=True, **self._metadata[cols]\n)\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.value_from","title":"value_from","text":"<pre><code>value_from(tsd, ep=None)\n</code></pre> <p>Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>Tsd</code> <p>The Tsd object holding the values to replace</p> required <code>ep</code> <code>IntervalSet</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>TsGroup</code> <p>TsGroup object with the new values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n</code></pre> <p>The variable tsd is a time series object containing the values to assign, for example the tracking data:</p> <pre><code>&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n&gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def value_from(self, tsd, ep=None):\n\"\"\"\n    Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument\n    Parameters\n    ----------\n    tsd : Tsd\n        The Tsd object holding the values to replace\n    ep : IntervalSet\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    TsGroup\n        TsGroup object with the new values\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    The variable tsd is a time series object containing the values to assign, for example the tracking data:\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n    &gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n    \"\"\"\nif ep is None:\nep = tsd.time_support\nnewgr = {}\nfor k in self.data:\nnewgr[k] = self.data[k].value_from(tsd, ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(newgr, time_support=ep, **self._metadata[cols])\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.count","title":"count","text":"<pre><code>count(*args, **kwargs)\n</code></pre> <p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsgroup.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsgroup.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsgroup.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsgroup.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>TsdFrame</code> <p>A TsdFrame with the columns being the index of each item in the TsGroup.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second for the first 100 seconds.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n&gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n&gt;&gt;&gt; bincount\n          0  1  2\nTime (s)\n0.05      0  0  0\n0.15      0  0  0\n0.25      0  0  1\n0.35      0  0  0\n0.45      0  0  0\n...      .. .. ..\n99.55     0  1  1\n99.65     0  0  0\n99.75     0  0  1\n99.85     0  0  0\n99.95     1  1  1\n[1000 rows x 3 columns]\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsgroup.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsgroup.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsgroup.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsgroup.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: TsdFrame\n        A TsdFrame with the columns being the index of each item in the TsGroup.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second for the first 100 seconds.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    &gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n    &gt;&gt;&gt; bincount\n              0  1  2\n    Time (s)\n    0.05      0  0  0\n    0.15      0  0  0\n    0.25      0  0  1\n    0.35      0  0  0\n    0.45      0  0  0\n    ...      .. .. ..\n    99.55     0  1  1\n    99.65     0  0  0\n    99.75     0  0  1\n    99.85     0  0  0\n    99.95     1  1  1\n    [1000 rows x 3 columns]\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = float(bin_size)\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\ntime_index, _ = jitcount(np.array([]), starts, ends, bin_size)\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jitcount(\nself.data[self.index[i]].index, starts, ends, bin_size\n)[1]\nelse:\ntime_index = starts + (ends - starts) / 2\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jittsrestrict_with_count(\nself.data[self.index[i]].index, starts, ends\n)[1]\ntoreturn = TsdFrame(t=time_index, d=count, time_support=ep, columns=self.index)\nreturn toreturn\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.to_tsd","title":"to_tsd","text":"<pre><code>to_tsd(*args)\n</code></pre> <p>Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>string, list, numpy.ndarray or pandas.Series</p> <code>()</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\nIndex    rate\n-------  ------\n0       1\n5       1\n</code></pre> <p>By default, the values of the Tsd is the index of the timestamp in the TsGroup:</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd()\nTime (s)\n0.0    0.0\n1.0    0.0\n2.0    5.0\n3.0    5.0\ndtype: float64\n</code></pre> <p>Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.</p> <pre><code>&gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n&gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\nTime (s)\n0.0    3.141593\n1.0    3.141593\n2.0    6.283185\n3.0    6.283185\ndtype: float64\n</code></pre> <p>Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\nTime (s)\n0.0   -1.0\n1.0   -1.0\n2.0    1.0\n3.0    1.0\ndtype: float64\n</code></pre> <p>The reverse operation can be done with the Tsd.to_tsgroup function :</p> <pre><code>&gt;&gt;&gt; my_tsd\nTime (s)\n0.0    0.0\n1.0    0.0\n2.0    5.0\n3.0    5.0\ndtype: float64\n&gt;&gt;&gt; my_tsd.to_tsgroup()\n  Index    rate\n-------  ------\n      0       1\n      5       1\n</code></pre> <p>Returns:</p> Type Description <code>Tsd</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>\"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata, \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def to_tsd(self, *args):\n\"\"\"\n    Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.\n    Parameters\n    ----------\n    *args\n        string, list, numpy.ndarray or pandas.Series\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\n    Index    rate\n    -------  ------\n    0       1\n    5       1\n    By default, the values of the Tsd is the index of the timestamp in the TsGroup:\n    &gt;&gt;&gt; tsgroup.to_tsd()\n    Time (s)\n    0.0    0.0\n    1.0    0.0\n    2.0    5.0\n    3.0    5.0\n    dtype: float64\n    Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.\n    &gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n    &gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\n    Time (s)\n    0.0    3.141593\n    1.0    3.141593\n    2.0    6.283185\n    3.0    6.283185\n    dtype: float64\n    Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :\n    &gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\n    Time (s)\n    0.0   -1.0\n    1.0   -1.0\n    2.0    1.0\n    3.0    1.0\n    dtype: float64\n    The reverse operation can be done with the Tsd.to_tsgroup function :\n    &gt;&gt;&gt; my_tsd\n    Time (s)\n    0.0    0.0\n    1.0    0.0\n    2.0    5.0\n    3.0    5.0\n    dtype: float64\n    &gt;&gt;&gt; my_tsd.to_tsgroup()\n      Index    rate\n    -------  ------\n          0       1\n          5       1\n    Returns\n    -------\n    Tsd\n    Raises\n    ------\n    RuntimeError\n        \"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes\n        \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object\n        \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata,\n        \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series\n    \"\"\"\nif len(args):\nif isinstance(args[0], pd.Series):\nif pd.Index.equals(self._metadata.index, args[0].index):\n_values = args[0].values.flatten()\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(args[0], (np.ndarray, list)):\nif len(self._metadata) == len(args[0]):\n_values = np.array(args[0])\nelse:\nraise RuntimeError(\"Values is not the same length.\")\nelif isinstance(args[0], str):\nif args[0] in self._metadata.columns:\n_values = self._metadata[args[0]].values\nelse:\nraise RuntimeError(\n\"Key {} not in metadata of TsGroup\".format(args[0])\n)\nelse:\npossible_keys = []\nfor k, d in self._metadata.dtypes.items():\nif \"int\" in str(d) or \"float\" in str(d):\npossible_keys.append(k)\nraise RuntimeError(\n\"Unknown argument format. Must be pandas.Series, numpy.ndarray or a string from one of the following values : [{}]\".format(\n\", \".join(possible_keys)\n)\n)\nelse:\n_values = self.index\nnt = 0\nfor n in self.index:\nnt += len(self[n])\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nk = 0\nfor n, v in zip(self.index, _values):\nkl = len(self[n])\ntimes[k : k + kl] = self[n].index\ndata[k : k + kl] = v\nk += kl\nidx = np.argsort(times)\ntoreturn = Tsd(t=times[idx], d=data[idx], time_support=self.time_support)\nreturn toreturn\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.getby_threshold","title":"getby_threshold","text":"<pre><code>getby_threshold(key, thr, op='&gt;')\n</code></pre> <p>Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <code>thr</code> <code>float</code> <p>THe value for thresholding</p> required <code>op</code> <code>str</code> <p>The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.</p> <code>'&gt;'</code> <p>Returns:</p> Type Description <code>TsGroup</code> <p>The new TsGroup</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise eror is operation is not recognized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n  Index    Freq. (Hz)\n-------  ------------\n      0             1\n      1             2\n      2             4\n</code></pre> <p>This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.</p> <pre><code>&gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n  Index    Freq. (Hz)\n-------  ------------\n      1             2\n      2             4\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_threshold(self, key, thr, op=\"&gt;\"):\n\"\"\"\n    Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    thr : float\n        THe value for thresholding\n    op : str, optional\n        The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.\n    Returns\n    -------\n    TsGroup\n        The new TsGroup\n    Raises\n    ------\n    RuntimeError\n        Raise eror is operation is not recognized.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n      Index    Freq. (Hz)\n    -------  ------------\n          0             1\n          1             2\n          2             4\n    This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.\n    &gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n      Index    Freq. (Hz)\n    -------  ------------\n          1             2\n          2             4\n    \"\"\"\nif op == \"&gt;\":\nix = list(self._metadata.index[self._metadata[key] &gt; thr])\nreturn self[ix]\nelif op == \"&lt;\":\nix = list(self._metadata.index[self._metadata[key] &lt; thr])\nreturn self[ix]\nelif op == \"&gt;=\":\nix = list(self._metadata.index[self._metadata[key] &gt;= thr])\nreturn self[ix]\nelif op == \"&lt;=\":\nix = list(self._metadata.index[self._metadata[key] &lt;= thr])\nreturn self[ix]\nelse:\nraise RuntimeError(\"Operation {} not recognized.\".format(op))\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.getby_intervals","title":"getby_intervals","text":"<pre><code>getby_intervals(key, bins)\n</code></pre> <p>Return a list of TsGroup binned.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <code>bins</code> <code>ndarray or list</code> <p>The bin intervals</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of TsGroup</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n  Index    Freq. (Hz)    alpha\n-------  ------------  -------\n      0             1        0\n      1             2        1\n      2             4        2\n</code></pre> <p>This exemple shows how to bin the TsGroup according to one metainfo key.</p> <pre><code>&gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n&gt;&gt;&gt; newtsgroup\n[  Index    Freq. (Hz)    alpha\n -------  ------------  -------\n       0             1        0,\n   Index    Freq. (Hz)    alpha\n -------  ------------  -------\n       1             2        1]\n</code></pre> <p>By default, the function returns the center of the bins.</p> <pre><code>&gt;&gt;&gt; bincenter\narray([0.5, 1.5])\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_intervals(self, key, bins):\n\"\"\"\n    Return a list of TsGroup binned.\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    bins : numpy.ndarray or list\n        The bin intervals\n    Returns\n    -------\n    list\n        A list of TsGroup\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n      Index    Freq. (Hz)    alpha\n    -------  ------------  -------\n          0             1        0\n          1             2        1\n          2             4        2\n    This exemple shows how to bin the TsGroup according to one metainfo key.\n    &gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n    &gt;&gt;&gt; newtsgroup\n    [  Index    Freq. (Hz)    alpha\n     -------  ------------  -------\n           0             1        0,\n       Index    Freq. (Hz)    alpha\n     -------  ------------  -------\n           1             2        1]\n    By default, the function returns the center of the bins.\n    &gt;&gt;&gt; bincenter\n    array([0.5, 1.5])\n    \"\"\"\nidx = np.digitize(self._metadata[key], bins) - 1\ngroups = self._metadata.index.groupby(idx)\nix = np.unique(list(groups.keys()))\nix = ix[ix &gt;= 0]\nix = ix[ix &lt; len(bins) - 1]\nxb = bins[0:-1] + np.diff(bins) / 2\nsliced = [self[list(groups[i])] for i in ix]\nreturn sliced, xb[ix]\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.getby_category","title":"getby_category","text":"<pre><code>getby_category(key)\n</code></pre> <p>Return a list of TsGroup grouped by category.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionnary of TsGroup</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n  Index    Freq. (Hz)    group\n-------  ------------  -------\n      0             1        0\n      1             2        1\n      2             4        1\n</code></pre> <p>This exemple shows how to group the TsGroup according to one metainfo key.</p> <pre><code>&gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n&gt;&gt;&gt; newtsgroup\n{0:   Index    Freq. (Hz)    group\n -------  ------------  -------\n       0             1        0,\n 1:   Index    Freq. (Hz)    group\n -------  ------------  -------\n       1             2        1\n       2             4        1}\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_category(self, key):\n\"\"\"\n    Return a list of TsGroup grouped by category.\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    Returns\n    -------\n    dict\n        A dictionnary of TsGroup\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n      Index    Freq. (Hz)    group\n    -------  ------------  -------\n          0             1        0\n          1             2        1\n          2             4        1\n    This exemple shows how to group the TsGroup according to one metainfo key.\n    &gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n    &gt;&gt;&gt; newtsgroup\n    {0:   Index    Freq. (Hz)    group\n     -------  ------------  -------\n           0             1        0,\n     1:   Index    Freq. (Hz)    group\n     -------  ------------  -------\n           1             2        1\n           2             4        1}\n    \"\"\"\ngroups = self._metadata.groupby(key).groups\nsliced = {k: self[list(groups[k])] for k in groups.keys()}\nreturn sliced\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.save","title":"save","text":"<pre><code>save(filename)\n</code></pre> <p>Save TsGroup object in npz format. The file will contain the timestamps, the data (if group of Tsd), group index, the time support and the metadata</p> <p>The main purpose of this function is to save small/medium sized TsGroup objects.</p> <p>The function will \"flatten\" the TsGroup by sorting all the timestamps and assigning to each the corresponding index. Typically, a TsGroup like this :</p> <pre><code>TsGroup({\n    0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n    1 : Tsd(t=[1, 5], d=[5, 6])\n})\n</code></pre> <p>will be saved as npz with the following keys:</p> <pre><code>{\n    't' : [0, 1, 2, 4, 5],\n    'd' : [1, 5, 2, 3, 5],\n    'index' : [0, 1, 0, 0, 1],\n    'start' : [0],\n    'end' : [5],\n    'type' : 'TsGroup'\n}\n</code></pre> <p>Metadata are saved by columns with the column name as the npz key. To avoid potential conflicts, make sure the columns name of the metadata are different from ['t', 'd', 'start', 'end', 'index']</p> <p>You can load the object with numpy.load. Default keys are 't', 'd'(optional), 'start', 'end', 'index' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsgroup = nap.TsGroup({\n    0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n    6 : nap.Ts(t=np.array([1.0, 5.0]))\n    },\n    group = np.array([0, 1]),\n    location = np.array(['right foot', 'left foot'])\n    )\n&gt;&gt;&gt; tsgroup\n  Index    rate    group  location\n-------  ------  -------  ----------\n      0     0.6        0  right foot\n      6     0.4        1  left foot\n&gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['rate', 'group', 'location', 't', 'index', 'start', 'end', 'type']\n&gt;&gt;&gt; print(file['index'])\n[0 6 0 0 6]\n</code></pre> <p>In the case where TsGroup is a set of Ts objects, it is very direct to recreate the TsGroup by using the function to_tsgroup :</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n&gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n&gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n&gt;&gt;&gt; tsgroup\n  Index    rate    group  location\n-------  ------  -------  ----------\n      0     0.6        0  right foot\n      6     0.4        1  left foot\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save TsGroup object in npz format. The file will contain the timestamps,\n    the data (if group of Tsd), group index, the time support and the metadata\n    The main purpose of this function is to save small/medium sized TsGroup\n    objects.\n    The function will \"flatten\" the TsGroup by sorting all the timestamps\n    and assigning to each the corresponding index. Typically, a TsGroup like\n    this :\n        TsGroup({\n            0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n            1 : Tsd(t=[1, 5], d=[5, 6])\n        })\n    will be saved as npz with the following keys:\n        {\n            't' : [0, 1, 2, 4, 5],\n            'd' : [1, 5, 2, 3, 5],\n            'index' : [0, 1, 0, 0, 1],\n            'start' : [0],\n            'end' : [5],\n            'type' : 'TsGroup'\n        }\n    Metadata are saved by columns with the column name as the npz key. To avoid\n    potential conflicts, make sure the columns name of the metadata are different\n    from ['t', 'd', 'start', 'end', 'index']\n    You can load the object with numpy.load. Default keys are 't', 'd'(optional),\n    'start', 'end', 'index' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsgroup = nap.TsGroup({\n        0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n        6 : nap.Ts(t=np.array([1.0, 5.0]))\n        },\n        group = np.array([0, 1]),\n        location = np.array(['right foot', 'left foot'])\n        )\n    &gt;&gt;&gt; tsgroup\n      Index    rate    group  location\n    -------  ------  -------  ----------\n          0     0.6        0  right foot\n          6     0.4        1  left foot\n    &gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['rate', 'group', 'location', 't', 'index', 'start', 'end', 'type']\n    &gt;&gt;&gt; print(file['index'])\n    [0 6 0 0 6]\n    In the case where TsGroup is a set of Ts objects, it is very direct to\n    recreate the TsGroup by using the function to_tsgroup :\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n    &gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n    &gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n    &gt;&gt;&gt; tsgroup\n      Index    rate    group  location\n    -------  ------  -------  ----------\n          0     0.6        0  right foot\n          6     0.4        1  left foot\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ndicttosave = {\"type\": np.array([\"TsGroup\"], dtype=np.str_)}\nfor k in self._metadata.columns:\nif k not in [\"t\", \"d\", \"start\", \"end\", \"index\"]:\ntmp = self._metadata[k].values\nif tmp.dtype == np.dtype(\"O\"):\ntmp = tmp.astype(np.str_)\ndicttosave[k] = tmp\n# We can't use to_tsd here in case tsgroup contains Tsd and not only Ts.\nnt = 0\nfor n in self.index:\nnt += len(self[n])\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nindex = np.zeros(nt, dtype=np.int64)\nk = 0\nfor n in self.index:\nkl = len(self[n])\ntimes[k : k + kl] = self[n].index\ndata[k : k + kl] = self[n].values\nindex[k : k + kl] = int(n)\nk += kl\nidx = np.argsort(times)\ntimes = times[idx]\nindex = index[idx]\ndicttosave[\"t\"] = times\ndicttosave[\"index\"] = index\nif not np.all(np.isnan(data)):\ndicttosave[\"d\"] = data[idx]\ndicttosave[\"start\"] = self.time_support.start.values\ndicttosave[\"end\"] = self.time_support.end.values\nnp.savez(filename, **dicttosave)\nreturn\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.union_intervals","title":"union_intervals","text":"<pre><code>union_intervals(i_sets)\n</code></pre> <p>Helper to merge intervals from ts_group</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def union_intervals(i_sets):\n\"\"\"\n    Helper to merge intervals from ts_group\n    \"\"\"\nn = len(i_sets)\nif n == 1:\nreturn i_sets[0]\nnew_start = np.zeros(0)\nnew_end = np.zeros(0)\nif n == 2:\nnew_start, new_end = jitunion(\ni_sets[0].start.values,\ni_sets[0].end.values,\ni_sets[1].start.values,\ni_sets[1].end.values,\n)\nif n &gt; 2:\nsizes = np.array([i_sets[i].shape[0] for i in range(n)])\nstartends = np.zeros((np.sum(sizes), 2))\nct = 0\nfor i in range(sizes.shape[0]):\nstartends[ct : ct + sizes[i], :] = i_sets[i].values\nct += sizes[i]\nnew_start, new_end = jitunion_isets(startends[:, 0], startends[:, 1])\nreturn IntervalSet(new_start, new_end)\n</code></pre>"},{"location":"old_pages/io.cnmfe/","title":"Io.cnmfe","text":""},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe","title":"pynapple.io.cnmfe","text":"<p> DEPRECATED: This will be removed in version 1.0.0. Check nwbmatic or neuroconv instead.</p> <p>Loaders for calcium imaging data with miniscope. Support CNMF-E in matlab, inscopix-cnmfe and minian.</p>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E","title":"CNMF_E","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E). The path folder should contain a file ending in .mat when calling Source2d.save_neurons</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>ndarray</code> <p>Spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class CNMF_E(BaseLoader):\n\"\"\"Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E).\n    The path folder should contain a file ending in .mat\n    when calling Source2d.save_neurons\n    Attributes\n    ----------\n    A : numpy.ndarray\n        Spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_cnmf_e(path)\nself.save_cnmfe_nwb(path)\ndef load_cnmf_e(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nfiles = os.listdir(path)\nmatfiles = [f for f in files if f.endswith(\".mat\")]\nif len(matfiles):\ndata = loadmat(os.path.join(path, matfiles[0]), struct_as_record=False)\nelse:\nraise RuntimeError(\"No mat file found in {}\".format(path))\nself.struct = data[\"neuron_results\"][0][0]\nC = self.struct.C.T\nself.A = self.struct.A.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = np.atleast_2d(self.A[i])\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_cnmf_e(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmf_e","title":"load_cnmf_e","text":"<pre><code>load_cnmf_e(path)\n</code></pre> <p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmf_e(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nfiles = os.listdir(path)\nmatfiles = [f for f in files if f.endswith(\".mat\")]\nif len(matfiles):\ndata = loadmat(os.path.join(path, matfiles[0]), struct_as_record=False)\nelse:\nraise RuntimeError(\"No mat file found in {}\".format(path))\nself.struct = data[\"neuron_results\"][0][0]\nC = self.struct.C.T\nself.A = self.struct.A.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nreturn None\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.save_cnmfe_nwb","title":"save_cnmfe_nwb","text":"<pre><code>save_cnmfe_nwb(path)\n</code></pre> <p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = np.atleast_2d(self.A[i])\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmfe_nwb","title":"load_cnmfe_nwb","text":"<pre><code>load_cnmfe_nwb(path)\n</code></pre> <p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian","title":"Minian","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader for data processed with Minian (https://github.com/denisecailab/minian). The path folder should contain a subfolder name minian.</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>ndarray</code> <p>Spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class Minian(BaseLoader):\n\"\"\"Loader for data processed with Minian (https://github.com/denisecailab/minian).\n    The path folder should contain a subfolder name minian.\n    Attributes\n    ----------\n    A : numpy.ndarray\n        Spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_minian(path)\nself.save_cnmfe_nwb(path)\ndef load_minian(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nminian_folder = os.path.join(path, \"minian\")\nif not os.path.exists(minian_folder):\nraise RuntimeError(\"Path {} does not contain a minian folder\".format(path))\ntry:\nimport zarr\nexcept ImportError as ie:\nprint(\"Please install module zarr for loading minian data\", ie)\nsys.exit()\ndata = zarr.open(minian_folder, \"r\")\nC = data[\"C.zarr\"][\"C\"][:]\nC = C.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nself.A = data[\"A.zarr\"][\"A\"][:]\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_minian(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.load_minian","title":"load_minian","text":"<pre><code>load_minian(path)\n</code></pre> <p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_minian(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nminian_folder = os.path.join(path, \"minian\")\nif not os.path.exists(minian_folder):\nraise RuntimeError(\"Path {} does not contain a minian folder\".format(path))\ntry:\nimport zarr\nexcept ImportError as ie:\nprint(\"Please install module zarr for loading minian data\", ie)\nsys.exit()\ndata = zarr.open(minian_folder, \"r\")\nC = data[\"C.zarr\"][\"C\"][:]\nC = C.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nself.A = data[\"A.zarr\"][\"A\"][:]\nreturn None\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.save_cnmfe_nwb","title":"save_cnmfe_nwb","text":"<pre><code>save_cnmfe_nwb(path)\n</code></pre> <p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.load_cnmfe_nwb","title":"load_cnmfe_nwb","text":"<pre><code>load_cnmfe_nwb(path)\n</code></pre> <p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE","title":"InscopixCNMFE","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe). The folder should contain a file ending with '_traces.csv' and a tiff file for spatial footprints.</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>ndarray</code> <p>The spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class InscopixCNMFE(BaseLoader):\n\"\"\"Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe).\n    The folder should contain a file ending with '_traces.csv'\n    and a tiff file for spatial footprints.\n    Attributes\n    ----------\n    A : np.ndarray\n        The spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_inscopix_cnmfe(path)\nself.save_cnmfe_nwb(path)\ndef load_inscopix_cnmfe(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nfiles = os.listdir(path)\ntracefile = [f for f in files if f.endswith(\"_traces.csv\")]\nif len(tracefile):\nC = pd.read_csv(os.path.join(path, tracefile[0]), index_col=0)\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*_traces.csv\")\n)\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C.values)\ntry:\nimport tifffile as tiff\nexcept ImportError as ie:\nprint(\"Please install module tifffile for loading inscopix-cnmfe data\", ie)\nsys.exit()\ntifffile = [f for f in files if f.endswith(\".tiff\")]\nif len(tifffile):\nself.A = tiff.imread(os.path.join(path, tifffile[0]))\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*.tiff\")\n)\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_inscopix_cnmfe(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_inscopix_cnmfe","title":"load_inscopix_cnmfe","text":"<pre><code>load_inscopix_cnmfe(path)\n</code></pre> <p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_inscopix_cnmfe(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nfiles = os.listdir(path)\ntracefile = [f for f in files if f.endswith(\"_traces.csv\")]\nif len(tracefile):\nC = pd.read_csv(os.path.join(path, tracefile[0]), index_col=0)\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*_traces.csv\")\n)\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C.values)\ntry:\nimport tifffile as tiff\nexcept ImportError as ie:\nprint(\"Please install module tifffile for loading inscopix-cnmfe data\", ie)\nsys.exit()\ntifffile = [f for f in files if f.endswith(\".tiff\")]\nif len(tifffile):\nself.A = tiff.imread(os.path.join(path, tifffile[0]))\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*.tiff\")\n)\nreturn None\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.save_cnmfe_nwb","title":"save_cnmfe_nwb","text":"<pre><code>save_cnmfe_nwb(path)\n</code></pre> <p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_cnmfe_nwb","title":"load_cnmfe_nwb","text":"<pre><code>load_cnmfe_nwb(path)\n</code></pre> <p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.folder/","title":"Io.folder","text":""},{"location":"old_pages/io.folder/#pynapple.io.folder","title":"pynapple.io.folder","text":"<p>The Folder class helps to navigate a hierarchical data tree.</p>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder","title":"Folder","text":"<p>             Bases: <code>UserDict</code></p> <p>Base class for all type of folders (i.e. Project, Subject, Sessions, ...). Handles files and sub-folders discovery</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>dict</code> <p>Dictionnary holidng all the pynapple objects found in the folder.</p> <code>name</code> <code>str</code> <p>Name of the folder</p> <code>npz_files</code> <code>list</code> <p>List of npz files found in the folder</p> <code>nwb_files</code> <code>list</code> <p>List of nwb files found in the folder</p> <code>path</code> <code>str</code> <p>Absolute path of the folder</p> <code>subfolds</code> <code>dict</code> <p>Dictionnary of all the subfolders</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>class Folder(UserDict):\n\"\"\"\n    Base class for all type of folders (i.e. Project, Subject, Sessions, ...).\n    Handles files and sub-folders discovery\n    Attributes\n    ----------\n    data : dict\n        Dictionnary holidng all the pynapple objects found in the folder.\n    name : str\n        Name of the folder\n    npz_files : list\n        List of npz files found in the folder\n    nwb_files : list\n        List of nwb files found in the folder\n    path : str\n        Absolute path of the folder\n    subfolds : dict\n        Dictionnary of all the subfolders\n    \"\"\"\ndef __init__(self, path):  # , exclude=(), max_depth=4):\n\"\"\"Initialize the Folder object\n        Parameters\n        ----------\n        path : str\n            Path to the folder\n        \"\"\"\npath = path.rstrip(\"/\")\nself.path = path\nself.name = os.path.basename(path)\nself._basic_view = Tree(\n\":open_file_folder: {}\".format(self.name), guide_style=\"blue\"\n)\nself._full_view = None\n# Search sub-folders\nsubfolds = [\nf.path\nfor f in os.scandir(path)\nif f.is_dir() and not f.name.startswith(\".\")\n]\nsubfolds.sort()\nself.subfolds = {}\nfor s in subfolds:\nsub = os.path.basename(s)\nself.subfolds[sub] = Folder(s)\nself._basic_view.add(\":open_file_folder: [blue]\" + sub)\n# Search files\nself.npz_files = _find_files(path, \"npz\")\nself.nwb_files = _find_files(path, \"nwb\")\nfor filename, file in self.npz_files.items():\nself._basic_view.add(\"[green]\" + file.name + \" \\t|\\t \" + file.type)\nfor file in self.nwb_files.values():\nself._basic_view.add(\"[magenta]\" + file.name + \" \\t|\\t NWB file\")\n# Putting everything together\nself.data = {**self.npz_files, **self.nwb_files, **self.subfolds}\nUserDict.__init__(self, self.data)\ndef __str__(self):\n\"\"\"View of the object\"\"\"\nwith Console() as console:\nconsole.print(self._basic_view)\nreturn \"\"\n# def __repr__(self):\n#     \"\"\"View of the object\"\"\"\n#     print(self._basic_view)\ndef __getitem__(self, key):\n\"\"\"Get subfolder or load file.\n        Parameters\n        ----------\n        key : str\n        Returns\n        -------\n        (Ts, Tsd, TsdFrame, TsGroup, IntervalSet, Folder or NWBFile)\n        Raises\n        ------\n        KeyError\n            If key is not in the dictionnary\n        \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], NPZFile):\ndata = self.data[key].load()\nself.data[key] = data\n# setattr(self, key, data)\nreturn data\nelif isinstance(self.data[key], NWBFile):\nreturn self.data[key]\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n# # # Gets called when an attribute is accessed\n# def __getattribute__(self, item):\n#     value = super(Folder, self).__getattribute__(item)\n#     if isinstance(value, NPZFile):\n#         data = value.load()\n#         setattr(self, item, data)\n#         self.data[item] = data\n#         return data\n#     else:\n#         return value\ndef _generate_tree_view(self):\ntree = Tree(\":open_file_folder: {}\".format(self.name), guide_style=\"blue\")\n# Folder\nfor fold in self.subfolds.keys():\ntree.add(\":open_file_folder: \" + fold)\n_walk_folder(tree.children[-1], self.subfolds[fold])\n# NPZ files\nfor file in self.npz_files.values():\ntree.add(\"[green]\" + file.name + \" \\t|\\t \" + file.type)\n# NWB files\nfor file in self.nwb_files.values():\ntree.add(\"[magenta]\" + file.name + \" \\t|\\t NWB file\")\nself._full_view = tree\ndef expand(self):\n\"\"\"Display the full tree view. Equivalent to Folder.view\"\"\"\nif not isinstance(self._full_view, Tree):\nself._generate_tree_view()\nwith Console() as console:\nconsole.print(self._full_view)\nreturn None\n@property\ndef view(self):\n\"\"\"Summary\"\"\"\nreturn self.expand()\ndef save(self, name, obj, description=\"\"):\n\"\"\"Save a pynapple object in the folder in a single file in uncompressed ``.npz`` format.\n        By default, the save function overwrite previously save file with the same name.\n        Parameters\n        ----------\n        name : str\n            Filename\n        obj : Ts, Tsd, TsdFrame, TsGroup or IntervalSet\n            Pynapple object.\n        description : str, optional\n            Metainformation added as a json sidecar.\n        \"\"\"\nfilepath = os.path.join(self.path, name)\nobj.save(filepath)\nself.npz_files[name] = NPZFile(filepath + \".npz\")\nself.data[name] = obj\nmetadata = {\"time\": str(datetime.now()), \"info\": str(description)}\nwith open(os.path.join(self.path, name + \".json\"), \"w\") as ff:\njson.dump(metadata, ff, indent=2)\n# regenerate the tree view\nself._generate_tree_view()\ndef load(self):\n\"\"\"Load all compatible NPZ files.\"\"\"\nfor k in self.npz_files.keys():\nself[k] = self.npz_files[k].load()\n# def add_metadata(self):\n#     \"\"\"Summary\"\"\"\n#     pass\ndef info(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n        Parameters\n        ----------\n        name : str\n            Name of the npz file\n        \"\"\"\nself.metadata(name)\ndef doc(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n        Parameters\n        ----------\n        name : str\n            Name of the npz file\n        \"\"\"\nself.metadata(name)\ndef metadata(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n        Parameters\n        ----------\n        name : str\n            Name of the npz file\n        \"\"\"\n# Search for json first\njson_filename = os.path.join(self.path, name + \".json\")\nif os.path.isfile(json_filename):\nwith open(json_filename, \"r\") as ff:\nmetadata = json.load(ff)\ntext = \"\\n\".join([\" : \".join(it) for it in metadata.items()])\npanel = Panel.fit(\ntext, border_style=\"green\", title=os.path.join(self.path, name + \".npz\")\n)\nelse:\npanel = Panel.fit(\n\"No metadata\",\nborder_style=\"red\",\ntitle=os.path.join(self.path, name + \".npz\"),\n)\nwith Console() as console:\nconsole.print(panel)\nreturn None\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.view","title":"view  <code>property</code>","text":"<pre><code>view\n</code></pre> <p>Summary</p>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Initialize the Folder object</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the folder</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def __init__(self, path):  # , exclude=(), max_depth=4):\n\"\"\"Initialize the Folder object\n    Parameters\n    ----------\n    path : str\n        Path to the folder\n    \"\"\"\npath = path.rstrip(\"/\")\nself.path = path\nself.name = os.path.basename(path)\nself._basic_view = Tree(\n\":open_file_folder: {}\".format(self.name), guide_style=\"blue\"\n)\nself._full_view = None\n# Search sub-folders\nsubfolds = [\nf.path\nfor f in os.scandir(path)\nif f.is_dir() and not f.name.startswith(\".\")\n]\nsubfolds.sort()\nself.subfolds = {}\nfor s in subfolds:\nsub = os.path.basename(s)\nself.subfolds[sub] = Folder(s)\nself._basic_view.add(\":open_file_folder: [blue]\" + sub)\n# Search files\nself.npz_files = _find_files(path, \"npz\")\nself.nwb_files = _find_files(path, \"nwb\")\nfor filename, file in self.npz_files.items():\nself._basic_view.add(\"[green]\" + file.name + \" \\t|\\t \" + file.type)\nfor file in self.nwb_files.values():\nself._basic_view.add(\"[magenta]\" + file.name + \" \\t|\\t NWB file\")\n# Putting everything together\nself.data = {**self.npz_files, **self.nwb_files, **self.subfolds}\nUserDict.__init__(self, self.data)\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>View of the object</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def __str__(self):\n\"\"\"View of the object\"\"\"\nwith Console() as console:\nconsole.print(self._basic_view)\nreturn \"\"\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key)\n</code></pre> <p>Get subfolder or load file.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required <p>Returns:</p> Type Description <code>(Ts, Tsd, TsdFrame, TsGroup, IntervalSet, Folder or NWBFile)</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If key is not in the dictionnary</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def __getitem__(self, key):\n\"\"\"Get subfolder or load file.\n    Parameters\n    ----------\n    key : str\n    Returns\n    -------\n    (Ts, Tsd, TsdFrame, TsGroup, IntervalSet, Folder or NWBFile)\n    Raises\n    ------\n    KeyError\n        If key is not in the dictionnary\n    \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], NPZFile):\ndata = self.data[key].load()\nself.data[key] = data\n# setattr(self, key, data)\nreturn data\nelif isinstance(self.data[key], NWBFile):\nreturn self.data[key]\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.expand","title":"expand","text":"<pre><code>expand()\n</code></pre> <p>Display the full tree view. Equivalent to Folder.view</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def expand(self):\n\"\"\"Display the full tree view. Equivalent to Folder.view\"\"\"\nif not isinstance(self._full_view, Tree):\nself._generate_tree_view()\nwith Console() as console:\nconsole.print(self._full_view)\nreturn None\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.save","title":"save","text":"<pre><code>save(name, obj, description='')\n</code></pre> <p>Save a pynapple object in the folder in a single file in uncompressed <code>.npz</code> format. By default, the save function overwrite previously save file with the same name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Filename</p> required <code>obj</code> <code>(Ts, Tsd, TsdFrame, TsGroup or IntervalSet)</code> <p>Pynapple object.</p> required <code>description</code> <code>str</code> <p>Metainformation added as a json sidecar.</p> <code>''</code> Source code in <code>pynapple/io/folder.py</code> <pre><code>def save(self, name, obj, description=\"\"):\n\"\"\"Save a pynapple object in the folder in a single file in uncompressed ``.npz`` format.\n    By default, the save function overwrite previously save file with the same name.\n    Parameters\n    ----------\n    name : str\n        Filename\n    obj : Ts, Tsd, TsdFrame, TsGroup or IntervalSet\n        Pynapple object.\n    description : str, optional\n        Metainformation added as a json sidecar.\n    \"\"\"\nfilepath = os.path.join(self.path, name)\nobj.save(filepath)\nself.npz_files[name] = NPZFile(filepath + \".npz\")\nself.data[name] = obj\nmetadata = {\"time\": str(datetime.now()), \"info\": str(description)}\nwith open(os.path.join(self.path, name + \".json\"), \"w\") as ff:\njson.dump(metadata, ff, indent=2)\n# regenerate the tree view\nself._generate_tree_view()\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.load","title":"load","text":"<pre><code>load()\n</code></pre> <p>Load all compatible NPZ files.</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def load(self):\n\"\"\"Load all compatible NPZ files.\"\"\"\nfor k in self.npz_files.keys():\nself[k] = self.npz_files[k].load()\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.info","title":"info","text":"<pre><code>info(name)\n</code></pre> <p>Display the metadata within the json sidecar of a NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the npz file</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def info(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n    Parameters\n    ----------\n    name : str\n        Name of the npz file\n    \"\"\"\nself.metadata(name)\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.doc","title":"doc","text":"<pre><code>doc(name)\n</code></pre> <p>Display the metadata within the json sidecar of a NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the npz file</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def doc(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n    Parameters\n    ----------\n    name : str\n        Name of the npz file\n    \"\"\"\nself.metadata(name)\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.metadata","title":"metadata","text":"<pre><code>metadata(name)\n</code></pre> <p>Display the metadata within the json sidecar of a NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the npz file</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def metadata(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n    Parameters\n    ----------\n    name : str\n        Name of the npz file\n    \"\"\"\n# Search for json first\njson_filename = os.path.join(self.path, name + \".json\")\nif os.path.isfile(json_filename):\nwith open(json_filename, \"r\") as ff:\nmetadata = json.load(ff)\ntext = \"\\n\".join([\" : \".join(it) for it in metadata.items()])\npanel = Panel.fit(\ntext, border_style=\"green\", title=os.path.join(self.path, name + \".npz\")\n)\nelse:\npanel = Panel.fit(\n\"No metadata\",\nborder_style=\"red\",\ntitle=os.path.join(self.path, name + \".npz\"),\n)\nwith Console() as console:\nconsole.print(panel)\nreturn None\n</code></pre>"},{"location":"old_pages/io.loader/","title":"Io.loader","text":""},{"location":"old_pages/io.loader/#pynapple.io.loader","title":"pynapple.io.loader","text":"<p>BaseLoader is the general class for loading session with pynapple.</p> <p>@author: Guillaume Viejo</p>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader","title":"BaseLoader","text":"<p>             Bases: <code>object</code></p> <p>General loader for epochs and tracking data</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>class BaseLoader(object):\n\"\"\"\n    General loader for epochs and tracking data\n    \"\"\"\ndef __init__(self, path=None):\nself.path = path\nstart_gui = True\n# Check if a pynapplenwb folder exist to bypass GUI\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nstart_gui = False\nself.load_data(path)\n# Starting the GUI\nif start_gui:\nwarnings.warn(\nget_deprecation_text(), category=DeprecationWarning, stacklevel=2\n)\napp = App()\nwindow = BaseLoaderGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# Extracting all the information from gui loader\nif window.status:\nself.session_information = window.session_information\nself.subject_information = window.subject_information\nself.name = self.session_information[\"name\"]\nself.tracking_frequency = window.tracking_frequency\nself.position = self._make_position(\nwindow.tracking_parameters,\nwindow.tracking_method,\nwindow.tracking_frequency,\nwindow.epochs,\nwindow.time_units_epochs,\nwindow.tracking_alignment,\n)\nself.epochs = self._make_epochs(window.epochs, window.time_units_epochs)\nself.time_support = self._join_epochs(\nwindow.epochs, window.time_units_epochs\n)\n# Save the data\nself.create_nwb_file(path)\ndef load_default_csv(self, csv_file):\n\"\"\"\n        Load tracking data. The default csv should have the time index in the first column in seconds.\n        If no header is provided, the column names will be the column index.\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\ndef load_optitrack_csv(self, csv_file):\n\"\"\"\n        Load tracking data exported with Optitrack.\n        By default, the function reads rows 4 and 5 to build the column names.\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Raises\n        ------\n        RuntimeError\n            If header names are unknown. Should be 'Position' and 'Rotation'\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\ndef load_dlc_csv(self, csv_file):\n\"\"\"\n        Load tracking data exported with DeepLabCut\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\ndef load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n        Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n        Parameters\n        ----------\n        ttl_file : str\n            File name\n        n_channels : int, optional\n            The number of channels in the binary file.\n        channel : int, optional\n            Which channel contains the TTL\n        bytes_size : int, optional\n            Bytes size of the binary file.\n        fs : float, optional\n            Sampling frequency of the binary file\n        Returns\n        -------\n        pd.Series\n            A series containing the time index of the TTL.\n        \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\ndef _make_position(\nself, parameters, method, frequency, epochs, time_units, alignment\n):\n\"\"\"\n        Make the position TSDFrame with the parameters extracted from the GUI.\n        \"\"\"\nif len(parameters.index) == 0:\nreturn None\nelse:\nif len(epochs) == 0:\nepochs.loc[0, \"start\"] = 0.0\nframes = []\ntime_supports_starts = []\ntime_support_ends = []\nfor i in range(len(parameters)):\nif method.lower() == \"optitrack\":\nposition = self.load_optitrack_csv(parameters.loc[i, \"csv\"])\nelif method.lower() == \"deep lab cut\":\nposition = self.load_dlc_csv(parameters.loc[i, \"csv\"])\nelif method.lower() == \"default\":\nposition = self.load_default_csv(parameters.loc[i, \"csv\"])\nif alignment.lower() == \"local\":\nstart_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"start\"], time_units\n)\nend_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"end\"], time_units\n)\ntimestamps = (\nposition.index.values\n+ nap.return_timestamps(start_epoch, \"s\")[0]\n)\n# Make sure timestamps are within the epochs\nidx = np.where(timestamps &lt; end_epoch)[0]\nposition = position.iloc[idx]\nposition.index = pd.Index(timestamps[idx])\nif alignment.lower() == \"ttl\":\nttl = self.load_ttl_pulse(\nttl_file=parameters.loc[i, \"ttl\"],\ntracking_frequency=frequency,\nn_channels=int(parameters.loc[i, \"n_channels\"]),\nchannel=int(parameters.loc[i, \"tracking_channel\"]),\nbytes_size=int(parameters.loc[i, \"bytes_size\"]),\nfs=float(parameters.loc[i, \"fs\"]),\nthreshold=float(parameters.loc[i, \"threshold\"]),\n)\nif len(ttl):\nlength = np.minimum(len(ttl), len(position))\nttl = ttl.iloc[0:length]\nposition = position.iloc[0:length]\nelse:\nraise RuntimeError(\n\"No ttl detected for {}\".format(parameters.loc[i, \"ttl\"])\n)\n# Make sure start epochs in seconds\n# start_epoch = format_timestamp(\n#     epochs.loc[parameters.loc[f, \"epoch\"], \"start\"], time_units\n# )\nstart_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"start\"], time_units\n)\ntimestamps = start_epoch + ttl.index.values\nposition.index = pd.Index(timestamps)\nframes.append(position)\ntime_supports_starts.append(position.index[0])\ntime_support_ends.append(position.index[-1])\nposition = pd.concat(frames)\ntime_supports = nap.IntervalSet(\nstart=time_supports_starts, end=time_support_ends, time_units=\"s\"\n)\n# Specific to optitrACK\nif set([\"rx\", \"ry\", \"rz\"]).issubset(position.columns):\nposition[[\"ry\", \"rx\", \"rz\"]] *= np.pi / 180\nposition[[\"ry\", \"rx\", \"rz\"]] += 2 * np.pi\nposition[[\"ry\", \"rx\", \"rz\"]] %= 2 * np.pi\nposition = nap.TsdFrame(\nt=position.index.values,\nd=position.values,\ncolumns=position.columns.values,\ntime_support=time_supports,\ntime_units=\"s\",\n)\nreturn position\ndef _make_epochs(self, epochs, time_units=\"s\"):\n\"\"\"\n        Split GUI epochs into dict of epochs\n        \"\"\"\nlabels = epochs.groupby(\"label\").groups\nisets = {}\nfor lbs in labels.keys():\ntmp = epochs.loc[labels[lbs]]\nisets[lbs] = nap.IntervalSet(\nstart=tmp[\"start\"], end=tmp[\"end\"], time_units=time_units\n)\nreturn isets\ndef _join_epochs(self, epochs, time_units=\"s\"):\n\"\"\"\n        To create the global time support of the data\n        \"\"\"\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nisets = nap.IntervalSet(\nstart=epochs[\"start\"].sort_values(),\nend=epochs[\"end\"].sort_values(),\ntime_units=time_units,\n)\niset = isets.merge_close_intervals(1, time_units=\"us\")\nif len(iset):\nreturn iset\nelse:\nreturn None\ndef create_nwb_file(self, path):\n\"\"\"\n        Initialize the NWB file in the folder pynapplenwb within the data folder.\n        Parameters\n        ----------\n        path : str\n            The path to save the data\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\ndef load_data(self, path):\n\"\"\"\n        Load NWB data save with pynapple in the pynapplenwb folder\n        Parameters\n        ----------\n        path : str\n            Path to the session folder\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\ndef save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n        Add epochs to the NWB file (e.g. ripples epochs)\n        See pynwb.epoch.TimeIntervals\n        Parameters\n        ----------\n        iset : IntervalSet\n            The intervalSet to save\n        name : str\n            The name in the nwb file\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\ndef save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n        Save timestamps in the NWB file (e.g. ripples time) with the time support.\n        See pynwb.base.TimeSeries\n        Parameters\n        ----------\n        tsd : TsdFrame\n            _\n        name : str\n            _\n        description : str, optional\n            _\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_intervals(self, name):\n\"\"\"\n        Load epochs from the NWB file (e.g. 'ripples')\n        Parameters\n        ----------\n        name : str\n            The name in the nwb file\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\ndef load_nwb_timeseries(self, name):\n\"\"\"\n        Load timestamps in the NWB file (e.g. ripples time)\n        Parameters\n        ----------\n        name : str\n            _\n        Returns\n        -------\n        Tsd\n            _\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"old_pages/io/","title":"Io","text":""},{"location":"old_pages/io/#pynapple.io.misc","title":"pynapple.io.misc","text":"<p>Various io functions</p>"},{"location":"old_pages/io/#pynapple.io.misc.load_file","title":"load_file","text":"<pre><code>load_file(path)\n</code></pre> <p>Load file. Current format supported is (npz,nwb,)</p> <p>.npz -&gt; If the file is compatible with a pynapple format, the function will return a pynapple object. Otherwise, the function will return the output of numpy.load</p> <p>.nwb -&gt; Return the pynapple.io.NWBFile class wrapping the NWBFile</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file</p> required <p>Returns:</p> Type Description <code>(Tsd, TsdFrame, Ts, IntervalSet, TsGroup, NWBFile)</code> <p>One of the 5 pynapple objects or pynapple.io.NWBFile</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file is missing</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_file(path):\n\"\"\"Load file. Current format supported is (npz,nwb,)\n    .npz -&gt; If the file is compatible with a pynapple format, the function will return a pynapple object.\n    Otherwise, the function will return the output of numpy.load\n    .nwb -&gt; Return the pynapple.io.NWBFile class wrapping the NWBFile\n    Parameters\n    ----------\n    path : str\n        Path to the file\n    Returns\n    -------\n    (Tsd, TsdFrame, Ts, IntervalSet, TsGroup, pynapple.io.NWBFile)\n        One of the 5 pynapple objects or pynapple.io.NWBFile\n    Raises\n    ------\n    FileNotFoundError\n        If file is missing\n    \"\"\"\nif os.path.isfile(path):\nif path.endswith(\".npz\"):\nreturn NPZFile(path).load()\nelif path.endswith(\".nwb\"):\nreturn NWBFile(path)\nelse:\nraise RuntimeError(\"File format not supported\")\nelse:\nraise FileNotFoundError(\"File {} does not exist\".format(path))\n</code></pre>"},{"location":"old_pages/io/#pynapple.io.misc.load_folder","title":"load_folder","text":"<pre><code>load_folder(path)\n</code></pre> <p>Load folder containing files or other folder. Pynapple will walk throught the subfolders to detect compatible npz files or nwb files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the folder</p> required <p>Returns:</p> Type Description <code>Folder</code> <p>A dictionnary-like class containing all the sub-folders and compatible files (i.e. npz, nwb)</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If folder is missing</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_folder(path):\n\"\"\"Load folder containing files or other folder.\n    Pynapple will walk throught the subfolders to detect compatible npz files\n    or nwb files.\n    Parameters\n    ----------\n    path : str\n        Path to the folder\n    Returns\n    -------\n    Folder\n        A dictionnary-like class containing all the sub-folders and compatible files (i.e. npz, nwb)\n    Raises\n    ------\n    RuntimeError\n        If folder is missing\n    \"\"\"\nif os.path.isdir(path):\nreturn Folder(path)\nelse:\nraise RuntimeError(\"Folder {} does not exist\".format(path))\n</code></pre>"},{"location":"old_pages/io/#pynapple.io.misc.load_session","title":"load_session","text":"<pre><code>load_session(path=None, session_type=None)\n</code></pre> <p>%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % WARNING : THIS FUNCTION IS DEPRECATED % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% General Loader for</p> <ul> <li> <p>Neurosuite</p> </li> <li> <p>Phy</p> </li> <li> <p>Minian</p> </li> <li> <p>Inscopix-cnmfe</p> </li> <li> <p>Matlab-cnmfe</p> </li> <li> <p>Suite2p</p> </li> <li>None for default session.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to load the data</p> <code>None</code> <code>session_type</code> <code>str</code> <p>Can be 'neurosuite', 'phy', 'minian', 'inscopix-cnmfe', 'cnmfe-matlab', 'suite2p' or None for default loader.</p> <code>None</code> <p>Returns:</p> Type Description <code>Session</code> <p>A class holding all the data from the session.</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_session(path=None, session_type=None):\n\"\"\"\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    % WARNING : THIS FUNCTION IS DEPRECATED %\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    General Loader for\n    - Neurosuite\\n\n    - Phy\\n\n    - Minian\\n\n    - Inscopix-cnmfe\\n\n    - Matlab-cnmfe\\n\n    - Suite2p\n    - None for default session.\n    Parameters\n    ----------\n    path : str, optional\n        The path to load the data\n    session_type : str, optional\n        Can be 'neurosuite', 'phy',\n        'minian', 'inscopix-cnmfe', 'cnmfe-matlab',\n        'suite2p' or None for default loader.\n    Returns\n    -------\n    Session\n        A class holding all the data from the session.\n    \"\"\"\nif path:\nif not os.path.isdir(path):\nraise RuntimeError(\"Path {} is not found.\".format(path))\nif isinstance(session_type, str):\nsession_type = session_type.lower()\nif session_type == \"neurosuite\":\nreturn NeuroSuite(path)\nelif session_type == \"phy\":\nreturn Phy(path)\nelif session_type == \"inscopix-cnmfe\":\nreturn InscopixCNMFE(path)\nelif session_type == \"minian\":\nreturn Minian(path)\nelif session_type == \"cnmfe-matlab\":\nreturn CNMF_E(path)\nelif session_type == \"suite2p\":\nreturn Suite2P(path)\nelse:\nreturn BaseLoader(path)\n</code></pre>"},{"location":"old_pages/io/#pynapple.io.misc.load_eeg","title":"load_eeg","text":"<pre><code>load_eeg(\nfilepath,\nchannel=None,\nn_channels=None,\nfrequency=None,\nprecision=\"int16\",\nbytes_size=2,\n)\n</code></pre> <p>Standalone function to load eeg/lfp/dat file in binary format.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the eeg file</p> required <code>channel</code> <code>int or list of int</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>n_channels</code> <code>int</code> <p>Number of channels</p> <code>None</code> <code>frequency</code> <code>float</code> <p>Sampling rate of the file</p> <code>None</code> <code>precision</code> <code>str</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p>"},{"location":"old_pages/io/#pynapple.io.misc.load_eeg--deleted-parameters","title":"Deleted Parameters","text":"<p>extension : str, optional     The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_eeg(\nfilepath,\nchannel=None,\nn_channels=None,\nfrequency=None,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Standalone function to load eeg/lfp/dat file in binary format.\n    Parameters\n    ----------\n    filepath : str\n        The path to the eeg file\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    n_channels : int, optional\n        Number of channels\n    frequency : float, optional\n        Sampling rate of the file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the binary file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    Deleted Parameters\n    ------------------\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    \"\"\"\n# Need to check if a xml file exists\npath = os.path.dirname(filepath)\nbasename = os.path.basename(filepath).split(\".\")[0]\nlistdir = os.listdir(path)\nif frequency is None or n_channels is None:\nif basename + \".xml\" in listdir:\nxmlpath = os.path.join(path, basename + \".xml\")\nxmldoc = minidom.parse(xmlpath)\nelse:\nraise RuntimeError(\n\"Can't find xml file; please specify sampling frequency or number of channels\"\n)\nif frequency is None:\nif filepath.endswith(\".dat\"):\nfs_dat = int(\nxmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nfrequency = fs_dat\nelif filepath.endswith((\".lfp\", \".eeg\")):\nfs_eeg = int(\nxmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nfrequency = fs_eeg\nif n_channels is None:\nn_channels = int(\nxmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn fp\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"old_pages/io/#pynapple.io.misc.append_NWB_LFP","title":"append_NWB_LFP","text":"<pre><code>append_NWB_LFP(path, lfp, channel=None)\n</code></pre> <p>Standalone function for adding lfp/eeg to already existing nwb files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data. The function will looks for a nwb file in path or in path/pynapplenwb.</p> required <code>lfp</code> <code>Tsd or TsdFrame</code> <p>Description</p> required <code>channel</code> <code>None</code> <p>channel number in int ff lfp is a Tsd</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the nwb file </p> <p>If no channel is specify when passing a Tsd</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def append_NWB_LFP(path, lfp, channel=None):\n\"\"\"Standalone function for adding lfp/eeg to already existing nwb files.\n    Parameters\n    ----------\n    path : str\n        The path to the data. The function will looks for a nwb file in path\n        or in path/pynapplenwb.\n    lfp : Tsd or TsdFrame\n        Description\n    channel : None, optional\n        channel number in int ff lfp is a Tsd\n    Raises\n    ------\n    RuntimeError\n        If can't find the nwb file \\n\n        If no channel is specify when passing a Tsd\n    \"\"\"\nnew_path = os.path.join(path, \"pynapplenwb\")\nnwb_path = \"\"\nif os.path.exists(new_path):\nnwbfilename = [f for f in os.listdir(new_path) if f.endswith(\".nwb\")]\nif len(nwbfilename):\nnwb_path = os.path.join(path, \"pynapplenwb\", nwbfilename[0])\nelse:\nnwbfilename = [f for f in os.listdir(path) if f.endswith(\".nwb\")]\nif len(nwbfilename):\nnwb_path = os.path.join(path, \"pynapplenwb\", nwbfilename[0])\nif len(nwb_path) == 0:\nraise RuntimeError(\"Can't find nwb file in {}\".format(path))\nif isinstance(lfp, nap.TsdFrame):\nchannels = lfp.columns.values\nelif isinstance(lfp, nap.Tsd):\nif isinstance(channel, int):\nchannels = [channel]\nelse:\nraise RuntimeError(\"Please specify which channel it is.\")\nio = NWBHDF5IO(nwb_path, \"r+\")\nnwbfile = io.read()\nall_table_region = nwbfile.create_electrode_table_region(\nregion=channels, description=\"\", name=\"electrodes\"\n)\nlfp_electrical_series = ElectricalSeries(\nname=\"ElectricalSeries\",\ndata=lfp.values,\ntimestamps=lfp.index.values,\nelectrodes=all_table_region,\n)\nlfp = LFP(electrical_series=lfp_electrical_series)\necephys_module = nwbfile.create_processing_module(\nname=\"ecephys\", description=\"processed extracellular electrophysiology data\"\n)\necephys_module.add(lfp)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/","title":"Io.neurosuite","text":""},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite","title":"pynapple.io.neurosuite","text":"<p> DEPRECATED: This will be removed in version 1.0.0. Check nwbmatic or neuroconv instead.</p> <p>Class and functions for loading data processed with the Neurosuite (Klusters, Neuroscope, NDmanager)</p> <p>@author: Guillaume Viejo</p>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite","title":"NeuroSuite","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader for kluster data</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>class NeuroSuite(BaseLoader):\n\"\"\"\n    Loader for kluster data\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Instantiate the data class from a neurosuite folder.\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nself.time_support = None\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_neurosuite = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_nwb_spikes(path)\nif success:\nloading_neurosuite = False\n# Bypass if data have already been transfered to nwb\nif loading_neurosuite:\nself.load_neurosuite_xml(path)\n# print(\"XML loaded\")\n# To label the electrodes groups\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.group_to_channel)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# print(\"GUI DONE\")\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_neurosuite_spikes(path, self.basename, self.time_support)\nself.save_data(path)\ndef load_neurosuite_spikes(self, path, basename, time_support=None, fs=20000.0):\n\"\"\"\n        Read the clus and res files and convert to NWB.\n        Instantiate automatically a TsGroup object.\n        Parameters\n        ----------\n        path : str\n            The path to the data\n        basename : str\n            Basename of the clu and res files.\n        time_support : IntevalSet, optional\n            The time support of the data\n        fs : float, optional\n            Sampling rate of the recording.\n        Raises\n        ------\n        RuntimeError\n            If number of clu and res are not equal.\n        \"\"\"\nfiles = os.listdir(path)\nclu_files = np.sort([f for f in files if \".clu.\" in f and f[0] != \".\"])\nres_files = np.sort([f for f in files if \".res.\" in f and f[0] != \".\"])\nclu1 = np.sort([int(f.split(\".\")[-1]) for f in clu_files])\nclu2 = np.sort([int(f.split(\".\")[-1]) for f in res_files])\nif len(clu_files) != len(res_files) or not (clu1 == clu2).any():\nraise RuntimeError(\n\"Not the same number of clu and res files in \" + path + \"; Exiting ...\"\n)\ncount = 0\nspikes = {}\ngroup = pd.Series(dtype=np.int32)\nfor i, s in zip(range(len(clu_files)), clu1):\nclu = np.genfromtxt(\nos.path.join(path, basename + \".clu.\" + str(s)), dtype=np.int32\n)[1:]\nif np.max(clu) &gt; 1:  # getting rid of mua and noise\nres = np.genfromtxt(os.path.join(path, basename + \".res.\" + str(s)))\ntmp = np.unique(clu).astype(int)\nidx_clu = tmp[tmp &gt; 1]\nidx_out = np.arange(count, count + len(idx_clu))\nfor j, k in zip(idx_clu, idx_out):\nt = res[clu == j] / fs\nspikes[k] = nap.Ts(t=t, time_units=\"s\")\ngroup.loc[k] = s\ncount += len(idx_clu)\ngroup = group - 1  # better to start it a 0\nself.spikes = nap.TsGroup(\nspikes, time_support=time_support, time_units=\"s\", group=group\n)\n# adding some information to help parse the neurons\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\ndef load_neurosuite_xml(self, path):\n\"\"\"\n        path should be the folder session containing the XML file\n        Function reads :\n        1. the number of channels\n        2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder\n            eeg file first if both are present or both are absent\n        3. the mappings shanks to channels as a dict\n        Parameters\n        ----------\n        path: str\n            The path to the data\n        Raises\n        ------\n        RuntimeError\n            If path does not contain the xml file.\n        \"\"\"\nlistdir = os.listdir(path)\nxmlfiles = [f for f in listdir if f.endswith(\".xml\")]\nif not len(xmlfiles):\nraise RuntimeError(\"Path {} contains no xml files;\".format(path))\nsys.exit()\nnew_path = os.path.join(path, xmlfiles[0])\nself.xmldoc = minidom.parse(new_path)\nself.nChannels = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nself.fs_dat = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nself.fs_eeg = int(\nself.xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nself.group_to_channel = {}\ngroups = (\nself.xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n.getElementsByTagName(\"channelGroups\")[0]\n.getElementsByTagName(\"group\")\n)\nfor i in range(len(groups)):\nself.group_to_channel[i] = np.array(\n[\nint(child.firstChild.data)\nfor child in groups[i].getElementsByTagName(\"channel\")\n]\n)\nreturn\ndef save_data(self, path):\n\"\"\"\n        Save the data to NWB format.\n        Parameters\n        ----------\n        path : str\n            The path to save the data\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.group_to_channel:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.group_to_channel[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_spikes(self, path):\n\"\"\"\n        Read the NWB spikes to extract the spike times.\n        Parameters\n        ----------\n        path : str\n            The path to the data\n        Returns\n        -------\n        TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn False\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\ndef load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n        Load the LFP.\n        Parameters\n        ----------\n        filename : str, optional\n            The filename of the lfp file.\n            It can be useful it multiple dat files are present in the data directory\n        channel : int or list of int, optional\n            The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n        extension : str, optional\n            The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n        frequency : float, optional\n            Default 1250 Hz for the eeg file\n        precision : str, optional\n            The precision of the binary file\n        bytes_size : int, optional\n            Bytes size of the lfp file\n        Raises\n        ------\n        RuntimeError\n            If can't find the lfp/eeg/dat file\n        Returns\n        -------\n        Tsd or TsdFrame\n            The lfp in a time series format\n        \"\"\"\nif filename is not None:\nfilepath = os.path.join(self.path, filename)\nelse:\nlistdir = os.listdir(self.path)\neegfile = [f for f in listdir if f.endswith(extension)]\nif not len(eegfile):\nraise RuntimeError(\n\"Path {} contains no {} files;\".format(self.path, extension)\n)\nfilepath = os.path.join(self.path, eegfile[0])\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\ndef read_neuroscope_intervals(self, name=None, path2file=None):\n\"\"\"\n        This function reads .evt files in which odd raws indicate the beginning\n        of the time series and the even raws are the ends.\n        If the file is present in the nwb, provide the just the name. If the file\n        is not present in the nwb, it loads the events from the nwb directory.\n        If just the path is provided but not the name, it takes the name from the file.\n        Parameters\n        ----------\n        name: str\n            name of the epoch in the nwb file, e.g. \"rem\" or desired name save\n            the data in the nwb.\n        path2file: str\n            Path of the file you want to load.\n        Returns\n        -------\n        IntervalSet\n            Contains two columns corresponding to the start and end of the intervals.\n        \"\"\"\nif name:\nisets = self.load_nwb_intervals(name)\nif isinstance(isets, nap.IntervalSet):\nreturn isets\nif name is not None and path2file is None:\npath2file = os.path.join(self.path, self.basename + \".\" + name + \".evt\")\nif path2file is not None:\ntry:\n# df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None)\ntmp = np.genfromtxt(path2file)[:, 0]\ndf = tmp.reshape(len(tmp) // 2, 2)\nexcept ValueError:\nprint(\"specify a valid name\")\nisets = nap.IntervalSet(df[:, 0], df[:, 1], time_units=\"ms\")\nif name is None:\nname = path2file.split(\".\")[-2]\nprint(\"*** saving file in the nwb as\", name)\nself.save_nwb_intervals(isets, name)\nelse:\nraise ValueError(\"specify a valid path\")\nreturn isets\ndef write_neuroscope_intervals(self, extension, isets, name):\n\"\"\"Write events to load with neuroscope (e.g. ripples start and ends)\n        Parameters\n        ----------\n        extension : str\n            The extension of the file (e.g. basename.evt.py.rip)\n        isets : IntervalSet\n            The IntervalSet to write\n        name : str\n            The name of the events (e.g. Ripples)\n        \"\"\"\nstart = isets.as_units(\"ms\")[\"start\"].values\nends = isets.as_units(\"ms\")[\"end\"].values\ndatatowrite = np.vstack((start, ends)).T.flatten()\nn = len(isets)\ntexttowrite = np.vstack(\n(\n(np.repeat(np.array([name + \" start\"]), n)),\n(np.repeat(np.array([name + \" end\"]), n)),\n)\n).T.flatten()\nevt_file = os.path.join(self.path, self.basename + extension)\nf = open(evt_file, \"w\")\nfor t, n in zip(datatowrite, texttowrite):\nf.writelines(\"{:1.6f}\".format(t) + \"\\t\" + n + \"\\n\")\nf.close()\nreturn\ndef load_mean_waveforms(self, epoch=None, waveform_window=None, spike_count=1000):\n\"\"\"\n        Load the mean waveforms from a dat file.\n        Parameters\n        ----------\n        epoch : IntervalSet\n            default = None\n            Restrict spikes to an epoch.\n        waveform_window : IntervalSet\n            default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms')\n            Limit waveform extraction before and after spike time\n        spike_count : int\n            default = 1000\n            Number of spikes used per neuron for the calculation of waveforms\n        Returns\n        -------\n        dictionary\n            the waveforms for all neurons\n        pandas.Series\n            the channel with the maximum waveform for each neuron\n        \"\"\"\nif not isinstance(waveform_window, nap.IntervalSet):\nwaveform_window = nap.IntervalSet(start=-0.5, end=1, time_units=\"ms\")\nspikes = self.spikes\nif not os.path.exists(self.path):  # check if path exists\nprint(\"The path \" + self.path + \" doesn't exist; Exiting ...\")\nsys.exit()\n# Load XML INFO\nself.load_neurosuite_xml(self.path)\nn_channels = self.nChannels\nfs = self.fs_dat\ngroup_to_channel = self.group_to_channel\ngroup = spikes.get_info(\"group\")\n# Check if there is an epoch, restrict spike times to epoch\nif epoch is not None:\nif type(epoch) is not nap.IntervalSet:\nprint(\"Epoch must be an IntervalSet\")\nsys.exit()\nelse:\nprint(\"Restricting spikes to epoch\")\nspikes = spikes.restrict(epoch)\nepstart = int(epoch.as_units(\"s\")[\"start\"].values[0] * fs)\nepend = int(epoch.as_units(\"s\")[\"end\"].values[0] * fs)\n# Find dat file\nfiles = os.listdir(self.path)\ndat_files = np.sort([f for f in files if \"dat\" in f and f[0] != \".\"])\n# Need n_samples collected in the entire recording from dat file to load\nfile = os.path.join(self.path, dat_files[0])\nf = open(\nfile, \"rb\"\n)  # open file to get number of samples collected in the entire recording\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\n# map to memory all samples for all channels, channels are numbered according to neuroscope number\nfp = np.memmap(file, np.int16, \"r\", shape=(n_samples, n_channels))\n# convert spike times to spikes in sample number\nsample_spikes = {\nneuron: (spikes[neuron].as_units(\"s\").index.values * fs).astype(\"int\")\nfor neuron in spikes\n}\n# prep for waveforms\noverlap = int(\nwaveform_window.tot_length(time_units=\"s\")\n)  # one spike's worth of overlap between windows\nwaveform_window = abs(np.array(waveform_window.as_units(\"s\"))[0] * fs).astype(\nint\n)  # convert time to sample number\nneuron_waveforms = {\nn: np.zeros([np.sum(waveform_window), len(group_to_channel[group[n]])])\nfor n in sample_spikes\n}\n# divide dat file into batches that slightly overlap for faster loading\nbatch_size = 3000000\nwindows = np.arange(0, int(endoffile / n_channels / bytes_size), batch_size)\nif epoch is not None:\nprint(\"Restricting dat file to epoch\")\nwindows = windows[(windows &gt;= epstart) &amp; (windows &lt;= epend)]\nbatches = []\nfor (\ni\n) in windows:  # make overlapping batches from the beginning to end of recording\nif i == windows[-1]:  # the last batch cannot overlap with the next one\nbatches.append([i, n_samples])\nelse:\nbatches.append([i, i + batch_size + overlap])\nbatches = [np.int32(batch) for batch in batches]\nsample_counted_spikes = {}\nfor index, neuron in enumerate(sample_spikes):\nif len(sample_spikes[neuron]) &gt;= spike_count:\nsample_counted_spikes[neuron] = np.array(\nnp.random.choice(list(sample_spikes[neuron]), spike_count)\n)\nelif len(sample_spikes[neuron]) &lt; spike_count:\nprint(\n\"Not enough spikes in neuron \" + str(index) + \"... using all spikes\"\n)\nsample_counted_spikes[neuron] = sample_spikes[neuron]\n# Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file\nspike_check = np.array(\n[\nint(spikes_neuron)\nfor spikes_neuron in sample_counted_spikes[neuron]\nfor neuron in sample_counted_spikes\n]\n)\nfor index, timestep in enumerate(batches):\nprint(\nf\"Extracting waveforms from dat file: window {index+1} / {len(windows)}\",\nend=\"\\r\",\n)\nif (\nlen(\nspike_check[\n(timestep[0] &lt; spike_check) &amp; (timestep[1] &gt; spike_check)\n]\n)\n== 0\n):\ncontinue  # if there are no spikes for any neurons in this batch, skip and go to the next one\n# Load dat file for timestep\ntmp = pd.DataFrame(\ndata=fp[timestep[0] : timestep[1], :],\ncolumns=np.arange(n_channels),\nindex=range(timestep[0], timestep[1]),\n)  # load dat file\n# Check if any spikes are present\nfor neuron in sample_counted_spikes:\nneurontmp = sample_counted_spikes[neuron]\ntmp2 = neurontmp[(timestep[0] &lt; neurontmp) &amp; (timestep[1] &gt; neurontmp)]\nif len(neurontmp) == 0:\ncontinue  # skip neuron if it has no spikes in this batch\ntmpn = tmp[\ngroup_to_channel[group[neuron]]\n]  # restrict dat file to the channel group of the neuron\nfor time in tmp2:  # add each spike waveform to neuron_waveform\nspikewindow = tmpn.loc[\ntime - waveform_window[0] : time + waveform_window[1] - 1\n]  # waveform for this spike time\ntry:\nneuron_waveforms[neuron] += spikewindow.values\nexcept (\nException\n):  # ignore if full waveform is not present in this batch\npass\nmeanwf = {\nn: pd.DataFrame(\ndata=np.array(neuron_waveforms[n]) / spike_count,\ncolumns=np.arange(len(group_to_channel[group[n]])),\nindex=np.array(np.arange(-waveform_window[0], waveform_window[1])) / fs,\n)\nfor n in sample_counted_spikes\n}\n# find the max channel for each neuron\nmaxch = pd.Series(\ndata=[meanwf[n][meanwf[n].loc[0].idxmin()].name for n in meanwf],\nindex=spikes.keys(),\n)\nreturn meanwf, maxch\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Instantiate the data class from a neurosuite folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Instantiate the data class from a neurosuite folder.\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nself.time_support = None\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_neurosuite = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_nwb_spikes(path)\nif success:\nloading_neurosuite = False\n# Bypass if data have already been transfered to nwb\nif loading_neurosuite:\nself.load_neurosuite_xml(path)\n# print(\"XML loaded\")\n# To label the electrodes groups\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.group_to_channel)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# print(\"GUI DONE\")\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_neurosuite_spikes(path, self.basename, self.time_support)\nself.save_data(path)\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_spikes","title":"load_neurosuite_spikes","text":"<pre><code>load_neurosuite_spikes(\npath, basename, time_support=None, fs=20000.0\n)\n</code></pre> <p>Read the clus and res files and convert to NWB. Instantiate automatically a TsGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <code>basename</code> <code>str</code> <p>Basename of the clu and res files.</p> required <code>time_support</code> <code>IntevalSet</code> <p>The time support of the data</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling rate of the recording.</p> <code>20000.0</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If number of clu and res are not equal.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_neurosuite_spikes(self, path, basename, time_support=None, fs=20000.0):\n\"\"\"\n    Read the clus and res files and convert to NWB.\n    Instantiate automatically a TsGroup object.\n    Parameters\n    ----------\n    path : str\n        The path to the data\n    basename : str\n        Basename of the clu and res files.\n    time_support : IntevalSet, optional\n        The time support of the data\n    fs : float, optional\n        Sampling rate of the recording.\n    Raises\n    ------\n    RuntimeError\n        If number of clu and res are not equal.\n    \"\"\"\nfiles = os.listdir(path)\nclu_files = np.sort([f for f in files if \".clu.\" in f and f[0] != \".\"])\nres_files = np.sort([f for f in files if \".res.\" in f and f[0] != \".\"])\nclu1 = np.sort([int(f.split(\".\")[-1]) for f in clu_files])\nclu2 = np.sort([int(f.split(\".\")[-1]) for f in res_files])\nif len(clu_files) != len(res_files) or not (clu1 == clu2).any():\nraise RuntimeError(\n\"Not the same number of clu and res files in \" + path + \"; Exiting ...\"\n)\ncount = 0\nspikes = {}\ngroup = pd.Series(dtype=np.int32)\nfor i, s in zip(range(len(clu_files)), clu1):\nclu = np.genfromtxt(\nos.path.join(path, basename + \".clu.\" + str(s)), dtype=np.int32\n)[1:]\nif np.max(clu) &gt; 1:  # getting rid of mua and noise\nres = np.genfromtxt(os.path.join(path, basename + \".res.\" + str(s)))\ntmp = np.unique(clu).astype(int)\nidx_clu = tmp[tmp &gt; 1]\nidx_out = np.arange(count, count + len(idx_clu))\nfor j, k in zip(idx_clu, idx_out):\nt = res[clu == j] / fs\nspikes[k] = nap.Ts(t=t, time_units=\"s\")\ngroup.loc[k] = s\ncount += len(idx_clu)\ngroup = group - 1  # better to start it a 0\nself.spikes = nap.TsGroup(\nspikes, time_support=time_support, time_units=\"s\", group=group\n)\n# adding some information to help parse the neurons\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_xml","title":"load_neurosuite_xml","text":"<pre><code>load_neurosuite_xml(path)\n</code></pre> <p>path should be the folder session containing the XML file</p> <p>Function reads : 1. the number of channels 2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder     eeg file first if both are present or both are absent 3. the mappings shanks to channels as a dict</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>The path to the data</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If path does not contain the xml file.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_neurosuite_xml(self, path):\n\"\"\"\n    path should be the folder session containing the XML file\n    Function reads :\n    1. the number of channels\n    2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder\n        eeg file first if both are present or both are absent\n    3. the mappings shanks to channels as a dict\n    Parameters\n    ----------\n    path: str\n        The path to the data\n    Raises\n    ------\n    RuntimeError\n        If path does not contain the xml file.\n    \"\"\"\nlistdir = os.listdir(path)\nxmlfiles = [f for f in listdir if f.endswith(\".xml\")]\nif not len(xmlfiles):\nraise RuntimeError(\"Path {} contains no xml files;\".format(path))\nsys.exit()\nnew_path = os.path.join(path, xmlfiles[0])\nself.xmldoc = minidom.parse(new_path)\nself.nChannels = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nself.fs_dat = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nself.fs_eeg = int(\nself.xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nself.group_to_channel = {}\ngroups = (\nself.xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n.getElementsByTagName(\"channelGroups\")[0]\n.getElementsByTagName(\"group\")\n)\nfor i in range(len(groups)):\nself.group_to_channel[i] = np.array(\n[\nint(child.firstChild.data)\nfor child in groups[i].getElementsByTagName(\"channel\")\n]\n)\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.save_data","title":"save_data","text":"<pre><code>save_data(path)\n</code></pre> <p>Save the data to NWB format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def save_data(self, path):\n\"\"\"\n    Save the data to NWB format.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.group_to_channel:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.group_to_channel[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_nwb_spikes","title":"load_nwb_spikes","text":"<pre><code>load_nwb_spikes(path)\n</code></pre> <p>Read the NWB spikes to extract the spike times.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_nwb_spikes(self, path):\n\"\"\"\n    Read the NWB spikes to extract the spike times.\n    Parameters\n    ----------\n    path : str\n        The path to the data\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn False\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_lfp","title":"load_lfp","text":"<pre><code>load_lfp(\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n)\n</code></pre> <p>Load the LFP.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename of the lfp file. It can be useful it multiple dat files are present in the data directory</p> <code>None</code> <code>channel</code> <code>int or list of int</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>extension</code> <code>str</code> <p>The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> <code>'.eeg'</code> <code>frequency</code> <code>float</code> <p>Default 1250 Hz for the eeg file</p> <code>1250.0</code> <code>precision</code> <code>str</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the lfp file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Load the LFP.\n    Parameters\n    ----------\n    filename : str, optional\n        The filename of the lfp file.\n        It can be useful it multiple dat files are present in the data directory\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    frequency : float, optional\n        Default 1250 Hz for the eeg file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the lfp file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    \"\"\"\nif filename is not None:\nfilepath = os.path.join(self.path, filename)\nelse:\nlistdir = os.listdir(self.path)\neegfile = [f for f in listdir if f.endswith(extension)]\nif not len(eegfile):\nraise RuntimeError(\n\"Path {} contains no {} files;\".format(self.path, extension)\n)\nfilepath = os.path.join(self.path, eegfile[0])\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.read_neuroscope_intervals","title":"read_neuroscope_intervals","text":"<pre><code>read_neuroscope_intervals(name=None, path2file=None)\n</code></pre> <p>This function reads .evt files in which odd raws indicate the beginning of the time series and the even raws are the ends. If the file is present in the nwb, provide the just the name. If the file is not present in the nwb, it loads the events from the nwb directory. If just the path is provided but not the name, it takes the name from the file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>name of the epoch in the nwb file, e.g. \"rem\" or desired name save the data in the nwb.</p> <code>None</code> <p>path2file: str     Path of the file you want to load.</p> <p>Returns:</p> Type Description <code>IntervalSet</code> <p>Contains two columns corresponding to the start and end of the intervals.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def read_neuroscope_intervals(self, name=None, path2file=None):\n\"\"\"\n    This function reads .evt files in which odd raws indicate the beginning\n    of the time series and the even raws are the ends.\n    If the file is present in the nwb, provide the just the name. If the file\n    is not present in the nwb, it loads the events from the nwb directory.\n    If just the path is provided but not the name, it takes the name from the file.\n    Parameters\n    ----------\n    name: str\n        name of the epoch in the nwb file, e.g. \"rem\" or desired name save\n        the data in the nwb.\n    path2file: str\n        Path of the file you want to load.\n    Returns\n    -------\n    IntervalSet\n        Contains two columns corresponding to the start and end of the intervals.\n    \"\"\"\nif name:\nisets = self.load_nwb_intervals(name)\nif isinstance(isets, nap.IntervalSet):\nreturn isets\nif name is not None and path2file is None:\npath2file = os.path.join(self.path, self.basename + \".\" + name + \".evt\")\nif path2file is not None:\ntry:\n# df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None)\ntmp = np.genfromtxt(path2file)[:, 0]\ndf = tmp.reshape(len(tmp) // 2, 2)\nexcept ValueError:\nprint(\"specify a valid name\")\nisets = nap.IntervalSet(df[:, 0], df[:, 1], time_units=\"ms\")\nif name is None:\nname = path2file.split(\".\")[-2]\nprint(\"*** saving file in the nwb as\", name)\nself.save_nwb_intervals(isets, name)\nelse:\nraise ValueError(\"specify a valid path\")\nreturn isets\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.write_neuroscope_intervals","title":"write_neuroscope_intervals","text":"<pre><code>write_neuroscope_intervals(extension, isets, name)\n</code></pre> <p>Write events to load with neuroscope (e.g. ripples start and ends)</p> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>The extension of the file (e.g. basename.evt.py.rip)</p> required <code>isets</code> <code>IntervalSet</code> <p>The IntervalSet to write</p> required <code>name</code> <code>str</code> <p>The name of the events (e.g. Ripples)</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def write_neuroscope_intervals(self, extension, isets, name):\n\"\"\"Write events to load with neuroscope (e.g. ripples start and ends)\n    Parameters\n    ----------\n    extension : str\n        The extension of the file (e.g. basename.evt.py.rip)\n    isets : IntervalSet\n        The IntervalSet to write\n    name : str\n        The name of the events (e.g. Ripples)\n    \"\"\"\nstart = isets.as_units(\"ms\")[\"start\"].values\nends = isets.as_units(\"ms\")[\"end\"].values\ndatatowrite = np.vstack((start, ends)).T.flatten()\nn = len(isets)\ntexttowrite = np.vstack(\n(\n(np.repeat(np.array([name + \" start\"]), n)),\n(np.repeat(np.array([name + \" end\"]), n)),\n)\n).T.flatten()\nevt_file = os.path.join(self.path, self.basename + extension)\nf = open(evt_file, \"w\")\nfor t, n in zip(datatowrite, texttowrite):\nf.writelines(\"{:1.6f}\".format(t) + \"\\t\" + n + \"\\n\")\nf.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_mean_waveforms","title":"load_mean_waveforms","text":"<pre><code>load_mean_waveforms(\nepoch=None, waveform_window=None, spike_count=1000\n)\n</code></pre> <p>Load the mean waveforms from a dat file.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>IntervalSet</code> <p>default = None Restrict spikes to an epoch.</p> <code>None</code> <code>waveform_window</code> <code>IntervalSet</code> <p>default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms') Limit waveform extraction before and after spike time</p> <code>None</code> <code>spike_count</code> <code>int</code> <p>default = 1000 Number of spikes used per neuron for the calculation of waveforms</p> <code>1000</code> <p>Returns:</p> Type Description <code>dictionary</code> <p>the waveforms for all neurons</p> <code>Series</code> <p>the channel with the maximum waveform for each neuron</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_mean_waveforms(self, epoch=None, waveform_window=None, spike_count=1000):\n\"\"\"\n    Load the mean waveforms from a dat file.\n    Parameters\n    ----------\n    epoch : IntervalSet\n        default = None\n        Restrict spikes to an epoch.\n    waveform_window : IntervalSet\n        default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms')\n        Limit waveform extraction before and after spike time\n    spike_count : int\n        default = 1000\n        Number of spikes used per neuron for the calculation of waveforms\n    Returns\n    -------\n    dictionary\n        the waveforms for all neurons\n    pandas.Series\n        the channel with the maximum waveform for each neuron\n    \"\"\"\nif not isinstance(waveform_window, nap.IntervalSet):\nwaveform_window = nap.IntervalSet(start=-0.5, end=1, time_units=\"ms\")\nspikes = self.spikes\nif not os.path.exists(self.path):  # check if path exists\nprint(\"The path \" + self.path + \" doesn't exist; Exiting ...\")\nsys.exit()\n# Load XML INFO\nself.load_neurosuite_xml(self.path)\nn_channels = self.nChannels\nfs = self.fs_dat\ngroup_to_channel = self.group_to_channel\ngroup = spikes.get_info(\"group\")\n# Check if there is an epoch, restrict spike times to epoch\nif epoch is not None:\nif type(epoch) is not nap.IntervalSet:\nprint(\"Epoch must be an IntervalSet\")\nsys.exit()\nelse:\nprint(\"Restricting spikes to epoch\")\nspikes = spikes.restrict(epoch)\nepstart = int(epoch.as_units(\"s\")[\"start\"].values[0] * fs)\nepend = int(epoch.as_units(\"s\")[\"end\"].values[0] * fs)\n# Find dat file\nfiles = os.listdir(self.path)\ndat_files = np.sort([f for f in files if \"dat\" in f and f[0] != \".\"])\n# Need n_samples collected in the entire recording from dat file to load\nfile = os.path.join(self.path, dat_files[0])\nf = open(\nfile, \"rb\"\n)  # open file to get number of samples collected in the entire recording\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\n# map to memory all samples for all channels, channels are numbered according to neuroscope number\nfp = np.memmap(file, np.int16, \"r\", shape=(n_samples, n_channels))\n# convert spike times to spikes in sample number\nsample_spikes = {\nneuron: (spikes[neuron].as_units(\"s\").index.values * fs).astype(\"int\")\nfor neuron in spikes\n}\n# prep for waveforms\noverlap = int(\nwaveform_window.tot_length(time_units=\"s\")\n)  # one spike's worth of overlap between windows\nwaveform_window = abs(np.array(waveform_window.as_units(\"s\"))[0] * fs).astype(\nint\n)  # convert time to sample number\nneuron_waveforms = {\nn: np.zeros([np.sum(waveform_window), len(group_to_channel[group[n]])])\nfor n in sample_spikes\n}\n# divide dat file into batches that slightly overlap for faster loading\nbatch_size = 3000000\nwindows = np.arange(0, int(endoffile / n_channels / bytes_size), batch_size)\nif epoch is not None:\nprint(\"Restricting dat file to epoch\")\nwindows = windows[(windows &gt;= epstart) &amp; (windows &lt;= epend)]\nbatches = []\nfor (\ni\n) in windows:  # make overlapping batches from the beginning to end of recording\nif i == windows[-1]:  # the last batch cannot overlap with the next one\nbatches.append([i, n_samples])\nelse:\nbatches.append([i, i + batch_size + overlap])\nbatches = [np.int32(batch) for batch in batches]\nsample_counted_spikes = {}\nfor index, neuron in enumerate(sample_spikes):\nif len(sample_spikes[neuron]) &gt;= spike_count:\nsample_counted_spikes[neuron] = np.array(\nnp.random.choice(list(sample_spikes[neuron]), spike_count)\n)\nelif len(sample_spikes[neuron]) &lt; spike_count:\nprint(\n\"Not enough spikes in neuron \" + str(index) + \"... using all spikes\"\n)\nsample_counted_spikes[neuron] = sample_spikes[neuron]\n# Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file\nspike_check = np.array(\n[\nint(spikes_neuron)\nfor spikes_neuron in sample_counted_spikes[neuron]\nfor neuron in sample_counted_spikes\n]\n)\nfor index, timestep in enumerate(batches):\nprint(\nf\"Extracting waveforms from dat file: window {index+1} / {len(windows)}\",\nend=\"\\r\",\n)\nif (\nlen(\nspike_check[\n(timestep[0] &lt; spike_check) &amp; (timestep[1] &gt; spike_check)\n]\n)\n== 0\n):\ncontinue  # if there are no spikes for any neurons in this batch, skip and go to the next one\n# Load dat file for timestep\ntmp = pd.DataFrame(\ndata=fp[timestep[0] : timestep[1], :],\ncolumns=np.arange(n_channels),\nindex=range(timestep[0], timestep[1]),\n)  # load dat file\n# Check if any spikes are present\nfor neuron in sample_counted_spikes:\nneurontmp = sample_counted_spikes[neuron]\ntmp2 = neurontmp[(timestep[0] &lt; neurontmp) &amp; (timestep[1] &gt; neurontmp)]\nif len(neurontmp) == 0:\ncontinue  # skip neuron if it has no spikes in this batch\ntmpn = tmp[\ngroup_to_channel[group[neuron]]\n]  # restrict dat file to the channel group of the neuron\nfor time in tmp2:  # add each spike waveform to neuron_waveform\nspikewindow = tmpn.loc[\ntime - waveform_window[0] : time + waveform_window[1] - 1\n]  # waveform for this spike time\ntry:\nneuron_waveforms[neuron] += spikewindow.values\nexcept (\nException\n):  # ignore if full waveform is not present in this batch\npass\nmeanwf = {\nn: pd.DataFrame(\ndata=np.array(neuron_waveforms[n]) / spike_count,\ncolumns=np.arange(len(group_to_channel[group[n]])),\nindex=np.array(np.arange(-waveform_window[0], waveform_window[1])) / fs,\n)\nfor n in sample_counted_spikes\n}\n# find the max channel for each neuron\nmaxch = pd.Series(\ndata=[meanwf[n][meanwf[n].loc[0].idxmin()].name for n in meanwf],\nindex=spikes.keys(),\n)\nreturn meanwf, maxch\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"old_pages/io.npz/","title":"Io.npz","text":""},{"location":"old_pages/io.npz/#pynapple.io.interface_npz","title":"pynapple.io.interface_npz","text":"<p>File classes help to validate and load pynapple objects or NWB files. Data are always lazy-loaded. Both classes behaves like dictionnary.</p>"},{"location":"old_pages/io.npz/#pynapple.io.interface_npz.NPZFile","title":"NPZFile","text":"<p>             Bases: <code>object</code></p> <p>Class that points to a NPZ file that can be loaded as a pynapple object. Objects have a save function in npz format as well as the Folder class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; tsd = nap.load_file(\"path/to/my_tsd.npz\")\n&gt;&gt;&gt; tsd\nTime (s)\n0.0    0\n0.1    1\n0.2    2\ndtype: int64\n</code></pre> Source code in <code>pynapple/io/interface_npz.py</code> <pre><code>class NPZFile(object):\n\"\"\"Class that points to a NPZ file that can be loaded as a pynapple object.\n    Objects have a save function in npz format as well as the Folder class.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; tsd = nap.load_file(\"path/to/my_tsd.npz\")\n    &gt;&gt;&gt; tsd\n    Time (s)\n    0.0    0\n    0.1    1\n    0.2    2\n    dtype: int64\n    \"\"\"\ndef __init__(self, path):\n\"\"\"Initialization of the NPZ file\n        Parameters\n        ----------\n        path : str\n            Valid path to a NPZ file\n        \"\"\"\nself.path = path\nself.name = os.path.basename(path)\nself.file = np.load(self.path, allow_pickle=True)\nself.type = \"\"\n# First check if type is explicitely defined\npossible = [\"Ts\", \"Tsd\", \"TsdFrame\", \"TsdTensor\", \"TsGroup\", \"IntervalSet\"]\nif \"type\" in self.file.keys():\nif len(self.file[\"type\"]) == 1:\nif isinstance(self.file[\"type\"][0], np.str_):\nif self.file[\"type\"] in possible:\nself.type = self.file[\"type\"][0]\n# Second check manually\nif self.type == \"\":\nk = set(self.file.keys())\nif {\"t\", \"start\", \"end\", \"index\"}.issubset(k):\nself.type = \"TsGroup\"\nelif {\"t\", \"d\", \"start\", \"end\", \"columns\"}.issubset(k):\nself.type = \"TsdFrame\"\nelif {\"t\", \"d\", \"start\", \"end\"}.issubset(k):\nif self.file[\"d\"].ndim == 1:\nself.type = \"Tsd\"\nelse:\nself.type = \"TsdTensor\"\nelif {\"t\", \"start\", \"end\"}.issubset(k):\nself.type = \"Ts\"\nelif {\"start\", \"end\"}.issubset(k):\nself.type = \"IntervalSet\"\nelse:\nself.type = \"npz\"\ndef load(self):\n\"\"\"Load the NPZ file\n        Returns\n        -------\n        (Tsd, Ts, TsdFrame, TsdTensor, TsGroup, IntervalSet)\n            A pynapple object\n        \"\"\"\nif self.type == \"npz\":\nreturn self.file\nelse:\ntime_support = nap.IntervalSet(self.file[\"start\"], self.file[\"end\"])\nif self.type == \"TsGroup\":\ntsd = nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"index\"], time_support=time_support\n)\ntsgroup = tsd.to_tsgroup()\nif \"d\" in self.file.keys():\nprint(\"TODO\")\nmetainfo = {}\nfor k in set(self.file.keys()) - {\n\"start\",\n\"end\",\n\"t\",\n\"index\",\n\"d\",\n\"rate\",\n}:\ntmp = self.file[k]\nif len(tmp) == len(tsgroup):\nmetainfo[k] = tmp\ntsgroup.set_info(**metainfo)\nreturn tsgroup\nelif self.type == \"TsdFrame\":\nreturn nap.TsdFrame(\nt=self.file[\"t\"],\nd=self.file[\"d\"],\ntime_support=time_support,\ncolumns=self.file[\"columns\"],\n)\nelif self.type == \"TsdTensor\":\nreturn nap.TsdTensor(\nt=self.file[\"t\"], d=self.file[\"d\"], time_support=time_support\n)\nelif self.type == \"Tsd\":\nreturn nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"d\"], time_support=time_support\n)\nelif self.type == \"Ts\":\nreturn nap.Ts(t=self.file[\"t\"], time_support=time_support)\nelif self.type == \"IntervalSet\":\nreturn time_support\nelse:\nreturn self.file\n</code></pre>"},{"location":"old_pages/io.npz/#pynapple.io.interface_npz.NPZFile.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Initialization of the NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Valid path to a NPZ file</p> required Source code in <code>pynapple/io/interface_npz.py</code> <pre><code>def __init__(self, path):\n\"\"\"Initialization of the NPZ file\n    Parameters\n    ----------\n    path : str\n        Valid path to a NPZ file\n    \"\"\"\nself.path = path\nself.name = os.path.basename(path)\nself.file = np.load(self.path, allow_pickle=True)\nself.type = \"\"\n# First check if type is explicitely defined\npossible = [\"Ts\", \"Tsd\", \"TsdFrame\", \"TsdTensor\", \"TsGroup\", \"IntervalSet\"]\nif \"type\" in self.file.keys():\nif len(self.file[\"type\"]) == 1:\nif isinstance(self.file[\"type\"][0], np.str_):\nif self.file[\"type\"] in possible:\nself.type = self.file[\"type\"][0]\n# Second check manually\nif self.type == \"\":\nk = set(self.file.keys())\nif {\"t\", \"start\", \"end\", \"index\"}.issubset(k):\nself.type = \"TsGroup\"\nelif {\"t\", \"d\", \"start\", \"end\", \"columns\"}.issubset(k):\nself.type = \"TsdFrame\"\nelif {\"t\", \"d\", \"start\", \"end\"}.issubset(k):\nif self.file[\"d\"].ndim == 1:\nself.type = \"Tsd\"\nelse:\nself.type = \"TsdTensor\"\nelif {\"t\", \"start\", \"end\"}.issubset(k):\nself.type = \"Ts\"\nelif {\"start\", \"end\"}.issubset(k):\nself.type = \"IntervalSet\"\nelse:\nself.type = \"npz\"\n</code></pre>"},{"location":"old_pages/io.npz/#pynapple.io.interface_npz.NPZFile.load","title":"load","text":"<pre><code>load()\n</code></pre> <p>Load the NPZ file</p> <p>Returns:</p> Type Description <code>(Tsd, Ts, TsdFrame, TsdTensor, TsGroup, IntervalSet)</code> <p>A pynapple object</p> Source code in <code>pynapple/io/interface_npz.py</code> <pre><code>def load(self):\n\"\"\"Load the NPZ file\n    Returns\n    -------\n    (Tsd, Ts, TsdFrame, TsdTensor, TsGroup, IntervalSet)\n        A pynapple object\n    \"\"\"\nif self.type == \"npz\":\nreturn self.file\nelse:\ntime_support = nap.IntervalSet(self.file[\"start\"], self.file[\"end\"])\nif self.type == \"TsGroup\":\ntsd = nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"index\"], time_support=time_support\n)\ntsgroup = tsd.to_tsgroup()\nif \"d\" in self.file.keys():\nprint(\"TODO\")\nmetainfo = {}\nfor k in set(self.file.keys()) - {\n\"start\",\n\"end\",\n\"t\",\n\"index\",\n\"d\",\n\"rate\",\n}:\ntmp = self.file[k]\nif len(tmp) == len(tsgroup):\nmetainfo[k] = tmp\ntsgroup.set_info(**metainfo)\nreturn tsgroup\nelif self.type == \"TsdFrame\":\nreturn nap.TsdFrame(\nt=self.file[\"t\"],\nd=self.file[\"d\"],\ntime_support=time_support,\ncolumns=self.file[\"columns\"],\n)\nelif self.type == \"TsdTensor\":\nreturn nap.TsdTensor(\nt=self.file[\"t\"], d=self.file[\"d\"], time_support=time_support\n)\nelif self.type == \"Tsd\":\nreturn nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"d\"], time_support=time_support\n)\nelif self.type == \"Ts\":\nreturn nap.Ts(t=self.file[\"t\"], time_support=time_support)\nelif self.type == \"IntervalSet\":\nreturn time_support\nelse:\nreturn self.file\n</code></pre>"},{"location":"old_pages/io.nwb/","title":"Io.nwb","text":""},{"location":"old_pages/io.nwb/#pynapple.io.interface_nwb","title":"pynapple.io.interface_nwb","text":"<p>Pynapple class to interface with NWB files. Data are always lazy-loaded. Object behaves like dictionary.</p>"},{"location":"old_pages/io.nwb/#pynapple.io.interface_nwb.NWBFile","title":"NWBFile","text":"<p>             Bases: <code>UserDict</code></p> <p>Class for reading NWB Files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; data = nap.load_file(\"my_file.nwb\")\n&gt;&gt;&gt; data[\"units\"]\n  Index    rate  location      group\n-------  ------  ----------  -------\n      0    1.0  brain        0\n      1    1.0  brain        0\n      2    1.0  brain        0\n</code></pre> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>class NWBFile(UserDict):\n\"\"\"Class for reading NWB Files.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; data = nap.load_file(\"my_file.nwb\")\n    &gt;&gt;&gt; data[\"units\"]\n      Index    rate  location      group\n    -------  ------  ----------  -------\n          0    1.0  brain        0\n          1    1.0  brain        0\n          2    1.0  brain        0\n    \"\"\"\n_f_eval = {\n\"IntervalSet\": _make_interval_set,\n\"Tsd\": _make_tsd,\n\"Ts\": _make_ts,\n\"TsdFrame\": _make_tsd_frame,\n\"TsdTensor\": _make_tsd_tensor,\n\"TsGroup\": _make_tsgroup,\n}\ndef __init__(self, file):\n\"\"\"\n        Parameters\n        ----------\n        file : str or pynwb.file.NWBFile\n            Valid file to a NWB file\n        Raises\n        ------\n        FileNotFoundError\n            If path is invalid\n        RuntimeError\n            If file is not an instance of NWBFile\n        \"\"\"\nif isinstance(file, str):\nif os.path.exists(file):\nself.path = file\nself.name = os.path.basename(file).split(\".\")[0]\nself.io = NWBHDF5IO(file, \"r\")\nself.nwb = self.io.read()\nelse:\nraise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), file)\nelif isinstance(file, pynwb.file.NWBFile):\nself.nwb = file\nself.name = self.nwb.session_id\nelse:\nraise RuntimeError(\n\"unrecognized argument. Please provide path to a valid NWB file or open NWB file.\"\n)\nself.data = _extract_compatible_data_from_nwbfile(self.nwb)\nself.key_to_id = {k: self.data[k][\"id\"] for k in self.data.keys()}\nself._view = [[k, self.data[k][\"type\"]] for k in self.data.keys()]\nUserDict.__init__(self, self.data)\ndef __str__(self):\ntitle = self.name if isinstance(self.name, str) else \"-\"\nheaders = [\"Keys\", \"Type\"]\nreturn (\ntitle\n+ \"\\n\"\n+ tabulate(self._view, headers=headers, tablefmt=\"mixed_outline\")\n)\n# self._view = Table(title=self.name)\n# self._view.add_column(\"Keys\", justify=\"left\", style=\"cyan\", no_wrap=True)\n# self._view.add_column(\"Type\", style=\"green\")\n# for k in self.data.keys():\n#     self._view.add_row(\n#         k,\n#         self.data[k][\"type\"],\n#     )\n# \"\"\"View of the object\"\"\"\n# with Console() as console:\n#     console.print(self._view)\n# return \"\"\ndef __repr__(self):\n\"\"\"View of the object\"\"\"\nreturn self.__str__()\ndef __getitem__(self, key):\n\"\"\"Get object from NWB\n        Parameters\n        ----------\n        key : str\n        Returns\n        -------\n        (Ts, Tsd, TsdFrame, TsGroup, IntervalSet or dict of IntervalSet)\n        Raises\n        ------\n        KeyError\n            If key is not in the dictionary\n        \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], dict) and \"id\" in self.data[key]:\nobj = self.nwb.objects[self.data[key][\"id\"]]\ntry:\ndata = self._f_eval[self.data[key][\"type\"]](obj)\nexcept Exception:\nwarnings.warn(\n\"Failed to build {}.\\n Returning the NWB object for manual inspection\".format(\nself.data[key][\"type\"]\n),\nstacklevel=2,\n)\ndata = obj\nself.data[key] = data\nreturn data\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n</code></pre>"},{"location":"old_pages/io.nwb/#pynapple.io.interface_nwb.NWBFile.__init__","title":"__init__","text":"<pre><code>__init__(file)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str or NWBFile</code> <p>Valid file to a NWB file</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If path is invalid</p> <code>RuntimeError</code> <p>If file is not an instance of NWBFile</p> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>def __init__(self, file):\n\"\"\"\n    Parameters\n    ----------\n    file : str or pynwb.file.NWBFile\n        Valid file to a NWB file\n    Raises\n    ------\n    FileNotFoundError\n        If path is invalid\n    RuntimeError\n        If file is not an instance of NWBFile\n    \"\"\"\nif isinstance(file, str):\nif os.path.exists(file):\nself.path = file\nself.name = os.path.basename(file).split(\".\")[0]\nself.io = NWBHDF5IO(file, \"r\")\nself.nwb = self.io.read()\nelse:\nraise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), file)\nelif isinstance(file, pynwb.file.NWBFile):\nself.nwb = file\nself.name = self.nwb.session_id\nelse:\nraise RuntimeError(\n\"unrecognized argument. Please provide path to a valid NWB file or open NWB file.\"\n)\nself.data = _extract_compatible_data_from_nwbfile(self.nwb)\nself.key_to_id = {k: self.data[k][\"id\"] for k in self.data.keys()}\nself._view = [[k, self.data[k][\"type\"]] for k in self.data.keys()]\nUserDict.__init__(self, self.data)\n</code></pre>"},{"location":"old_pages/io.nwb/#pynapple.io.interface_nwb.NWBFile.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>View of the object</p> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>def __repr__(self):\n\"\"\"View of the object\"\"\"\nreturn self.__str__()\n</code></pre>"},{"location":"old_pages/io.nwb/#pynapple.io.interface_nwb.NWBFile.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key)\n</code></pre> <p>Get object from NWB</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required <p>Returns:</p> Type Description <code>(Ts, Tsd, TsdFrame, TsGroup, IntervalSet or dict of IntervalSet)</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If key is not in the dictionary</p> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>def __getitem__(self, key):\n\"\"\"Get object from NWB\n    Parameters\n    ----------\n    key : str\n    Returns\n    -------\n    (Ts, Tsd, TsdFrame, TsGroup, IntervalSet or dict of IntervalSet)\n    Raises\n    ------\n    KeyError\n        If key is not in the dictionary\n    \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], dict) and \"id\" in self.data[key]:\nobj = self.nwb.objects[self.data[key][\"id\"]]\ntry:\ndata = self._f_eval[self.data[key][\"type\"]](obj)\nexcept Exception:\nwarnings.warn(\n\"Failed to build {}.\\n Returning the NWB object for manual inspection\".format(\nself.data[key][\"type\"]\n),\nstacklevel=2,\n)\ndata = obj\nself.data[key] = data\nreturn data\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n</code></pre>"},{"location":"old_pages/io.phy/","title":"Io.phy","text":""},{"location":"old_pages/io.phy/#pynapple.io.phy","title":"pynapple.io.phy","text":"<p> DEPRECATED: This will be removed in version 1.0.0. Check nwbmatic or neuroconv instead.</p> <p>Class and functions for loading data processed with Phy2</p> <p>@author: Sara Mahallati, Guillaume Viejo</p>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy","title":"Phy","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader for Phy data</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>class Phy(BaseLoader):\n\"\"\"\n    Loader for Phy data\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Instantiate the data class from a Phy folder.\n        Parameters\n        ----------\n        path : str or Path object\n            The path to the data.\n        \"\"\"\nself.time_support = None\nself.sample_rate = None\nself.n_channels_dat = None\nself.channel_map = None\nself.ch_to_sh = None\nself.spikes = None\nself.channel_positions = None\nsuper().__init__(path)\n# This path stuff should happen only once in the parent class\nself.path = Path(path)\nself.basename = self.path.name\nself.nwb_path = self.path / \"pynapplenwb\"\n# from what I can see in the loading function, only one nwb file per folder:\ntry:\nself.nwb_file = list(self.nwb_path.glob(\"*.nwb\"))[0]\nexcept IndexError:\nself.nwb_file = None\n# Need to check if nwb file exists and if data are there\n# if self.path is not None:  -&gt; are there any cases where this is None?\nif self.nwb_file is not None:\nloaded_spikes = self.load_nwb_spikes()\nif loaded_spikes is not None:\nreturn\n# Bypass if data have already been transferred to nwb\nself.load_phy_params()\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.channel_map)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_phy_spikes(self.time_support)\nself.save_data()\napp.quit()\ndef load_phy_params(self):\n\"\"\"\n        path should be the folder session containing the params.py file\n        Function reads :\n        1. the number of channels\n        2. the sampling frequency of the dat file\n        Raises\n        ------\n        AssertionError\n            If path does not contain the params file or channel_map.npy\n        \"\"\"\nassert (\nself.path / \"params.py\"\n).exists(), f\"Can't find params.py in {self.path}\"\n# It is strongly recommended not to conflate parameters and code! Also, there's a library called params.\n# I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py!\n# In this way we just read the file, and we don't have to add to sys to import...\n# TODO maybe remove this\nsys.path.append(str(self.path))\nimport params as params\nself.sample_rate = params.sample_rate\nself.n_channels_dat = params.n_channels_dat\nassert (\nself.path / \"channel_map.npy\"\n).exists(), f\"Can't find channel_map.npy in {self.path}\"\nchannel_map = np.load(self.path / \"channel_map.npy\")\nif (self.path / \"channel_shanks.npy\").exists():\nchannel_shank = np.load(self.path / \"channel_shanks.npy\")\nn_shanks = len(np.unique(channel_shank))\nself.channel_map = {\ni: channel_map[channel_shank == i] for i in range(n_shanks)\n}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=channel_shank.flatten(),\n)\nelse:\nself.channel_map = {i: channel_map[i] for i in range(len(channel_map))}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=np.hstack(\n[\nnp.ones(len(channel_map[i]), dtype=int) * i\nfor i in range(len(channel_map))\n]\n),\n)\nreturn\ndef load_phy_spikes(self, time_support=None):\n\"\"\"\n        Load Phy spike times and convert to NWB.\n        Instantiate automatically a TsGroup object.\n        The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv\n        Parameters\n        ----------\n        path : Path object\n            The path to the data\n        time_support : IntevalSet, optional\n            The time support of the data\n        Raises\n        ------\n        RuntimeError\n            If files are missing.\n            The function needs :\n            - cluster_info.tsv or cluster_group.tsv\n            - spike_times.npy\n            - spike_clusters.npy\n            - channel_positions.npy\n            - templates.npy\n        \"\"\"\n# Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used:\nhas_cluster_info = False\nif (self.path / \"cluster_info.tsv\").exists():\ncluster_info_file = self.path / \"cluster_info.tsv\"\nhas_cluster_info = True\nelif (self.path / \"cluster_group.tsv\").exists():\ncluster_info_file = self.path / \"cluster_group.tsv\"\nelse:\nraise RuntimeError(\n\"Can't find cluster_info.tsv or cluster_group.tsv in {};\".format(\nself.path\n)\n)\ncluster_info = pd.read_csv(cluster_info_file, sep=\"\\t\", index_col=\"cluster_id\")\n# In my processed data with KiloSort 3.0, the column is named KSLabel\nif \"group\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.group == \"good\"].index.values\nelif \"KSLabel\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.KSLabel == \"good\"].index.values\nelse:\nraise RuntimeError(\n\"Can't find column group or KSLabel in {};\".format(cluster_info_file)\n)\nspike_times = np.load(self.path / \"spike_times.npy\")\nspike_clusters = np.load(self.path / \"spike_clusters.npy\")\nspikes = {}\nfor n in cluster_id_good:\nspikes[n] = nap.Ts(\nt=spike_times[spike_clusters == n] / self.sample_rate,\ntime_support=time_support,\n)\nself.spikes = nap.TsGroup(spikes, time_support=time_support)\n# Adding the position of the electrodes in case\nself.channel_positions = np.load(self.path / \"channel_positions.npy\")\n# Adding shank group info from cluster_info if present\nif has_cluster_info:\ngroup = cluster_info.loc[cluster_id_good, \"sh\"]\nself.spikes.set_info(group=group)\nelse:\ntemplate = np.load(self.path / \"templates.npy\")\ntemplate = template[cluster_id_good]\nch = np.power(template, 2).max(1).argmax(1)\ngroup = pd.Series(index=cluster_id_good, data=self.ch_to_sh[ch].values)\nself.spikes.set_info(group=group)\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\ndef save_data(self):\n\"\"\"Save the data to NWB format.\"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.channel_map:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.channel_map[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_spikes(self):\n\"\"\"Read the NWB spikes to extract the spike times.\n        Returns\n        -------\n        TYPE\n            Description\n        \"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn None\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\ndef load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n        Load the LFP.\n        Parameters\n        ----------\n        filename : str, optional\n            The filename of the lfp file.\n            It can be useful it multiple dat files are present in the data directory\n        channel : int or list of int, optional\n            The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n        extension : str, optional\n            The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n        frequency : float, optional\n            Default 1250 Hz for the eeg file\n        precision : str, optional\n            The precision of the binary file\n        bytes_size : int, optional\n            Bytes size of the lfp file\n        Raises\n        ------\n        RuntimeError\n            If can't find the lfp/eeg/dat file\n        Returns\n        -------\n        Tsd or TsdFrame\n            The lfp in a time series format\n        \"\"\"\nif filename is not None:\nfilepath = self.path / filename\nelse:\ntry:\nfilepath = list(self.path.glob(f\"*{extension}\"))[0]\nexcept IndexError:\nraise RuntimeError(f\"Path {self.path} contains no {extension} files;\")\n# is it possible that this is a leftover from neurosuite data?\n# This is not implemented for this class.\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Instantiate the data class from a Phy folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path object</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/phy.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Instantiate the data class from a Phy folder.\n    Parameters\n    ----------\n    path : str or Path object\n        The path to the data.\n    \"\"\"\nself.time_support = None\nself.sample_rate = None\nself.n_channels_dat = None\nself.channel_map = None\nself.ch_to_sh = None\nself.spikes = None\nself.channel_positions = None\nsuper().__init__(path)\n# This path stuff should happen only once in the parent class\nself.path = Path(path)\nself.basename = self.path.name\nself.nwb_path = self.path / \"pynapplenwb\"\n# from what I can see in the loading function, only one nwb file per folder:\ntry:\nself.nwb_file = list(self.nwb_path.glob(\"*.nwb\"))[0]\nexcept IndexError:\nself.nwb_file = None\n# Need to check if nwb file exists and if data are there\n# if self.path is not None:  -&gt; are there any cases where this is None?\nif self.nwb_file is not None:\nloaded_spikes = self.load_nwb_spikes()\nif loaded_spikes is not None:\nreturn\n# Bypass if data have already been transferred to nwb\nself.load_phy_params()\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.channel_map)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_phy_spikes(self.time_support)\nself.save_data()\napp.quit()\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_phy_params","title":"load_phy_params","text":"<pre><code>load_phy_params()\n</code></pre> <p>path should be the folder session containing the params.py file</p> <p>Function reads : 1. the number of channels 2. the sampling frequency of the dat file</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If path does not contain the params file or channel_map.npy</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_phy_params(self):\n\"\"\"\n    path should be the folder session containing the params.py file\n    Function reads :\n    1. the number of channels\n    2. the sampling frequency of the dat file\n    Raises\n    ------\n    AssertionError\n        If path does not contain the params file or channel_map.npy\n    \"\"\"\nassert (\nself.path / \"params.py\"\n).exists(), f\"Can't find params.py in {self.path}\"\n# It is strongly recommended not to conflate parameters and code! Also, there's a library called params.\n# I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py!\n# In this way we just read the file, and we don't have to add to sys to import...\n# TODO maybe remove this\nsys.path.append(str(self.path))\nimport params as params\nself.sample_rate = params.sample_rate\nself.n_channels_dat = params.n_channels_dat\nassert (\nself.path / \"channel_map.npy\"\n).exists(), f\"Can't find channel_map.npy in {self.path}\"\nchannel_map = np.load(self.path / \"channel_map.npy\")\nif (self.path / \"channel_shanks.npy\").exists():\nchannel_shank = np.load(self.path / \"channel_shanks.npy\")\nn_shanks = len(np.unique(channel_shank))\nself.channel_map = {\ni: channel_map[channel_shank == i] for i in range(n_shanks)\n}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=channel_shank.flatten(),\n)\nelse:\nself.channel_map = {i: channel_map[i] for i in range(len(channel_map))}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=np.hstack(\n[\nnp.ones(len(channel_map[i]), dtype=int) * i\nfor i in range(len(channel_map))\n]\n),\n)\nreturn\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_phy_spikes","title":"load_phy_spikes","text":"<pre><code>load_phy_spikes(time_support=None)\n</code></pre> <p>Load Phy spike times and convert to NWB. Instantiate automatically a TsGroup object. The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path object</code> <p>The path to the data</p> required <code>time_support</code> <code>IntevalSet</code> <p>The time support of the data</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If files are missing. The function needs : - cluster_info.tsv or cluster_group.tsv - spike_times.npy - spike_clusters.npy - channel_positions.npy - templates.npy</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_phy_spikes(self, time_support=None):\n\"\"\"\n    Load Phy spike times and convert to NWB.\n    Instantiate automatically a TsGroup object.\n    The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv\n    Parameters\n    ----------\n    path : Path object\n        The path to the data\n    time_support : IntevalSet, optional\n        The time support of the data\n    Raises\n    ------\n    RuntimeError\n        If files are missing.\n        The function needs :\n        - cluster_info.tsv or cluster_group.tsv\n        - spike_times.npy\n        - spike_clusters.npy\n        - channel_positions.npy\n        - templates.npy\n    \"\"\"\n# Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used:\nhas_cluster_info = False\nif (self.path / \"cluster_info.tsv\").exists():\ncluster_info_file = self.path / \"cluster_info.tsv\"\nhas_cluster_info = True\nelif (self.path / \"cluster_group.tsv\").exists():\ncluster_info_file = self.path / \"cluster_group.tsv\"\nelse:\nraise RuntimeError(\n\"Can't find cluster_info.tsv or cluster_group.tsv in {};\".format(\nself.path\n)\n)\ncluster_info = pd.read_csv(cluster_info_file, sep=\"\\t\", index_col=\"cluster_id\")\n# In my processed data with KiloSort 3.0, the column is named KSLabel\nif \"group\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.group == \"good\"].index.values\nelif \"KSLabel\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.KSLabel == \"good\"].index.values\nelse:\nraise RuntimeError(\n\"Can't find column group or KSLabel in {};\".format(cluster_info_file)\n)\nspike_times = np.load(self.path / \"spike_times.npy\")\nspike_clusters = np.load(self.path / \"spike_clusters.npy\")\nspikes = {}\nfor n in cluster_id_good:\nspikes[n] = nap.Ts(\nt=spike_times[spike_clusters == n] / self.sample_rate,\ntime_support=time_support,\n)\nself.spikes = nap.TsGroup(spikes, time_support=time_support)\n# Adding the position of the electrodes in case\nself.channel_positions = np.load(self.path / \"channel_positions.npy\")\n# Adding shank group info from cluster_info if present\nif has_cluster_info:\ngroup = cluster_info.loc[cluster_id_good, \"sh\"]\nself.spikes.set_info(group=group)\nelse:\ntemplate = np.load(self.path / \"templates.npy\")\ntemplate = template[cluster_id_good]\nch = np.power(template, 2).max(1).argmax(1)\ngroup = pd.Series(index=cluster_id_good, data=self.ch_to_sh[ch].values)\nself.spikes.set_info(group=group)\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.save_data","title":"save_data","text":"<pre><code>save_data()\n</code></pre> <p>Save the data to NWB format.</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def save_data(self):\n\"\"\"Save the data to NWB format.\"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.channel_map:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.channel_map[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_nwb_spikes","title":"load_nwb_spikes","text":"<pre><code>load_nwb_spikes()\n</code></pre> <p>Read the NWB spikes to extract the spike times.</p> <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_nwb_spikes(self):\n\"\"\"Read the NWB spikes to extract the spike times.\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn None\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_lfp","title":"load_lfp","text":"<pre><code>load_lfp(\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n)\n</code></pre> <p>Load the LFP.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename of the lfp file. It can be useful it multiple dat files are present in the data directory</p> <code>None</code> <code>channel</code> <code>int or list of int</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>extension</code> <code>str</code> <p>The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> <code>'.eeg'</code> <code>frequency</code> <code>float</code> <p>Default 1250 Hz for the eeg file</p> <code>1250.0</code> <code>precision</code> <code>str</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the lfp file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Load the LFP.\n    Parameters\n    ----------\n    filename : str, optional\n        The filename of the lfp file.\n        It can be useful it multiple dat files are present in the data directory\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    frequency : float, optional\n        Default 1250 Hz for the eeg file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the lfp file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    \"\"\"\nif filename is not None:\nfilepath = self.path / filename\nelse:\ntry:\nfilepath = list(self.path.glob(f\"*{extension}\"))[0]\nexcept IndexError:\nraise RuntimeError(f\"Path {self.path} contains no {extension} files;\")\n# is it possible that this is a leftover from neurosuite data?\n# This is not implemented for this class.\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"old_pages/io.suite2p/","title":"Io.suite2p","text":""},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p","title":"pynapple.io.suite2p","text":"<p> DEPRECATED: This will be removed in version 1.0.0. Check nwbmatic or neuroconv instead.</p> <p>Loader for Suite2P https://github.com/MouseLand/suite2p</p>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P","title":"Suite2P","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader for data processed with Suite2P.</p> <p>Pynapple will try to look for data in this order :</p> <ol> <li> <p>pynapplenwb/session_name.nwb</p> </li> <li> <p>suite2p/plane/.npy</p> </li> </ol> <p>Attributes:</p> Name Type Description <code>F</code> <code>TsdFrame</code> <p>Fluorescence traces (timepoints x ROIs) for all planes</p> <code>Fneu</code> <code>TsdFrame</code> <p>Neuropil fluorescence traces (timepoints x ROIs) for all planes</p> <code>spks</code> <code>TsdFrame</code> <p>Deconvolved traces (timepoints x ROIS) for all planes</p> <code>plane_info</code> <code>DataFrame</code> <p>Contains plane identity of each cell</p> <code>stats</code> <code>dict</code> <p>dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells (Can be smaller when loading from the NWB file)</p> <code>ops</code> <code>dict</code> <p>Parameters from Suite2p. (Can be smaller when loading from the NWB file)</p> <code>iscell</code> <code>ndarray</code> <p>Cell classification</p> Source code in <code>pynapple/io/suite2p.py</code> <pre><code>class Suite2P(BaseLoader):\n\"\"\"Loader for data processed with Suite2P.\n    Pynapple will try to look for data in this order :\n    1. pynapplenwb/session_name.nwb\n    2. suite2p/plane*/*.npy\n    Attributes\n    ----------\n    F : TsdFrame\n        Fluorescence traces (timepoints x ROIs) for all planes\n    Fneu : TsdFrame\n        Neuropil fluorescence traces (timepoints x ROIs) for all planes\n    spks : TsdFrame\n        Deconvolved traces (timepoints x ROIS) for all planes\n    plane_info : pandas.DataFrame\n        Contains plane identity of each cell\n    stats : dict\n        dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells\n        (Can be smaller when loading from the NWB file)\n    ops : dict\n        Parameters from Suite2p. (Can be smaller when loading from the NWB file)\n    iscell : numpy.ndarray\n        Cell classification\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_suite2p_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_suite2p(path)\nself.save_suite2p_nwb(path)\ndef load_suite2p(self, path):\n\"\"\"\n        Looking for suite2/plane*\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.path_suite2p = os.path.join(path, \"suite2p\")\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ndata = {\n\"F\": [],\n\"Fneu\": [],\n\"spks\": [],\n}\nplane_info = []\nself.stats = {}\nself.pops = {}\nself.iscells = {}\nself.planes = []\nif os.path.exists(self.path_suite2p):\nplanes = glob.glob(os.path.join(self.path_suite2p, \"plane*\"))\nif len(planes):\n# count = 0\nfor plane_dir in planes:\nn = int(os.path.basename(plane_dir)[-1])\nself.planes.append(n)\n# Loading iscell.npy\ntry:\niscell = np.load(\nos.path.join(plane_dir, \"iscell.npy\"), allow_pickle=True\n)\nidx = np.where(iscell.astype(\"int\")[:, 0])[0]\nplane_info.append(np.ones(len(idx), dtype=\"int\") * n)\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading F.npy, Fneu.py and spks.npy\nfor obj in [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]:\ntry:\nname = obj.split(\".\")[0]\ntmp = np.load(\nos.path.join(plane_dir, obj), allow_pickle=True\n)\ndata[name].append(tmp[idx])\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading stat.npy and ops.npy\ntry:\nstat = np.load(\nos.path.join(plane_dir, \"stat.npy\"), allow_pickle=True\n)\nops = np.load(\nos.path.join(plane_dir, \"ops.npy\"), allow_pickle=True\n).item()\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Saving stat, ops and iscell\nself.stats[n] = stat\nself.pops[n] = ops\nself.iscells[n] = iscell\n# count += len(idx)\nelse:\nwarnings.warn(\n\"Couldn't find planes in %s\" % self.path_suite2p, stacklevel=2\n)\nsys.exit()\nelse:\nwarnings.warn(\"No suite2p folder in %s\" % path, stacklevel=2)\nsys.exit()\n# Calcium transients\ndata[\"F\"] = np.transpose(np.vstack(data[\"F\"]))\ndata[\"Fneu\"] = np.transpose(np.vstack(data[\"Fneu\"]))\ndata[\"spks\"] = np.transpose(np.vstack(data[\"spks\"]))\ntime_index = np.arange(0, len(data[\"F\"])) / self.sampling_rate\nself.F = nap.TsdFrame(t=time_index, d=data[\"F\"])\nself.Fneu = nap.TsdFrame(t=time_index, d=data[\"Fneu\"])\nself.spks = nap.TsdFrame(t=time_index, d=data[\"spks\"])\nself.ops = self.pops[0]\nself.iscell = np.vstack([self.iscells[k] for k in self.iscells.keys()])\n# Metadata\nself.plane_info = pd.DataFrame.from_dict({\"plane\": np.hstack(plane_info)})\nreturn\ndef save_suite2p_nwb(self, path):\n\"\"\"\n        Save the data to NWB. To ensure continuity, this function is based on :\n        https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nmultiplane = True if len(self.planes) &gt; 1 else False\nops = self.pops[list(self.pops.keys())[0]]\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice = nwbfile.create_device(\nname=self.ophys_information[\"device\"][\"name\"],\ndescription=self.ophys_information[\"device\"][\"description\"],\nmanufacturer=self.ophys_information[\"device\"][\"manufacturer\"],\n)\nimaging_plane = nwbfile.create_imaging_plane(\nname=self.ophys_information[\"ImagingPlane\"][\"name\"],\noptical_channel=OpticalChannel(\nname=self.ophys_information[\"OpticalChannel\"][\"name\"],\ndescription=self.ophys_information[\"OpticalChannel\"][\"description\"],\nemission_lambda=float(\nself.ophys_information[\"OpticalChannel\"][\"emission_lambda\"]\n),\n),\nimaging_rate=self.sampling_rate,\ndescription=self.ophys_information[\"ImagingPlane\"][\"description\"],\ndevice=device,\nexcitation_lambda=float(\nself.ophys_information[\"ImagingPlane\"][\"excitation_lambda\"]\n),\nindicator=self.ophys_information[\"ImagingPlane\"][\"indicator\"],\nlocation=self.ophys_information[\"ImagingPlane\"][\"location\"],\ngrid_spacing=([2.0, 2.0, 30.0] if multiplane else [2.0, 2.0]),\ngrid_spacing_unit=\"microns\",\n)\n# link to external data\nimage_series = TwoPhotonSeries(\nname=\"TwoPhotonSeries\",\ndimension=[ops[\"Ly\"], ops[\"Lx\"]],\nexternal_file=(ops[\"filelist\"] if \"filelist\" in ops else [\"\"]),\nimaging_plane=imaging_plane,\nstarting_frame=[0],\nformat=\"external\",\nstarting_time=0.0,\nrate=ops[\"fs\"] * ops[\"nplanes\"],\n)\nnwbfile.add_acquisition(image_series)\n# processing\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=self.ophys_information[\"PlaneSegmentation\"][\"name\"],\ndescription=self.ophys_information[\"PlaneSegmentation\"][\"description\"],\nimaging_plane=imaging_plane,\n# reference_images=image_series,\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nophys_module.add(img_seg)\nfile_strs = [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]\ntraces = []\nncells = np.zeros(len(self.pops), dtype=np.int_)\nNfr = np.array([self.pops[k][\"nframes\"] for k in self.pops.keys()]).max()\nfor iplane, ops in self.pops.items():\nif iplane == 0:\niscell = self.iscells[iplane]\nfor fstr in file_strs:\ntraces.append(np.load(os.path.join(ops[\"save_path\"], fstr)))\nPlaneCellsIdx = iplane * np.ones(len(iscell))\nelse:\niscell = np.append(\niscell,\nself.iscells[iplane],\naxis=0,\n)\nfor i, fstr in enumerate(file_strs):\ntrace = np.load(os.path.join(ops[\"save_path\"], fstr))\nif trace.shape[1] &lt; Nfr:\nfcat = np.zeros(\n(trace.shape[0], Nfr - trace.shape[1]), \"float32\"\n)\ntrace = np.concatenate((trace, fcat), axis=1)\ntraces[i] = np.append(traces[i], trace, axis=0)\nPlaneCellsIdx = np.append(\nPlaneCellsIdx, iplane * np.ones(len(iscell) - len(PlaneCellsIdx))\n)\nstat = self.stats[iplane]\nncells[iplane] = len(stat)\nfor n in range(ncells[iplane]):\nif multiplane:\npixel_mask = np.array(\n[\nstat[n][\"ypix\"],\nstat[n][\"xpix\"],\niplane * np.ones(stat[n][\"npix\"]),\nstat[n][\"lam\"],\n]\n)\nps.add_roi(voxel_mask=pixel_mask.T)\nelse:\npixel_mask = np.array(\n[stat[n][\"ypix\"], stat[n][\"xpix\"], stat[n][\"lam\"]]\n)\nps.add_roi(pixel_mask=pixel_mask.T)\nps.add_column(\"iscell\", \"two columns - iscell &amp; probcell\", iscell)\nrt_region = []\nfor iplane, ops in self.pops.items():\nif iplane == 0:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(0, ncells[iplane]),\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\nelse:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(\nnp.sum(ncells[:iplane]),\nncells[iplane] + np.sum(ncells[:iplane]),\n)\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\n# FLUORESCENCE (all are required)\nname_strs = [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\nfor i, (fstr, nstr) in enumerate(zip(file_strs, name_strs)):\nfor iplane, ops in self.pops.items():\nroi_resp_series = RoiResponseSeries(\nname=f\"plane{int(iplane)}\",\ndata=traces[i][PlaneCellsIdx == iplane],\nrois=rt_region[iplane],\nunit=\"lumens\",\nrate=ops[\"fs\"],\n)\nif iplane == 0:\nfl = Fluorescence(roi_response_series=roi_resp_series, name=nstr)\nelse:\nfl.add_roi_response_series(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_suite2p_nwb(self, path):\n\"\"\"\n        Load suite2p data from NWB\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\nophys = nwbfile.processing[\"ophys\"]\n#################################################################\n# STATS, OPS and ISCELL\n#################################################################\ndims = nwbfile.acquisition[\"TwoPhotonSeries\"].dimension[:]\nself.ops = {\"Ly\": dims[0], \"Lx\": dims[1]}\nself.rate = nwbfile.acquisition[\n\"TwoPhotonSeries\"\n].imaging_plane.imaging_rate\nself.stats = {0: {}}\nself.iscell = ophys[\"ImageSegmentation\"][\"PlaneSegmentation\"][\n\"iscell\"\n].data[:]\ninfo = pd.DataFrame(\ndata=self.iscell[:, 0].astype(\"int\"), columns=[\"iscell\"]\n)\n#################################################################\n# ROIS\n#################################################################\ntry:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"pixel_mask\"]\nmultiplane = False\nexcept Exception:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"voxel_mask\"]\nmultiplane = True\nidx = np.where(self.iscell[:, 0])[0]\ninfo[\"plane\"] = 0\nfor n in range(len(rois)):\nroi = pd.DataFrame(rois[n])\nif \"z\" in roi.columns:\npl = roi[\"z\"][0]\nelse:\npl = 0\ninfo.loc[n, \"plane\"] = pl\nif pl not in self.stats.keys():\nself.stats[pl] = {}\nif n in idx:\nself.stats[pl][n] = {\n\"xpix\": roi[\"y\"].values,\n\"ypix\": roi[\"x\"].values,\n\"lam\": roi[\"weight\"].values,\n}\n#################################################################\n# Time Series\n#################################################################\nfields = np.intersect1d(\n[\"Fluorescence\", \"Neuropil\", \"Deconvolved\"],\nlist(ophys.fields[\"data_interfaces\"].keys()),\n)\nif len(fields) == 0:\nprint(\n\"No \" + \" or \".join([\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]),\n\"found in nwb {}\".format(self.nwbfilepath),\n)\nreturn False\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\ndata = {}\nif multiplane:\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\nelse:\nplanes = [0]\nfor k, name in zip(\n[\"F\", \"Fneu\", \"spks\"], [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n):\ntmp = []\ntimestamps = []\nfor i, n in enumerate(planes):\nif multiplane:\npl = \"plane{}\".format(n)\nelse:\npl = name  # This doesn't make sense\ntokeep = info[\"iscell\"][info[\"plane\"] == n].values == 1\nd = np.transpose(ophys[name][pl].data[:][tokeep])\nif ophys[name][pl].timestamps is not None:\nt = ophys[name][pl].timestamps[:]\nelse:\nt = (np.arange(0, len(d)) / self.rate) + ophys[name][\npl\n].starting_time\ntmp.append(d)\ntimestamps.append(t)\ndata[k] = nap.TsdFrame(t=timestamps[0], d=np.hstack(tmp))\nif \"F\" in data.keys():\nself.F = data[\"F\"]\nif \"Fneu\" in data.keys():\nself.Fneu = data[\"Fneu\"]\nif \"spks\" in data.keys():\nself.spks = data[\"spks\"]\nself.plane_info = pd.DataFrame(\ndata=info[\"plane\"][info[\"iscell\"] == 1].values, columns=[\"plane\"]\n)\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_suite2p_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_suite2p(path)\nself.save_suite2p_nwb(path)\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p","title":"load_suite2p","text":"<pre><code>load_suite2p(path)\n</code></pre> <p>Looking for suite2/plane*</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def load_suite2p(self, path):\n\"\"\"\n    Looking for suite2/plane*\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.path_suite2p = os.path.join(path, \"suite2p\")\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ndata = {\n\"F\": [],\n\"Fneu\": [],\n\"spks\": [],\n}\nplane_info = []\nself.stats = {}\nself.pops = {}\nself.iscells = {}\nself.planes = []\nif os.path.exists(self.path_suite2p):\nplanes = glob.glob(os.path.join(self.path_suite2p, \"plane*\"))\nif len(planes):\n# count = 0\nfor plane_dir in planes:\nn = int(os.path.basename(plane_dir)[-1])\nself.planes.append(n)\n# Loading iscell.npy\ntry:\niscell = np.load(\nos.path.join(plane_dir, \"iscell.npy\"), allow_pickle=True\n)\nidx = np.where(iscell.astype(\"int\")[:, 0])[0]\nplane_info.append(np.ones(len(idx), dtype=\"int\") * n)\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading F.npy, Fneu.py and spks.npy\nfor obj in [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]:\ntry:\nname = obj.split(\".\")[0]\ntmp = np.load(\nos.path.join(plane_dir, obj), allow_pickle=True\n)\ndata[name].append(tmp[idx])\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading stat.npy and ops.npy\ntry:\nstat = np.load(\nos.path.join(plane_dir, \"stat.npy\"), allow_pickle=True\n)\nops = np.load(\nos.path.join(plane_dir, \"ops.npy\"), allow_pickle=True\n).item()\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Saving stat, ops and iscell\nself.stats[n] = stat\nself.pops[n] = ops\nself.iscells[n] = iscell\n# count += len(idx)\nelse:\nwarnings.warn(\n\"Couldn't find planes in %s\" % self.path_suite2p, stacklevel=2\n)\nsys.exit()\nelse:\nwarnings.warn(\"No suite2p folder in %s\" % path, stacklevel=2)\nsys.exit()\n# Calcium transients\ndata[\"F\"] = np.transpose(np.vstack(data[\"F\"]))\ndata[\"Fneu\"] = np.transpose(np.vstack(data[\"Fneu\"]))\ndata[\"spks\"] = np.transpose(np.vstack(data[\"spks\"]))\ntime_index = np.arange(0, len(data[\"F\"])) / self.sampling_rate\nself.F = nap.TsdFrame(t=time_index, d=data[\"F\"])\nself.Fneu = nap.TsdFrame(t=time_index, d=data[\"Fneu\"])\nself.spks = nap.TsdFrame(t=time_index, d=data[\"spks\"])\nself.ops = self.pops[0]\nself.iscell = np.vstack([self.iscells[k] for k in self.iscells.keys()])\n# Metadata\nself.plane_info = pd.DataFrame.from_dict({\"plane\": np.hstack(plane_info)})\nreturn\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.save_suite2p_nwb","title":"save_suite2p_nwb","text":"<pre><code>save_suite2p_nwb(path)\n</code></pre> <p>Save the data to NWB. To ensure continuity, this function is based on : https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def save_suite2p_nwb(self, path):\n\"\"\"\n    Save the data to NWB. To ensure continuity, this function is based on :\n    https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nmultiplane = True if len(self.planes) &gt; 1 else False\nops = self.pops[list(self.pops.keys())[0]]\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice = nwbfile.create_device(\nname=self.ophys_information[\"device\"][\"name\"],\ndescription=self.ophys_information[\"device\"][\"description\"],\nmanufacturer=self.ophys_information[\"device\"][\"manufacturer\"],\n)\nimaging_plane = nwbfile.create_imaging_plane(\nname=self.ophys_information[\"ImagingPlane\"][\"name\"],\noptical_channel=OpticalChannel(\nname=self.ophys_information[\"OpticalChannel\"][\"name\"],\ndescription=self.ophys_information[\"OpticalChannel\"][\"description\"],\nemission_lambda=float(\nself.ophys_information[\"OpticalChannel\"][\"emission_lambda\"]\n),\n),\nimaging_rate=self.sampling_rate,\ndescription=self.ophys_information[\"ImagingPlane\"][\"description\"],\ndevice=device,\nexcitation_lambda=float(\nself.ophys_information[\"ImagingPlane\"][\"excitation_lambda\"]\n),\nindicator=self.ophys_information[\"ImagingPlane\"][\"indicator\"],\nlocation=self.ophys_information[\"ImagingPlane\"][\"location\"],\ngrid_spacing=([2.0, 2.0, 30.0] if multiplane else [2.0, 2.0]),\ngrid_spacing_unit=\"microns\",\n)\n# link to external data\nimage_series = TwoPhotonSeries(\nname=\"TwoPhotonSeries\",\ndimension=[ops[\"Ly\"], ops[\"Lx\"]],\nexternal_file=(ops[\"filelist\"] if \"filelist\" in ops else [\"\"]),\nimaging_plane=imaging_plane,\nstarting_frame=[0],\nformat=\"external\",\nstarting_time=0.0,\nrate=ops[\"fs\"] * ops[\"nplanes\"],\n)\nnwbfile.add_acquisition(image_series)\n# processing\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=self.ophys_information[\"PlaneSegmentation\"][\"name\"],\ndescription=self.ophys_information[\"PlaneSegmentation\"][\"description\"],\nimaging_plane=imaging_plane,\n# reference_images=image_series,\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nophys_module.add(img_seg)\nfile_strs = [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]\ntraces = []\nncells = np.zeros(len(self.pops), dtype=np.int_)\nNfr = np.array([self.pops[k][\"nframes\"] for k in self.pops.keys()]).max()\nfor iplane, ops in self.pops.items():\nif iplane == 0:\niscell = self.iscells[iplane]\nfor fstr in file_strs:\ntraces.append(np.load(os.path.join(ops[\"save_path\"], fstr)))\nPlaneCellsIdx = iplane * np.ones(len(iscell))\nelse:\niscell = np.append(\niscell,\nself.iscells[iplane],\naxis=0,\n)\nfor i, fstr in enumerate(file_strs):\ntrace = np.load(os.path.join(ops[\"save_path\"], fstr))\nif trace.shape[1] &lt; Nfr:\nfcat = np.zeros(\n(trace.shape[0], Nfr - trace.shape[1]), \"float32\"\n)\ntrace = np.concatenate((trace, fcat), axis=1)\ntraces[i] = np.append(traces[i], trace, axis=0)\nPlaneCellsIdx = np.append(\nPlaneCellsIdx, iplane * np.ones(len(iscell) - len(PlaneCellsIdx))\n)\nstat = self.stats[iplane]\nncells[iplane] = len(stat)\nfor n in range(ncells[iplane]):\nif multiplane:\npixel_mask = np.array(\n[\nstat[n][\"ypix\"],\nstat[n][\"xpix\"],\niplane * np.ones(stat[n][\"npix\"]),\nstat[n][\"lam\"],\n]\n)\nps.add_roi(voxel_mask=pixel_mask.T)\nelse:\npixel_mask = np.array(\n[stat[n][\"ypix\"], stat[n][\"xpix\"], stat[n][\"lam\"]]\n)\nps.add_roi(pixel_mask=pixel_mask.T)\nps.add_column(\"iscell\", \"two columns - iscell &amp; probcell\", iscell)\nrt_region = []\nfor iplane, ops in self.pops.items():\nif iplane == 0:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(0, ncells[iplane]),\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\nelse:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(\nnp.sum(ncells[:iplane]),\nncells[iplane] + np.sum(ncells[:iplane]),\n)\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\n# FLUORESCENCE (all are required)\nname_strs = [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\nfor i, (fstr, nstr) in enumerate(zip(file_strs, name_strs)):\nfor iplane, ops in self.pops.items():\nroi_resp_series = RoiResponseSeries(\nname=f\"plane{int(iplane)}\",\ndata=traces[i][PlaneCellsIdx == iplane],\nrois=rt_region[iplane],\nunit=\"lumens\",\nrate=ops[\"fs\"],\n)\nif iplane == 0:\nfl = Fluorescence(roi_response_series=roi_resp_series, name=nstr)\nelse:\nfl.add_roi_response_series(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p_nwb","title":"load_suite2p_nwb","text":"<pre><code>load_suite2p_nwb(path)\n</code></pre> <p>Load suite2p data from NWB</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def load_suite2p_nwb(self, path):\n\"\"\"\n    Load suite2p data from NWB\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\nophys = nwbfile.processing[\"ophys\"]\n#################################################################\n# STATS, OPS and ISCELL\n#################################################################\ndims = nwbfile.acquisition[\"TwoPhotonSeries\"].dimension[:]\nself.ops = {\"Ly\": dims[0], \"Lx\": dims[1]}\nself.rate = nwbfile.acquisition[\n\"TwoPhotonSeries\"\n].imaging_plane.imaging_rate\nself.stats = {0: {}}\nself.iscell = ophys[\"ImageSegmentation\"][\"PlaneSegmentation\"][\n\"iscell\"\n].data[:]\ninfo = pd.DataFrame(\ndata=self.iscell[:, 0].astype(\"int\"), columns=[\"iscell\"]\n)\n#################################################################\n# ROIS\n#################################################################\ntry:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"pixel_mask\"]\nmultiplane = False\nexcept Exception:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"voxel_mask\"]\nmultiplane = True\nidx = np.where(self.iscell[:, 0])[0]\ninfo[\"plane\"] = 0\nfor n in range(len(rois)):\nroi = pd.DataFrame(rois[n])\nif \"z\" in roi.columns:\npl = roi[\"z\"][0]\nelse:\npl = 0\ninfo.loc[n, \"plane\"] = pl\nif pl not in self.stats.keys():\nself.stats[pl] = {}\nif n in idx:\nself.stats[pl][n] = {\n\"xpix\": roi[\"y\"].values,\n\"ypix\": roi[\"x\"].values,\n\"lam\": roi[\"weight\"].values,\n}\n#################################################################\n# Time Series\n#################################################################\nfields = np.intersect1d(\n[\"Fluorescence\", \"Neuropil\", \"Deconvolved\"],\nlist(ophys.fields[\"data_interfaces\"].keys()),\n)\nif len(fields) == 0:\nprint(\n\"No \" + \" or \".join([\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]),\n\"found in nwb {}\".format(self.nwbfilepath),\n)\nreturn False\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\ndata = {}\nif multiplane:\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\nelse:\nplanes = [0]\nfor k, name in zip(\n[\"F\", \"Fneu\", \"spks\"], [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n):\ntmp = []\ntimestamps = []\nfor i, n in enumerate(planes):\nif multiplane:\npl = \"plane{}\".format(n)\nelse:\npl = name  # This doesn't make sense\ntokeep = info[\"iscell\"][info[\"plane\"] == n].values == 1\nd = np.transpose(ophys[name][pl].data[:][tokeep])\nif ophys[name][pl].timestamps is not None:\nt = ophys[name][pl].timestamps[:]\nelse:\nt = (np.arange(0, len(d)) / self.rate) + ophys[name][\npl\n].starting_time\ntmp.append(d)\ntimestamps.append(t)\ndata[k] = nap.TsdFrame(t=timestamps[0], d=np.hstack(tmp))\nif \"F\" in data.keys():\nself.F = data[\"F\"]\nif \"Fneu\" in data.keys():\nself.Fneu = data[\"Fneu\"]\nif \"spks\" in data.keys():\nself.spks = data[\"spks\"]\nself.plane_info = pd.DataFrame(\ndata=info[\"plane\"][info[\"iscell\"] == 1].values, columns=[\"plane\"]\n)\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"old_pages/process.correlograms/","title":"Process.correlograms","text":""},{"location":"old_pages/process.correlograms/#pynapple.process.correlograms","title":"pynapple.process.correlograms","text":""},{"location":"old_pages/process.correlograms/#pynapple.process.correlograms.cross_correlogram","title":"cross_correlogram","text":"<pre><code>cross_correlogram(t1, t2, binsize, windowsize)\n</code></pre> <p>Performs the discrete cross-correlogram of two time series. The units should be in s for all arguments. Return the firing rate of the series t2 relative to the timings of t1. See compute_crosscorrelogram, compute_autocorrelogram and compute_eventcorrelogram for wrappers of this function.</p> <p>Parameters:</p> Name Type Description Default <code>t1</code> <code>ndarray</code> <p>The timestamps of the reference time series (in seconds)</p> required <code>t2</code> <code>ndarray</code> <p>The timestamps of the target time series (in seconds)</p> required <code>binsize</code> <code>float</code> <p>The bin size (in seconds)</p> required <code>windowsize</code> <code>float</code> <p>The window size (in seconds)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The cross-correlogram</p> <code>ndarray</code> <p>Center of the bins (in s)</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>@jit(nopython=True)\ndef cross_correlogram(t1, t2, binsize, windowsize):\n\"\"\"\n    Performs the discrete cross-correlogram of two time series.\n    The units should be in s for all arguments.\n    Return the firing rate of the series t2 relative to the timings of t1.\n    See compute_crosscorrelogram, compute_autocorrelogram and compute_eventcorrelogram\n    for wrappers of this function.\n    Parameters\n    ----------\n    t1 : numpy.ndarray\n        The timestamps of the reference time series (in seconds)\n    t2 : numpy.ndarray\n        The timestamps of the target time series (in seconds)\n    binsize : float\n        The bin size (in seconds)\n    windowsize : float\n        The window size (in seconds)\n    Returns\n    -------\n    numpy.ndarray\n        The cross-correlogram\n    numpy.ndarray\n        Center of the bins (in s)\n    \"\"\"\n# nbins = ((windowsize//binsize)*2)\nnt1 = len(t1)\nnt2 = len(t2)\nnbins = int((windowsize * 2) // binsize)\nif np.floor(nbins / 2) * 2 == nbins:\nnbins = nbins + 1\nw = (nbins / 2) * binsize\nC = np.zeros(nbins)\ni2 = 0\nfor i1 in range(nt1):\nlbound = t1[i1] - w\nwhile i2 &lt; nt2 and t2[i2] &lt; lbound:\ni2 = i2 + 1\nwhile i2 &gt; 0 and t2[i2 - 1] &gt; lbound:\ni2 = i2 - 1\nrbound = lbound\nleftb = i2\nfor j in range(nbins):\nk = 0\nrbound = rbound + binsize\nwhile leftb &lt; nt2 and t2[leftb] &lt; rbound:\nleftb = leftb + 1\nk = k + 1\nC[j] += k\nC = C / (nt1 * binsize)\nm = -w + binsize / 2\nB = np.zeros(nbins)\nfor j in range(nbins):\nB[j] = m + j * binsize\nreturn C, B\n</code></pre>"},{"location":"old_pages/process.correlograms/#pynapple.process.correlograms.compute_autocorrelogram","title":"compute_autocorrelogram","text":"<pre><code>compute_autocorrelogram(\ngroup,\nbinsize,\nwindowsize,\nep=None,\nnorm=True,\ntime_units=\"s\",\n)\n</code></pre> <p>Computes the autocorrelogram of a group of Ts/Tsd objects. The group can be passed directly as a TsGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects to auto-correlate</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which auto-corrs are computed. If None, the epoch is the time support of the group.</p> <code>None</code> <code>norm</code> <code>bool</code> <p>If True, autocorrelograms are normalized to baseline (i.e. divided by the average rate)  If False, autoorrelograms are returned as the rate (Hz) of the time series (relative to itself)</p> <code>True</code> <code>time_units</code> <code>str</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_autocorrelogram(\ngroup, binsize, windowsize, ep=None, norm=True, time_units=\"s\"\n):\n\"\"\"\n    Computes the autocorrelogram of a group of Ts/Tsd objects.\n    The group can be passed directly as a TsGroup object.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects to auto-correlate\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which auto-corrs are computed.\n        If None, the epoch is the time support of the group.\n    norm : bool, optional\n         If True, autocorrelograms are normalized to baseline (i.e. divided by the average rate)\n         If False, autoorrelograms are returned as the rate (Hz) of the time series (relative to itself)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup\n    \"\"\"\nif type(group) is nap.TsGroup:\nif isinstance(ep, nap.IntervalSet):\nnewgroup = group.restrict(ep)\nelse:\nnewgroup = group\nelse:\nraise RuntimeError(\"Unknown format for group\")\nautocorrs = {}\nbinsize = nap.TsIndex.format_timestamps(\nnp.array([binsize], dtype=np.float64), time_units\n)[0]\nwindowsize = nap.TsIndex.format_timestamps(\nnp.array([windowsize], dtype=np.float64), time_units\n)[0]\nfor n in newgroup.keys():\nspk_time = newgroup[n].index\nauc, times = cross_correlogram(spk_time, spk_time, binsize, windowsize)\nautocorrs[n] = pd.Series(index=np.round(times, 6), data=auc, dtype=\"float\")\nautocorrs = pd.DataFrame.from_dict(autocorrs)\nif norm:\nautocorrs = autocorrs / newgroup.get_info(\"rate\")\n# Bug here\nif 0 in autocorrs.index:\nautocorrs.loc[0] = 0.0\nreturn autocorrs.astype(\"float\")\n</code></pre>"},{"location":"old_pages/process.correlograms/#pynapple.process.correlograms.compute_crosscorrelogram","title":"compute_crosscorrelogram","text":"<pre><code>compute_crosscorrelogram(\ngroup,\nbinsize,\nwindowsize,\nep=None,\nnorm=True,\ntime_units=\"s\",\nreverse=False,\n)\n</code></pre> <p>Computes all the pairwise cross-correlograms for TsGroup or list/tuple of two TsGroup.</p> <p>If input is TsGroup only, the reference Ts/Tsd and target are chosen based on the builtin itertools.combinations function. For example if indexes are [0,1,2], the function computes cross-correlograms for the pairs (0,1), (0, 2), and (1, 2). The left index gives the reference time series. To reverse the order, set reverse=True.</p> <p>If input is tuple/list of TsGroup, for example group=(group1, group2), the reference for each pairs comes from group1.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup or tuple/list of two TsGroups</code> required <p>binsize : float     The bin size. Default is second.     If different, specify with the parameter time_units ('s' [default], 'ms', 'us'). windowsize : float     The window size. Default is second.     If different, specify with the parameter time_units ('s' [default], 'ms', 'us'). ep : IntervalSet     The epoch on which cross-corrs are computed.     If None, the epoch is the time support of the group. norm : bool, optional     If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)     If False, cross-orrelograms are returned as the rate (Hz) of the target time series ((relative to the reference time series) time_units : str, optional     The time units of the parameters. They have to be consistent for binsize and windowsize.     ('s' [default], 'ms', 'us'). reverse : bool, optional     To reverse the pair order if input is TsGroup</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup or tuple/list of two TsGroups</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_crosscorrelogram(\ngroup, binsize, windowsize, ep=None, norm=True, time_units=\"s\", reverse=False\n):\n\"\"\"\n    Computes all the pairwise cross-correlograms for TsGroup or list/tuple of two TsGroup.\n    If input is TsGroup only, the reference Ts/Tsd and target are chosen based on the builtin itertools.combinations function.\n    For example if indexes are [0,1,2], the function computes cross-correlograms\n    for the pairs (0,1), (0, 2), and (1, 2). The left index gives the reference time series.\n    To reverse the order, set reverse=True.\n    If input is tuple/list of TsGroup, for example group=(group1, group2), the reference for each pairs comes from group1.\n    Parameters\n    ----------\n    group : TsGroup or tuple/list of two TsGroups\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which cross-corrs are computed.\n        If None, the epoch is the time support of the group.\n    norm : bool, optional\n        If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)\n        If False, cross-orrelograms are returned as the rate (Hz) of the target time series ((relative to the reference time series)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    reverse : bool, optional\n        To reverse the pair order if input is TsGroup\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup or tuple/list of two TsGroups\n    \"\"\"\ncrosscorrs = {}\nbinsize = nap.TsIndex.format_timestamps(\nnp.array([binsize], dtype=np.float64), time_units\n)[0]\nwindowsize = nap.TsIndex.format_timestamps(\nnp.array([windowsize], dtype=np.float64), time_units\n)[0]\nif isinstance(group, nap.TsGroup):\nif isinstance(ep, nap.IntervalSet):\nnewgroup = group.restrict(ep)\nelse:\nnewgroup = group\nneurons = list(newgroup.keys())\npairs = list(combinations(neurons, 2))\nif reverse:\npairs = list(map(lambda n: (n[1], n[0]), pairs))\nfor i, j in pairs:\nspk1 = newgroup[i].index\nspk2 = newgroup[j].index\nauc, times = cross_correlogram(spk1, spk2, binsize, windowsize)\ncrosscorrs[(i, j)] = pd.Series(index=times, data=auc, dtype=\"float\")\ncrosscorrs = pd.DataFrame.from_dict(crosscorrs)\nif norm:\nfreq = newgroup.get_info(\"rate\")\nfreq2 = pd.Series(\nindex=pairs, data=list(map(lambda n: freq.loc[n[1]], pairs))\n)\ncrosscorrs = crosscorrs / freq2\nelif (\nisinstance(group, (tuple, list))\nand len(group) == 2\nand all(map(lambda g: isinstance(g, nap.TsGroup), group))\n):\nif isinstance(ep, nap.IntervalSet):\nnewgroup = [group[i].restrict(ep) for i in range(2)]\nelse:\nnewgroup = group\npairs = product(list(newgroup[0].keys()), list(newgroup[1].keys()))\nfor i, j in pairs:\nspk1 = newgroup[0][i].index\nspk2 = newgroup[1][j].index\nauc, times = cross_correlogram(spk1, spk2, binsize, windowsize)\nif norm:\nauc /= newgroup[1][j].rate\ncrosscorrs[(i, j)] = pd.Series(index=times, data=auc, dtype=\"float\")\ncrosscorrs = pd.DataFrame.from_dict(crosscorrs)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nreturn crosscorrs.astype(\"float\")\n</code></pre>"},{"location":"old_pages/process.correlograms/#pynapple.process.correlograms.compute_eventcorrelogram","title":"compute_eventcorrelogram","text":"<pre><code>compute_eventcorrelogram(\ngroup,\nevent,\nbinsize,\nwindowsize,\nep=None,\nnorm=True,\ntime_units=\"s\",\n)\n</code></pre> <p>Computes the correlograms of a group of Ts/Tsd objects with another single Ts/Tsd object The time of reference is the event times.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects to correlate with the event</p> required <code>event</code> <code>Ts / Tsd</code> <p>The event to correlate the each of the time series in the group with.</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which cross-corrs are computed. If None, the epoch is the time support of the event.</p> <code>None</code> <code>norm</code> <code>bool</code> <p>If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series) If False, cross-orrelograms are returned as the rate (Hz) of the target time series (relative to the event time series)</p> <code>True</code> <code>time_units</code> <code>str</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_eventcorrelogram(\ngroup, event, binsize, windowsize, ep=None, norm=True, time_units=\"s\"\n):\n\"\"\"\n    Computes the correlograms of a group of Ts/Tsd objects with another single Ts/Tsd object\n    The time of reference is the event times.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects to correlate with the event\n    event : Ts/Tsd\n        The event to correlate the each of the time series in the group with.\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which cross-corrs are computed.\n        If None, the epoch is the time support of the event.\n    norm : bool, optional\n        If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)\n        If False, cross-orrelograms are returned as the rate (Hz) of the target time series (relative to the event time series)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup\n    \"\"\"\nif ep is None:\nep = event.time_support\ntsd1 = event.index\nelse:\ntsd1 = event.restrict(ep).index\nif type(group) is nap.TsGroup:\nnewgroup = group.restrict(ep)\nelse:\nraise RuntimeError(\"Unknown format for group\")\ncrosscorrs = {}\nbinsize = nap.TsIndex.format_timestamps(\nnp.array([binsize], dtype=np.float64), time_units\n)[0]\nwindowsize = nap.TsIndex.format_timestamps(\nnp.array([windowsize], dtype=np.float64), time_units\n)[0]\nfor n in newgroup.keys():\nspk_time = newgroup[n].index\nauc, times = cross_correlogram(tsd1, spk_time, binsize, windowsize)\ncrosscorrs[n] = pd.Series(index=times, data=auc, dtype=\"float\")\ncrosscorrs = pd.DataFrame.from_dict(crosscorrs)\nif norm:\ncrosscorrs = crosscorrs / newgroup.get_info(\"rate\")\nreturn crosscorrs.astype(\"float\")\n</code></pre>"},{"location":"old_pages/process.decoding/","title":"Process.decoding","text":""},{"location":"old_pages/process.decoding/#pynapple.process.decoding","title":"pynapple.process.decoding","text":""},{"location":"old_pages/process.decoding/#pynapple.process.decoding.decode_1d","title":"decode_1d","text":"<pre><code>decode_1d(\ntuning_curves,\ngroup,\nep,\nbin_size,\ntime_units=\"s\",\nfeature=None,\n)\n</code></pre> <p>Performs Bayesian decoding over a one dimensional feature. See: Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.</p> <p>Parameters:</p> Name Type Description Default <code>tuning_curves</code> <code>DataFrame</code> <p>Each column is the tuning curve of one neuron relative to the feature. Index should be the center of the bin.</p> required <code>group</code> <code>TsGroup or dict of Ts/Tsd object.</code> <p>A group of neurons with the same index as tuning curves column names.</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which decoding is computed</p> required <code>bin_size</code> <code>float</code> <p>Bin size. Default is second. Use the parameter time_units to change it.</p> required <code>time_units</code> <code>str</code> <p>Time unit of the bin size ('s' [default], 'ms', 'us').</p> <code>'s'</code> <code>feature</code> <code>Tsd</code> <p>The 1d feature used to compute the tuning curves. Used to correct for occupancy. If feature is not passed, the occupancy is uniform.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tsd</code> <p>The decoded feature</p> <code>TsdFrame</code> <p>The probability distribution of the decoded feature for each time bin</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a dict of Ts/Tsd or TsGroup. If different size of neurons for tuning_curves and group. If indexes don't match between tuning_curves and group.</p> Source code in <code>pynapple/process/decoding.py</code> <pre><code>def decode_1d(tuning_curves, group, ep, bin_size, time_units=\"s\", feature=None):\n\"\"\"\n    Performs Bayesian decoding over a one dimensional feature.\n    See:\n    Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J.\n    (1998). Interpreting neuronal population activity by\n    reconstruction: unified framework with application to\n    hippocampal place cells. Journal of neurophysiology, 79(2),\n    1017-1044.\n    Parameters\n    ----------\n    tuning_curves : pandas.DataFrame\n        Each column is the tuning curve of one neuron relative to the feature.\n        Index should be the center of the bin.\n    group : TsGroup or dict of Ts/Tsd object.\n        A group of neurons with the same index as tuning curves column names.\n    ep : IntervalSet\n        The epoch on which decoding is computed\n    bin_size : float\n        Bin size. Default is second. Use the parameter time_units to change it.\n    time_units : str, optional\n        Time unit of the bin size ('s' [default], 'ms', 'us').\n    feature : Tsd, optional\n        The 1d feature used to compute the tuning curves. Used to correct for occupancy.\n        If feature is not passed, the occupancy is uniform.\n    Returns\n    -------\n    Tsd\n        The decoded feature\n    TsdFrame\n        The probability distribution of the decoded feature for each time bin\n    Raises\n    ------\n    RuntimeError\n        If group is not a dict of Ts/Tsd or TsGroup.\n        If different size of neurons for tuning_curves and group.\n        If indexes don't match between tuning_curves and group.\n    \"\"\"\nif isinstance(group, dict):\nnewgroup = nap.TsGroup(group, time_support=ep)\nelif isinstance(group, nap.TsGroup):\nnewgroup = group.restrict(ep)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nif tuning_curves.shape[1] != len(newgroup):\nraise RuntimeError(\"Different shapes for tuning_curves and group\")\nif not np.all(tuning_curves.columns.values == np.array(newgroup.keys())):\nraise RuntimeError(\"Difference indexes for tuning curves and group keys\")\n# Bin spikes\ncount = newgroup.count(bin_size, ep, time_units)\n# Occupancy\nif feature is None:\noccupancy = np.ones(tuning_curves.shape[0])\nelif isinstance(feature, nap.Tsd):\ndiff = np.diff(tuning_curves.index.values)\nbins = tuning_curves.index.values[:-1] - diff / 2\nbins = np.hstack(\n(bins, [bins[-1] + diff[-1], bins[-1] + 2 * diff[-1]])\n)  # assuming the size of the last 2 bins is equal\noccupancy, _ = np.histogram(feature.values, bins)\nelse:\nraise RuntimeError(\"Unknown format for feature in decode_1d\")\n# Transforming to pure numpy array\ntc = tuning_curves.values\nct = count.values\nbin_size_s = nap.TsIndex.format_timestamps(\nnp.array([bin_size], dtype=np.float64), time_units\n)[0]\np1 = np.exp(-bin_size_s * tc.sum(1))\np2 = occupancy / occupancy.sum()\nct2 = np.tile(ct[:, np.newaxis, :], (1, tc.shape[0], 1))\np3 = np.prod(tc**ct2, -1)\np = p1 * p2 * p3\np = p / p.sum(1)[:, np.newaxis]\nidxmax = np.argmax(p, 1)\np = nap.TsdFrame(\nt=count.index, d=p, time_support=ep, columns=tuning_curves.index.values\n)\ndecoded = nap.Tsd(\nt=count.index, d=tuning_curves.index.values[idxmax], time_support=ep\n)\nreturn decoded, p\n</code></pre>"},{"location":"old_pages/process.decoding/#pynapple.process.decoding.decode_2d","title":"decode_2d","text":"<pre><code>decode_2d(\ntuning_curves,\ngroup,\nep,\nbin_size,\nxy,\ntime_units=\"s\",\nfeatures=None,\n)\n</code></pre> <p>Performs Bayesian decoding over a two dimensional feature. See: Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.</p> <p>Parameters:</p> Name Type Description Default <code>tuning_curves</code> <code>dict</code> <p>Dictionnay of 2d tuning curves (one for each neuron).</p> required <code>group</code> <code>TsGroup or dict of Ts/Tsd object.</code> <p>A group of neurons with the same keys as tuning_curves dictionnary.</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which decoding is computed</p> required <code>bin_size</code> <code>float</code> <p>Bin size. Default is second. Use the parameter time_units to change it.</p> required <code>xy</code> <code>tuple</code> <p>A tuple of bin positions for the tuning curves i.e. xy=(x,y)</p> required <code>time_units</code> <code>str</code> <p>Time unit of the bin size ('s' [default], 'ms', 'us').</p> <code>'s'</code> <code>features</code> <code>TsdFrame</code> <p>The 2 columns features used to compute the tuning curves. Used to correct for occupancy. If feature is not passed, the occupancy is uniform.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tsd</code> <p>The decoded feature in 2d</p> <code>ndarray</code> <p>The probability distribution of the decoded trajectory for each time bin</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a dict of Ts/Tsd or TsGroup. If different size of neurons for tuning_curves and group. If indexes don't match between tuning_curves and group.</p> Source code in <code>pynapple/process/decoding.py</code> <pre><code>def decode_2d(tuning_curves, group, ep, bin_size, xy, time_units=\"s\", features=None):\n\"\"\"\n    Performs Bayesian decoding over a two dimensional feature.\n    See:\n    Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J.\n    (1998). Interpreting neuronal population activity by\n    reconstruction: unified framework with application to\n    hippocampal place cells. Journal of neurophysiology, 79(2),\n    1017-1044.\n    Parameters\n    ----------\n    tuning_curves : dict\n        Dictionnay of 2d tuning curves (one for each neuron).\n    group : TsGroup or dict of Ts/Tsd object.\n        A group of neurons with the same keys as tuning_curves dictionnary.\n    ep : IntervalSet\n        The epoch on which decoding is computed\n    bin_size : float\n        Bin size. Default is second. Use the parameter time_units to change it.\n    xy : tuple\n        A tuple of bin positions for the tuning curves i.e. xy=(x,y)\n    time_units : str, optional\n        Time unit of the bin size ('s' [default], 'ms', 'us').\n    features : TsdFrame\n        The 2 columns features used to compute the tuning curves. Used to correct for occupancy.\n        If feature is not passed, the occupancy is uniform.\n    Returns\n    -------\n    Tsd\n        The decoded feature in 2d\n    numpy.ndarray\n        The probability distribution of the decoded trajectory for each time bin\n    Raises\n    ------\n    RuntimeError\n        If group is not a dict of Ts/Tsd or TsGroup.\n        If different size of neurons for tuning_curves and group.\n        If indexes don't match between tuning_curves and group.\n    \"\"\"\nif type(group) is dict:\nnewgroup = nap.TsGroup(group, time_support=ep)\nnumcells = len(newgroup)\nelif type(group) is nap.TsGroup:\nnewgroup = group.restrict(ep)\nnumcells = len(newgroup)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nif len(tuning_curves) != numcells:\nraise RuntimeError(\"Different shapes for tuning_curves and group\")\nif not np.all(np.array(list(tuning_curves.keys())) == np.array(newgroup.keys())):\nraise RuntimeError(\"Difference indexes for tuning curves and group keys\")\n# Bin spikes\n# if type(newgroup) is not nap.TsdFrame:\ncount = newgroup.count(bin_size, ep, time_units)\n# else:\n#     #Spikes already \"binned\" with continuous TsdFrame input\n#     count = newgroup\nindexes = list(tuning_curves.keys())\n# Occupancy\nif features is None:\noccupancy = np.ones_like(tuning_curves[indexes[0]]).flatten()\nelse:\nbinsxy = []\nfor i in range(len(xy)):\ndiff = np.diff(xy[i])\nbins = xy[i][:-1] - diff / 2\nbins = np.hstack(\n(bins, [bins[-1] + diff[-1], bins[-1] + 2 * diff[-1]])\n)  # assuming the size of the last 2 bins is equal\nbinsxy.append(bins)\noccupancy, _, _ = np.histogram2d(\nfeatures[:, 0].values, features[:, 1].values, [binsxy[0], binsxy[1]]\n)\noccupancy = occupancy.flatten()\n# Transforming to pure numpy array\ntc = np.array([tuning_curves[i] for i in tuning_curves.keys()])\ntc = tc.reshape(tc.shape[0], np.prod(tc.shape[1:]))\ntc = tc.T\nct = count.values\nbin_size_s = nap.TsIndex.format_timestamps(\nnp.array([bin_size], dtype=np.float64), time_units\n)[0]\np1 = np.exp(-bin_size_s * np.nansum(tc, 1))\np2 = occupancy / occupancy.sum()\nct2 = np.tile(ct[:, np.newaxis, :], (1, tc.shape[0], 1))\np3 = np.nanprod(tc**ct2, -1)\np = p1 * p2 * p3\np = p / p.sum(1)[:, np.newaxis]\nidxmax = np.argmax(p, 1)\np = p.reshape(p.shape[0], len(xy[0]), len(xy[1]))\nidxmax2d = np.unravel_index(idxmax, (len(xy[0]), len(xy[1])))\nif features is not None:\ncols = features.columns\nelse:\ncols = np.arange(2)\ndecoded = nap.TsdFrame(\nt=count.index,\nd=np.vstack((xy[0][idxmax2d[0]], xy[1][idxmax2d[1]])).T,\ntime_support=ep,\ncolumns=cols,\n)\nreturn decoded, p\n</code></pre>"},{"location":"old_pages/process.perievent/","title":"Process.perievent","text":""},{"location":"old_pages/process.perievent/#pynapple.process.perievent","title":"pynapple.process.perievent","text":""},{"location":"old_pages/process.perievent/#pynapple.process.perievent.compute_perievent","title":"compute_perievent","text":"<pre><code>compute_perievent(data, tref, minmax, time_unit='s')\n</code></pre> <p>Center ts/tsd/tsgroup object around the timestamps given by the tref argument. minmax indicates the start and end of the window.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Ts / Tsd / TsGroup</code> <p>The data to align to tref. If Ts/Tsd, returns a TsGroup. If TsGroup, returns a dictionnary of TsGroup</p> required <code>tref</code> <code>Ts / Tsd</code> <p>The timestamps of the event to align to</p> required <code>minmax</code> <code>tuple or int or float</code> <p>The window size. Can be unequal on each side i.e. (-500, 1000).</p> required <code>time_unit</code> <code>str</code> <p>Time units of the minmax ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A TsGroup if data is a Ts/Tsd or a dictionnary of TsGroup if data is a TsGroup.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if tref is not a Ts/Tsd object or if data is not a Ts/Tsd or TsGroup</p> Source code in <code>pynapple/process/perievent.py</code> <pre><code>def compute_perievent(data, tref, minmax, time_unit=\"s\"):\n\"\"\"\n    Center ts/tsd/tsgroup object around the timestamps given by the tref argument.\n    minmax indicates the start and end of the window.\n    Parameters\n    ----------\n    data : Ts/Tsd/TsGroup\n        The data to align to tref.\n        If Ts/Tsd, returns a TsGroup.\n        If TsGroup, returns a dictionnary of TsGroup\n    tref : Ts/Tsd\n        The timestamps of the event to align to\n    minmax : tuple or int or float\n        The window size. Can be unequal on each side i.e. (-500, 1000).\n    time_unit : str, optional\n        Time units of the minmax ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    dict\n        A TsGroup if data is a Ts/Tsd or\n        a dictionnary of TsGroup if data is a TsGroup.\n    Raises\n    ------\n    RuntimeError\n        if tref is not a Ts/Tsd object or if data is not a Ts/Tsd or TsGroup\n    \"\"\"\nif not isinstance(tref, (nap.Ts, nap.Tsd)):\nraise RuntimeError(\"tref should be a Tsd object.\")\nif isinstance(minmax, float) or isinstance(minmax, int):\nminmax = np.array([minmax, minmax], dtype=np.float64)\nwindow = np.abs(nap.TsIndex.format_timestamps(np.array(minmax), time_unit))\ntime_support = nap.IntervalSet(start=-window[0], end=window[1])\nif isinstance(data, nap.TsGroup):\ntoreturn = {}\nfor n in data.index:\ntoreturn[n] = _align_tsd(data[n], tref, window, time_support)\nreturn toreturn\nelif isinstance(data, (nap.Ts, nap.Tsd)):\nreturn _align_tsd(data, tref, window, time_support)\nelse:\nraise RuntimeError(\"Unknown format for data\")\n</code></pre>"},{"location":"old_pages/process.perievent/#pynapple.process.perievent.compute_event_trigger_average","title":"compute_event_trigger_average","text":"<pre><code>compute_event_trigger_average(\ngroup, feature, binsize, windowsize, ep, time_units=\"s\"\n)\n</code></pre> <p>Bin the spike train in binsize and compute the Spike Trigger Average (STA) within windowsize. If C is the spike count matrix and feature is a Tsd array, the function computes the Hankel matrix H from windowsize=(-t1,+t2) by offseting the Tsd array.</p> <p>The STA is then defined as the dot product between H and C divided by the number of spikes.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects that hold the trigger time.</p> required <code>feature</code> <code>Tsd</code> <p>The 1-dimensional feature to average</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which STA are computed</p> required <code>time_units</code> <code>str</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>TsdFrame</code> <p>A TsdFrame of Spike-Trigger Average. Each column is an element from the group.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if group is not a Ts/Tsd or TsGroup</p> Source code in <code>pynapple/process/perievent.py</code> <pre><code>def compute_event_trigger_average(\ngroup, feature, binsize, windowsize, ep, time_units=\"s\"\n):\n\"\"\"\n    Bin the spike train in binsize and compute the Spike Trigger Average (STA) within windowsize.\n    If C is the spike count matrix and feature is a Tsd array, the function computes\n    the Hankel matrix H from windowsize=(-t1,+t2) by offseting the Tsd array.\n    The STA is then defined as the dot product between H and C divided by the number of spikes.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects that hold the trigger time.\n    feature : Tsd\n        The 1-dimensional feature to average\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which STA are computed\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    TsdFrame\n        A TsdFrame of Spike-Trigger Average. Each column is an element from the group.\n    Raises\n    ------\n    RuntimeError\n        if group is not a Ts/Tsd or TsGroup\n    \"\"\"\nif type(group) is not nap.TsGroup:\nraise RuntimeError(\"Unknown format for group\")\nbinsize = nap.TsIndex.format_timestamps(\nnp.array([binsize], dtype=np.float64), time_units\n)[0]\nstart = np.abs(\nnap.TsIndex.format_timestamps(\nnp.array([windowsize[0]], dtype=np.float64), time_units\n)[0]\n)\nend = np.abs(\nnap.TsIndex.format_timestamps(\nnp.array([windowsize[1]], dtype=np.float64), time_units\n)[0]\n)\nidx1 = -np.arange(0, start + binsize, binsize)[::-1][:-1]\nidx2 = np.arange(0, end + binsize, binsize)[1:]\ntime_idx = np.hstack((idx1, np.zeros(1), idx2))\ncount = group.count(binsize, ep)\ntmp = feature.bin_average(binsize, ep)\n# Build the Hankel matrix\nn_p = len(idx1)\nn_f = len(idx2)\npad_tmp = np.pad(tmp, (n_p, n_f))\noffset_tmp = hankel(pad_tmp, pad_tmp[-(n_p + n_f + 1) :])[0 : len(tmp)]\nsta = np.dot(offset_tmp.T, count.values)\nsta = sta / np.sum(count, 0)\nsta = nap.TsdFrame(t=time_idx, d=sta, columns=group.index)\nreturn sta\n</code></pre>"},{"location":"old_pages/process.randomize/","title":"Process.randomize","text":""},{"location":"old_pages/process.randomize/#pynapple.process.randomize","title":"pynapple.process.randomize","text":""},{"location":"old_pages/process.randomize/#pynapple.process.randomize.shift_timestamps","title":"shift_timestamps","text":"<pre><code>shift_timestamps(ts, min_shift=0.0, max_shift=None)\n</code></pre> <p>Shifts all the time stamps of a random amount between min_shift and max_shift, wrapping the end of the time support to the beginning.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to shift. If TsGroup, shifts all Ts in the group independently.</p> required <code>min_shift</code> <code>float</code> <p>minimum shift (default: 0 )</p> <code>0.0</code> <code>max_shift</code> <code>float</code> <p>maximum shift, (default: length of time support)</p> <code>None</code> <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The randomly shifted timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def shift_timestamps(ts, min_shift=0.0, max_shift=None):\n\"\"\"\n    Shifts all the time stamps of a random amount between min_shift and max_shift, wrapping the\n    end of the time support to the beginning.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to shift. If TsGroup, shifts all Ts in the group independently.\n    min_shift : float, optional\n        minimum shift (default: 0 )\n    max_shift : float, optional\n        maximum shift, (default: length of time support)\n    Returns\n    -------\n    Ts or TsGroup\n        The randomly shifted timestamps\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _shift_ts,\nnap.ts_group.TsGroup: _shift_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nstrategy = strategies[type(ts)]\nreturn strategy(ts, min_shift, max_shift)\n</code></pre>"},{"location":"old_pages/process.randomize/#pynapple.process.randomize.shuffle_ts_intervals","title":"shuffle_ts_intervals","text":"<pre><code>shuffle_ts_intervals(ts, min_shift=0.0, max_shift=None)\n</code></pre> <p>Randomizes the timestamps by shuffling the intervals between them.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to randomize. If TsGroup, randomizes all Ts in the group independently.</p> required <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The randomized timestamps, with shuffled intervals</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def shuffle_ts_intervals(ts, min_shift=0.0, max_shift=None):\n\"\"\"\n    Randomizes the timestamps by shuffling the intervals between them.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to randomize. If TsGroup, randomizes all Ts in the group independently.\n    Returns\n    -------\n    Ts or TsGroup\n        The randomized timestamps, with shuffled intervals\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _shuffle_intervals_ts,\nnap.ts_group.TsGroup: _shuffle_intervals_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nstrategy = strategies[type(ts)]\nreturn strategy(ts)\n</code></pre>"},{"location":"old_pages/process.randomize/#pynapple.process.randomize.jitter_timestamps","title":"jitter_timestamps","text":"<pre><code>jitter_timestamps(ts, max_jitter=None, keep_tsupport=False)\n</code></pre> <p>Jitters each time stamp independently of random amounts uniformly drawn between -max_jitter and max_jitter.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to jitter. If TsGroup, jitter is applied to each element of the group.</p> required <code>max_jitter</code> <code>float</code> <p>maximum jitter</p> <code>None</code> <code>keep_tsupport</code> <p>If True, keep time support of the input. The number of timestamps will not be conserved. If False, the time support is inferred from the jittered timestamps. The number of tmestamps is conserved. (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The jittered timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def jitter_timestamps(ts, max_jitter=None, keep_tsupport=False):\n\"\"\"\n    Jitters each time stamp independently of random amounts uniformly drawn between -max_jitter and max_jitter.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to jitter. If TsGroup, jitter is applied to each element of the group.\n    max_jitter : float\n        maximum jitter\n    keep_tsupport: bool, optional\n        If True, keep time support of the input. The number of timestamps will not be conserved.\n        If False, the time support is inferred from the jittered timestamps. The number of tmestamps\n        is conserved. (default: False)\n    Returns\n    -------\n    Ts or TsGroup\n        The jittered timestamps\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _jitter_ts,\nnap.ts_group.TsGroup: _jitter_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nif max_jitter is None:\nraise TypeError(\"missing required argument: max_jitter \")\nstrategy = strategies[type(ts)]\nreturn strategy(ts, max_jitter, keep_tsupport)\n</code></pre>"},{"location":"old_pages/process.randomize/#pynapple.process.randomize.resample_timestamps","title":"resample_timestamps","text":"<pre><code>resample_timestamps(ts)\n</code></pre> <p>Resamples the timestamps in the time support, with uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to resample. If TsGroup, each Ts object in the group is independently resampled, in the time support of the whole group.</p> required <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The resampled timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def resample_timestamps(ts):\n\"\"\"\n    Resamples the timestamps in the time support, with uniform distribution.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to resample. If TsGroup, each Ts object in the group is independently\n        resampled, in the time support of the whole group.\n    Returns\n    -------\n    Ts or TsGroup\n        The resampled timestamps\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _resample_ts,\nnap.ts_group.TsGroup: _resample_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nstrategy = strategies[type(ts)]\nreturn strategy(ts)\n</code></pre>"},{"location":"old_pages/process.tuning_curves/","title":"Process.tuning curves","text":""},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves","title":"pynapple.process.tuning_curves","text":"<p>Summary</p>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_discrete_tuning_curves","title":"compute_discrete_tuning_curves","text":"<pre><code>compute_discrete_tuning_curves(group, dict_ep)\n</code></pre> <pre><code>Compute discrete tuning curves of a TsGroup using a dictionnary of epochs.\n</code></pre> <p>The function returns a pandas DataFrame with each row being a key of the dictionnary of epochs and each column being a neurons.</p> <p>This function can typically being used for a set of stimulus being presented for multiple epochs. An example of the dictionnary is :</p> <pre><code>&gt;&gt;&gt; dict_ep =  {\n        \"stim0\": nap.IntervalSet(start=0, end=1),\n        \"stim1\":nap.IntervalSet(start=2, end=3)\n    }\n</code></pre> <p>In this case, the function will return a pandas DataFrame :</p> <pre><code>&gt;&gt;&gt; tc\n           neuron0    neuron1    neuron2\nstim0        0 Hz       1 Hz       2 Hz\nstim1        3 Hz       4 Hz       5 Hz\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>dict_ep</code> <code>dict</code> <p>Dictionary of IntervalSets</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Table of firing rate for each neuron and each IntervalSet</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_discrete_tuning_curves(group, dict_ep):\n\"\"\"\n        Compute discrete tuning curves of a TsGroup using a dictionnary of epochs.\n    The function returns a pandas DataFrame with each row being a key of the dictionnary of epochs\n    and each column being a neurons.\n       This function can typically being used for a set of stimulus being presented for multiple epochs.\n    An example of the dictionnary is :\n        &gt;&gt;&gt; dict_ep =  {\n                \"stim0\": nap.IntervalSet(start=0, end=1),\n                \"stim1\":nap.IntervalSet(start=2, end=3)\n            }\n    In this case, the function will return a pandas DataFrame :\n        &gt;&gt;&gt; tc\n                   neuron0    neuron1    neuron2\n        stim0        0 Hz       1 Hz       2 Hz\n        stim1        3 Hz       4 Hz       5 Hz\n    Parameters\n    ----------\n    group : nap.TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    dict_ep : dict\n        Dictionary of IntervalSets\n    Returns\n    -------\n    pandas.DataFrame\n        Table of firing rate for each neuron and each IntervalSet\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object.\n    \"\"\"\nif not isinstance(group, nap.TsGroup):\nraise RuntimeError(\"Unknown format for group\")\nidx = np.sort(list(dict_ep.keys()))\ntuning_curves = pd.DataFrame(index=idx, columns=list(group.keys()), data=0)\nfor k in dict_ep.keys():\nif not isinstance(dict_ep[k], nap.IntervalSet):\nraise RuntimeError(\"Key {} in dict_ep is not an IntervalSet\".format(k))\nfor n in group.keys():\ntuning_curves.loc[k, n] = float(len(group[n].restrict(dict_ep[k])))\ntuning_curves.loc[k] = tuning_curves.loc[k] / dict_ep[k].tot_length(\"s\")\nreturn tuning_curves\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_1d_tuning_curves","title":"compute_1d_tuning_curves","text":"<pre><code>compute_1d_tuning_curves(\ngroup, feature, nb_bins, ep=None, minmax=None\n)\n</code></pre> <p>Computes 1-dimensional tuning curves relative to a 1d feature.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>feature</code> <code>Tsd</code> <p>The 1-dimensional target feature (e.g. head-direction)</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curve</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame to hold the tuning curves</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None):\n\"\"\"\n    Computes 1-dimensional tuning curves relative to a 1d feature.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    feature : Tsd\n        The 1-dimensional target feature (e.g. head-direction)\n    nb_bins : int\n        Number of bins in the tuning curve\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame to hold the tuning curves\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object.\n    \"\"\"\nif not isinstance(group, nap.TsGroup):\nraise RuntimeError(\"Unknown format for group\")\nif minmax is None:\nbins = np.linspace(np.min(feature), np.max(feature), nb_bins + 1)\nelse:\nbins = np.linspace(minmax[0], minmax[1], nb_bins + 1)\nidx = bins[0:-1] + np.diff(bins) / 2\ntuning_curves = pd.DataFrame(index=idx, columns=list(group.keys()))\nif isinstance(ep, nap.IntervalSet):\ngroup_value = group.value_from(feature, ep)\noccupancy, _ = np.histogram(feature.restrict(ep).values, bins)\nelse:\ngroup_value = group.value_from(feature)\noccupancy, _ = np.histogram(feature.values, bins)\nfor k in group_value:\ncount, _ = np.histogram(group_value[k].values, bins)\ncount = count / occupancy\ncount[np.isnan(count)] = 0.0\ntuning_curves[k] = count\ntuning_curves[k] = count * feature.rate\nreturn tuning_curves\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_2d_tuning_curves","title":"compute_2d_tuning_curves","text":"<pre><code>compute_2d_tuning_curves(\ngroup, feature, nb_bins, ep=None, minmax=None\n)\n</code></pre> <p>Computes 2-dimensional tuning curves relative to a 2d feature</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>feature</code> <code>TsdFrame</code> <p>The 2d feature (i.e. 2 columns features).</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curves</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list</code> <p>The min and max boundaries of the tuning curves given as: (minx, maxx, miny, maxy) If None, the boundaries are inferred from the target variable</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: </p> <p>tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).</p> <p>xy (list): List of bins center in the two dimensions</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object or if feature is not 2 columns only.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None):\n\"\"\"\n    Computes 2-dimensional tuning curves relative to a 2d feature\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    feature : TsdFrame\n        The 2d feature (i.e. 2 columns features).\n    nb_bins : int\n        Number of bins in the tuning curves\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves given as:\n        (minx, maxx, miny, maxy)\n        If None, the boundaries are inferred from the target variable\n    Returns\n    -------\n    tuple\n        A tuple containing: \\n\n        tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).\\n\n        xy (list): List of bins center in the two dimensions\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object or if feature is not 2 columns only.\n    \"\"\"\nif feature.shape[1] != 2:\nraise RuntimeError(\"feature should have 2 columns only.\")\nif type(group) is not nap.TsGroup:\nraise RuntimeError(\"Unknown format for group\")\nif isinstance(ep, nap.IntervalSet):\nfeature = feature.restrict(ep)\nelse:\nep = feature.time_support\ncols = list(feature.columns)\ngroups_value = {}\nbinsxy = {}\nfor i, c in enumerate(cols):\ngroups_value[c] = group.value_from(feature.loc[c], ep)\nif minmax is None:\nbins = np.linspace(\nnp.min(feature.loc[c]), np.max(feature.loc[c]), nb_bins + 1\n)\nelse:\nbins = np.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins + 1)\nbinsxy[c] = bins\noccupancy, _, _ = np.histogram2d(\nfeature.loc[cols[0]].values.flatten(),\nfeature.loc[cols[1]].values.flatten(),\n[binsxy[cols[0]], binsxy[cols[1]]],\n)\ntc = {}\nfor n in group.keys():\ncount, _, _ = np.histogram2d(\ngroups_value[cols[0]][n].values.flatten(),\ngroups_value[cols[1]][n].values.flatten(),\n[binsxy[cols[0]], binsxy[cols[1]]],\n)\ncount = count / occupancy\n# count[np.isnan(count)] = 0.0\ntc[n] = count * feature.rate\nxy = [binsxy[c][0:-1] + np.diff(binsxy[c]) / 2 for c in binsxy.keys()]\nreturn tc, xy\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_1d_mutual_info","title":"compute_1d_mutual_info","text":"<pre><code>compute_1d_mutual_info(\ntc, feature, ep=None, minmax=None, bitssec=False\n)\n</code></pre> <p>Mutual information as defined in</p> <p>Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993). An information-theoretic approach to deciphering the hippocampal code. In Advances in neural information processing systems (pp. 1030-1037).</p> <p>Parameters:</p> Name Type Description Default <code>tc</code> <code>DataFrame or ndarray</code> <p>Tuning curves in columns</p> required <code>feature</code> <code>Tsd</code> <p>The feature that was used to compute the tuning curves</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch over which the tuning curves were computed If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <code>bitssec</code> <code>bool</code> <p>By default, the function return bits per spikes. Set to true for bits per seconds</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Spatial Information (default is bits/spikes)</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_mutual_info(tc, feature, ep=None, minmax=None, bitssec=False):\n\"\"\"\n    Mutual information as defined in\n    Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993).\n    An information-theoretic approach to deciphering the hippocampal code.\n    In Advances in neural information processing systems (pp. 1030-1037).\n    Parameters\n    ----------\n    tc : pandas.DataFrame or numpy.ndarray\n        Tuning curves in columns\n    feature : Tsd\n        The feature that was used to compute the tuning curves\n    ep : IntervalSet, optional\n        The epoch over which the tuning curves were computed\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n    bitssec : bool, optional\n        By default, the function return bits per spikes.\n        Set to true for bits per seconds\n    Returns\n    -------\n    pandas.DataFrame\n        Spatial Information (default is bits/spikes)\n    \"\"\"\nif isinstance(tc, pd.DataFrame):\ncolumns = tc.columns.values\nfx = np.atleast_2d(tc.values)\nelif isinstance(tc, np.ndarray):\nfx = np.atleast_2d(tc)\ncolumns = np.arange(tc.shape[1])\nnb_bins = tc.shape[0] + 1\nif minmax is None:\nbins = np.linspace(np.min(feature), np.max(feature), nb_bins)\nelse:\nbins = np.linspace(minmax[0], minmax[1], nb_bins)\nif isinstance(ep, nap.IntervalSet):\noccupancy, _ = np.histogram(feature.restrict(ep).values, bins)\nelse:\noccupancy, _ = np.histogram(feature.values, bins)\noccupancy = occupancy / occupancy.sum()\noccupancy = occupancy[:, np.newaxis]\nfr = np.sum(fx * occupancy, 0)\nfxfr = fx / fr\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nlogfx = np.log2(fxfr)\nlogfx[np.isinf(logfx)] = 0.0\nSI = np.sum(occupancy * fx * logfx, 0)\nif bitssec:\nSI = pd.DataFrame(index=columns, columns=[\"SI\"], data=SI)\nreturn SI\nelse:\nSI = SI / fr\nSI = pd.DataFrame(index=columns, columns=[\"SI\"], data=SI)\nreturn SI\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_2d_mutual_info","title":"compute_2d_mutual_info","text":"<pre><code>compute_2d_mutual_info(\ntc, features, ep=None, minmax=None, bitssec=False\n)\n</code></pre> <p>Mutual information as defined in</p> <p>Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993). An information-theoretic approach to deciphering the hippocampal code. In Advances in neural information processing systems (pp. 1030-1037).</p> <p>Parameters:</p> Name Type Description Default <code>tc</code> <code>dict or ndarray</code> <p>If array, first dimension should be the neuron</p> required <code>features</code> <code>TsdFrame</code> <p>The 2 columns features that were used to compute the tuning curves</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch over which the tuning curves were computed If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target features</p> <code>None</code> <code>bitssec</code> <code>bool</code> <p>By default, the function return bits per spikes. Set to true for bits per seconds</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Spatial Information (default is bits/spikes)</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_mutual_info(tc, features, ep=None, minmax=None, bitssec=False):\n\"\"\"\n    Mutual information as defined in\n    Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993).\n    An information-theoretic approach to deciphering the hippocampal code.\n    In Advances in neural information processing systems (pp. 1030-1037).\n    Parameters\n    ----------\n    tc : dict or numpy.ndarray\n        If array, first dimension should be the neuron\n    features : TsdFrame\n        The 2 columns features that were used to compute the tuning curves\n    ep : IntervalSet, optional\n        The epoch over which the tuning curves were computed\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target features\n    bitssec : bool, optional\n        By default, the function return bits per spikes.\n        Set to true for bits per seconds\n    Returns\n    -------\n    pandas.DataFrame\n        Spatial Information (default is bits/spikes)\n    \"\"\"\n# A bit tedious here\nif type(tc) is dict:\nfx = np.array([tc[i] for i in tc.keys()])\nidx = list(tc.keys())\nelif type(tc) is np.ndarray:\nfx = tc\nidx = np.arange(len(tc))\nnb_bins = (fx.shape[1] + 1, fx.shape[2] + 1)\ncols = features.columns\nbins = []\nfor i, c in enumerate(cols):\nif minmax is None:\nbins.append(\nnp.linspace(\nnp.min(features.loc[c]), np.max(features.loc[c]), nb_bins[i]\n)\n)\nelse:\nbins.append(\nnp.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins[i])\n)\nif isinstance(ep, nap.IntervalSet):\nfeatures = features.restrict(ep)\noccupancy, _, _ = np.histogram2d(\nfeatures.loc[cols[0]].values.flatten(),\nfeatures.loc[cols[1]].values.flatten(),\n[bins[0], bins[1]],\n)\noccupancy = occupancy / occupancy.sum()\nfr = np.nansum(fx * occupancy, (1, 2))\nfr = fr[:, np.newaxis, np.newaxis]\nfxfr = fx / fr\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nlogfx = np.log2(fxfr)\nlogfx[np.isinf(logfx)] = 0.0\nSI = np.nansum(occupancy * fx * logfx, (1, 2))\nif bitssec:\nSI = pd.DataFrame(index=idx, columns=[\"SI\"], data=SI)\nreturn SI\nelse:\nSI = SI / fr[:, 0, 0]\nSI = pd.DataFrame(index=idx, columns=[\"SI\"], data=SI)\nreturn SI\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_1d_tuning_curves_continous","title":"compute_1d_tuning_curves_continous","text":"<pre><code>compute_1d_tuning_curves_continous(\ntsdframe, feature, nb_bins, ep=None, minmax=None\n)\n</code></pre> <p>Computes 1-dimensional tuning curves relative to a feature with continous data.</p> <p>Parameters:</p> Name Type Description Default <code>tsdframe</code> <code>Tsd or TsdFrame</code> <p>Input data (e.g. continus calcium data where each column is the calcium activity of one neuron)</p> required <code>feature</code> <code>Tsd</code> <p>The feature (one column)</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curves</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame to hold the tuning curves</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tsdframe is not a Tsd or a TsdFrame object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_tuning_curves_continous(\ntsdframe, feature, nb_bins, ep=None, minmax=None\n):\n\"\"\"\n    Computes 1-dimensional tuning curves relative to a feature with continous data.\n    Parameters\n    ----------\n    tsdframe : Tsd or TsdFrame\n        Input data (e.g. continus calcium data\n        where each column is the calcium activity of one neuron)\n    feature : Tsd\n        The feature (one column)\n    nb_bins : int\n        Number of bins in the tuning curves\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame to hold the tuning curves\n    Raises\n    ------\n    RuntimeError\n        If tsdframe is not a Tsd or a TsdFrame object.\n    \"\"\"\nif not isinstance(tsdframe, (nap.Tsd, nap.TsdFrame)):\nraise RuntimeError(\"Unknown format for tsdframe.\")\nif isinstance(ep, nap.IntervalSet):\nfeature = feature.restrict(ep)\ntsdframe = tsdframe.restrict(ep)\nelse:\ntsdframe = tsdframe.restrict(feature.time_support)\nif minmax is None:\nbins = np.linspace(np.min(feature), np.max(feature), nb_bins + 1)\nelse:\nbins = np.linspace(minmax[0], minmax[1], nb_bins + 1)\nalign_times = tsdframe.value_from(feature)\nidx = np.digitize(align_times.values, bins) - 1\ntmp = tsdframe.as_dataframe().groupby(idx).mean()\ntmp = tmp.reindex(np.arange(0, len(bins) - 1))\ntmp.index = pd.Index(bins[0:-1] + np.diff(bins) / 2)\ntmp = tmp.fillna(0)\nreturn pd.DataFrame(tmp)\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_2d_tuning_curves_continuous","title":"compute_2d_tuning_curves_continuous","text":"<pre><code>compute_2d_tuning_curves_continuous(\ntsdframe, features, nb_bins, ep=None, minmax=None\n)\n</code></pre> <p>Computes 2-dimensional tuning curves relative to a 2d feature with continous data.</p> <p>Parameters:</p> Name Type Description Default <code>tsdframe</code> <code>Tsd or TsdFrame</code> <p>Input data (e.g. continuous calcium data where each column is the calcium activity of one neuron)</p> required <code>features</code> <code>TsdFrame</code> <p>The 2d feature (two columns)</p> required <code>nb_bins</code> <code>int or tuple</code> <p>Number of bins in the tuning curves (separate for 2 feature dimensions if tuple provided)</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list</code> <p>The min and max boundaries of the tuning curves. Should be a tuple of minx, maxx, miny, maxy If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: </p> <p>tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).</p> <p>xy (list): List of bins center in the two dimensions</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tsdframe is not a Tsd/TsdFrame or if features is not 2 columns</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_tuning_curves_continuous(\ntsdframe, features, nb_bins, ep=None, minmax=None\n):\n\"\"\"\n    Computes 2-dimensional tuning curves relative to a 2d feature with continous data.\n    Parameters\n    ----------\n    tsdframe : Tsd or TsdFrame\n        Input data (e.g. continuous calcium data\n        where each column is the calcium activity of one neuron)\n    features : TsdFrame\n        The 2d feature (two columns)\n    nb_bins : int or tuple\n        Number of bins in the tuning curves (separate for 2 feature dimensions if tuple provided)\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        Should be a tuple of minx, maxx, miny, maxy\n        If None, the boundaries are inferred from the target feature\n    Returns\n    -------\n    tuple\n        A tuple containing: \\n\n        tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).\\n\n        xy (list): List of bins center in the two dimensions\n    Raises\n    ------\n    RuntimeError\n        If tsdframe is not a Tsd/TsdFrame or if features is not 2 columns\n    \"\"\"\nif not isinstance(tsdframe, (nap.Tsd, nap.TsdFrame)):\nraise RuntimeError(\"Unknown format for tsdframe.\")\nif not isinstance(features, nap.TsdFrame):\nraise RuntimeError(\"Unknown format for features.\")\nif isinstance(ep, nap.IntervalSet):\nfeatures = features.restrict(ep)\ntsdframe = tsdframe.restrict(ep)\nelse:\ntsdframe = tsdframe.restrict(features.time_support)\nif features.shape[1] != 2:\nraise RuntimeError(\"features input is not 2 columns.\")\nif isinstance(nb_bins, int):\nnb_bins = (nb_bins, nb_bins)\nelif len(nb_bins) != 2:\nraise RuntimeError(\"nb_bins should be int or tuple of 2 ints\")\ncols = list(features.columns)\nbinsxy = {}\nidxs = {}\nfor i, c in enumerate(cols):\nif minmax is None:\nbins = np.linspace(\nnp.min(features.loc[c]), np.max(features.loc[c]), nb_bins[i] + 1\n)\nelse:\nbins = np.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins[i] + 1)\nalign_times = tsdframe.value_from(features.loc[c], ep)\nidxs[c] = np.digitize(align_times.values.flatten(), bins) - 1\nbinsxy[c] = bins\nidxs = pd.DataFrame(idxs)\ntc_np = np.zeros((tsdframe.shape[1], nb_bins[0], nb_bins[1])) * np.nan\nfor k, tmp in idxs.groupby(cols):\nif (0 &lt;= k[0] &lt; nb_bins[0]) and (0 &lt;= k[1] &lt; nb_bins[1]):\ntc_np[:, k[0], k[1]] = np.mean(tsdframe[tmp.index].values, 0)\ntc_np[np.isnan(tc_np)] = 0.0\nxy = [binsxy[c][0:-1] + np.diff(binsxy[c]) / 2 for c in binsxy.keys()]\ntc = {c: tc_np[i] for i, c in enumerate(tsdframe.columns)}\nreturn tc, xy\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_1d_poisson_glm","title":"compute_1d_poisson_glm","text":"<pre><code>compute_1d_poisson_glm(\ngroup,\nfeature,\nbinsize,\nwindowsize,\nep,\ntime_units=\"s\",\nniter=100,\ntolerance=1e-05,\n)\n</code></pre> <p>Poisson GLM</p> <p>Warning : this function is still experimental!</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>Spike trains</p> required <code>feature</code> <code>Tsd</code> <p>The regressors</p> required <code>binsize</code> <code>float</code> <p>Bin size</p> required <code>windowsize</code> <code>Float</code> <p>The window for offsetting the regressors</p> required <code>ep</code> <code>IntervalSet</code> <p>On which epoch to perfom the GLM</p> required <code>time_units</code> <code>str</code> <p>Time units of binsize and windowsize</p> <code>'s'</code> <code>niter</code> <code>int</code> <p>Number of iteration for fitting the GLM</p> <code>100</code> <code>tolerance</code> <code>float</code> <p>Tolerance for stopping the IRLS</p> <code>1e-05</code> <p>Returns:</p> Type Description <code>tuple</code> <p>regressors : TsdFrame</p> <p>offset : pandas.Series</p> <p>prediction : TsdFrame</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if group is not a TsGroup</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_poisson_glm(\ngroup, feature, binsize, windowsize, ep, time_units=\"s\", niter=100, tolerance=1e-5\n):\n\"\"\"\n    Poisson GLM\n    Warning : this function is still experimental!\n    Parameters\n    ----------\n    group : TsGroup\n        Spike trains\n    feature : Tsd\n        The regressors\n    binsize : float\n        Bin size\n    windowsize : Float\n        The window for offsetting the regressors\n    ep : IntervalSet, optional\n        On which epoch to perfom the GLM\n    time_units : str, optional\n        Time units of binsize and windowsize\n    niter : int, optional\n        Number of iteration for fitting the GLM\n    tolerance : float, optional\n        Tolerance for stopping the IRLS\n    Returns\n    -------\n    tuple\n        regressors : TsdFrame\\n\n        offset : pandas.Series\\n\n        prediction : TsdFrame\\n\n    Raises\n    ------\n    RuntimeError\n        if group is not a TsGroup\n    \"\"\"\nif type(group) is nap.TsGroup:\nnewgroup = group.restrict(ep)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nbinsize = nap.TsIndex.format_timestamps(binsize, time_units)[0]\nwindowsize = nap.TsIndex.format_timestamps(windowsize, time_units)[0]\n# Bin the spike train\ncount = newgroup.count(binsize)\n# Downsample the feature to binsize\ntidx = []\ndfeat = []\nfor i in ep.index:\nbins = np.arange(ep.start[i], ep.end[i] + binsize, binsize)\nidx = np.digitize(feature.index.values, bins) - 1\ntmp = feature.groupby(idx).mean()\ntidx.append(bins[0:-1] + np.diff(bins) / 2)\ndfeat.append(tmp)\ndfeat = nap.Tsd(t=np.hstack(tidx), d=np.hstack(dfeat), time_support=ep)\n# Build the Hankel matrix\nnt = np.abs(windowsize // binsize).astype(\"int\") + 1\nX = hankel(\nnp.hstack((np.zeros(nt - 1), dfeat.values))[: -nt + 1], dfeat.values[-nt:]\n)\nX = np.hstack((np.ones((len(dfeat), 1)), X))\n# Fitting GLM for each neuron\nregressors = []\nfor i, n in enumerate(group.keys()):\nprint(\"Fitting Poisson GLM for unit %i\" % n)\nb = nap.jitted_functions.jit_poisson_IRLS(\nX, count[n].values, niter=niter, tolerance=tolerance\n)\nregressors.append(b)\nregressors = np.array(regressors).T\noffset = regressors[0]\nregressors = regressors[1:]\nregressors = nap.TsdFrame(\nt=np.arange(-nt + 1, 1) * binsize, d=regressors, columns=list(group.keys())\n)\noffset = pd.Series(index=group.keys(), data=offset)\nprediction = nap.TsdFrame(\nt=dfeat.index.values,\nd=np.exp(np.dot(X[:, 1:], regressors.values) + offset.values) * binsize,\n)\nreturn (regressors, offset, prediction)\n</code></pre>"},{"location":"reference/SUMMARY/","title":"Modules","text":"<ul> <li>core<ul> <li>interval_set</li> <li>jitted_functions</li> <li>time_index</li> <li>time_series</li> <li>time_units</li> <li>ts_group</li> </ul> </li> <li>io<ul> <li>cnmfe</li> <li>ephys_gui</li> <li>folder</li> <li>interface_npz</li> <li>interface_nwb</li> <li>loader</li> <li>loader_gui</li> <li>misc</li> <li>neurosuite</li> <li>ophys_gui</li> <li>phy</li> <li>suite2p</li> </ul> </li> <li>process<ul> <li>correlograms</li> <li>decoding</li> <li>perievent</li> <li>randomize</li> <li>tuning_curves</li> </ul> </li> </ul>"},{"location":"reference/core/","title":"Core","text":""},{"location":"reference/core/#pynapple.core","title":"pynapple.core","text":""},{"location":"reference/core/interval_set/","title":"Interval set","text":""},{"location":"reference/core/interval_set/#pynapple.core.interval_set","title":"pynapple.core.interval_set","text":""},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet","title":"IntervalSet","text":"<p>             Bases: <code>DataFrame</code></p> <p>A subclass of pandas.DataFrame representing a (irregular) set of time intervals in elapsed time, with relative operations</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>class IntervalSet(pd.DataFrame):\n# class IntervalSet():\n\"\"\"\n    A subclass of pandas.DataFrame representing a (irregular) set of time intervals in elapsed time, with relative operations\n    \"\"\"\ndef __init__(self, start, end=None, time_units=\"s\", **kwargs):\n\"\"\"\n        IntervalSet initializer\n        If start and end and not aligned, meaning that \\n\n        1. len(start) != len(end)\n        2. end[i] &gt; start[i]\n        3. start[i+1] &gt; end[i]\n        4. start and end are not sorted,\n        IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point\n        Parameters\n        ----------\n        start : numpy.ndarray or number or pandas.DataFrame\n            Beginning of intervals\n        end : numpy.ndarray or number, optional\n            Ends of intervals\n        time_units : str, optional\n            Time unit of the intervals ('us', 'ms', 's' [default])\n        **kwargs\n            Additional parameters passed ot pandas.DataFrame\n        Returns\n        -------\n        IntervalSet\n            _\n        Raises\n        ------\n        RuntimeError\n            Description\n        ValueError\n            If a pandas.DataFrame is passed, it should contains\n            a column 'start' and a column 'end'.\n        \"\"\"\nif end is None:\ndf = pd.DataFrame(start)\nif \"start\" not in df.columns or \"end\" not in df.columns:\nraise ValueError(\"wrong columns name\")\nstart = df[\"start\"].values.astype(np.float64)\nend = df[\"end\"].values.astype(np.float64)\nstart = TsIndex.sort_timestamps(\nTsIndex.format_timestamps(start.ravel(), time_units)\n)\nend = TsIndex.sort_timestamps(\nTsIndex.format_timestamps(end.ravel(), time_units)\n)\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\nself._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\nreturn\nstart = np.array(start).astype(np.float64)\nend = np.array(end).astype(np.float64)\nstart = TsIndex.format_timestamps(np.array(start).ravel(), time_units)\nend = TsIndex.format_timestamps(np.array(end).ravel(), time_units)\nif len(start) != len(end):\nraise RuntimeError(\"Starts end ends are not of the same length\")\nif not (np.diff(start) &gt; 0).all():\nwarnings.warn(\"start is not sorted.\", stacklevel=2)\nstart = np.sort(start)\nif not (np.diff(end) &gt; 0).all():\nwarnings.warn(\"end is not sorted.\", stacklevel=2)\nend = np.sort(end)\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\n# self._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\ndef __repr__(self):\nreturn self.as_units(\"s\").__repr__()\ndef __str__(self):\nreturn self.__repr__()\ndef time_span(self):\n\"\"\"\n        Time span of the interval set.\n        Returns\n        -------\n        out: IntervalSet\n            an IntervalSet with a single interval encompassing the whole IntervalSet\n        \"\"\"\ns = self[\"start\"][0]\ne = self[\"end\"].iloc[-1]\nreturn IntervalSet(s, e)\ndef tot_length(self, time_units=\"s\"):\n\"\"\"\n        Total elapsed time in the set.\n        Parameters\n        ----------\n        time_units : None, optional\n            The time units to return the result in ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: float\n            _\n        \"\"\"\ntot_l = (self[\"end\"] - self[\"start\"]).sum()\nreturn TsIndex.return_timestamps(np.array([tot_l]), time_units)[0]\ndef intersect(self, a):\n\"\"\"\n        set intersection of IntervalSet\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to intersect self with\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitintersect(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\ndef union(self, a):\n\"\"\"\n        set union of IntervalSet\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to union self with\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitunion(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\ndef set_diff(self, a):\n\"\"\"\n        set difference of IntervalSet\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to set-substract from self\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitdiff(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\ndef in_interval(self, tsd):\n\"\"\"\n        finds out in which element of the interval set each point in a time series fits.\n        NaNs for those that don't fit an interval\n        Parameters\n        ----------\n        tsd : Tsd\n            The tsd to be binned\n        Returns\n        -------\n        out: numpy.ndarray\n            an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet\n        \"\"\"\ntimes = tsd.index\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nreturn jitin_interval(times, starts, ends)\ndef drop_short_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Drops the short intervals in the interval set.\n        Parameters\n        ----------\n        threshold : numeric\n            Time threshold for \"short\" intervals\n        time_units : None, optional\n            The time units for the treshold ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: IntervalSet\n            A copied IntervalSet with the dropped intervals\n        \"\"\"\nthreshold = TsIndex.format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &gt; threshold].reset_index(\ndrop=True\n)\ndef drop_long_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Drops the long intervals in the interval set.\n        Parameters\n        ----------\n        threshold : numeric\n            Time threshold for \"long\" intervals\n        time_units : None, optional\n            The time units for the treshold ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: IntervalSet\n            A copied IntervalSet with the dropped intervals\n        \"\"\"\nthreshold = TsIndex.format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &lt; threshold].reset_index(\ndrop=True\n)\ndef as_units(self, units=\"s\"):\n\"\"\"\n        returns a DataFrame with time expressed in the desired unit\n        Parameters\n        ----------\n        units : None, optional\n            'us', 'ms', or 's' [default]\n        Returns\n        -------\n        out: pandas.DataFrame\n            DataFrame with adjusted times\n        \"\"\"\ndata = self.values.copy()\ndata = TsIndex.return_timestamps(data, units)\nif units == \"us\":\ndata = data.astype(np.int64)\ndf = pd.DataFrame(index=self.index.values, data=data, columns=self.columns)\nreturn df\ndef merge_close_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Merges intervals that are very close.\n        Parameters\n        ----------\n        threshold : numeric\n            time threshold for the closeness of the intervals\n        time_units : None, optional\n            time units for the threshold ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: IntervalSet\n            a copied IntervalSet with merged intervals\n        \"\"\"\nif len(self) == 0:\nreturn IntervalSet(start=[], end=[])\nthreshold = TsIndex.format_timestamps(\nnp.array((threshold,), dtype=np.float64).ravel(), time_units\n)[0]\nstart = self[\"start\"].values\nend = self[\"end\"].values\ntojoin = (start[1:] - end[0:-1]) &gt; threshold\nstart = np.hstack((start[0], start[1:][tojoin]))\nend = np.hstack((end[0:-1][tojoin], end[-1]))\nreturn IntervalSet(start=start, end=end)\ndef get_intervals_center(self, alpha=0.5):\n\"\"\"\n        Returns by default the centers of each intervals.\n        It is possible to bias the midpoint by changing the alpha parameter between [0, 1]\n        For each epoch:\n        t = start + (end-start)*alpha\n        Parameters\n        ----------\n        alpha : float, optional\n            The midpoint within each interval.\n        Returns\n        -------\n        Ts\n            Timestamps object\n        \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nif not isinstance(alpha, float):\nraise RuntimeError(\"Parameter alpha should be float type\")\nalpha = np.clip(alpha, 0, 1)\nt = starts + (ends - starts) * alpha\nreturn time_series.Ts(t=t, time_support=self)\ndef save(self, filename):\n\"\"\"\n        Save IntervalSet object in npz format. The file will contain the starts and ends.\n        The main purpose of this function is to save small/medium sized IntervalSet\n        objects. For example, you determined some epochs for one session that you want to save\n        to avoid recomputing them.\n        You can load the object with numpy.load. Keys are 'start', 'end' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n        &gt;&gt;&gt; ep.save(\"my_ep.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['start', 'end', 'type']\n        &gt;&gt;&gt; print(file['start'])\n        [0. 10. 20.]\n        It is then easy to recreate the IntervalSet object.\n        &gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n           start   end\n        0    0.0   5.0\n        1   10.0  12.0\n        2   20.0  33.0\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nstart=self.start.values,\nend=self.end.values,\ntype=np.array([\"IntervalSet\"], dtype=np.str_),\n)\nreturn\n@property\ndef _constructor(self):\nreturn IntervalSet\n@property\ndef starts(self):\n\"\"\"Return the starts of the IntervalSet as a Ts object\n        Returns\n        -------\n        Ts\n            The starts of the IntervalSet\n        \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nreturn time_series.Ts(t=self.values[:, 0], time_support=self)\n@property\ndef ends(self):\n\"\"\"Return the ends of the IntervalSet as a Ts object\n        Returns\n        -------\n        Ts\n            The ends of the IntervalSet\n        \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nreturn time_series.Ts(t=self.values[:, 1], time_support=self)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.starts","title":"starts  <code>property</code>","text":"<pre><code>starts\n</code></pre> <p>Return the starts of the IntervalSet as a Ts object</p> <p>Returns:</p> Type Description <code>Ts</code> <p>The starts of the IntervalSet</p>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.ends","title":"ends  <code>property</code>","text":"<pre><code>ends\n</code></pre> <p>Return the ends of the IntervalSet as a Ts object</p> <p>Returns:</p> Type Description <code>Ts</code> <p>The ends of the IntervalSet</p>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.__init__","title":"__init__","text":"<pre><code>__init__(start, end=None, time_units='s', **kwargs)\n</code></pre> <p>IntervalSet initializer</p> <p>If start and end and not aligned, meaning that </p> <ol> <li>len(start) != len(end)</li> <li>end[i] &gt; start[i]</li> <li>start[i+1] &gt; end[i]</li> <li>start and end are not sorted,</li> </ol> <p>IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>ndarray or number or DataFrame</code> <p>Beginning of intervals</p> required <code>end</code> <code>ndarray or number</code> <p>Ends of intervals</p> <code>None</code> <code>time_units</code> <code>str</code> <p>Time unit of the intervals ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>**kwargs</code> <p>Additional parameters passed ot pandas.DataFrame</p> <code>{}</code> <p>Returns:</p> Type Description <code>IntervalSet</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Description</p> <code>ValueError</code> <p>If a pandas.DataFrame is passed, it should contains a column 'start' and a column 'end'.</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def __init__(self, start, end=None, time_units=\"s\", **kwargs):\n\"\"\"\n    IntervalSet initializer\n    If start and end and not aligned, meaning that \\n\n    1. len(start) != len(end)\n    2. end[i] &gt; start[i]\n    3. start[i+1] &gt; end[i]\n    4. start and end are not sorted,\n    IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point\n    Parameters\n    ----------\n    start : numpy.ndarray or number or pandas.DataFrame\n        Beginning of intervals\n    end : numpy.ndarray or number, optional\n        Ends of intervals\n    time_units : str, optional\n        Time unit of the intervals ('us', 'ms', 's' [default])\n    **kwargs\n        Additional parameters passed ot pandas.DataFrame\n    Returns\n    -------\n    IntervalSet\n        _\n    Raises\n    ------\n    RuntimeError\n        Description\n    ValueError\n        If a pandas.DataFrame is passed, it should contains\n        a column 'start' and a column 'end'.\n    \"\"\"\nif end is None:\ndf = pd.DataFrame(start)\nif \"start\" not in df.columns or \"end\" not in df.columns:\nraise ValueError(\"wrong columns name\")\nstart = df[\"start\"].values.astype(np.float64)\nend = df[\"end\"].values.astype(np.float64)\nstart = TsIndex.sort_timestamps(\nTsIndex.format_timestamps(start.ravel(), time_units)\n)\nend = TsIndex.sort_timestamps(\nTsIndex.format_timestamps(end.ravel(), time_units)\n)\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\nself._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\nreturn\nstart = np.array(start).astype(np.float64)\nend = np.array(end).astype(np.float64)\nstart = TsIndex.format_timestamps(np.array(start).ravel(), time_units)\nend = TsIndex.format_timestamps(np.array(end).ravel(), time_units)\nif len(start) != len(end):\nraise RuntimeError(\"Starts end ends are not of the same length\")\nif not (np.diff(start) &gt; 0).all():\nwarnings.warn(\"start is not sorted.\", stacklevel=2)\nstart = np.sort(start)\nif not (np.diff(end) &gt; 0).all():\nwarnings.warn(\"end is not sorted.\", stacklevel=2)\nend = np.sort(end)\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\n# self._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.time_span","title":"time_span","text":"<pre><code>time_span()\n</code></pre> <p>Time span of the interval set.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>an IntervalSet with a single interval encompassing the whole IntervalSet</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def time_span(self):\n\"\"\"\n    Time span of the interval set.\n    Returns\n    -------\n    out: IntervalSet\n        an IntervalSet with a single interval encompassing the whole IntervalSet\n    \"\"\"\ns = self[\"start\"][0]\ne = self[\"end\"].iloc[-1]\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.tot_length","title":"tot_length","text":"<pre><code>tot_length(time_units='s')\n</code></pre> <p>Total elapsed time in the set.</p> <p>Parameters:</p> Name Type Description Default <code>time_units</code> <code>None</code> <p>The time units to return the result in ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def tot_length(self, time_units=\"s\"):\n\"\"\"\n    Total elapsed time in the set.\n    Parameters\n    ----------\n    time_units : None, optional\n        The time units to return the result in ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: float\n        _\n    \"\"\"\ntot_l = (self[\"end\"] - self[\"start\"]).sum()\nreturn TsIndex.return_timestamps(np.array([tot_l]), time_units)[0]\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.intersect","title":"intersect","text":"<pre><code>intersect(a)\n</code></pre> <p>set intersection of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to intersect self with</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def intersect(self, a):\n\"\"\"\n    set intersection of IntervalSet\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to intersect self with\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitintersect(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.union","title":"union","text":"<pre><code>union(a)\n</code></pre> <p>set union of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to union self with</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def union(self, a):\n\"\"\"\n    set union of IntervalSet\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to union self with\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitunion(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.set_diff","title":"set_diff","text":"<pre><code>set_diff(a)\n</code></pre> <p>set difference of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to set-substract from self</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def set_diff(self, a):\n\"\"\"\n    set difference of IntervalSet\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to set-substract from self\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitdiff(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.in_interval","title":"in_interval","text":"<pre><code>in_interval(tsd)\n</code></pre> <p>finds out in which element of the interval set each point in a time series fits.</p> <p>NaNs for those that don't fit an interval</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>Tsd</code> <p>The tsd to be binned</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def in_interval(self, tsd):\n\"\"\"\n    finds out in which element of the interval set each point in a time series fits.\n    NaNs for those that don't fit an interval\n    Parameters\n    ----------\n    tsd : Tsd\n        The tsd to be binned\n    Returns\n    -------\n    out: numpy.ndarray\n        an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet\n    \"\"\"\ntimes = tsd.index\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nreturn jitin_interval(times, starts, ends)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.drop_short_intervals","title":"drop_short_intervals","text":"<pre><code>drop_short_intervals(threshold, time_units='s')\n</code></pre> <p>Drops the short intervals in the interval set.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>Time threshold for \"short\" intervals</p> required <code>time_units</code> <code>None</code> <p>The time units for the treshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>A copied IntervalSet with the dropped intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def drop_short_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Drops the short intervals in the interval set.\n    Parameters\n    ----------\n    threshold : numeric\n        Time threshold for \"short\" intervals\n    time_units : None, optional\n        The time units for the treshold ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: IntervalSet\n        A copied IntervalSet with the dropped intervals\n    \"\"\"\nthreshold = TsIndex.format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &gt; threshold].reset_index(\ndrop=True\n)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.drop_long_intervals","title":"drop_long_intervals","text":"<pre><code>drop_long_intervals(threshold, time_units='s')\n</code></pre> <p>Drops the long intervals in the interval set.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>Time threshold for \"long\" intervals</p> required <code>time_units</code> <code>None</code> <p>The time units for the treshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>A copied IntervalSet with the dropped intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def drop_long_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Drops the long intervals in the interval set.\n    Parameters\n    ----------\n    threshold : numeric\n        Time threshold for \"long\" intervals\n    time_units : None, optional\n        The time units for the treshold ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: IntervalSet\n        A copied IntervalSet with the dropped intervals\n    \"\"\"\nthreshold = TsIndex.format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &lt; threshold].reset_index(\ndrop=True\n)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.as_units","title":"as_units","text":"<pre><code>as_units(units='s')\n</code></pre> <p>returns a DataFrame with time expressed in the desired unit</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>None</code> <p>'us', 'ms', or 's' [default]</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>DataFrame</code> <p>DataFrame with adjusted times</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    returns a DataFrame with time expressed in the desired unit\n    Parameters\n    ----------\n    units : None, optional\n        'us', 'ms', or 's' [default]\n    Returns\n    -------\n    out: pandas.DataFrame\n        DataFrame with adjusted times\n    \"\"\"\ndata = self.values.copy()\ndata = TsIndex.return_timestamps(data, units)\nif units == \"us\":\ndata = data.astype(np.int64)\ndf = pd.DataFrame(index=self.index.values, data=data, columns=self.columns)\nreturn df\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.merge_close_intervals","title":"merge_close_intervals","text":"<pre><code>merge_close_intervals(threshold, time_units='s')\n</code></pre> <p>Merges intervals that are very close.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>time threshold for the closeness of the intervals</p> required <code>time_units</code> <code>None</code> <p>time units for the threshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>a copied IntervalSet with merged intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def merge_close_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Merges intervals that are very close.\n    Parameters\n    ----------\n    threshold : numeric\n        time threshold for the closeness of the intervals\n    time_units : None, optional\n        time units for the threshold ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: IntervalSet\n        a copied IntervalSet with merged intervals\n    \"\"\"\nif len(self) == 0:\nreturn IntervalSet(start=[], end=[])\nthreshold = TsIndex.format_timestamps(\nnp.array((threshold,), dtype=np.float64).ravel(), time_units\n)[0]\nstart = self[\"start\"].values\nend = self[\"end\"].values\ntojoin = (start[1:] - end[0:-1]) &gt; threshold\nstart = np.hstack((start[0], start[1:][tojoin]))\nend = np.hstack((end[0:-1][tojoin], end[-1]))\nreturn IntervalSet(start=start, end=end)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.get_intervals_center","title":"get_intervals_center","text":"<pre><code>get_intervals_center(alpha=0.5)\n</code></pre> <p>Returns by default the centers of each intervals.</p> <p>It is possible to bias the midpoint by changing the alpha parameter between [0, 1] For each epoch: t = start + (end-start)*alpha</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>The midpoint within each interval.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Ts</code> <p>Timestamps object</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def get_intervals_center(self, alpha=0.5):\n\"\"\"\n    Returns by default the centers of each intervals.\n    It is possible to bias the midpoint by changing the alpha parameter between [0, 1]\n    For each epoch:\n    t = start + (end-start)*alpha\n    Parameters\n    ----------\n    alpha : float, optional\n        The midpoint within each interval.\n    Returns\n    -------\n    Ts\n        Timestamps object\n    \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nif not isinstance(alpha, float):\nraise RuntimeError(\"Parameter alpha should be float type\")\nalpha = np.clip(alpha, 0, 1)\nt = starts + (ends - starts) * alpha\nreturn time_series.Ts(t=t, time_support=self)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.save","title":"save","text":"<pre><code>save(filename)\n</code></pre> <p>Save IntervalSet object in npz format. The file will contain the starts and ends.</p> <p>The main purpose of this function is to save small/medium sized IntervalSet objects. For example, you determined some epochs for one session that you want to save to avoid recomputing them.</p> <p>You can load the object with numpy.load. Keys are 'start', 'end' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n&gt;&gt;&gt; ep.save(\"my_ep.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['start', 'end', 'type']\n&gt;&gt;&gt; print(file['start'])\n[0. 10. 20.]\n</code></pre> <p>It is then easy to recreate the IntervalSet object.</p> <pre><code>&gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n   start   end\n0    0.0   5.0\n1   10.0  12.0\n2   20.0  33.0\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save IntervalSet object in npz format. The file will contain the starts and ends.\n    The main purpose of this function is to save small/medium sized IntervalSet\n    objects. For example, you determined some epochs for one session that you want to save\n    to avoid recomputing them.\n    You can load the object with numpy.load. Keys are 'start', 'end' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n    &gt;&gt;&gt; ep.save(\"my_ep.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['start', 'end', 'type']\n    &gt;&gt;&gt; print(file['start'])\n    [0. 10. 20.]\n    It is then easy to recreate the IntervalSet object.\n    &gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n       start   end\n    0    0.0   5.0\n    1   10.0  12.0\n    2   20.0  33.0\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nstart=self.start.values,\nend=self.end.values,\ntype=np.array([\"IntervalSet\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.jitfix_iset","title":"jitfix_iset","text":"<pre><code>jitfix_iset(start, end)\n</code></pre> <p>0 - &gt; \"Some starts and ends are equal. Removing 1 microsecond!\", 1 - &gt; \"Some ends precede the relative start. Dropping them!\", 2 - &gt; \"Some starts precede the previous end. Joining them!\", 3 - &gt; \"Some epochs have no duration\"</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>ndarray</code> <p>Description</p> required <code>end</code> <code>ndarray</code> <p>Description</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>@jit(nopython=True)\ndef jitfix_iset(start, end):\n\"\"\"\n    0 - &gt; \"Some starts and ends are equal. Removing 1 microsecond!\",\n    1 - &gt; \"Some ends precede the relative start. Dropping them!\",\n    2 - &gt; \"Some starts precede the previous end. Joining them!\",\n    3 - &gt; \"Some epochs have no duration\"\n    Parameters\n    ----------\n    start : numpy.ndarray\n        Description\n    end : numpy.ndarray\n        Description\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nto_warn = np.zeros(4, dtype=np.bool_)\nm = start.shape[0]\ndata = np.zeros((m, 2), dtype=np.float64)\ni = 0\nct = 0\nwhile i &lt; m:\nnewstart = start[i]\nnewend = end[i]\nwhile i &lt; m:\nif end[i] == start[i]:\nto_warn[3] = True\ni += 1\nelse:\nnewstart = start[i]\nnewend = end[i]\nbreak\nwhile i &lt; m:\nif end[i] &lt; start[i]:\nto_warn[1] = True\ni += 1\nelse:\nnewstart = start[i]\nnewend = end[i]\nbreak\nwhile i &lt; m - 1:\nif start[i + 1] &lt; end[i]:\nto_warn[2] = True\ni += 1\nnewend = max(end[i - 1], end[i])\nelse:\nbreak\nif i &lt; m - 1:\nif newend == start[i + 1]:\nto_warn[0] = True\nnewend -= 1.0e-6\ndata[ct, 0] = newstart\ndata[ct, 1] = newend\nct += 1\ni += 1\ndata = data[0:ct]\nreturn (data, to_warn)\n</code></pre>"},{"location":"reference/core/jitted_functions/","title":"Jitted functions","text":""},{"location":"reference/core/jitted_functions/#pynapple.core.jitted_functions","title":"pynapple.core.jitted_functions","text":""},{"location":"reference/core/time_index/","title":"Time index","text":""},{"location":"reference/core/time_index/#pynapple.core.time_index","title":"pynapple.core.time_index","text":"<p>Similar to pd.Index, TsIndex holds the timestamps associated with the data of a time series. This class deals with conversion between different time units for all pynapple objects as well as making sure that timestamps are property sorted before initializing any objects.       - <code>us</code>: microseconds     - <code>ms</code>: milliseconds     - <code>s</code>: seconds  (overall default)</p>"},{"location":"reference/core/time_index/#pynapple.core.time_index.TsIndex","title":"TsIndex","text":"<p>             Bases: <code>ndarray</code></p> <p>Holder for timestamps. Similar to pandas.Index. Subclass numpy.ndarray</p> Source code in <code>pynapple/core/time_index.py</code> <pre><code>class TsIndex(np.ndarray):\n\"\"\"\n    Holder for timestamps. Similar to pandas.Index. Subclass numpy.ndarray\n    \"\"\"\n@staticmethod\ndef format_timestamps(t, units=\"s\"):\n\"\"\"\n        Converts time index in pynapple in a default format\n        Parameters\n        ----------\n        t : numpy.ndarray\n            a vector of times\n        units\n            the units in which times are given\n        Returns\n        -------\n        t : np.ndarray\n            times in standard pynapple format\n        Raises\n        ------\n        ValueError\n            Description\n        \"\"\"\nif units == \"s\":\nt = np.around(t, 9)\nelif units == \"ms\":\nt = np.around(t / 1.0e3, 9)\nelif units == \"us\":\nt = np.around(t / 1.0e6, 9)\nelse:\nraise ValueError(\"unrecognized time units type\")\nreturn t\n@staticmethod\ndef return_timestamps(t, units=\"s\"):\n\"\"\"\n        Converts time index in pynapple in a particular format\n        Parameters\n        ----------\n        t : numpy.ndarray\n            a vector (or scalar) of times\n        units\n            the units in which times are given\n        Returns\n        -------\n        t : numpy.ndarray\n            times in standard pynapple format\n        Raises\n        ------\n        ValueError\n            IF units is not in ['s', 'ms', 'us']\n        \"\"\"\nif units == \"s\":\nt = np.around(t, 9)\nelif units == \"ms\":\nt = np.around(t * 1.0e3, 9)\nelif units == \"us\":\nt = np.around(t * 1.0e6, 9)\nelse:\nraise ValueError(\"unrecognized time units type\")\nreturn t\n@staticmethod\ndef sort_timestamps(t, give_warning=True):\n\"\"\"\n        Raise warning if timestamps are not sorted\n        Parameters\n        ----------\n        t : numpy.ndarray\n            a vector of times\n        give_warning : bool, optional\n            If timestamps are not sorted\n        Returns\n        -------\n        numpy.ndarray\n            Description\n        \"\"\"\nif not (np.diff(t) &gt;= 0).all():\nif give_warning:\nwarn(\"timestamps are not sorted\", UserWarning)\nt = np.sort(t)\nreturn t\ndef __new__(cls, t, time_units=\"s\"):\nt = t.astype(np.float64).flatten()\nt = TsIndex.format_timestamps(t, time_units)\nt = TsIndex.sort_timestamps(t)\nobj = np.asarray(t).view(cls)\nreturn obj\n@property\ndef values(self):\n\"\"\"Returns the index as a ndarray\n        Returns\n        -------\n        numpy.ndarray\n            The timestamps in seconds\n        \"\"\"\nreturn np.asarray(self)\ndef __setitem__(self, *args, **kwargs):\nraise RuntimeError(\"TsIndex object is not mutable.\")\ndef to_numpy(self):\n\"\"\"Return the index as a ndarray. Useful for matplotlib.\n        Returns\n        -------\n        numpy.ndarray\n            The timestamps in seconds\n        \"\"\"\nreturn self.values\ndef in_units(self, time_units=\"s\"):\n\"\"\"Return the index as a ndarray in the desired units\n        Returns\n        -------\n        numpy.ndarray\n            The timestamps in seconds\n        \"\"\"\nreturn TsIndex.return_timestamps(self.values, time_units)\n</code></pre>"},{"location":"reference/core/time_index/#pynapple.core.time_index.TsIndex.values","title":"values  <code>property</code>","text":"<pre><code>values\n</code></pre> <p>Returns the index as a ndarray</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The timestamps in seconds</p>"},{"location":"reference/core/time_index/#pynapple.core.time_index.TsIndex.format_timestamps","title":"format_timestamps  <code>staticmethod</code>","text":"<pre><code>format_timestamps(t, units='s')\n</code></pre> <p>Converts time index in pynapple in a default format</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray</code> <p>a vector of times</p> required <code>units</code> <p>the units in which times are given</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>t</code> <code>ndarray</code> <p>times in standard pynapple format</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Description</p> Source code in <code>pynapple/core/time_index.py</code> <pre><code>@staticmethod\ndef format_timestamps(t, units=\"s\"):\n\"\"\"\n    Converts time index in pynapple in a default format\n    Parameters\n    ----------\n    t : numpy.ndarray\n        a vector of times\n    units\n        the units in which times are given\n    Returns\n    -------\n    t : np.ndarray\n        times in standard pynapple format\n    Raises\n    ------\n    ValueError\n        Description\n    \"\"\"\nif units == \"s\":\nt = np.around(t, 9)\nelif units == \"ms\":\nt = np.around(t / 1.0e3, 9)\nelif units == \"us\":\nt = np.around(t / 1.0e6, 9)\nelse:\nraise ValueError(\"unrecognized time units type\")\nreturn t\n</code></pre>"},{"location":"reference/core/time_index/#pynapple.core.time_index.TsIndex.return_timestamps","title":"return_timestamps  <code>staticmethod</code>","text":"<pre><code>return_timestamps(t, units='s')\n</code></pre> <p>Converts time index in pynapple in a particular format</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray</code> <p>a vector (or scalar) of times</p> required <code>units</code> <p>the units in which times are given</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>t</code> <code>ndarray</code> <p>times in standard pynapple format</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>IF units is not in ['s', 'ms', 'us']</p> Source code in <code>pynapple/core/time_index.py</code> <pre><code>@staticmethod\ndef return_timestamps(t, units=\"s\"):\n\"\"\"\n    Converts time index in pynapple in a particular format\n    Parameters\n    ----------\n    t : numpy.ndarray\n        a vector (or scalar) of times\n    units\n        the units in which times are given\n    Returns\n    -------\n    t : numpy.ndarray\n        times in standard pynapple format\n    Raises\n    ------\n    ValueError\n        IF units is not in ['s', 'ms', 'us']\n    \"\"\"\nif units == \"s\":\nt = np.around(t, 9)\nelif units == \"ms\":\nt = np.around(t * 1.0e3, 9)\nelif units == \"us\":\nt = np.around(t * 1.0e6, 9)\nelse:\nraise ValueError(\"unrecognized time units type\")\nreturn t\n</code></pre>"},{"location":"reference/core/time_index/#pynapple.core.time_index.TsIndex.sort_timestamps","title":"sort_timestamps  <code>staticmethod</code>","text":"<pre><code>sort_timestamps(t, give_warning=True)\n</code></pre> <p>Raise warning if timestamps are not sorted</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray</code> <p>a vector of times</p> required <code>give_warning</code> <code>bool</code> <p>If timestamps are not sorted</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Description</p> Source code in <code>pynapple/core/time_index.py</code> <pre><code>@staticmethod\ndef sort_timestamps(t, give_warning=True):\n\"\"\"\n    Raise warning if timestamps are not sorted\n    Parameters\n    ----------\n    t : numpy.ndarray\n        a vector of times\n    give_warning : bool, optional\n        If timestamps are not sorted\n    Returns\n    -------\n    numpy.ndarray\n        Description\n    \"\"\"\nif not (np.diff(t) &gt;= 0).all():\nif give_warning:\nwarn(\"timestamps are not sorted\", UserWarning)\nt = np.sort(t)\nreturn t\n</code></pre>"},{"location":"reference/core/time_index/#pynapple.core.time_index.TsIndex.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy()\n</code></pre> <p>Return the index as a ndarray. Useful for matplotlib.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The timestamps in seconds</p> Source code in <code>pynapple/core/time_index.py</code> <pre><code>def to_numpy(self):\n\"\"\"Return the index as a ndarray. Useful for matplotlib.\n    Returns\n    -------\n    numpy.ndarray\n        The timestamps in seconds\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_index/#pynapple.core.time_index.TsIndex.in_units","title":"in_units","text":"<pre><code>in_units(time_units='s')\n</code></pre> <p>Return the index as a ndarray in the desired units</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>The timestamps in seconds</p> Source code in <code>pynapple/core/time_index.py</code> <pre><code>def in_units(self, time_units=\"s\"):\n\"\"\"Return the index as a ndarray in the desired units\n    Returns\n    -------\n    numpy.ndarray\n        The timestamps in seconds\n    \"\"\"\nreturn TsIndex.return_timestamps(self.values, time_units)\n</code></pre>"},{"location":"reference/core/time_series/","title":"Time series","text":""},{"location":"reference/core/time_series/#pynapple.core.time_series","title":"pynapple.core.time_series","text":""},{"location":"reference/core/time_series/#pynapple.core.time_series--pynapple-time-series","title":"Pynapple time series","text":"<p>Pynapple time series are containers specialized for neurophysiological time series.</p> <p>They provides standardized time representation, plus various functions for manipulating times series with identical sampling frequency.</p> <p>Multiple time series object are avaible depending on the shape of the data.</p> <ul> <li><code>TsdTensor</code> : for data with of more than 2 dimensions, typically movies.</li> <li><code>TsdFrame</code> : for column-based data. It can be easily converted to a pandas.DataFrame. Columns can be labelled and selected similar to pandas.</li> <li><code>Tsd</code> : One-dimensional time series. It can be converted to a pandas.Series.</li> <li><code>Ts</code> : For timestamps data only.</li> </ul> <p>Most of the same functions are available through all classes. Objects behaves like numpy.ndarray. Slicing can be done the same way for example  <code>tsd[0:10]</code> returns the first 10 rows. Similarly, you can call any numpy functions like <code>np.mean(tsd, 1)</code>.</p>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor","title":"TsdTensor","text":"<p>             Bases: <code>NDArrayOperatorsMixin</code>, <code>_AbstractTsd</code></p> <p>TsdTensor</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class TsdTensor(NDArrayOperatorsMixin, _AbstractTsd):\n\"\"\"\n    TsdTensor\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, d, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n        TsdTensor initializer\n        Parameters\n        ----------\n        t : numpy.ndarray\n            the time index t\n        d : numpy.ndarray\n            The data\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default]).\n        time_support : IntervalSet, optional\n            The time support of the TsdFrame object\n        \"\"\"\nif isinstance(t, np.ndarray) and d is None:\nraise RuntimeError(\"Missing argument d when initializing TsdTensor\")\nif isinstance(t, (list, tuple)):\nt = np.array(t)\nif isinstance(d, (list, tuple)):\nd = np.array(d)\nassert (\nd.ndim &gt;= 3\n), \"Data should have more than 2 dimensions. If ndim &lt; 3, use TsdFrame or Tsd object\"\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index) != len(d):\nraise ValueError(\n\"Length of values \"\nf\"({len(d)}) \"\n\"does not match length of index \"\nf\"({len(self.index)})\"\n)\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt, d = jitrestrict(self.index.values, d, starts, ends)\nself.index = TsIndex(t)\nself.values = d\nelse:\ntime_support = IntervalSet(start=self.index[0], end=self.index[-1])\nself.values = d\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.values = np.empty(0)\nself.time_support = IntervalSet(start=[], end=[])\nself.nap_class = self.__class__.__name__\nself.dtype = self.values.dtype\ndef __repr__(self):\nheaders = [\"Time (s)\", \"\"]\nbottom = \"dtype: {}\".format(self.dtype) + \", shape: {}\".format(self.shape)\nif len(self):\ndef create_str(array):\nif array.ndim == 1:\nreturn (\n\"[\" + array[0].__repr__() + \" ... \" + array[0].__repr__() + \"]\"\n)\nelse:\nreturn \"[\" + create_str(array[0]) + \" ...]\"\n_str_ = []\nif self.shape[0] &lt; 100:\nfor i, array in zip(self.index, self.values):\n_str_.append([i.__repr__(), create_str(array)])\nelse:\nfor i, array in zip(self.index[0:5], self.values[0:5]):\n_str_.append([i.__repr__(), create_str(array)])\n_str_.append([\"...\", \"\"])\nfor i, array in zip(self.index[-5:], self.values[-5:]):\n_str_.append([i.__repr__(), create_str(array)])\nreturn tabulate(_str_, headers=headers, colalign=(\"left\",)) + \"\\n\" + bottom\nelse:\nreturn tabulate([], headers=headers) + \"\\n\" + bottom\ndef save(self, filename):\n\"\"\"\n        Save TsdTensor object in npz format. The file will contain the timestamps, the\n        data and the time support.\n        The main purpose of this function is to save small/medium sized time series\n        objects. For example, you extracted several channels from your recording and\n        filtered them. You can save the filtered channels as a npz to avoid\n        reprocessing it.\n        You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type'\n        and 'columns' for columns names.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsdtensor = nap.TsdTensor(t=np.array([0., 1.]), d = np.zeros((2,3,4)))\n        &gt;&gt;&gt; tsdtensor.save(\"my_path/my_tsdtensor.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_tsdtensor.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'd', 'start', 'end', ''type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1.]\n        It is then easy to recreate the TsdTensor object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.TsdTensor(t=file['t'], d=file['d'], time_support=time_support)\n        Time (s)\n        0.0       [[[0.0 ...]]]\n        1.0       [[[0.0 ...]]]\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([self.nap_class], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor.times","title":"times","text":"<pre><code>times(units='s')\n</code></pre> <p>The time index of the object, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>the time indexes</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the object, returned as np.double in the desired time units.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.ndarray\n        the time indexes\n    \"\"\"\nreturn self.index.in_units(units)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor.as_array","title":"as_array","text":"<pre><code>as_array()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_array(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor.data","title":"data","text":"<pre><code>data()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy()\n</code></pre> <p>Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling <code>plot(tsd)</code></p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def to_numpy(self):\n\"\"\"\n    Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling `plot(tsd)`\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor.start_time","title":"start_time","text":"<pre><code>start_time(units='s')\n</code></pre> <p>The first time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def start_time(self, units=\"s\"):\n\"\"\"\n    The first time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[0]\nelse:\nreturn None\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor.end_time","title":"end_time","text":"<pre><code>end_time(units='s')\n</code></pre> <p>The last time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def end_time(self, units=\"s\"):\n\"\"\"\n    The last time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[-1]\nelse:\nreturn None\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor.value_from","title":"value_from","text":"<pre><code>value_from(data, ep=None)\n</code></pre> <p>Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>The object holding the values to replace.</p> required <code>ep</code> <code>IntervalSet(optional)</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>Object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, data, ep=None):\n\"\"\"\n    Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument\n    Parameters\n    ----------\n    data : Tsd/TsdFrame/TsdTensor\n        The object holding the values to replace.\n    ep : IntervalSet (optional)\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    out : Tsd/TsdFrame/TsdTensor\n        Object with the new values\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n    newts is the same size as ts restrict to ep.\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\nif not isinstance(data, (TsdTensor, TsdFrame, Tsd)):\nraise RuntimeError(\n\"The time series to align to should be Tsd/TsdFrame/TsdTensor.\"\n)\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nif data_target_array.ndim == 1:\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\nelse:\nt, d, ns, ne = jitvaluefromtensor(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nif isinstance(data, TsdFrame):\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nreturn data.__class__(t, d, time_support=time_support)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor.count","title":"count","text":"<pre><code>count(*args, **kwargs)\n</code></pre> <p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsd.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsd.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsd.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsd.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; bincount = ts.count(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n&gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n</code></pre> <p>And bincount automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bincount.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  100.0  800.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsd.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsd.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsd.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsd.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; bincount = ts.count(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n    &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n    And bincount automatically inherit ep as time support:\n    &gt;&gt;&gt; bincount.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  100.0  800.0\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\ntime_array = self.index.values\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\nt, d = jitcount(time_array, starts, ends, bin_size)\nelse:\n_, d = jittsrestrict_with_count(time_array, starts, ends)\nt = starts + (ends - starts) / 2\nreturn Tsd(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor.restrict","title":"restrict","text":"<pre><code>restrict(iset)\n</code></pre> <p>Restricts a time series object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>(Ts, Tsd, TsdFrame or TsdTensor)</code> <p>Tsd object restricted to ep</p> <p>Examples:</p> <p>The Ts object is restrict to the intervals defined by ep.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n&gt;&gt;&gt; newts = ts.restrict(ep)\n</code></pre> <p>The time support of newts automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newts.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0    0.0  500.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, iset):\n\"\"\"\n    Restricts a time series object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    iset : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    out: Ts, Tsd, TsdFrame or TsdTensor\n        Tsd object restricted to ep\n    Examples\n    --------\n    The Ts object is restrict to the intervals defined by ep.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n    &gt;&gt;&gt; newts = ts.restrict(ep)\n    The time support of newts automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newts.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0    0.0  500.0\n    \"\"\"\nassert isinstance(iset, IntervalSet), \"Argument should be IntervalSet\"\ntime_array = self.index.values\nstarts = iset.start.values\nends = iset.end.values\nif isinstance(self.values, np.ndarray):\ndata_array = self.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=iset, columns=self.columns)\nelse:\nreturn self.__class__(t=t, d=d, time_support=iset)\nelse:\nt = jittsrestrict(time_array, starts, ends)\nreturn Ts(t, time_support=iset)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor.bin_average","title":"bin_average","text":"<pre><code>bin_average(bin_size, ep=None, time_units='s')\n</code></pre> <p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>(Tsd, TsdFrame, TsdTensor)</code> <p>A Tsd object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsd automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsd.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd, TsdFrame, TsdTensor\n        A Tsd object indexed by the center of the bins and holding the averaged data points.\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n    And bintsd automatically inherit ep as time support:\n    &gt;&gt;&gt; bintsd.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nif data_array.ndim &gt; 1:\nt, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\nelse:\nt, d = jitbin(time_array, data_array, starts, ends, bin_size)\nif d.ndim == 1:\nreturn Tsd(t=t, d=d, time_support=ep)\nelif d.ndim == 2:\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=ep, columns=self.columns)\nelse:\nreturn TsdFrame(t=t, d=d, time_support=ep)\nelse:\nreturn TsdTensor(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy the data, index and time support</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def copy(self):\n\"\"\"Copy the data, index and time support\"\"\"\nreturn self.__class__(\nt=self.index.copy(), d=self.values.copy(), time_support=self.time_support\n)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor.__init__","title":"__init__","text":"<pre><code>__init__(t, d, time_units='s', time_support=None, **kwargs)\n</code></pre> <p>TsdTensor initializer</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray</code> <p>the time index t</p> required <code>d</code> <code>ndarray</code> <p>The data</p> required <code>time_units</code> <code>str</code> <p>The time units in which times are specified ('us', 'ms', 's' [default]).</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the TsdFrame object</p> <code>None</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, d, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n    TsdTensor initializer\n    Parameters\n    ----------\n    t : numpy.ndarray\n        the time index t\n    d : numpy.ndarray\n        The data\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default]).\n    time_support : IntervalSet, optional\n        The time support of the TsdFrame object\n    \"\"\"\nif isinstance(t, np.ndarray) and d is None:\nraise RuntimeError(\"Missing argument d when initializing TsdTensor\")\nif isinstance(t, (list, tuple)):\nt = np.array(t)\nif isinstance(d, (list, tuple)):\nd = np.array(d)\nassert (\nd.ndim &gt;= 3\n), \"Data should have more than 2 dimensions. If ndim &lt; 3, use TsdFrame or Tsd object\"\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index) != len(d):\nraise ValueError(\n\"Length of values \"\nf\"({len(d)}) \"\n\"does not match length of index \"\nf\"({len(self.index)})\"\n)\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt, d = jitrestrict(self.index.values, d, starts, ends)\nself.index = TsIndex(t)\nself.values = d\nelse:\ntime_support = IntervalSet(start=self.index[0], end=self.index[-1])\nself.values = d\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.values = np.empty(0)\nself.time_support = IntervalSet(start=[], end=[])\nself.nap_class = self.__class__.__name__\nself.dtype = self.values.dtype\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdTensor.save","title":"save","text":"<pre><code>save(filename)\n</code></pre> <p>Save TsdTensor object in npz format. The file will contain the timestamps, the data and the time support.</p> <p>The main purpose of this function is to save small/medium sized time series objects. For example, you extracted several channels from your recording and filtered them. You can save the filtered channels as a npz to avoid reprocessing it.</p> <p>You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type' and 'columns' for columns names.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsdtensor = nap.TsdTensor(t=np.array([0., 1.]), d = np.zeros((2,3,4)))\n&gt;&gt;&gt; tsdtensor.save(\"my_path/my_tsdtensor.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_tsdtensor.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'd', 'start', 'end', ''type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1.]\n</code></pre> <p>It is then easy to recreate the TsdTensor object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.TsdTensor(t=file['t'], d=file['d'], time_support=time_support)\nTime (s)\n0.0       [[[0.0 ...]]]\n1.0       [[[0.0 ...]]]\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save TsdTensor object in npz format. The file will contain the timestamps, the\n    data and the time support.\n    The main purpose of this function is to save small/medium sized time series\n    objects. For example, you extracted several channels from your recording and\n    filtered them. You can save the filtered channels as a npz to avoid\n    reprocessing it.\n    You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type'\n    and 'columns' for columns names.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsdtensor = nap.TsdTensor(t=np.array([0., 1.]), d = np.zeros((2,3,4)))\n    &gt;&gt;&gt; tsdtensor.save(\"my_path/my_tsdtensor.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_tsdtensor.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'd', 'start', 'end', ''type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1.]\n    It is then easy to recreate the TsdTensor object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.TsdTensor(t=file['t'], d=file['d'], time_support=time_support)\n    Time (s)\n    0.0       [[[0.0 ...]]]\n    1.0       [[[0.0 ...]]]\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([self.nap_class], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame","title":"TsdFrame","text":"<p>             Bases: <code>NDArrayOperatorsMixin</code>, <code>_AbstractTsd</code></p> <p>TsdFrame</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class TsdFrame(NDArrayOperatorsMixin, _AbstractTsd):\n\"\"\"\n    TsdFrame\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, d=None, time_units=\"s\", time_support=None, columns=None):\n\"\"\"\n        TsdFrame initializer\n        A pandas.DataFrame can be passed directly\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.DataFrame\n            the time index t,  or a pandas.DataFrame (if d is None)\n        d : numpy.ndarray\n            The data\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default]).\n        time_support : IntervalSet, optional\n            The time support of the TsdFrame object\n        columns : iterables\n            Column names\n        \"\"\"\nif isinstance(t, np.ndarray) and d is None:\nraise RuntimeError(\"Missing argument d when initializing TsdFrame\")\nc = columns\nif isinstance(t, pd.DataFrame):\nd = t.values\nc = t.columns.values\nt = t.index.values\nif isinstance(t, (list, tuple)):\nt = np.array(t)\nif isinstance(d, (list, tuple)):\nd = np.array(d)\nassert d.ndim &lt;= 2, \"Data should be 1 or 2 dimensional\"\nif d.ndim == 1:\nd = d[:, np.newaxis]\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index) != len(d):\nraise ValueError(\n\"Length of values \"\nf\"({len(d)}) \"\n\"does not match length of index \"\nf\"({len(self.index)})\"\n)\nif c is None or len(c) != d.shape[1]:\nc = np.arange(d.shape[1], dtype=\"int\")\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt, d = jitrestrict(self.index.values, d, starts, ends)\nself.index = TsIndex(t)\nself.values = d\nelse:\ntime_support = IntervalSet(start=self.index[0], end=self.index[-1])\nself.values = d\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.values = np.empty(0)\nself.time_support = IntervalSet(start=[], end=[])\nself.columns = pd.Index(c)\nself.nap_class = self.__class__.__name__\nself.dtype = self.values.dtype\n@property\ndef loc(self):\nreturn _TsdFrameSliceHelper(self)\ndef __repr__(self):\nheaders = [\"Time (s)\"] + [str(k) for k in self.columns]\nbottom = \"dtype: {}\".format(self.dtype) + \", shape: {}\".format(self.shape)\nmax_cols = 5\ntry:\nmax_cols = os.get_terminal_size()[0] // 16\nexcept Exception:\nimport shutil\nmax_cols = shutil.get_terminal_size().columns // 16\nelse:\npass\nif self.shape[1] &gt; max_cols:\nheaders = headers[0 : max_cols + 1] + [\"...\"]\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nif len(self):\ntable = []\nend = [\"...\"] if self.shape[1] &gt; max_cols else []\nif len(self) &gt; 51:\nfor i, array in zip(self.index[0:5], self.values[0:5, 0:max_cols]):\ntable.append([i] + [k for k in array] + end)\ntable.append([\"...\"])\nfor i, array in zip(self.index[-5:], self.values[-5:, 0:max_cols]):\ntable.append([i] + [k for k in array] + end)\nreturn tabulate(table, headers=headers) + \"\\n\" + bottom\nelse:\nfor i, array in zip(self.index, self.values[:, 0:max_cols]):\ntable.append([i] + [k for k in array] + end)\nreturn tabulate(table, headers=headers) + \"\\n\" + bottom\nelse:\nreturn tabulate([], headers=headers) + \"\\n\" + bottom\ndef __getitem__(self, key, *args, **kwargs):\nif (\nisinstance(key, str)\nor hasattr(key, \"__iter__\")\nand all([isinstance(k, str) for k in key])\n):\nreturn self.loc[key]\nelse:\nreturn super().__getitem__(key, *args, **kwargs)\ndef __setitem__(self, key, value):\ntry:\nif isinstance(key, str):\nnew_key = self.columns.get_indexer([key])\nself.values.__setitem__((slice(None, None, None), new_key[0]), value)\nelif hasattr(key, \"__iter__\") and all([isinstance(k, str) for k in key]):\nnew_key = self.columns.get_indexer(key)\nself.values.__setitem__((slice(None, None, None), new_key), value)\nelse:\nself.values.__setitem__(key, value)\nexcept IndexError:\nraise IndexError\ndef as_dataframe(self):\n\"\"\"\n        Convert the TsdFrame object to a pandas.DataFrame object.\n        Returns\n        -------\n        out: pandas.DataFrame\n            _\n        \"\"\"\nreturn pd.DataFrame(\nindex=self.index.values, data=self.values, columns=self.columns\n)\ndef as_units(self, units=\"s\"):\n\"\"\"\n        Returns a DataFrame with time expressed in the desired unit.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        pandas.DataFrame\n            the series object with adjusted times\n        \"\"\"\nt = self.index.in_units(units)\nif units == \"us\":\nt = t.astype(np.int64)\ndf = pd.DataFrame(index=t, data=self.values)\ndf.index.name = \"Time (\" + str(units) + \")\"\ndf.columns = self.columns.copy()\nreturn df\ndef save(self, filename):\n\"\"\"\n        Save TsdFrame object in npz format. The file will contain the timestamps, the\n        data and the time support.\n        The main purpose of this function is to save small/medium sized time series\n        objects. For example, you extracted several channels from your recording and\n        filtered them. You can save the filtered channels as a npz to avoid\n        reprocessing it.\n        You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type'\n        and 'columns' for columns names.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n        &gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'd', 'start', 'end', 'columns', 'type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1.]\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n                  a  b\n        Time (s)\n        0.0       2  3\n        1.0       4  5\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ncols_name = self.columns\nif cols_name.dtype == np.dtype(\"O\"):\ncols_name = cols_name.astype(str)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ncolumns=cols_name,\ntype=np.array([\"TsdFrame\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.times","title":"times","text":"<pre><code>times(units='s')\n</code></pre> <p>The time index of the object, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>the time indexes</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the object, returned as np.double in the desired time units.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.ndarray\n        the time indexes\n    \"\"\"\nreturn self.index.in_units(units)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.as_array","title":"as_array","text":"<pre><code>as_array()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_array(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.data","title":"data","text":"<pre><code>data()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy()\n</code></pre> <p>Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling <code>plot(tsd)</code></p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def to_numpy(self):\n\"\"\"\n    Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling `plot(tsd)`\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.start_time","title":"start_time","text":"<pre><code>start_time(units='s')\n</code></pre> <p>The first time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def start_time(self, units=\"s\"):\n\"\"\"\n    The first time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[0]\nelse:\nreturn None\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.end_time","title":"end_time","text":"<pre><code>end_time(units='s')\n</code></pre> <p>The last time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def end_time(self, units=\"s\"):\n\"\"\"\n    The last time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[-1]\nelse:\nreturn None\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.value_from","title":"value_from","text":"<pre><code>value_from(data, ep=None)\n</code></pre> <p>Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>The object holding the values to replace.</p> required <code>ep</code> <code>IntervalSet(optional)</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>Object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, data, ep=None):\n\"\"\"\n    Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument\n    Parameters\n    ----------\n    data : Tsd/TsdFrame/TsdTensor\n        The object holding the values to replace.\n    ep : IntervalSet (optional)\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    out : Tsd/TsdFrame/TsdTensor\n        Object with the new values\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n    newts is the same size as ts restrict to ep.\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\nif not isinstance(data, (TsdTensor, TsdFrame, Tsd)):\nraise RuntimeError(\n\"The time series to align to should be Tsd/TsdFrame/TsdTensor.\"\n)\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nif data_target_array.ndim == 1:\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\nelse:\nt, d, ns, ne = jitvaluefromtensor(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nif isinstance(data, TsdFrame):\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nreturn data.__class__(t, d, time_support=time_support)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.count","title":"count","text":"<pre><code>count(*args, **kwargs)\n</code></pre> <p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsd.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsd.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsd.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsd.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; bincount = ts.count(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n&gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n</code></pre> <p>And bincount automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bincount.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  100.0  800.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsd.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsd.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsd.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsd.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; bincount = ts.count(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n    &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n    And bincount automatically inherit ep as time support:\n    &gt;&gt;&gt; bincount.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  100.0  800.0\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\ntime_array = self.index.values\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\nt, d = jitcount(time_array, starts, ends, bin_size)\nelse:\n_, d = jittsrestrict_with_count(time_array, starts, ends)\nt = starts + (ends - starts) / 2\nreturn Tsd(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.restrict","title":"restrict","text":"<pre><code>restrict(iset)\n</code></pre> <p>Restricts a time series object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>(Ts, Tsd, TsdFrame or TsdTensor)</code> <p>Tsd object restricted to ep</p> <p>Examples:</p> <p>The Ts object is restrict to the intervals defined by ep.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n&gt;&gt;&gt; newts = ts.restrict(ep)\n</code></pre> <p>The time support of newts automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newts.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0    0.0  500.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, iset):\n\"\"\"\n    Restricts a time series object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    iset : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    out: Ts, Tsd, TsdFrame or TsdTensor\n        Tsd object restricted to ep\n    Examples\n    --------\n    The Ts object is restrict to the intervals defined by ep.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n    &gt;&gt;&gt; newts = ts.restrict(ep)\n    The time support of newts automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newts.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0    0.0  500.0\n    \"\"\"\nassert isinstance(iset, IntervalSet), \"Argument should be IntervalSet\"\ntime_array = self.index.values\nstarts = iset.start.values\nends = iset.end.values\nif isinstance(self.values, np.ndarray):\ndata_array = self.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=iset, columns=self.columns)\nelse:\nreturn self.__class__(t=t, d=d, time_support=iset)\nelse:\nt = jittsrestrict(time_array, starts, ends)\nreturn Ts(t, time_support=iset)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.bin_average","title":"bin_average","text":"<pre><code>bin_average(bin_size, ep=None, time_units='s')\n</code></pre> <p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>(Tsd, TsdFrame, TsdTensor)</code> <p>A Tsd object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsd automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsd.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd, TsdFrame, TsdTensor\n        A Tsd object indexed by the center of the bins and holding the averaged data points.\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n    And bintsd automatically inherit ep as time support:\n    &gt;&gt;&gt; bintsd.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nif data_array.ndim &gt; 1:\nt, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\nelse:\nt, d = jitbin(time_array, data_array, starts, ends, bin_size)\nif d.ndim == 1:\nreturn Tsd(t=t, d=d, time_support=ep)\nelif d.ndim == 2:\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=ep, columns=self.columns)\nelse:\nreturn TsdFrame(t=t, d=d, time_support=ep)\nelse:\nreturn TsdTensor(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy the data, index and time support</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def copy(self):\n\"\"\"Copy the data, index and time support\"\"\"\nreturn self.__class__(\nt=self.index.copy(), d=self.values.copy(), time_support=self.time_support\n)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.__init__","title":"__init__","text":"<pre><code>__init__(\nt,\nd=None,\ntime_units=\"s\",\ntime_support=None,\ncolumns=None,\n)\n</code></pre> <p>TsdFrame initializer A pandas.DataFrame can be passed directly</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray or DataFrame</code> <p>the time index t,  or a pandas.DataFrame (if d is None)</p> required <code>d</code> <code>ndarray</code> <p>The data</p> <code>None</code> <code>time_units</code> <code>str</code> <p>The time units in which times are specified ('us', 'ms', 's' [default]).</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the TsdFrame object</p> <code>None</code> <code>columns</code> <code>iterables</code> <p>Column names</p> <code>None</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, d=None, time_units=\"s\", time_support=None, columns=None):\n\"\"\"\n    TsdFrame initializer\n    A pandas.DataFrame can be passed directly\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.DataFrame\n        the time index t,  or a pandas.DataFrame (if d is None)\n    d : numpy.ndarray\n        The data\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default]).\n    time_support : IntervalSet, optional\n        The time support of the TsdFrame object\n    columns : iterables\n        Column names\n    \"\"\"\nif isinstance(t, np.ndarray) and d is None:\nraise RuntimeError(\"Missing argument d when initializing TsdFrame\")\nc = columns\nif isinstance(t, pd.DataFrame):\nd = t.values\nc = t.columns.values\nt = t.index.values\nif isinstance(t, (list, tuple)):\nt = np.array(t)\nif isinstance(d, (list, tuple)):\nd = np.array(d)\nassert d.ndim &lt;= 2, \"Data should be 1 or 2 dimensional\"\nif d.ndim == 1:\nd = d[:, np.newaxis]\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index) != len(d):\nraise ValueError(\n\"Length of values \"\nf\"({len(d)}) \"\n\"does not match length of index \"\nf\"({len(self.index)})\"\n)\nif c is None or len(c) != d.shape[1]:\nc = np.arange(d.shape[1], dtype=\"int\")\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt, d = jitrestrict(self.index.values, d, starts, ends)\nself.index = TsIndex(t)\nself.values = d\nelse:\ntime_support = IntervalSet(start=self.index[0], end=self.index[-1])\nself.values = d\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.values = np.empty(0)\nself.time_support = IntervalSet(start=[], end=[])\nself.columns = pd.Index(c)\nself.nap_class = self.__class__.__name__\nself.dtype = self.values.dtype\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.as_dataframe","title":"as_dataframe","text":"<pre><code>as_dataframe()\n</code></pre> <p>Convert the TsdFrame object to a pandas.DataFrame object.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_dataframe(self):\n\"\"\"\n    Convert the TsdFrame object to a pandas.DataFrame object.\n    Returns\n    -------\n    out: pandas.DataFrame\n        _\n    \"\"\"\nreturn pd.DataFrame(\nindex=self.index.values, data=self.values, columns=self.columns\n)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.as_units","title":"as_units","text":"<pre><code>as_units(units='s')\n</code></pre> <p>Returns a DataFrame with time expressed in the desired unit.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>the series object with adjusted times</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    Returns a DataFrame with time expressed in the desired unit.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    pandas.DataFrame\n        the series object with adjusted times\n    \"\"\"\nt = self.index.in_units(units)\nif units == \"us\":\nt = t.astype(np.int64)\ndf = pd.DataFrame(index=t, data=self.values)\ndf.index.name = \"Time (\" + str(units) + \")\"\ndf.columns = self.columns.copy()\nreturn df\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.save","title":"save","text":"<pre><code>save(filename)\n</code></pre> <p>Save TsdFrame object in npz format. The file will contain the timestamps, the data and the time support.</p> <p>The main purpose of this function is to save small/medium sized time series objects. For example, you extracted several channels from your recording and filtered them. You can save the filtered channels as a npz to avoid reprocessing it.</p> <p>You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type' and 'columns' for columns names.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n&gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'd', 'start', 'end', 'columns', 'type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1.]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n          a  b\nTime (s)\n0.0       2  3\n1.0       4  5\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save TsdFrame object in npz format. The file will contain the timestamps, the\n    data and the time support.\n    The main purpose of this function is to save small/medium sized time series\n    objects. For example, you extracted several channels from your recording and\n    filtered them. You can save the filtered channels as a npz to avoid\n    reprocessing it.\n    You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type'\n    and 'columns' for columns names.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n    &gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'd', 'start', 'end', 'columns', 'type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1.]\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n              a  b\n    Time (s)\n    0.0       2  3\n    1.0       4  5\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ncols_name = self.columns\nif cols_name.dtype == np.dtype(\"O\"):\ncols_name = cols_name.astype(str)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ncolumns=cols_name,\ntype=np.array([\"TsdFrame\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd","title":"Tsd","text":"<p>             Bases: <code>NDArrayOperatorsMixin</code>, <code>_AbstractTsd</code></p> <p>A container around numpy.ndarray specialized for neurophysiology time series.</p> <p>Tsd provides standardized time representation, plus various functions for manipulating times series.</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class Tsd(NDArrayOperatorsMixin, _AbstractTsd):\n\"\"\"\n    A container around numpy.ndarray specialized for neurophysiology time series.\n    Tsd provides standardized time representation, plus various functions for manipulating times series.\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, d=None, time_units=\"s\", time_support=None):\n\"\"\"\n        Tsd Initializer.\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.Series\n            An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n        d : numpy.ndarray, optional\n            The data of the time series\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default])\n        time_support : IntervalSet, optional\n            The time support of the tsd object\n        \"\"\"\nif isinstance(t, np.ndarray) and d is None:\nraise RuntimeError(\"Missing argument d when initializing Tsd\")\nif isinstance(t, pd.Series):\nd = t.values\nt = t.index.values\nif isinstance(t, (list, tuple)):\nt = np.array(t)\nif isinstance(d, (list, tuple)):\nd = np.array(d)\nassert d.ndim == 1, \"Data should be 1 dimension\"\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index) != len(d):\nraise ValueError(\n\"Length of values \"\nf\"({len(d)}) \"\n\"does not match length of index \"\nf\"({len(self.index)})\"\n)\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt, d = jitrestrict(self.index.values, d, starts, ends)\nself.index = TsIndex(t)\nself.values = d\nelse:\ntime_support = IntervalSet(start=self.index[0], end=self.index[-1])\nself.values = d\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.values = np.empty(0)\nself.time_support = IntervalSet(start=[], end=[])\nself.nap_class = self.__class__.__name__\nself.dtype = self.values.dtype\ndef __repr__(self):\nheaders = [\"Time (s)\", \"\"]\nbottom = \"dtype: {}\".format(self.dtype) + \", shape: {}\".format(self.shape)\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nif len(self):\nif len(self) &lt; 51:\nreturn (\ntabulate(\nnp.vstack((self.index, self.values)).T,\nheaders=headers,\ncolalign=(\"left\",),\n)\n+ \"\\n\"\n+ bottom\n)\nelse:\ntable = []\nfor i, v in zip(self.index[0:5], self.values[0:5]):\ntable.append([i, v])\ntable.append([\"...\"])\nfor i, array in zip(self.index[-5:], self.values[-5:]):\ntable.append([i, v])\nreturn (\ntabulate(table, headers=headers, colalign=(\"left\",))\n+ \"\\n\"\n+ bottom\n)\nelse:\nreturn tabulate([], headers=headers) + \"\\n\" + bottom\ndef as_series(self):\n\"\"\"\n        Convert the Ts/Tsd object to a pandas.Series object.\n        Returns\n        -------\n        out: pandas.Series\n            _\n        \"\"\"\nreturn pd.Series(\nindex=self.index.values, data=self.values, copy=True, dtype=\"float64\"\n)\ndef as_units(self, units=\"s\"):\n\"\"\"\n        Returns a pandas Series with time expressed in the desired unit.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        pandas.Series\n            the series object with adjusted times\n        \"\"\"\nss = self.as_series()\nt = self.index.in_units(units)\nif units == \"us\":\nt = t.astype(np.int64)\nss.index = t\nss.index.name = \"Time (\" + str(units) + \")\"\nreturn ss\ndef threshold(self, thr, method=\"above\"):\n\"\"\"\n        Apply a threshold function to the tsd to return a new tsd\n        with the time support being the epochs above/below/&gt;=/&lt;= the threshold\n        Parameters\n        ----------\n        thr : float\n            The threshold value\n        method : str, optional\n            The threshold method (above/below/aboveequal/belowequal)\n        Returns\n        -------\n        out: Tsd\n            All the time points below/ above/greater than equal to/less than equal to the threshold\n        Raises\n        ------\n        ValueError\n            Raise an error if method is not 'below' or 'above'\n        RuntimeError\n            Raise an error if thr is too high/low and no epochs is found.\n        Examples\n        --------\n        This example finds all epoch above 0.5 within the tsd object.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n        &gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n        The epochs with the times above/below the threshold can be accessed through the time support:\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n        &gt;&gt;&gt; tsd.threshold(50).time_support\n        &gt;&gt;&gt;    start   end\n        &gt;&gt;&gt; 0   50.5  99.0\n        \"\"\"\ntime_array = self.index.values\ndata_array = self.values\nstarts = self.time_support.start.values\nends = self.time_support.end.values\nif method not in [\"above\", \"below\", \"aboveequal\", \"belowequal\"]:\nraise ValueError(\n\"Method {} for thresholding is not accepted.\".format(method)\n)\nt, d, ns, ne = jitthreshold(time_array, data_array, starts, ends, thr, method)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\ndef to_tsgroup(self):\n\"\"\"\n        Convert Tsd to a TsGroup by grouping timestamps with the same values.\n        By default, the values are converted to integers.\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\n        Time (s)\n        0.0    0\n        1.0    2\n        2.0    0\n        3.0    1\n        dtype: int64\n        &gt;&gt;&gt; tsd.to_tsgroup()\n        Index    rate\n        -------  ------\n            0    0.67\n            1    0.33\n            2    0.33\n        The reverse operation can be done with the TsGroup.to_tsd function :\n        &gt;&gt;&gt; tsgroup.to_tsd()\n        Time (s)\n        0.0    0.0\n        1.0    2.0\n        2.0    0.0\n        3.0    1.0\n        dtype: float64\n        Returns\n        -------\n        TsGroup\n            Grouped timestamps\n        \"\"\"\nts_group = importlib.import_module(\".ts_group\", \"pynapple.core\")\nt = self.index.values\nd = self.values.astype(\"int\")\nidx = np.unique(d)\ngroup = {}\nfor k in idx:\ngroup[k] = Ts(t=t[d == k], time_support=self.time_support)\nreturn ts_group.TsGroup(group, time_support=self.time_support)\ndef save(self, filename):\n\"\"\"\n        Save Tsd object in npz format. The file will contain the timestamps, the\n        data and the time support.\n        The main purpose of this function is to save small/medium sized time series\n        objects. For example, you extracted one channel from your recording and\n        filtered it. You can save the filtered channel as a npz to avoid\n        reprocessing it.\n        You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n        &gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'd', 'start', 'end', 'type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1.]\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\n        Time (s)\n        0.0    2\n        1.0    3\n        dtype: int64\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([self.nap_class], dtype=np.str_),\n)\nreturn\ndef interpolate(self, ts, ep=None, left=None, right=None):\n\"\"\"Wrapper of the numpy linear interpolation method. See https://numpy.org/doc/stable/reference/generated/numpy.interp.html for an explanation of the parameters.\n        The argument ts should be Ts, Tsd, TsdFrame, TsdTensor to ensure interpolating from sorted timestamps in the right unit,\n        Parameters\n        ----------\n        ts : Ts, Tsd or TsdFrame\n            The object holding the timestamps\n        ep : IntervalSet, optional\n            The epochs to use to interpolate. If None, the time support of Tsd is used.\n        left : None, optional\n            Value to return for ts &lt; tsd[0], default is tsd[0].\n        right : None, optional\n            Value to return for ts &gt; tsd[-1], default is tsd[-1].\n        \"\"\"\nif not isinstance(ts, (Ts, Tsd, TsdFrame)):\nraise RuntimeError(\n\"First argument should be an instance of Ts, Tsd or TsdFrame\"\n)\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nnew_t = ts.restrict(ep).index\nnew_d = np.empty(len(new_t))\nnew_d.fill(np.nan)\nstart = 0\nfor i in range(len(ep)):\nt = ts.restrict(ep.loc[[i]])\ntmp = self.restrict(ep.loc[[i]])\nif len(t) and len(tmp):\nnew_d[start : start + len(t)] = np.interp(\nt.index.values, tmp.index.values, tmp.values, left=left, right=right\n)\nstart += len(t)\nreturn Tsd(t=new_t, d=new_d, time_support=ep)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.times","title":"times","text":"<pre><code>times(units='s')\n</code></pre> <p>The time index of the object, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>the time indexes</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the object, returned as np.double in the desired time units.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.ndarray\n        the time indexes\n    \"\"\"\nreturn self.index.in_units(units)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.as_array","title":"as_array","text":"<pre><code>as_array()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_array(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.data","title":"data","text":"<pre><code>data()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy()\n</code></pre> <p>Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling <code>plot(tsd)</code></p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def to_numpy(self):\n\"\"\"\n    Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling `plot(tsd)`\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.start_time","title":"start_time","text":"<pre><code>start_time(units='s')\n</code></pre> <p>The first time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def start_time(self, units=\"s\"):\n\"\"\"\n    The first time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[0]\nelse:\nreturn None\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.end_time","title":"end_time","text":"<pre><code>end_time(units='s')\n</code></pre> <p>The last time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def end_time(self, units=\"s\"):\n\"\"\"\n    The last time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[-1]\nelse:\nreturn None\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.value_from","title":"value_from","text":"<pre><code>value_from(data, ep=None)\n</code></pre> <p>Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>The object holding the values to replace.</p> required <code>ep</code> <code>IntervalSet(optional)</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>Object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, data, ep=None):\n\"\"\"\n    Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument\n    Parameters\n    ----------\n    data : Tsd/TsdFrame/TsdTensor\n        The object holding the values to replace.\n    ep : IntervalSet (optional)\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    out : Tsd/TsdFrame/TsdTensor\n        Object with the new values\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n    newts is the same size as ts restrict to ep.\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\nif not isinstance(data, (TsdTensor, TsdFrame, Tsd)):\nraise RuntimeError(\n\"The time series to align to should be Tsd/TsdFrame/TsdTensor.\"\n)\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nif data_target_array.ndim == 1:\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\nelse:\nt, d, ns, ne = jitvaluefromtensor(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nif isinstance(data, TsdFrame):\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nreturn data.__class__(t, d, time_support=time_support)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.count","title":"count","text":"<pre><code>count(*args, **kwargs)\n</code></pre> <p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsd.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsd.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsd.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsd.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; bincount = ts.count(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n&gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n</code></pre> <p>And bincount automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bincount.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  100.0  800.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsd.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsd.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsd.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsd.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; bincount = ts.count(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n    &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n    And bincount automatically inherit ep as time support:\n    &gt;&gt;&gt; bincount.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  100.0  800.0\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\ntime_array = self.index.values\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\nt, d = jitcount(time_array, starts, ends, bin_size)\nelse:\n_, d = jittsrestrict_with_count(time_array, starts, ends)\nt = starts + (ends - starts) / 2\nreturn Tsd(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.restrict","title":"restrict","text":"<pre><code>restrict(iset)\n</code></pre> <p>Restricts a time series object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>(Ts, Tsd, TsdFrame or TsdTensor)</code> <p>Tsd object restricted to ep</p> <p>Examples:</p> <p>The Ts object is restrict to the intervals defined by ep.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n&gt;&gt;&gt; newts = ts.restrict(ep)\n</code></pre> <p>The time support of newts automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newts.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0    0.0  500.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, iset):\n\"\"\"\n    Restricts a time series object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    iset : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    out: Ts, Tsd, TsdFrame or TsdTensor\n        Tsd object restricted to ep\n    Examples\n    --------\n    The Ts object is restrict to the intervals defined by ep.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n    &gt;&gt;&gt; newts = ts.restrict(ep)\n    The time support of newts automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newts.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0    0.0  500.0\n    \"\"\"\nassert isinstance(iset, IntervalSet), \"Argument should be IntervalSet\"\ntime_array = self.index.values\nstarts = iset.start.values\nends = iset.end.values\nif isinstance(self.values, np.ndarray):\ndata_array = self.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=iset, columns=self.columns)\nelse:\nreturn self.__class__(t=t, d=d, time_support=iset)\nelse:\nt = jittsrestrict(time_array, starts, ends)\nreturn Ts(t, time_support=iset)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.bin_average","title":"bin_average","text":"<pre><code>bin_average(bin_size, ep=None, time_units='s')\n</code></pre> <p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>(Tsd, TsdFrame, TsdTensor)</code> <p>A Tsd object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsd automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsd.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd, TsdFrame, TsdTensor\n        A Tsd object indexed by the center of the bins and holding the averaged data points.\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n    And bintsd automatically inherit ep as time support:\n    &gt;&gt;&gt; bintsd.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nif data_array.ndim &gt; 1:\nt, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\nelse:\nt, d = jitbin(time_array, data_array, starts, ends, bin_size)\nif d.ndim == 1:\nreturn Tsd(t=t, d=d, time_support=ep)\nelif d.ndim == 2:\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=ep, columns=self.columns)\nelse:\nreturn TsdFrame(t=t, d=d, time_support=ep)\nelse:\nreturn TsdTensor(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy the data, index and time support</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def copy(self):\n\"\"\"Copy the data, index and time support\"\"\"\nreturn self.__class__(\nt=self.index.copy(), d=self.values.copy(), time_support=self.time_support\n)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.__init__","title":"__init__","text":"<pre><code>__init__(t, d=None, time_units='s', time_support=None)\n</code></pre> <p>Tsd Initializer.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray or Series</code> <p>An object transformable in a time series, or a pandas.Series equivalent (if d is None)</p> required <code>d</code> <code>ndarray</code> <p>The data of the time series</p> <code>None</code> <code>time_units</code> <code>str</code> <p>The time units in which times are specified ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the tsd object</p> <code>None</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, d=None, time_units=\"s\", time_support=None):\n\"\"\"\n    Tsd Initializer.\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.Series\n        An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n    d : numpy.ndarray, optional\n        The data of the time series\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default])\n    time_support : IntervalSet, optional\n        The time support of the tsd object\n    \"\"\"\nif isinstance(t, np.ndarray) and d is None:\nraise RuntimeError(\"Missing argument d when initializing Tsd\")\nif isinstance(t, pd.Series):\nd = t.values\nt = t.index.values\nif isinstance(t, (list, tuple)):\nt = np.array(t)\nif isinstance(d, (list, tuple)):\nd = np.array(d)\nassert d.ndim == 1, \"Data should be 1 dimension\"\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index) != len(d):\nraise ValueError(\n\"Length of values \"\nf\"({len(d)}) \"\n\"does not match length of index \"\nf\"({len(self.index)})\"\n)\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt, d = jitrestrict(self.index.values, d, starts, ends)\nself.index = TsIndex(t)\nself.values = d\nelse:\ntime_support = IntervalSet(start=self.index[0], end=self.index[-1])\nself.values = d\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.values = np.empty(0)\nself.time_support = IntervalSet(start=[], end=[])\nself.nap_class = self.__class__.__name__\nself.dtype = self.values.dtype\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.as_series","title":"as_series","text":"<pre><code>as_series()\n</code></pre> <p>Convert the Ts/Tsd object to a pandas.Series object.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>Series</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_series(self):\n\"\"\"\n    Convert the Ts/Tsd object to a pandas.Series object.\n    Returns\n    -------\n    out: pandas.Series\n        _\n    \"\"\"\nreturn pd.Series(\nindex=self.index.values, data=self.values, copy=True, dtype=\"float64\"\n)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.as_units","title":"as_units","text":"<pre><code>as_units(units='s')\n</code></pre> <p>Returns a pandas Series with time expressed in the desired unit.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Type Description <code>Series</code> <p>the series object with adjusted times</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    Returns a pandas Series with time expressed in the desired unit.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    pandas.Series\n        the series object with adjusted times\n    \"\"\"\nss = self.as_series()\nt = self.index.in_units(units)\nif units == \"us\":\nt = t.astype(np.int64)\nss.index = t\nss.index.name = \"Time (\" + str(units) + \")\"\nreturn ss\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.threshold","title":"threshold","text":"<pre><code>threshold(thr, method='above')\n</code></pre> <p>Apply a threshold function to the tsd to return a new tsd with the time support being the epochs above/below/&gt;=/&lt;= the threshold</p> <p>Parameters:</p> Name Type Description Default <code>thr</code> <code>float</code> <p>The threshold value</p> required <code>method</code> <code>str</code> <p>The threshold method (above/below/aboveequal/belowequal)</p> <code>'above'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>All the time points below/ above/greater than equal to/less than equal to the threshold</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raise an error if method is not 'below' or 'above'</p> <code>RuntimeError</code> <p>Raise an error if thr is too high/low and no epochs is found.</p> <p>Examples:</p> <p>This example finds all epoch above 0.5 within the tsd object.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n</code></pre> <p>The epochs with the times above/below the threshold can be accessed through the time support:</p> <pre><code>&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n&gt;&gt;&gt; tsd.threshold(50).time_support\n&gt;&gt;&gt;    start   end\n&gt;&gt;&gt; 0   50.5  99.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def threshold(self, thr, method=\"above\"):\n\"\"\"\n    Apply a threshold function to the tsd to return a new tsd\n    with the time support being the epochs above/below/&gt;=/&lt;= the threshold\n    Parameters\n    ----------\n    thr : float\n        The threshold value\n    method : str, optional\n        The threshold method (above/below/aboveequal/belowequal)\n    Returns\n    -------\n    out: Tsd\n        All the time points below/ above/greater than equal to/less than equal to the threshold\n    Raises\n    ------\n    ValueError\n        Raise an error if method is not 'below' or 'above'\n    RuntimeError\n        Raise an error if thr is too high/low and no epochs is found.\n    Examples\n    --------\n    This example finds all epoch above 0.5 within the tsd object.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n    The epochs with the times above/below the threshold can be accessed through the time support:\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n    &gt;&gt;&gt; tsd.threshold(50).time_support\n    &gt;&gt;&gt;    start   end\n    &gt;&gt;&gt; 0   50.5  99.0\n    \"\"\"\ntime_array = self.index.values\ndata_array = self.values\nstarts = self.time_support.start.values\nends = self.time_support.end.values\nif method not in [\"above\", \"below\", \"aboveequal\", \"belowequal\"]:\nraise ValueError(\n\"Method {} for thresholding is not accepted.\".format(method)\n)\nt, d, ns, ne = jitthreshold(time_array, data_array, starts, ends, thr, method)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.to_tsgroup","title":"to_tsgroup","text":"<pre><code>to_tsgroup()\n</code></pre> <p>Convert Tsd to a TsGroup by grouping timestamps with the same values. By default, the values are converted to integers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\nTime (s)\n0.0    0\n1.0    2\n2.0    0\n3.0    1\ndtype: int64\n</code></pre> <pre><code>&gt;&gt;&gt; tsd.to_tsgroup()\nIndex    rate\n-------  ------\n    0    0.67\n    1    0.33\n    2    0.33\n</code></pre> <p>The reverse operation can be done with the TsGroup.to_tsd function :</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd()\nTime (s)\n0.0    0.0\n1.0    2.0\n2.0    0.0\n3.0    1.0\ndtype: float64\n</code></pre> <p>Returns:</p> Type Description <code>TsGroup</code> <p>Grouped timestamps</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def to_tsgroup(self):\n\"\"\"\n    Convert Tsd to a TsGroup by grouping timestamps with the same values.\n    By default, the values are converted to integers.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\n    Time (s)\n    0.0    0\n    1.0    2\n    2.0    0\n    3.0    1\n    dtype: int64\n    &gt;&gt;&gt; tsd.to_tsgroup()\n    Index    rate\n    -------  ------\n        0    0.67\n        1    0.33\n        2    0.33\n    The reverse operation can be done with the TsGroup.to_tsd function :\n    &gt;&gt;&gt; tsgroup.to_tsd()\n    Time (s)\n    0.0    0.0\n    1.0    2.0\n    2.0    0.0\n    3.0    1.0\n    dtype: float64\n    Returns\n    -------\n    TsGroup\n        Grouped timestamps\n    \"\"\"\nts_group = importlib.import_module(\".ts_group\", \"pynapple.core\")\nt = self.index.values\nd = self.values.astype(\"int\")\nidx = np.unique(d)\ngroup = {}\nfor k in idx:\ngroup[k] = Ts(t=t[d == k], time_support=self.time_support)\nreturn ts_group.TsGroup(group, time_support=self.time_support)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.save","title":"save","text":"<pre><code>save(filename)\n</code></pre> <p>Save Tsd object in npz format. The file will contain the timestamps, the data and the time support.</p> <p>The main purpose of this function is to save small/medium sized time series objects. For example, you extracted one channel from your recording and filtered it. You can save the filtered channel as a npz to avoid reprocessing it.</p> <p>You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n&gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'd', 'start', 'end', 'type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1.]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\nTime (s)\n0.0    2\n1.0    3\ndtype: int64\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save Tsd object in npz format. The file will contain the timestamps, the\n    data and the time support.\n    The main purpose of this function is to save small/medium sized time series\n    objects. For example, you extracted one channel from your recording and\n    filtered it. You can save the filtered channel as a npz to avoid\n    reprocessing it.\n    You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n    &gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'd', 'start', 'end', 'type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1.]\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\n    Time (s)\n    0.0    2\n    1.0    3\n    dtype: int64\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([self.nap_class], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.interpolate","title":"interpolate","text":"<pre><code>interpolate(ts, ep=None, left=None, right=None)\n</code></pre> <p>Wrapper of the numpy linear interpolation method. See https://numpy.org/doc/stable/reference/generated/numpy.interp.html for an explanation of the parameters. The argument ts should be Ts, Tsd, TsdFrame, TsdTensor to ensure interpolating from sorted timestamps in the right unit,</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>(Ts, Tsd or TsdFrame)</code> <p>The object holding the timestamps</p> required <code>ep</code> <code>IntervalSet</code> <p>The epochs to use to interpolate. If None, the time support of Tsd is used.</p> <code>None</code> <code>left</code> <code>None</code> <p>Value to return for ts &lt; tsd[0], default is tsd[0].</p> <code>None</code> <code>right</code> <code>None</code> <p>Value to return for ts &gt; tsd[-1], default is tsd[-1].</p> <code>None</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def interpolate(self, ts, ep=None, left=None, right=None):\n\"\"\"Wrapper of the numpy linear interpolation method. See https://numpy.org/doc/stable/reference/generated/numpy.interp.html for an explanation of the parameters.\n    The argument ts should be Ts, Tsd, TsdFrame, TsdTensor to ensure interpolating from sorted timestamps in the right unit,\n    Parameters\n    ----------\n    ts : Ts, Tsd or TsdFrame\n        The object holding the timestamps\n    ep : IntervalSet, optional\n        The epochs to use to interpolate. If None, the time support of Tsd is used.\n    left : None, optional\n        Value to return for ts &lt; tsd[0], default is tsd[0].\n    right : None, optional\n        Value to return for ts &gt; tsd[-1], default is tsd[-1].\n    \"\"\"\nif not isinstance(ts, (Ts, Tsd, TsdFrame)):\nraise RuntimeError(\n\"First argument should be an instance of Ts, Tsd or TsdFrame\"\n)\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nnew_t = ts.restrict(ep).index\nnew_d = np.empty(len(new_t))\nnew_d.fill(np.nan)\nstart = 0\nfor i in range(len(ep)):\nt = ts.restrict(ep.loc[[i]])\ntmp = self.restrict(ep.loc[[i]])\nif len(t) and len(tmp):\nnew_d[start : start + len(t)] = np.interp(\nt.index.values, tmp.index.values, tmp.values, left=left, right=right\n)\nstart += len(t)\nreturn Tsd(t=new_t, d=new_d, time_support=ep)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts","title":"Ts","text":"<p>             Bases: <code>_AbstractTsd</code></p> <p>Timestamps only object for a time series with only time index,</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class Ts(_AbstractTsd):\n\"\"\"\n    Timestamps only object for a time series with only time index,\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, time_units=\"s\", time_support=None):\n\"\"\"\n        Ts Initializer\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.Series\n            An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default])\n        time_support : IntervalSet, optional\n            The time support of the Ts object\n        \"\"\"\nif isinstance(t, Number):\nt = np.array([t])\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt = jittsrestrict(self.index.values, starts, ends)\nself.index = TsIndex(t)\nelse:\ntime_support = IntervalSet(start=t[0], end=t[-1])\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.time_support = IntervalSet(start=[], end=[])\nself.values = None\nself.nap_class = self.__class__.__name__\ndef __repr__(self):\nupper = \"Time (s)\"\nif len(self) &lt; 100:\n_str_ = \"\\n\".join([i.__repr__() for i in self.index])\nelse:\n_str_ = \"\\n\".join(\n[i.__repr__() for i in self.index[0:5]]\n+ [\"...\"]\n+ [i.__repr__() for i in self.index[-5:]]\n)\nbottom = \"shape: {}\".format(len(self.index))\nreturn \"\\n\".join((upper, _str_, bottom))\ndef __getitem__(self, key):\ntry:\nif isinstance(key, tuple):\nindex = self.index.__getitem__(key[0])\nelse:\nindex = self.index.__getitem__(key)\nif isinstance(index, Number):\nindex = np.array([index])\nif isinstance(index, np.ndarray):\nreturn Ts(t=index, time_support=self.time_support)\nelse:\nreturn None\nexcept RuntimeError:\nraise IndexError\ndef __setitem__(self, key, value):\npass\ndef as_series(self):\n\"\"\"\n        Convert the Ts/Tsd object to a pandas.Series object.\n        Returns\n        -------\n        out: pandas.Series\n            _\n        \"\"\"\nreturn pd.Series(index=self.index.values, dtype=\"object\")\ndef as_units(self, units=\"s\"):\n\"\"\"\n        Returns a pandas Series with time expressed in the desired unit.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        pandas.Series\n            the series object with adjusted times\n        \"\"\"\nt = self.index.in_units(units)\nif units == \"us\":\nt = t.astype(np.int64)\nss = pd.Series(index=t, dtype=\"object\")\nss.index.name = \"Time (\" + str(units) + \")\"\nreturn ss\ndef fillna(self, value):\n\"\"\"\n        Similar to pandas fillna function.\n        Parameters\n        ----------\n        value : Number\n            Value for filling\n        Returns\n        -------\n        Tsd\n        \"\"\"\nassert isinstance(value, Number), \"Only a scalar can be passed to fillna\"\nd = np.empty(len(self))\nd.fill(value)\nreturn Tsd(t=self.index, d=d, time_support=self.time_support)\ndef save(self, filename):\n\"\"\"\n        Save Ts object in npz format. The file will contain the timestamps and\n        the time support.\n        The main purpose of this function is to save small/medium sized timestamps\n        object.\n        You can load the object with numpy.load. Keys are 't', 'start' and 'end' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n        &gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'start', 'end', 'type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1. 1.5]\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\n        Time (s)\n        0.0\n        1.0\n        1.5\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([\"Ts\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.times","title":"times","text":"<pre><code>times(units='s')\n</code></pre> <p>The time index of the object, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>the time indexes</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the object, returned as np.double in the desired time units.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.ndarray\n        the time indexes\n    \"\"\"\nreturn self.index.in_units(units)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.as_array","title":"as_array","text":"<pre><code>as_array()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_array(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.data","title":"data","text":"<pre><code>data()\n</code></pre> <p>Return the data as a numpy.ndarray</p> <p>Returns:</p> Name Type Description <code>out</code> <code>ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    Return the data as a numpy.ndarray\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy()\n</code></pre> <p>Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling <code>plot(tsd)</code></p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def to_numpy(self):\n\"\"\"\n    Return the data as a numpy.ndarray. Mostly useful for matplotlib plotting when calling `plot(tsd)`\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.start_time","title":"start_time","text":"<pre><code>start_time(units='s')\n</code></pre> <p>The first time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def start_time(self, units=\"s\"):\n\"\"\"\n    The first time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[0]\nelse:\nreturn None\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.end_time","title":"end_time","text":"<pre><code>end_time(units='s')\n</code></pre> <p>The last time index in the time series object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def end_time(self, units=\"s\"):\n\"\"\"\n    The last time index in the time series object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nif len(self.index):\nreturn self.times(units=units)[-1]\nelse:\nreturn None\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.value_from","title":"value_from","text":"<pre><code>value_from(data, ep=None)\n</code></pre> <p>Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>The object holding the values to replace.</p> required <code>ep</code> <code>IntervalSet(optional)</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd / TsdFrame / TsdTensor</code> <p>Object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, data, ep=None):\n\"\"\"\n    Replace the value with the closest value from Tsd/TsdFrame/TsdTensor argument\n    Parameters\n    ----------\n    data : Tsd/TsdFrame/TsdTensor\n        The object holding the values to replace.\n    ep : IntervalSet (optional)\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    out : Tsd/TsdFrame/TsdTensor\n        Object with the new values\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n    newts is the same size as ts restrict to ep.\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\nif not isinstance(data, (TsdTensor, TsdFrame, Tsd)):\nraise RuntimeError(\n\"The time series to align to should be Tsd/TsdFrame/TsdTensor.\"\n)\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nif data_target_array.ndim == 1:\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\nelse:\nt, d, ns, ne = jitvaluefromtensor(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nif isinstance(data, TsdFrame):\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nreturn data.__class__(t, d, time_support=time_support)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.count","title":"count","text":"<pre><code>count(*args, **kwargs)\n</code></pre> <p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsd.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsd.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsd.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsd.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; bincount = ts.count(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n&gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n</code></pre> <p>And bincount automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bincount.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  100.0  800.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsd.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsd.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsd.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsd.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; bincount = ts.count(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n    &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n    And bincount automatically inherit ep as time support:\n    &gt;&gt;&gt; bincount.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  100.0  800.0\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\ntime_array = self.index.values\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\nt, d = jitcount(time_array, starts, ends, bin_size)\nelse:\n_, d = jittsrestrict_with_count(time_array, starts, ends)\nt = starts + (ends - starts) / 2\nreturn Tsd(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.restrict","title":"restrict","text":"<pre><code>restrict(iset)\n</code></pre> <p>Restricts a time series object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>(Ts, Tsd, TsdFrame or TsdTensor)</code> <p>Tsd object restricted to ep</p> <p>Examples:</p> <p>The Ts object is restrict to the intervals defined by ep.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n&gt;&gt;&gt; newts = ts.restrict(ep)\n</code></pre> <p>The time support of newts automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newts.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0    0.0  500.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, iset):\n\"\"\"\n    Restricts a time series object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    iset : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    out: Ts, Tsd, TsdFrame or TsdTensor\n        Tsd object restricted to ep\n    Examples\n    --------\n    The Ts object is restrict to the intervals defined by ep.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n    &gt;&gt;&gt; newts = ts.restrict(ep)\n    The time support of newts automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newts.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0    0.0  500.0\n    \"\"\"\nassert isinstance(iset, IntervalSet), \"Argument should be IntervalSet\"\ntime_array = self.index.values\nstarts = iset.start.values\nends = iset.end.values\nif isinstance(self.values, np.ndarray):\ndata_array = self.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=iset, columns=self.columns)\nelse:\nreturn self.__class__(t=t, d=d, time_support=iset)\nelse:\nt = jittsrestrict(time_array, starts, ends)\nreturn Ts(t, time_support=iset)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.bin_average","title":"bin_average","text":"<pre><code>bin_average(bin_size, ep=None, time_units='s')\n</code></pre> <p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>(Tsd, TsdFrame, TsdTensor)</code> <p>A Tsd object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsd automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsd.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd, TsdFrame, TsdTensor\n        A Tsd object indexed by the center of the bins and holding the averaged data points.\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n    And bintsd automatically inherit ep as time support:\n    &gt;&gt;&gt; bintsd.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nif data_array.ndim &gt; 1:\nt, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\nelse:\nt, d = jitbin(time_array, data_array, starts, ends, bin_size)\nif d.ndim == 1:\nreturn Tsd(t=t, d=d, time_support=ep)\nelif d.ndim == 2:\nif hasattr(self, \"columns\"):\nreturn TsdFrame(t=t, d=d, time_support=ep, columns=self.columns)\nelse:\nreturn TsdFrame(t=t, d=d, time_support=ep)\nelse:\nreturn TsdTensor(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.copy","title":"copy","text":"<pre><code>copy()\n</code></pre> <p>Copy the data, index and time support</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def copy(self):\n\"\"\"Copy the data, index and time support\"\"\"\nreturn self.__class__(\nt=self.index.copy(), d=self.values.copy(), time_support=self.time_support\n)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.__init__","title":"__init__","text":"<pre><code>__init__(t, time_units='s', time_support=None)\n</code></pre> <p>Ts Initializer</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>ndarray or Series</code> <p>An object transformable in a time series, or a pandas.Series equivalent (if d is None)</p> required <code>time_units</code> <code>str</code> <p>The time units in which times are specified ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the Ts object</p> <code>None</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, time_units=\"s\", time_support=None):\n\"\"\"\n    Ts Initializer\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.Series\n        An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default])\n    time_support : IntervalSet, optional\n        The time support of the Ts object\n    \"\"\"\nif isinstance(t, Number):\nt = np.array([t])\nif isinstance(t, TsIndex):\nself.index = t\nelse:\n# Checking timestamps\nself.index = TsIndex(t, time_units)\nif len(self.index):\nif isinstance(time_support, IntervalSet):\nstarts = time_support.start.values\nends = time_support.end.values\nt = jittsrestrict(self.index.values, starts, ends)\nself.index = TsIndex(t)\nelse:\ntime_support = IntervalSet(start=t[0], end=t[-1])\nself.time_support = time_support\nself.rate = self.index.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\nself.rate = np.NaN\nself.time_support = IntervalSet(start=[], end=[])\nself.values = None\nself.nap_class = self.__class__.__name__\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.as_series","title":"as_series","text":"<pre><code>as_series()\n</code></pre> <p>Convert the Ts/Tsd object to a pandas.Series object.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>Series</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_series(self):\n\"\"\"\n    Convert the Ts/Tsd object to a pandas.Series object.\n    Returns\n    -------\n    out: pandas.Series\n        _\n    \"\"\"\nreturn pd.Series(index=self.index.values, dtype=\"object\")\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.as_units","title":"as_units","text":"<pre><code>as_units(units='s')\n</code></pre> <p>Returns a pandas Series with time expressed in the desired unit.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Type Description <code>Series</code> <p>the series object with adjusted times</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    Returns a pandas Series with time expressed in the desired unit.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    pandas.Series\n        the series object with adjusted times\n    \"\"\"\nt = self.index.in_units(units)\nif units == \"us\":\nt = t.astype(np.int64)\nss = pd.Series(index=t, dtype=\"object\")\nss.index.name = \"Time (\" + str(units) + \")\"\nreturn ss\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.fillna","title":"fillna","text":"<pre><code>fillna(value)\n</code></pre> <p>Similar to pandas fillna function.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Number</code> <p>Value for filling</p> required <p>Returns:</p> Type Description <code>Tsd</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def fillna(self, value):\n\"\"\"\n    Similar to pandas fillna function.\n    Parameters\n    ----------\n    value : Number\n        Value for filling\n    Returns\n    -------\n    Tsd\n    \"\"\"\nassert isinstance(value, Number), \"Only a scalar can be passed to fillna\"\nd = np.empty(len(self))\nd.fill(value)\nreturn Tsd(t=self.index, d=d, time_support=self.time_support)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.save","title":"save","text":"<pre><code>save(filename)\n</code></pre> <p>Save Ts object in npz format. The file will contain the timestamps and the time support.</p> <p>The main purpose of this function is to save small/medium sized timestamps object.</p> <p>You can load the object with numpy.load. Keys are 't', 'start' and 'end' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n&gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'start', 'end', 'type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1. 1.5]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\nTime (s)\n0.0\n1.0\n1.5\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save Ts object in npz format. The file will contain the timestamps and\n    the time support.\n    The main purpose of this function is to save small/medium sized timestamps\n    object.\n    You can load the object with numpy.load. Keys are 't', 'start' and 'end' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n    &gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'start', 'end', 'type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1. 1.5]\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\n    Time (s)\n    0.0\n    1.0\n    1.5\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([\"Ts\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"reference/core/time_units/","title":"Time units","text":""},{"location":"reference/core/time_units/#pynapple.core.time_units","title":"pynapple.core.time_units","text":"<p>DEPRECATED This class deals with conversion between different time units for all pynapple objects. It also provides a context manager that tweaks the default time units to the supported units:</p> <ul> <li>'us': microseconds</li> <li>'ms': milliseconds</li> <li>'s': seconds  (overall default)</li> </ul>"},{"location":"reference/core/time_units/#pynapple.core.time_units.format_timestamps","title":"format_timestamps","text":"<pre><code>format_timestamps(t, units='s')\n</code></pre> <p>Converts time index in pynapple in a default format</p> <p>Args:     t: a vector (or scalar) of times     units: the units in which times are given</p> <p>Returns:     t: times in standard pynapple format</p> Source code in <code>pynapple/core/time_units.py</code> <pre><code>def format_timestamps(t, units=\"s\"):\n\"\"\"\n    Converts time index in pynapple in a default format\n    Args:\n        t: a vector (or scalar) of times\n        units: the units in which times are given\n    Returns:\n        t: times in standard pynapple format\n    \"\"\"\nif units == \"s\":\nt = np.around(t, 9)\nelif units == \"ms\":\nt = np.around(t / 1.0e3, 9)\nelif units == \"us\":\nt = np.around(t / 1.0e6, 9)\nelse:\nraise ValueError(\"unrecognized time units type\")\nreturn t\n</code></pre>"},{"location":"reference/core/time_units/#pynapple.core.time_units.return_timestamps","title":"return_timestamps","text":"<pre><code>return_timestamps(t, units='s')\n</code></pre> <p>Converts time index in pynapple in a particular format</p> <p>Args:     t: a vector (or scalar) of times     units: the units in which times are given</p> <p>Returns:     t: times in standard pynapple format</p> Source code in <code>pynapple/core/time_units.py</code> <pre><code>def return_timestamps(t, units=\"s\"):\n\"\"\"\n    Converts time index in pynapple in a particular format\n    Args:\n        t: a vector (or scalar) of times\n        units: the units in which times are given\n    Returns:\n        t: times in standard pynapple format\n    \"\"\"\nif units == \"s\":\nt = np.around(t, 9)\nelif units == \"ms\":\nt = np.around(t * 1.0e3, 9)\nelif units == \"us\":\nt = np.around(t * 1.0e6, 9)\nelse:\nraise ValueError(\"unrecognized time units type\")\nreturn t\n</code></pre>"},{"location":"reference/core/ts_group/","title":"Ts group","text":""},{"location":"reference/core/ts_group/#pynapple.core.ts_group","title":"pynapple.core.ts_group","text":""},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup","title":"TsGroup","text":"<p>             Bases: <code>UserDict</code></p> <p>The TsGroup is a dictionnary-like object to hold multiple <code>Ts</code> or <code>Tsd</code> objects with different time index.</p> <p>Attributes:</p> Name Type Description <code>time_support</code> <code>IntervalSet</code> <p>The time support of the TsGroup</p> <code>rates</code> <code>Series</code> <p>The rate of each element of the TsGroup</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>class TsGroup(UserDict):\n\"\"\"\n    The TsGroup is a dictionnary-like object to hold multiple [`Ts`][pynapple.core.time_series.Ts] or [`Tsd`][pynapple.core.time_series.Tsd] objects with different time index.\n    Attributes\n    ----------\n    time_support: IntervalSet\n        The time support of the TsGroup\n    rates : pandas.Series\n        The rate of each element of the TsGroup\n    \"\"\"\ndef __init__(\nself, data, time_support=None, time_units=\"s\", bypass_check=False, **kwargs\n):\n\"\"\"\n        TsGroup Initializer\n        Parameters\n        ----------\n        data : dict\n            Dictionnary containing Ts/Tsd objects\n        time_support : IntervalSet, optional\n            The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed.\n            If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.\n        time_units : str, optional\n            Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).\n        bypass_check: bool, optional\n            To avoid checking that each element is within time_support.\n            Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand\n        **kwargs\n            Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray.\n            Note that the index should match the index of the input dictionnary.\n        Raises\n        ------\n        RuntimeError\n            Raise error if the union of time support of Ts/Tsd object is empty.\n        \"\"\"\nself._initialized = False\nself.index = np.sort(list(data.keys()))\nself._metadata = pd.DataFrame(index=self.index, columns=[\"rate\"], dtype=\"float\")\n# Transform elements to Ts/Tsd objects\nfor k in self.index:\nif isinstance(data[k], (np.ndarray, list)):\nwarnings.warn(\n\"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\nstacklevel=2,\n)\ndata[k] = Ts(\nt=data[k], time_support=time_support, time_units=time_units\n)\n# If time_support is passed, all elements of data are restricted prior to init\nif isinstance(time_support, IntervalSet):\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nelse:\n# Otherwise do the union of all time supports\ntime_support = union_intervals([data[k].time_support for k in self.index])\nif len(time_support) == 0:\nraise RuntimeError(\n\"Union of time supports is empty. Consider passing a time support as argument.\"\n)\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nUserDict.__init__(self, data)\n# Making the TsGroup non mutable\nself._initialized = True\n# Trying to add argument as metainfo\nself.set_info(**kwargs)\n\"\"\"\n    Base functions\n    \"\"\"\ndef __setitem__(self, key, value):\nif self._initialized:\nraise RuntimeError(\"TsGroup object is not mutable.\")\nself._metadata.loc[int(key), \"rate\"] = float(value.rate)\nsuper().__setitem__(int(key), value)\n# if self.__contains__(key):\n#     raise KeyError(\"Key {} already in group index.\".format(key))\n# else:\n# if isinstance(value, (Ts, Tsd)):\n#     self._metadata.loc[int(key), \"rate\"] = value.rate\n#     super().__setitem__(int(key), value)\n# elif isinstance(value, (np.ndarray, list)):\n#     warnings.warn(\n#         \"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\n#         stacklevel=2,\n#     )\n#     tmp = Ts(t=value, time_units=\"s\")\n#     self._metadata.loc[int(key), \"rate\"] = tmp.rate\n#     super().__setitem__(int(key), tmp)\n# else:\n#     raise ValueError(\"Value with key {} is not an iterable.\".format(key))\ndef __getitem__(self, key):\nif key.__hash__:\nif self.__contains__(key):\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\nelse:\nmetadata = self._metadata.loc[key, self._metadata.columns.drop(\"rate\")]\nreturn TsGroup(\n{k: self[k] for k in key}, time_support=self.time_support, **metadata\n)\ndef __repr__(self):\ncols = self._metadata.columns.drop(\"rate\")\nheaders = [\"Index\", \"rate\"] + [c for c in cols]\nlines = []\nfor i in self.data.keys():\nlines.append(\n[str(i), \"%.2f\" % self._metadata.loc[i, \"rate\"]]\n+ [self._metadata.loc[i, c] for c in cols]\n)\nreturn tabulate(lines, headers=headers)\ndef __str__(self):\nreturn self.__repr__()\ndef keys(self):\n\"\"\"\n        Return index/keys of TsGroup\n        Returns\n        -------\n        list\n            List of keys\n        \"\"\"\nreturn list(self.data.keys())\ndef items(self):\n\"\"\"\n        Return a list of key/object.\n        Returns\n        -------\n        list\n            List of tuples\n        \"\"\"\nreturn list(self.data.items())\ndef values(self):\n\"\"\"\n        Return a list of all the Ts/Tsd objects in the TsGroup\n        Returns\n        -------\n        list\n            List of Ts/Tsd objects\n        \"\"\"\nreturn list(self.data.values())\n@property\ndef rates(self):\n\"\"\"\n        Return the rates of each element of the group in Hz\n        \"\"\"\nreturn self._metadata[\"rate\"]\n#######################\n# Metadata\n#######################\n@property\ndef metadata_columns(self):\n\"\"\"\n        Returns list of metadata columns\n        -------\n        \"\"\"\nreturn list(self._metadata.columns)\ndef set_info(self, *args, **kwargs):\n\"\"\"\n        Add metadata informations about the TsGroup.\n        Metadata are saved as a DataFrame.\n        Parameters\n        ----------\n        *args\n            pandas.Dataframe or list of pandas.DataFrame\n        **kwargs\n            Can be either pandas.Series or numpy.ndarray\n        Raises\n        ------\n        RuntimeError\n            Raise an error if\n                no column labels are found when passing simple arguments,\n                indexes are not equals for a pandas series,\n                not the same length when passing numpy array.\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        To add metadata with a pandas.DataFrame:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n        &gt;&gt;&gt; tsgroup.set_info(structs)\n        &gt;&gt;&gt; tsgroup\n          Index    Freq. (Hz)  struct\n        -------  ------------  --------\n              0             1  pfc\n              1             2  pfc\n              2             4  ca1\n        To add metadata with a pd.Series or numpy.ndarray:\n        &gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n        &gt;&gt;&gt; tsgroup.set_info(hd=hd)\n        &gt;&gt;&gt; tsgroup\n          Index    Freq. (Hz)  struct      hd\n        -------  ------------  --------  ----\n              0             1  pfc          0\n              1             2  pfc          1\n              2             4  ca1          1\n        \"\"\"\nif len(args):\nfor arg in args:\nif isinstance(arg, pd.DataFrame):\nif pd.Index.equals(self._metadata.index, arg.index):\nself._metadata = self._metadata.join(arg)\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(arg, (pd.Series, np.ndarray)):\nraise RuntimeError(\"Columns needs to be labelled for metadata\")\nif len(kwargs):\nfor k, v in kwargs.items():\nif isinstance(v, pd.Series):\nif pd.Index.equals(self._metadata.index, v.index):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(v, np.ndarray):\nif len(self._metadata) == len(v):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Array is not the same length.\")\nreturn\ndef get_info(self, key):\n\"\"\"\n        Returns the metainfo located in one column.\n        The key for the column frequency is \"rate\".\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        Returns\n        -------\n        pandas.Series\n            The metainfo\n        \"\"\"\nif key in [\"freq\", \"frequency\"]:\nkey = \"rate\"\nreturn self._metadata[key]\n#################################\n# Generic functions of Tsd objects\n#################################\ndef restrict(self, ep):\n\"\"\"\n        Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object\n        Parameters\n        ----------\n        ep : IntervalSet\n            the IntervalSet object\n        Returns\n        -------\n        TsGroup\n            TsGroup object restricted to ep\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        &gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n        All objects within the TsGroup automatically inherit the epochs defined by ep.\n        &gt;&gt;&gt; newtsgroup.time_support\n           start    end\n        0    0.0  100.0\n        &gt;&gt;&gt; newtsgroup[0].time_support\n           start    end\n        0    0.0  100.0\n        \"\"\"\nnewgr = {}\nfor k in self.index:\nnewgr[k] = self.data[k].restrict(ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(\nnewgr, time_support=ep, bypass_check=True, **self._metadata[cols]\n)\ndef value_from(self, tsd, ep=None):\n\"\"\"\n        Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument\n        Parameters\n        ----------\n        tsd : Tsd\n            The Tsd object holding the values to replace\n        ep : IntervalSet\n            The IntervalSet object to restrict the operation.\n            If None, the time support of the tsd input object is used.\n        Returns\n        -------\n        TsGroup\n            TsGroup object with the new values\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        The variable tsd is a time series object containing the values to assign, for example the tracking data:\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n        &gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n        \"\"\"\nif ep is None:\nep = tsd.time_support\nnewgr = {}\nfor k in self.data:\nnewgr[k] = self.data[k].value_from(tsd, ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(newgr, time_support=ep, **self._metadata[cols])\ndef count(self, *args, **kwargs):\n\"\"\"\n        Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n        You can call this function in multiple ways :\n        1. *tsgroup.count(bin_size=1, time_units = 'ms')*\n        -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n        2. *tsgroup.count(1, ep=my_epochs)*\n        -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n        3. *tsgroup.count(ep=my_bins)*\n        -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n        4. *tsgroup.count()*\n        -&gt; Count occurent of events within each epoch of the time support.\n        bin_size should be seconds unless specified.\n        If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n        Parameters\n        ----------\n        bin_size : None or float, optional\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: TsdFrame\n            A TsdFrame with the columns being the index of each item in the TsGroup.\n        Examples\n        --------\n        This example shows how to count events within bins of 0.1 second for the first 100 seconds.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        &gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n        &gt;&gt;&gt; bincount\n                  0  1  2\n        Time (s)\n        0.05      0  0  0\n        0.15      0  0  0\n        0.25      0  0  1\n        0.35      0  0  0\n        0.45      0  0  0\n        ...      .. .. ..\n        99.55     0  1  1\n        99.65     0  0  0\n        99.75     0  0  1\n        99.85     0  0  0\n        99.95     1  1  1\n        [1000 rows x 3 columns]\n        \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = float(bin_size)\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\ntime_index, _ = jitcount(np.array([]), starts, ends, bin_size)\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jitcount(\nself.data[self.index[i]].index, starts, ends, bin_size\n)[1]\nelse:\ntime_index = starts + (ends - starts) / 2\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jittsrestrict_with_count(\nself.data[self.index[i]].index, starts, ends\n)[1]\ntoreturn = TsdFrame(t=time_index, d=count, time_support=ep, columns=self.index)\nreturn toreturn\ndef to_tsd(self, *args):\n\"\"\"\n        Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.\n        Parameters\n        ----------\n        *args\n            string, list, numpy.ndarray or pandas.Series\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\n        Index    rate\n        -------  ------\n        0       1\n        5       1\n        By default, the values of the Tsd is the index of the timestamp in the TsGroup:\n        &gt;&gt;&gt; tsgroup.to_tsd()\n        Time (s)\n        0.0    0.0\n        1.0    0.0\n        2.0    5.0\n        3.0    5.0\n        dtype: float64\n        Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.\n        &gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n        &gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\n        Time (s)\n        0.0    3.141593\n        1.0    3.141593\n        2.0    6.283185\n        3.0    6.283185\n        dtype: float64\n        Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :\n        &gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\n        Time (s)\n        0.0   -1.0\n        1.0   -1.0\n        2.0    1.0\n        3.0    1.0\n        dtype: float64\n        The reverse operation can be done with the Tsd.to_tsgroup function :\n        &gt;&gt;&gt; my_tsd\n        Time (s)\n        0.0    0.0\n        1.0    0.0\n        2.0    5.0\n        3.0    5.0\n        dtype: float64\n        &gt;&gt;&gt; my_tsd.to_tsgroup()\n          Index    rate\n        -------  ------\n              0       1\n              5       1\n        Returns\n        -------\n        Tsd\n        Raises\n        ------\n        RuntimeError\n            \"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes\n            \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object\n            \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata,\n            \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series\n        \"\"\"\nif len(args):\nif isinstance(args[0], pd.Series):\nif pd.Index.equals(self._metadata.index, args[0].index):\n_values = args[0].values.flatten()\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(args[0], (np.ndarray, list)):\nif len(self._metadata) == len(args[0]):\n_values = np.array(args[0])\nelse:\nraise RuntimeError(\"Values is not the same length.\")\nelif isinstance(args[0], str):\nif args[0] in self._metadata.columns:\n_values = self._metadata[args[0]].values\nelse:\nraise RuntimeError(\n\"Key {} not in metadata of TsGroup\".format(args[0])\n)\nelse:\npossible_keys = []\nfor k, d in self._metadata.dtypes.items():\nif \"int\" in str(d) or \"float\" in str(d):\npossible_keys.append(k)\nraise RuntimeError(\n\"Unknown argument format. Must be pandas.Series, numpy.ndarray or a string from one of the following values : [{}]\".format(\n\", \".join(possible_keys)\n)\n)\nelse:\n_values = self.index\nnt = 0\nfor n in self.index:\nnt += len(self[n])\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nk = 0\nfor n, v in zip(self.index, _values):\nkl = len(self[n])\ntimes[k : k + kl] = self[n].index\ndata[k : k + kl] = v\nk += kl\nidx = np.argsort(times)\ntoreturn = Tsd(t=times[idx], d=data[idx], time_support=self.time_support)\nreturn toreturn\n\"\"\"\n    Special slicing of metadata\n    \"\"\"\ndef getby_threshold(self, key, thr, op=\"&gt;\"):\n\"\"\"\n        Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        thr : float\n            THe value for thresholding\n        op : str, optional\n            The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.\n        Returns\n        -------\n        TsGroup\n            The new TsGroup\n        Raises\n        ------\n        RuntimeError\n            Raise eror is operation is not recognized.\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n          Index    Freq. (Hz)\n        -------  ------------\n              0             1\n              1             2\n              2             4\n        This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.\n        &gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n          Index    Freq. (Hz)\n        -------  ------------\n              1             2\n              2             4\n        \"\"\"\nif op == \"&gt;\":\nix = list(self._metadata.index[self._metadata[key] &gt; thr])\nreturn self[ix]\nelif op == \"&lt;\":\nix = list(self._metadata.index[self._metadata[key] &lt; thr])\nreturn self[ix]\nelif op == \"&gt;=\":\nix = list(self._metadata.index[self._metadata[key] &gt;= thr])\nreturn self[ix]\nelif op == \"&lt;=\":\nix = list(self._metadata.index[self._metadata[key] &lt;= thr])\nreturn self[ix]\nelse:\nraise RuntimeError(\"Operation {} not recognized.\".format(op))\ndef getby_intervals(self, key, bins):\n\"\"\"\n        Return a list of TsGroup binned.\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        bins : numpy.ndarray or list\n            The bin intervals\n        Returns\n        -------\n        list\n            A list of TsGroup\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n          Index    Freq. (Hz)    alpha\n        -------  ------------  -------\n              0             1        0\n              1             2        1\n              2             4        2\n        This exemple shows how to bin the TsGroup according to one metainfo key.\n        &gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n        &gt;&gt;&gt; newtsgroup\n        [  Index    Freq. (Hz)    alpha\n         -------  ------------  -------\n               0             1        0,\n           Index    Freq. (Hz)    alpha\n         -------  ------------  -------\n               1             2        1]\n        By default, the function returns the center of the bins.\n        &gt;&gt;&gt; bincenter\n        array([0.5, 1.5])\n        \"\"\"\nidx = np.digitize(self._metadata[key], bins) - 1\ngroups = self._metadata.index.groupby(idx)\nix = np.unique(list(groups.keys()))\nix = ix[ix &gt;= 0]\nix = ix[ix &lt; len(bins) - 1]\nxb = bins[0:-1] + np.diff(bins) / 2\nsliced = [self[list(groups[i])] for i in ix]\nreturn sliced, xb[ix]\ndef getby_category(self, key):\n\"\"\"\n        Return a list of TsGroup grouped by category.\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        Returns\n        -------\n        dict\n            A dictionnary of TsGroup\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n          Index    Freq. (Hz)    group\n        -------  ------------  -------\n              0             1        0\n              1             2        1\n              2             4        1\n        This exemple shows how to group the TsGroup according to one metainfo key.\n        &gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n        &gt;&gt;&gt; newtsgroup\n        {0:   Index    Freq. (Hz)    group\n         -------  ------------  -------\n               0             1        0,\n         1:   Index    Freq. (Hz)    group\n         -------  ------------  -------\n               1             2        1\n               2             4        1}\n        \"\"\"\ngroups = self._metadata.groupby(key).groups\nsliced = {k: self[list(groups[k])] for k in groups.keys()}\nreturn sliced\ndef save(self, filename):\n\"\"\"\n        Save TsGroup object in npz format. The file will contain the timestamps,\n        the data (if group of Tsd), group index, the time support and the metadata\n        The main purpose of this function is to save small/medium sized TsGroup\n        objects.\n        The function will \"flatten\" the TsGroup by sorting all the timestamps\n        and assigning to each the corresponding index. Typically, a TsGroup like\n        this :\n            TsGroup({\n                0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n                1 : Tsd(t=[1, 5], d=[5, 6])\n            })\n        will be saved as npz with the following keys:\n            {\n                't' : [0, 1, 2, 4, 5],\n                'd' : [1, 5, 2, 3, 5],\n                'index' : [0, 1, 0, 0, 1],\n                'start' : [0],\n                'end' : [5],\n                'type' : 'TsGroup'\n            }\n        Metadata are saved by columns with the column name as the npz key. To avoid\n        potential conflicts, make sure the columns name of the metadata are different\n        from ['t', 'd', 'start', 'end', 'index']\n        You can load the object with numpy.load. Default keys are 't', 'd'(optional),\n        'start', 'end', 'index' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsgroup = nap.TsGroup({\n            0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n            6 : nap.Ts(t=np.array([1.0, 5.0]))\n            },\n            group = np.array([0, 1]),\n            location = np.array(['right foot', 'left foot'])\n            )\n        &gt;&gt;&gt; tsgroup\n          Index    rate    group  location\n        -------  ------  -------  ----------\n              0     0.6        0  right foot\n              6     0.4        1  left foot\n        &gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['rate', 'group', 'location', 't', 'index', 'start', 'end', 'type']\n        &gt;&gt;&gt; print(file['index'])\n        [0 6 0 0 6]\n        In the case where TsGroup is a set of Ts objects, it is very direct to\n        recreate the TsGroup by using the function to_tsgroup :\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n        &gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n        &gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n        &gt;&gt;&gt; tsgroup\n          Index    rate    group  location\n        -------  ------  -------  ----------\n              0     0.6        0  right foot\n              6     0.4        1  left foot\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ndicttosave = {\"type\": np.array([\"TsGroup\"], dtype=np.str_)}\nfor k in self._metadata.columns:\nif k not in [\"t\", \"d\", \"start\", \"end\", \"index\"]:\ntmp = self._metadata[k].values\nif tmp.dtype == np.dtype(\"O\"):\ntmp = tmp.astype(np.str_)\ndicttosave[k] = tmp\n# We can't use to_tsd here in case tsgroup contains Tsd and not only Ts.\nnt = 0\nfor n in self.index:\nnt += len(self[n])\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nindex = np.zeros(nt, dtype=np.int64)\nk = 0\nfor n in self.index:\nkl = len(self[n])\ntimes[k : k + kl] = self[n].index\ndata[k : k + kl] = self[n].values\nindex[k : k + kl] = int(n)\nk += kl\nidx = np.argsort(times)\ntimes = times[idx]\nindex = index[idx]\ndicttosave[\"t\"] = times\ndicttosave[\"index\"] = index\nif not np.all(np.isnan(data)):\ndicttosave[\"d\"] = data[idx]\ndicttosave[\"start\"] = self.time_support.start.values\ndicttosave[\"end\"] = self.time_support.end.values\nnp.savez(filename, **dicttosave)\nreturn\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.rates","title":"rates  <code>property</code>","text":"<pre><code>rates\n</code></pre> <p>Return the rates of each element of the group in Hz</p>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.metadata_columns","title":"metadata_columns  <code>property</code>","text":"<pre><code>metadata_columns\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.metadata_columns--returns-list-of-metadata-columns","title":"Returns list of metadata columns","text":""},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.__init__","title":"__init__","text":"<pre><code>__init__(\ndata,\ntime_support=None,\ntime_units=\"s\",\nbypass_check=False,\n**kwargs\n)\n</code></pre> <p>TsGroup Initializer</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionnary containing Ts/Tsd objects</p> required <code>time_support</code> <code>IntervalSet</code> <p>The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed. If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.</p> <code>None</code> <code>time_units</code> <code>str</code> <p>Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).</p> <code>'s'</code> <code>bypass_check</code> <p>To avoid checking that each element is within time_support. Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand</p> <code>False</code> <code>**kwargs</code> <p>Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray. Note that the index should match the index of the input dictionnary.</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise error if the union of time support of Ts/Tsd object is empty.</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def __init__(\nself, data, time_support=None, time_units=\"s\", bypass_check=False, **kwargs\n):\n\"\"\"\n    TsGroup Initializer\n    Parameters\n    ----------\n    data : dict\n        Dictionnary containing Ts/Tsd objects\n    time_support : IntervalSet, optional\n        The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed.\n        If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.\n    time_units : str, optional\n        Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).\n    bypass_check: bool, optional\n        To avoid checking that each element is within time_support.\n        Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand\n    **kwargs\n        Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray.\n        Note that the index should match the index of the input dictionnary.\n    Raises\n    ------\n    RuntimeError\n        Raise error if the union of time support of Ts/Tsd object is empty.\n    \"\"\"\nself._initialized = False\nself.index = np.sort(list(data.keys()))\nself._metadata = pd.DataFrame(index=self.index, columns=[\"rate\"], dtype=\"float\")\n# Transform elements to Ts/Tsd objects\nfor k in self.index:\nif isinstance(data[k], (np.ndarray, list)):\nwarnings.warn(\n\"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\nstacklevel=2,\n)\ndata[k] = Ts(\nt=data[k], time_support=time_support, time_units=time_units\n)\n# If time_support is passed, all elements of data are restricted prior to init\nif isinstance(time_support, IntervalSet):\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nelse:\n# Otherwise do the union of all time supports\ntime_support = union_intervals([data[k].time_support for k in self.index])\nif len(time_support) == 0:\nraise RuntimeError(\n\"Union of time supports is empty. Consider passing a time support as argument.\"\n)\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nUserDict.__init__(self, data)\n# Making the TsGroup non mutable\nself._initialized = True\n# Trying to add argument as metainfo\nself.set_info(**kwargs)\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.keys","title":"keys","text":"<pre><code>keys()\n</code></pre> <p>Return index/keys of TsGroup</p> <p>Returns:</p> Type Description <code>list</code> <p>List of keys</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def keys(self):\n\"\"\"\n    Return index/keys of TsGroup\n    Returns\n    -------\n    list\n        List of keys\n    \"\"\"\nreturn list(self.data.keys())\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.items","title":"items","text":"<pre><code>items()\n</code></pre> <p>Return a list of key/object.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of tuples</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def items(self):\n\"\"\"\n    Return a list of key/object.\n    Returns\n    -------\n    list\n        List of tuples\n    \"\"\"\nreturn list(self.data.items())\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.values","title":"values","text":"<pre><code>values()\n</code></pre> <p>Return a list of all the Ts/Tsd objects in the TsGroup</p> <p>Returns:</p> Type Description <code>list</code> <p>List of Ts/Tsd objects</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def values(self):\n\"\"\"\n    Return a list of all the Ts/Tsd objects in the TsGroup\n    Returns\n    -------\n    list\n        List of Ts/Tsd objects\n    \"\"\"\nreturn list(self.data.values())\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.set_info","title":"set_info","text":"<pre><code>set_info(*args, **kwargs)\n</code></pre> <p>Add metadata informations about the TsGroup. Metadata are saved as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>pandas.Dataframe or list of pandas.DataFrame</p> <code>()</code> <code>**kwargs</code> <p>Can be either pandas.Series or numpy.ndarray</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise an error if     no column labels are found when passing simple arguments,     indexes are not equals for a pandas series,     not the same length when passing numpy array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n</code></pre> <p>To add metadata with a pandas.DataFrame:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n&gt;&gt;&gt; tsgroup.set_info(structs)\n&gt;&gt;&gt; tsgroup\n  Index    Freq. (Hz)  struct\n-------  ------------  --------\n      0             1  pfc\n      1             2  pfc\n      2             4  ca1\n</code></pre> <p>To add metadata with a pd.Series or numpy.ndarray:</p> <pre><code>&gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n&gt;&gt;&gt; tsgroup.set_info(hd=hd)\n&gt;&gt;&gt; tsgroup\n  Index    Freq. (Hz)  struct      hd\n-------  ------------  --------  ----\n      0             1  pfc          0\n      1             2  pfc          1\n      2             4  ca1          1\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def set_info(self, *args, **kwargs):\n\"\"\"\n    Add metadata informations about the TsGroup.\n    Metadata are saved as a DataFrame.\n    Parameters\n    ----------\n    *args\n        pandas.Dataframe or list of pandas.DataFrame\n    **kwargs\n        Can be either pandas.Series or numpy.ndarray\n    Raises\n    ------\n    RuntimeError\n        Raise an error if\n            no column labels are found when passing simple arguments,\n            indexes are not equals for a pandas series,\n            not the same length when passing numpy array.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    To add metadata with a pandas.DataFrame:\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n    &gt;&gt;&gt; tsgroup.set_info(structs)\n    &gt;&gt;&gt; tsgroup\n      Index    Freq. (Hz)  struct\n    -------  ------------  --------\n          0             1  pfc\n          1             2  pfc\n          2             4  ca1\n    To add metadata with a pd.Series or numpy.ndarray:\n    &gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n    &gt;&gt;&gt; tsgroup.set_info(hd=hd)\n    &gt;&gt;&gt; tsgroup\n      Index    Freq. (Hz)  struct      hd\n    -------  ------------  --------  ----\n          0             1  pfc          0\n          1             2  pfc          1\n          2             4  ca1          1\n    \"\"\"\nif len(args):\nfor arg in args:\nif isinstance(arg, pd.DataFrame):\nif pd.Index.equals(self._metadata.index, arg.index):\nself._metadata = self._metadata.join(arg)\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(arg, (pd.Series, np.ndarray)):\nraise RuntimeError(\"Columns needs to be labelled for metadata\")\nif len(kwargs):\nfor k, v in kwargs.items():\nif isinstance(v, pd.Series):\nif pd.Index.equals(self._metadata.index, v.index):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(v, np.ndarray):\nif len(self._metadata) == len(v):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Array is not the same length.\")\nreturn\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.get_info","title":"get_info","text":"<pre><code>get_info(key)\n</code></pre> <p>Returns the metainfo located in one column. The key for the column frequency is \"rate\".</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <p>Returns:</p> Type Description <code>Series</code> <p>The metainfo</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def get_info(self, key):\n\"\"\"\n    Returns the metainfo located in one column.\n    The key for the column frequency is \"rate\".\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    Returns\n    -------\n    pandas.Series\n        The metainfo\n    \"\"\"\nif key in [\"freq\", \"frequency\"]:\nkey = \"rate\"\nreturn self._metadata[key]\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.restrict","title":"restrict","text":"<pre><code>restrict(ep)\n</code></pre> <p>Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>ep</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Type Description <code>TsGroup</code> <p>TsGroup object restricted to ep</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n&gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n</code></pre> <p>All objects within the TsGroup automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newtsgroup.time_support\n   start    end\n0    0.0  100.0\n&gt;&gt;&gt; newtsgroup[0].time_support\n   start    end\n0    0.0  100.0\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def restrict(self, ep):\n\"\"\"\n    Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    ep : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    TsGroup\n        TsGroup object restricted to ep\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    &gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n    All objects within the TsGroup automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newtsgroup.time_support\n       start    end\n    0    0.0  100.0\n    &gt;&gt;&gt; newtsgroup[0].time_support\n       start    end\n    0    0.0  100.0\n    \"\"\"\nnewgr = {}\nfor k in self.index:\nnewgr[k] = self.data[k].restrict(ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(\nnewgr, time_support=ep, bypass_check=True, **self._metadata[cols]\n)\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.value_from","title":"value_from","text":"<pre><code>value_from(tsd, ep=None)\n</code></pre> <p>Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>Tsd</code> <p>The Tsd object holding the values to replace</p> required <code>ep</code> <code>IntervalSet</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>TsGroup</code> <p>TsGroup object with the new values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n</code></pre> <p>The variable tsd is a time series object containing the values to assign, for example the tracking data:</p> <pre><code>&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n&gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def value_from(self, tsd, ep=None):\n\"\"\"\n    Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument\n    Parameters\n    ----------\n    tsd : Tsd\n        The Tsd object holding the values to replace\n    ep : IntervalSet\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    TsGroup\n        TsGroup object with the new values\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    The variable tsd is a time series object containing the values to assign, for example the tracking data:\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n    &gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n    \"\"\"\nif ep is None:\nep = tsd.time_support\nnewgr = {}\nfor k in self.data:\nnewgr[k] = self.data[k].value_from(tsd, ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(newgr, time_support=ep, **self._metadata[cols])\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.count","title":"count","text":"<pre><code>count(*args, **kwargs)\n</code></pre> <p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsgroup.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsgroup.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsgroup.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsgroup.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>TsdFrame</code> <p>A TsdFrame with the columns being the index of each item in the TsGroup.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second for the first 100 seconds.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n&gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n&gt;&gt;&gt; bincount\n          0  1  2\nTime (s)\n0.05      0  0  0\n0.15      0  0  0\n0.25      0  0  1\n0.35      0  0  0\n0.45      0  0  0\n...      .. .. ..\n99.55     0  1  1\n99.65     0  0  0\n99.75     0  0  1\n99.85     0  0  0\n99.95     1  1  1\n[1000 rows x 3 columns]\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsgroup.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsgroup.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsgroup.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsgroup.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: TsdFrame\n        A TsdFrame with the columns being the index of each item in the TsGroup.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second for the first 100 seconds.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    &gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n    &gt;&gt;&gt; bincount\n              0  1  2\n    Time (s)\n    0.05      0  0  0\n    0.15      0  0  0\n    0.25      0  0  1\n    0.35      0  0  0\n    0.45      0  0  0\n    ...      .. .. ..\n    99.55     0  1  1\n    99.65     0  0  0\n    99.75     0  0  1\n    99.85     0  0  0\n    99.95     1  1  1\n    [1000 rows x 3 columns]\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = float(bin_size)\nbin_size = TsIndex.format_timestamps(np.array([bin_size]), time_units)[0]\ntime_index, _ = jitcount(np.array([]), starts, ends, bin_size)\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jitcount(\nself.data[self.index[i]].index, starts, ends, bin_size\n)[1]\nelse:\ntime_index = starts + (ends - starts) / 2\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jittsrestrict_with_count(\nself.data[self.index[i]].index, starts, ends\n)[1]\ntoreturn = TsdFrame(t=time_index, d=count, time_support=ep, columns=self.index)\nreturn toreturn\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.to_tsd","title":"to_tsd","text":"<pre><code>to_tsd(*args)\n</code></pre> <p>Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>string, list, numpy.ndarray or pandas.Series</p> <code>()</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\nIndex    rate\n-------  ------\n0       1\n5       1\n</code></pre> <p>By default, the values of the Tsd is the index of the timestamp in the TsGroup:</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd()\nTime (s)\n0.0    0.0\n1.0    0.0\n2.0    5.0\n3.0    5.0\ndtype: float64\n</code></pre> <p>Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.</p> <pre><code>&gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n&gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\nTime (s)\n0.0    3.141593\n1.0    3.141593\n2.0    6.283185\n3.0    6.283185\ndtype: float64\n</code></pre> <p>Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\nTime (s)\n0.0   -1.0\n1.0   -1.0\n2.0    1.0\n3.0    1.0\ndtype: float64\n</code></pre> <p>The reverse operation can be done with the Tsd.to_tsgroup function :</p> <pre><code>&gt;&gt;&gt; my_tsd\nTime (s)\n0.0    0.0\n1.0    0.0\n2.0    5.0\n3.0    5.0\ndtype: float64\n&gt;&gt;&gt; my_tsd.to_tsgroup()\n  Index    rate\n-------  ------\n      0       1\n      5       1\n</code></pre> <p>Returns:</p> Type Description <code>Tsd</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>\"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata, \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def to_tsd(self, *args):\n\"\"\"\n    Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.\n    Parameters\n    ----------\n    *args\n        string, list, numpy.ndarray or pandas.Series\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\n    Index    rate\n    -------  ------\n    0       1\n    5       1\n    By default, the values of the Tsd is the index of the timestamp in the TsGroup:\n    &gt;&gt;&gt; tsgroup.to_tsd()\n    Time (s)\n    0.0    0.0\n    1.0    0.0\n    2.0    5.0\n    3.0    5.0\n    dtype: float64\n    Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.\n    &gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n    &gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\n    Time (s)\n    0.0    3.141593\n    1.0    3.141593\n    2.0    6.283185\n    3.0    6.283185\n    dtype: float64\n    Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :\n    &gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\n    Time (s)\n    0.0   -1.0\n    1.0   -1.0\n    2.0    1.0\n    3.0    1.0\n    dtype: float64\n    The reverse operation can be done with the Tsd.to_tsgroup function :\n    &gt;&gt;&gt; my_tsd\n    Time (s)\n    0.0    0.0\n    1.0    0.0\n    2.0    5.0\n    3.0    5.0\n    dtype: float64\n    &gt;&gt;&gt; my_tsd.to_tsgroup()\n      Index    rate\n    -------  ------\n          0       1\n          5       1\n    Returns\n    -------\n    Tsd\n    Raises\n    ------\n    RuntimeError\n        \"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes\n        \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object\n        \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata,\n        \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series\n    \"\"\"\nif len(args):\nif isinstance(args[0], pd.Series):\nif pd.Index.equals(self._metadata.index, args[0].index):\n_values = args[0].values.flatten()\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(args[0], (np.ndarray, list)):\nif len(self._metadata) == len(args[0]):\n_values = np.array(args[0])\nelse:\nraise RuntimeError(\"Values is not the same length.\")\nelif isinstance(args[0], str):\nif args[0] in self._metadata.columns:\n_values = self._metadata[args[0]].values\nelse:\nraise RuntimeError(\n\"Key {} not in metadata of TsGroup\".format(args[0])\n)\nelse:\npossible_keys = []\nfor k, d in self._metadata.dtypes.items():\nif \"int\" in str(d) or \"float\" in str(d):\npossible_keys.append(k)\nraise RuntimeError(\n\"Unknown argument format. Must be pandas.Series, numpy.ndarray or a string from one of the following values : [{}]\".format(\n\", \".join(possible_keys)\n)\n)\nelse:\n_values = self.index\nnt = 0\nfor n in self.index:\nnt += len(self[n])\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nk = 0\nfor n, v in zip(self.index, _values):\nkl = len(self[n])\ntimes[k : k + kl] = self[n].index\ndata[k : k + kl] = v\nk += kl\nidx = np.argsort(times)\ntoreturn = Tsd(t=times[idx], d=data[idx], time_support=self.time_support)\nreturn toreturn\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.getby_threshold","title":"getby_threshold","text":"<pre><code>getby_threshold(key, thr, op='&gt;')\n</code></pre> <p>Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <code>thr</code> <code>float</code> <p>THe value for thresholding</p> required <code>op</code> <code>str</code> <p>The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.</p> <code>'&gt;'</code> <p>Returns:</p> Type Description <code>TsGroup</code> <p>The new TsGroup</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise eror is operation is not recognized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n  Index    Freq. (Hz)\n-------  ------------\n      0             1\n      1             2\n      2             4\n</code></pre> <p>This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.</p> <pre><code>&gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n  Index    Freq. (Hz)\n-------  ------------\n      1             2\n      2             4\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_threshold(self, key, thr, op=\"&gt;\"):\n\"\"\"\n    Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    thr : float\n        THe value for thresholding\n    op : str, optional\n        The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.\n    Returns\n    -------\n    TsGroup\n        The new TsGroup\n    Raises\n    ------\n    RuntimeError\n        Raise eror is operation is not recognized.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n      Index    Freq. (Hz)\n    -------  ------------\n          0             1\n          1             2\n          2             4\n    This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.\n    &gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n      Index    Freq. (Hz)\n    -------  ------------\n          1             2\n          2             4\n    \"\"\"\nif op == \"&gt;\":\nix = list(self._metadata.index[self._metadata[key] &gt; thr])\nreturn self[ix]\nelif op == \"&lt;\":\nix = list(self._metadata.index[self._metadata[key] &lt; thr])\nreturn self[ix]\nelif op == \"&gt;=\":\nix = list(self._metadata.index[self._metadata[key] &gt;= thr])\nreturn self[ix]\nelif op == \"&lt;=\":\nix = list(self._metadata.index[self._metadata[key] &lt;= thr])\nreturn self[ix]\nelse:\nraise RuntimeError(\"Operation {} not recognized.\".format(op))\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.getby_intervals","title":"getby_intervals","text":"<pre><code>getby_intervals(key, bins)\n</code></pre> <p>Return a list of TsGroup binned.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <code>bins</code> <code>ndarray or list</code> <p>The bin intervals</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of TsGroup</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n  Index    Freq. (Hz)    alpha\n-------  ------------  -------\n      0             1        0\n      1             2        1\n      2             4        2\n</code></pre> <p>This exemple shows how to bin the TsGroup according to one metainfo key.</p> <pre><code>&gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n&gt;&gt;&gt; newtsgroup\n[  Index    Freq. (Hz)    alpha\n -------  ------------  -------\n       0             1        0,\n   Index    Freq. (Hz)    alpha\n -------  ------------  -------\n       1             2        1]\n</code></pre> <p>By default, the function returns the center of the bins.</p> <pre><code>&gt;&gt;&gt; bincenter\narray([0.5, 1.5])\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_intervals(self, key, bins):\n\"\"\"\n    Return a list of TsGroup binned.\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    bins : numpy.ndarray or list\n        The bin intervals\n    Returns\n    -------\n    list\n        A list of TsGroup\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n      Index    Freq. (Hz)    alpha\n    -------  ------------  -------\n          0             1        0\n          1             2        1\n          2             4        2\n    This exemple shows how to bin the TsGroup according to one metainfo key.\n    &gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n    &gt;&gt;&gt; newtsgroup\n    [  Index    Freq. (Hz)    alpha\n     -------  ------------  -------\n           0             1        0,\n       Index    Freq. (Hz)    alpha\n     -------  ------------  -------\n           1             2        1]\n    By default, the function returns the center of the bins.\n    &gt;&gt;&gt; bincenter\n    array([0.5, 1.5])\n    \"\"\"\nidx = np.digitize(self._metadata[key], bins) - 1\ngroups = self._metadata.index.groupby(idx)\nix = np.unique(list(groups.keys()))\nix = ix[ix &gt;= 0]\nix = ix[ix &lt; len(bins) - 1]\nxb = bins[0:-1] + np.diff(bins) / 2\nsliced = [self[list(groups[i])] for i in ix]\nreturn sliced, xb[ix]\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.getby_category","title":"getby_category","text":"<pre><code>getby_category(key)\n</code></pre> <p>Return a list of TsGroup grouped by category.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionnary of TsGroup</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n  Index    Freq. (Hz)    group\n-------  ------------  -------\n      0             1        0\n      1             2        1\n      2             4        1\n</code></pre> <p>This exemple shows how to group the TsGroup according to one metainfo key.</p> <pre><code>&gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n&gt;&gt;&gt; newtsgroup\n{0:   Index    Freq. (Hz)    group\n -------  ------------  -------\n       0             1        0,\n 1:   Index    Freq. (Hz)    group\n -------  ------------  -------\n       1             2        1\n       2             4        1}\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_category(self, key):\n\"\"\"\n    Return a list of TsGroup grouped by category.\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    Returns\n    -------\n    dict\n        A dictionnary of TsGroup\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n      Index    Freq. (Hz)    group\n    -------  ------------  -------\n          0             1        0\n          1             2        1\n          2             4        1\n    This exemple shows how to group the TsGroup according to one metainfo key.\n    &gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n    &gt;&gt;&gt; newtsgroup\n    {0:   Index    Freq. (Hz)    group\n     -------  ------------  -------\n           0             1        0,\n     1:   Index    Freq. (Hz)    group\n     -------  ------------  -------\n           1             2        1\n           2             4        1}\n    \"\"\"\ngroups = self._metadata.groupby(key).groups\nsliced = {k: self[list(groups[k])] for k in groups.keys()}\nreturn sliced\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.save","title":"save","text":"<pre><code>save(filename)\n</code></pre> <p>Save TsGroup object in npz format. The file will contain the timestamps, the data (if group of Tsd), group index, the time support and the metadata</p> <p>The main purpose of this function is to save small/medium sized TsGroup objects.</p> <p>The function will \"flatten\" the TsGroup by sorting all the timestamps and assigning to each the corresponding index. Typically, a TsGroup like this :</p> <pre><code>TsGroup({\n    0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n    1 : Tsd(t=[1, 5], d=[5, 6])\n})\n</code></pre> <p>will be saved as npz with the following keys:</p> <pre><code>{\n    't' : [0, 1, 2, 4, 5],\n    'd' : [1, 5, 2, 3, 5],\n    'index' : [0, 1, 0, 0, 1],\n    'start' : [0],\n    'end' : [5],\n    'type' : 'TsGroup'\n}\n</code></pre> <p>Metadata are saved by columns with the column name as the npz key. To avoid potential conflicts, make sure the columns name of the metadata are different from ['t', 'd', 'start', 'end', 'index']</p> <p>You can load the object with numpy.load. Default keys are 't', 'd'(optional), 'start', 'end', 'index' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsgroup = nap.TsGroup({\n    0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n    6 : nap.Ts(t=np.array([1.0, 5.0]))\n    },\n    group = np.array([0, 1]),\n    location = np.array(['right foot', 'left foot'])\n    )\n&gt;&gt;&gt; tsgroup\n  Index    rate    group  location\n-------  ------  -------  ----------\n      0     0.6        0  right foot\n      6     0.4        1  left foot\n&gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['rate', 'group', 'location', 't', 'index', 'start', 'end', 'type']\n&gt;&gt;&gt; print(file['index'])\n[0 6 0 0 6]\n</code></pre> <p>In the case where TsGroup is a set of Ts objects, it is very direct to recreate the TsGroup by using the function to_tsgroup :</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n&gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n&gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n&gt;&gt;&gt; tsgroup\n  Index    rate    group  location\n-------  ------  -------  ----------\n      0     0.6        0  right foot\n      6     0.4        1  left foot\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save TsGroup object in npz format. The file will contain the timestamps,\n    the data (if group of Tsd), group index, the time support and the metadata\n    The main purpose of this function is to save small/medium sized TsGroup\n    objects.\n    The function will \"flatten\" the TsGroup by sorting all the timestamps\n    and assigning to each the corresponding index. Typically, a TsGroup like\n    this :\n        TsGroup({\n            0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n            1 : Tsd(t=[1, 5], d=[5, 6])\n        })\n    will be saved as npz with the following keys:\n        {\n            't' : [0, 1, 2, 4, 5],\n            'd' : [1, 5, 2, 3, 5],\n            'index' : [0, 1, 0, 0, 1],\n            'start' : [0],\n            'end' : [5],\n            'type' : 'TsGroup'\n        }\n    Metadata are saved by columns with the column name as the npz key. To avoid\n    potential conflicts, make sure the columns name of the metadata are different\n    from ['t', 'd', 'start', 'end', 'index']\n    You can load the object with numpy.load. Default keys are 't', 'd'(optional),\n    'start', 'end', 'index' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsgroup = nap.TsGroup({\n        0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n        6 : nap.Ts(t=np.array([1.0, 5.0]))\n        },\n        group = np.array([0, 1]),\n        location = np.array(['right foot', 'left foot'])\n        )\n    &gt;&gt;&gt; tsgroup\n      Index    rate    group  location\n    -------  ------  -------  ----------\n          0     0.6        0  right foot\n          6     0.4        1  left foot\n    &gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['rate', 'group', 'location', 't', 'index', 'start', 'end', 'type']\n    &gt;&gt;&gt; print(file['index'])\n    [0 6 0 0 6]\n    In the case where TsGroup is a set of Ts objects, it is very direct to\n    recreate the TsGroup by using the function to_tsgroup :\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n    &gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n    &gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n    &gt;&gt;&gt; tsgroup\n      Index    rate    group  location\n    -------  ------  -------  ----------\n          0     0.6        0  right foot\n          6     0.4        1  left foot\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ndicttosave = {\"type\": np.array([\"TsGroup\"], dtype=np.str_)}\nfor k in self._metadata.columns:\nif k not in [\"t\", \"d\", \"start\", \"end\", \"index\"]:\ntmp = self._metadata[k].values\nif tmp.dtype == np.dtype(\"O\"):\ntmp = tmp.astype(np.str_)\ndicttosave[k] = tmp\n# We can't use to_tsd here in case tsgroup contains Tsd and not only Ts.\nnt = 0\nfor n in self.index:\nnt += len(self[n])\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nindex = np.zeros(nt, dtype=np.int64)\nk = 0\nfor n in self.index:\nkl = len(self[n])\ntimes[k : k + kl] = self[n].index\ndata[k : k + kl] = self[n].values\nindex[k : k + kl] = int(n)\nk += kl\nidx = np.argsort(times)\ntimes = times[idx]\nindex = index[idx]\ndicttosave[\"t\"] = times\ndicttosave[\"index\"] = index\nif not np.all(np.isnan(data)):\ndicttosave[\"d\"] = data[idx]\ndicttosave[\"start\"] = self.time_support.start.values\ndicttosave[\"end\"] = self.time_support.end.values\nnp.savez(filename, **dicttosave)\nreturn\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.union_intervals","title":"union_intervals","text":"<pre><code>union_intervals(i_sets)\n</code></pre> <p>Helper to merge intervals from ts_group</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def union_intervals(i_sets):\n\"\"\"\n    Helper to merge intervals from ts_group\n    \"\"\"\nn = len(i_sets)\nif n == 1:\nreturn i_sets[0]\nnew_start = np.zeros(0)\nnew_end = np.zeros(0)\nif n == 2:\nnew_start, new_end = jitunion(\ni_sets[0].start.values,\ni_sets[0].end.values,\ni_sets[1].start.values,\ni_sets[1].end.values,\n)\nif n &gt; 2:\nsizes = np.array([i_sets[i].shape[0] for i in range(n)])\nstartends = np.zeros((np.sum(sizes), 2))\nct = 0\nfor i in range(sizes.shape[0]):\nstartends[ct : ct + sizes[i], :] = i_sets[i].values\nct += sizes[i]\nnew_start, new_end = jitunion_isets(startends[:, 0], startends[:, 1])\nreturn IntervalSet(new_start, new_end)\n</code></pre>"},{"location":"reference/io/","title":"Io","text":""},{"location":"reference/io/#pynapple.io","title":"pynapple.io","text":""},{"location":"reference/io/cnmfe/","title":"Cnmfe","text":""},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe","title":"pynapple.io.cnmfe","text":"<p> DEPRECATED: This will be removed in version 1.0.0. Check nwbmatic or neuroconv instead.</p> <p>Loaders for calcium imaging data with miniscope. Support CNMF-E in matlab, inscopix-cnmfe and minian.</p>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E","title":"CNMF_E","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E). The path folder should contain a file ending in .mat when calling Source2d.save_neurons</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>ndarray</code> <p>Spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class CNMF_E(BaseLoader):\n\"\"\"Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E).\n    The path folder should contain a file ending in .mat\n    when calling Source2d.save_neurons\n    Attributes\n    ----------\n    A : numpy.ndarray\n        Spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_cnmf_e(path)\nself.save_cnmfe_nwb(path)\ndef load_cnmf_e(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nfiles = os.listdir(path)\nmatfiles = [f for f in files if f.endswith(\".mat\")]\nif len(matfiles):\ndata = loadmat(os.path.join(path, matfiles[0]), struct_as_record=False)\nelse:\nraise RuntimeError(\"No mat file found in {}\".format(path))\nself.struct = data[\"neuron_results\"][0][0]\nC = self.struct.C.T\nself.A = self.struct.A.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = np.atleast_2d(self.A[i])\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_cnmf_e(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmf_e","title":"load_cnmf_e","text":"<pre><code>load_cnmf_e(path)\n</code></pre> <p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmf_e(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nfiles = os.listdir(path)\nmatfiles = [f for f in files if f.endswith(\".mat\")]\nif len(matfiles):\ndata = loadmat(os.path.join(path, matfiles[0]), struct_as_record=False)\nelse:\nraise RuntimeError(\"No mat file found in {}\".format(path))\nself.struct = data[\"neuron_results\"][0][0]\nC = self.struct.C.T\nself.A = self.struct.A.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nreturn None\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.save_cnmfe_nwb","title":"save_cnmfe_nwb","text":"<pre><code>save_cnmfe_nwb(path)\n</code></pre> <p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = np.atleast_2d(self.A[i])\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmfe_nwb","title":"load_cnmfe_nwb","text":"<pre><code>load_cnmfe_nwb(path)\n</code></pre> <p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian","title":"Minian","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader for data processed with Minian (https://github.com/denisecailab/minian). The path folder should contain a subfolder name minian.</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>ndarray</code> <p>Spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class Minian(BaseLoader):\n\"\"\"Loader for data processed with Minian (https://github.com/denisecailab/minian).\n    The path folder should contain a subfolder name minian.\n    Attributes\n    ----------\n    A : numpy.ndarray\n        Spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_minian(path)\nself.save_cnmfe_nwb(path)\ndef load_minian(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nminian_folder = os.path.join(path, \"minian\")\nif not os.path.exists(minian_folder):\nraise RuntimeError(\"Path {} does not contain a minian folder\".format(path))\ntry:\nimport zarr\nexcept ImportError as ie:\nprint(\"Please install module zarr for loading minian data\", ie)\nsys.exit()\ndata = zarr.open(minian_folder, \"r\")\nC = data[\"C.zarr\"][\"C\"][:]\nC = C.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nself.A = data[\"A.zarr\"][\"A\"][:]\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_minian(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.load_minian","title":"load_minian","text":"<pre><code>load_minian(path)\n</code></pre> <p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_minian(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nminian_folder = os.path.join(path, \"minian\")\nif not os.path.exists(minian_folder):\nraise RuntimeError(\"Path {} does not contain a minian folder\".format(path))\ntry:\nimport zarr\nexcept ImportError as ie:\nprint(\"Please install module zarr for loading minian data\", ie)\nsys.exit()\ndata = zarr.open(minian_folder, \"r\")\nC = data[\"C.zarr\"][\"C\"][:]\nC = C.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nself.A = data[\"A.zarr\"][\"A\"][:]\nreturn None\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.save_cnmfe_nwb","title":"save_cnmfe_nwb","text":"<pre><code>save_cnmfe_nwb(path)\n</code></pre> <p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.load_cnmfe_nwb","title":"load_cnmfe_nwb","text":"<pre><code>load_cnmfe_nwb(path)\n</code></pre> <p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE","title":"InscopixCNMFE","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe). The folder should contain a file ending with '_traces.csv' and a tiff file for spatial footprints.</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>ndarray</code> <p>The spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class InscopixCNMFE(BaseLoader):\n\"\"\"Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe).\n    The folder should contain a file ending with '_traces.csv'\n    and a tiff file for spatial footprints.\n    Attributes\n    ----------\n    A : np.ndarray\n        The spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_inscopix_cnmfe(path)\nself.save_cnmfe_nwb(path)\ndef load_inscopix_cnmfe(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nfiles = os.listdir(path)\ntracefile = [f for f in files if f.endswith(\"_traces.csv\")]\nif len(tracefile):\nC = pd.read_csv(os.path.join(path, tracefile[0]), index_col=0)\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*_traces.csv\")\n)\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C.values)\ntry:\nimport tifffile as tiff\nexcept ImportError as ie:\nprint(\"Please install module tifffile for loading inscopix-cnmfe data\", ie)\nsys.exit()\ntifffile = [f for f in files if f.endswith(\".tiff\")]\nif len(tifffile):\nself.A = tiff.imread(os.path.join(path, tifffile[0]))\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*.tiff\")\n)\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_inscopix_cnmfe(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_inscopix_cnmfe","title":"load_inscopix_cnmfe","text":"<pre><code>load_inscopix_cnmfe(path)\n</code></pre> <p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_inscopix_cnmfe(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nfiles = os.listdir(path)\ntracefile = [f for f in files if f.endswith(\"_traces.csv\")]\nif len(tracefile):\nC = pd.read_csv(os.path.join(path, tracefile[0]), index_col=0)\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*_traces.csv\")\n)\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C.values)\ntry:\nimport tifffile as tiff\nexcept ImportError as ie:\nprint(\"Please install module tifffile for loading inscopix-cnmfe data\", ie)\nsys.exit()\ntifffile = [f for f in files if f.endswith(\".tiff\")]\nif len(tifffile):\nself.A = tiff.imread(os.path.join(path, tifffile[0]))\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*.tiff\")\n)\nreturn None\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.save_cnmfe_nwb","title":"save_cnmfe_nwb","text":"<pre><code>save_cnmfe_nwb(path)\n</code></pre> <p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_cnmfe_nwb","title":"load_cnmfe_nwb","text":"<pre><code>load_cnmfe_nwb(path)\n</code></pre> <p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/ephys_gui/","title":"Ephys gui","text":""},{"location":"reference/io/ephys_gui/#pynapple.io.ephys_gui","title":"pynapple.io.ephys_gui","text":"<p>Summary</p>"},{"location":"reference/io/folder/","title":"Folder","text":""},{"location":"reference/io/folder/#pynapple.io.folder","title":"pynapple.io.folder","text":"<p>The Folder class helps to navigate a hierarchical data tree.</p>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder","title":"Folder","text":"<p>             Bases: <code>UserDict</code></p> <p>Base class for all type of folders (i.e. Project, Subject, Sessions, ...). Handles files and sub-folders discovery</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>dict</code> <p>Dictionnary holidng all the pynapple objects found in the folder.</p> <code>name</code> <code>str</code> <p>Name of the folder</p> <code>npz_files</code> <code>list</code> <p>List of npz files found in the folder</p> <code>nwb_files</code> <code>list</code> <p>List of nwb files found in the folder</p> <code>path</code> <code>str</code> <p>Absolute path of the folder</p> <code>subfolds</code> <code>dict</code> <p>Dictionnary of all the subfolders</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>class Folder(UserDict):\n\"\"\"\n    Base class for all type of folders (i.e. Project, Subject, Sessions, ...).\n    Handles files and sub-folders discovery\n    Attributes\n    ----------\n    data : dict\n        Dictionnary holidng all the pynapple objects found in the folder.\n    name : str\n        Name of the folder\n    npz_files : list\n        List of npz files found in the folder\n    nwb_files : list\n        List of nwb files found in the folder\n    path : str\n        Absolute path of the folder\n    subfolds : dict\n        Dictionnary of all the subfolders\n    \"\"\"\ndef __init__(self, path):  # , exclude=(), max_depth=4):\n\"\"\"Initialize the Folder object\n        Parameters\n        ----------\n        path : str\n            Path to the folder\n        \"\"\"\npath = path.rstrip(\"/\")\nself.path = path\nself.name = os.path.basename(path)\nself._basic_view = Tree(\n\":open_file_folder: {}\".format(self.name), guide_style=\"blue\"\n)\nself._full_view = None\n# Search sub-folders\nsubfolds = [\nf.path\nfor f in os.scandir(path)\nif f.is_dir() and not f.name.startswith(\".\")\n]\nsubfolds.sort()\nself.subfolds = {}\nfor s in subfolds:\nsub = os.path.basename(s)\nself.subfolds[sub] = Folder(s)\nself._basic_view.add(\":open_file_folder: [blue]\" + sub)\n# Search files\nself.npz_files = _find_files(path, \"npz\")\nself.nwb_files = _find_files(path, \"nwb\")\nfor filename, file in self.npz_files.items():\nself._basic_view.add(\"[green]\" + file.name + \" \\t|\\t \" + file.type)\nfor file in self.nwb_files.values():\nself._basic_view.add(\"[magenta]\" + file.name + \" \\t|\\t NWB file\")\n# Putting everything together\nself.data = {**self.npz_files, **self.nwb_files, **self.subfolds}\nUserDict.__init__(self, self.data)\ndef __str__(self):\n\"\"\"View of the object\"\"\"\nwith Console() as console:\nconsole.print(self._basic_view)\nreturn \"\"\n# def __repr__(self):\n#     \"\"\"View of the object\"\"\"\n#     print(self._basic_view)\ndef __getitem__(self, key):\n\"\"\"Get subfolder or load file.\n        Parameters\n        ----------\n        key : str\n        Returns\n        -------\n        (Ts, Tsd, TsdFrame, TsGroup, IntervalSet, Folder or NWBFile)\n        Raises\n        ------\n        KeyError\n            If key is not in the dictionnary\n        \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], NPZFile):\ndata = self.data[key].load()\nself.data[key] = data\n# setattr(self, key, data)\nreturn data\nelif isinstance(self.data[key], NWBFile):\nreturn self.data[key]\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n# # # Gets called when an attribute is accessed\n# def __getattribute__(self, item):\n#     value = super(Folder, self).__getattribute__(item)\n#     if isinstance(value, NPZFile):\n#         data = value.load()\n#         setattr(self, item, data)\n#         self.data[item] = data\n#         return data\n#     else:\n#         return value\ndef _generate_tree_view(self):\ntree = Tree(\":open_file_folder: {}\".format(self.name), guide_style=\"blue\")\n# Folder\nfor fold in self.subfolds.keys():\ntree.add(\":open_file_folder: \" + fold)\n_walk_folder(tree.children[-1], self.subfolds[fold])\n# NPZ files\nfor file in self.npz_files.values():\ntree.add(\"[green]\" + file.name + \" \\t|\\t \" + file.type)\n# NWB files\nfor file in self.nwb_files.values():\ntree.add(\"[magenta]\" + file.name + \" \\t|\\t NWB file\")\nself._full_view = tree\ndef expand(self):\n\"\"\"Display the full tree view. Equivalent to Folder.view\"\"\"\nif not isinstance(self._full_view, Tree):\nself._generate_tree_view()\nwith Console() as console:\nconsole.print(self._full_view)\nreturn None\n@property\ndef view(self):\n\"\"\"Summary\"\"\"\nreturn self.expand()\ndef save(self, name, obj, description=\"\"):\n\"\"\"Save a pynapple object in the folder in a single file in uncompressed ``.npz`` format.\n        By default, the save function overwrite previously save file with the same name.\n        Parameters\n        ----------\n        name : str\n            Filename\n        obj : Ts, Tsd, TsdFrame, TsGroup or IntervalSet\n            Pynapple object.\n        description : str, optional\n            Metainformation added as a json sidecar.\n        \"\"\"\nfilepath = os.path.join(self.path, name)\nobj.save(filepath)\nself.npz_files[name] = NPZFile(filepath + \".npz\")\nself.data[name] = obj\nmetadata = {\"time\": str(datetime.now()), \"info\": str(description)}\nwith open(os.path.join(self.path, name + \".json\"), \"w\") as ff:\njson.dump(metadata, ff, indent=2)\n# regenerate the tree view\nself._generate_tree_view()\ndef load(self):\n\"\"\"Load all compatible NPZ files.\"\"\"\nfor k in self.npz_files.keys():\nself[k] = self.npz_files[k].load()\n# def add_metadata(self):\n#     \"\"\"Summary\"\"\"\n#     pass\ndef info(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n        Parameters\n        ----------\n        name : str\n            Name of the npz file\n        \"\"\"\nself.metadata(name)\ndef doc(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n        Parameters\n        ----------\n        name : str\n            Name of the npz file\n        \"\"\"\nself.metadata(name)\ndef metadata(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n        Parameters\n        ----------\n        name : str\n            Name of the npz file\n        \"\"\"\n# Search for json first\njson_filename = os.path.join(self.path, name + \".json\")\nif os.path.isfile(json_filename):\nwith open(json_filename, \"r\") as ff:\nmetadata = json.load(ff)\ntext = \"\\n\".join([\" : \".join(it) for it in metadata.items()])\npanel = Panel.fit(\ntext, border_style=\"green\", title=os.path.join(self.path, name + \".npz\")\n)\nelse:\npanel = Panel.fit(\n\"No metadata\",\nborder_style=\"red\",\ntitle=os.path.join(self.path, name + \".npz\"),\n)\nwith Console() as console:\nconsole.print(panel)\nreturn None\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.view","title":"view  <code>property</code>","text":"<pre><code>view\n</code></pre> <p>Summary</p>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Initialize the Folder object</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the folder</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def __init__(self, path):  # , exclude=(), max_depth=4):\n\"\"\"Initialize the Folder object\n    Parameters\n    ----------\n    path : str\n        Path to the folder\n    \"\"\"\npath = path.rstrip(\"/\")\nself.path = path\nself.name = os.path.basename(path)\nself._basic_view = Tree(\n\":open_file_folder: {}\".format(self.name), guide_style=\"blue\"\n)\nself._full_view = None\n# Search sub-folders\nsubfolds = [\nf.path\nfor f in os.scandir(path)\nif f.is_dir() and not f.name.startswith(\".\")\n]\nsubfolds.sort()\nself.subfolds = {}\nfor s in subfolds:\nsub = os.path.basename(s)\nself.subfolds[sub] = Folder(s)\nself._basic_view.add(\":open_file_folder: [blue]\" + sub)\n# Search files\nself.npz_files = _find_files(path, \"npz\")\nself.nwb_files = _find_files(path, \"nwb\")\nfor filename, file in self.npz_files.items():\nself._basic_view.add(\"[green]\" + file.name + \" \\t|\\t \" + file.type)\nfor file in self.nwb_files.values():\nself._basic_view.add(\"[magenta]\" + file.name + \" \\t|\\t NWB file\")\n# Putting everything together\nself.data = {**self.npz_files, **self.nwb_files, **self.subfolds}\nUserDict.__init__(self, self.data)\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>View of the object</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def __str__(self):\n\"\"\"View of the object\"\"\"\nwith Console() as console:\nconsole.print(self._basic_view)\nreturn \"\"\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key)\n</code></pre> <p>Get subfolder or load file.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required <p>Returns:</p> Type Description <code>(Ts, Tsd, TsdFrame, TsGroup, IntervalSet, Folder or NWBFile)</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If key is not in the dictionnary</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def __getitem__(self, key):\n\"\"\"Get subfolder or load file.\n    Parameters\n    ----------\n    key : str\n    Returns\n    -------\n    (Ts, Tsd, TsdFrame, TsGroup, IntervalSet, Folder or NWBFile)\n    Raises\n    ------\n    KeyError\n        If key is not in the dictionnary\n    \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], NPZFile):\ndata = self.data[key].load()\nself.data[key] = data\n# setattr(self, key, data)\nreturn data\nelif isinstance(self.data[key], NWBFile):\nreturn self.data[key]\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.expand","title":"expand","text":"<pre><code>expand()\n</code></pre> <p>Display the full tree view. Equivalent to Folder.view</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def expand(self):\n\"\"\"Display the full tree view. Equivalent to Folder.view\"\"\"\nif not isinstance(self._full_view, Tree):\nself._generate_tree_view()\nwith Console() as console:\nconsole.print(self._full_view)\nreturn None\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.save","title":"save","text":"<pre><code>save(name, obj, description='')\n</code></pre> <p>Save a pynapple object in the folder in a single file in uncompressed <code>.npz</code> format. By default, the save function overwrite previously save file with the same name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Filename</p> required <code>obj</code> <code>(Ts, Tsd, TsdFrame, TsGroup or IntervalSet)</code> <p>Pynapple object.</p> required <code>description</code> <code>str</code> <p>Metainformation added as a json sidecar.</p> <code>''</code> Source code in <code>pynapple/io/folder.py</code> <pre><code>def save(self, name, obj, description=\"\"):\n\"\"\"Save a pynapple object in the folder in a single file in uncompressed ``.npz`` format.\n    By default, the save function overwrite previously save file with the same name.\n    Parameters\n    ----------\n    name : str\n        Filename\n    obj : Ts, Tsd, TsdFrame, TsGroup or IntervalSet\n        Pynapple object.\n    description : str, optional\n        Metainformation added as a json sidecar.\n    \"\"\"\nfilepath = os.path.join(self.path, name)\nobj.save(filepath)\nself.npz_files[name] = NPZFile(filepath + \".npz\")\nself.data[name] = obj\nmetadata = {\"time\": str(datetime.now()), \"info\": str(description)}\nwith open(os.path.join(self.path, name + \".json\"), \"w\") as ff:\njson.dump(metadata, ff, indent=2)\n# regenerate the tree view\nself._generate_tree_view()\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.load","title":"load","text":"<pre><code>load()\n</code></pre> <p>Load all compatible NPZ files.</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def load(self):\n\"\"\"Load all compatible NPZ files.\"\"\"\nfor k in self.npz_files.keys():\nself[k] = self.npz_files[k].load()\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.info","title":"info","text":"<pre><code>info(name)\n</code></pre> <p>Display the metadata within the json sidecar of a NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the npz file</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def info(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n    Parameters\n    ----------\n    name : str\n        Name of the npz file\n    \"\"\"\nself.metadata(name)\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.doc","title":"doc","text":"<pre><code>doc(name)\n</code></pre> <p>Display the metadata within the json sidecar of a NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the npz file</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def doc(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n    Parameters\n    ----------\n    name : str\n        Name of the npz file\n    \"\"\"\nself.metadata(name)\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.metadata","title":"metadata","text":"<pre><code>metadata(name)\n</code></pre> <p>Display the metadata within the json sidecar of a NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the npz file</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def metadata(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n    Parameters\n    ----------\n    name : str\n        Name of the npz file\n    \"\"\"\n# Search for json first\njson_filename = os.path.join(self.path, name + \".json\")\nif os.path.isfile(json_filename):\nwith open(json_filename, \"r\") as ff:\nmetadata = json.load(ff)\ntext = \"\\n\".join([\" : \".join(it) for it in metadata.items()])\npanel = Panel.fit(\ntext, border_style=\"green\", title=os.path.join(self.path, name + \".npz\")\n)\nelse:\npanel = Panel.fit(\n\"No metadata\",\nborder_style=\"red\",\ntitle=os.path.join(self.path, name + \".npz\"),\n)\nwith Console() as console:\nconsole.print(panel)\nreturn None\n</code></pre>"},{"location":"reference/io/interface_npz/","title":"Interface npz","text":""},{"location":"reference/io/interface_npz/#pynapple.io.interface_npz","title":"pynapple.io.interface_npz","text":"<p>File classes help to validate and load pynapple objects or NWB files. Data are always lazy-loaded. Both classes behaves like dictionnary.</p>"},{"location":"reference/io/interface_npz/#pynapple.io.interface_npz.NPZFile","title":"NPZFile","text":"<p>             Bases: <code>object</code></p> <p>Class that points to a NPZ file that can be loaded as a pynapple object. Objects have a save function in npz format as well as the Folder class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; tsd = nap.load_file(\"path/to/my_tsd.npz\")\n&gt;&gt;&gt; tsd\nTime (s)\n0.0    0\n0.1    1\n0.2    2\ndtype: int64\n</code></pre> Source code in <code>pynapple/io/interface_npz.py</code> <pre><code>class NPZFile(object):\n\"\"\"Class that points to a NPZ file that can be loaded as a pynapple object.\n    Objects have a save function in npz format as well as the Folder class.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; tsd = nap.load_file(\"path/to/my_tsd.npz\")\n    &gt;&gt;&gt; tsd\n    Time (s)\n    0.0    0\n    0.1    1\n    0.2    2\n    dtype: int64\n    \"\"\"\ndef __init__(self, path):\n\"\"\"Initialization of the NPZ file\n        Parameters\n        ----------\n        path : str\n            Valid path to a NPZ file\n        \"\"\"\nself.path = path\nself.name = os.path.basename(path)\nself.file = np.load(self.path, allow_pickle=True)\nself.type = \"\"\n# First check if type is explicitely defined\npossible = [\"Ts\", \"Tsd\", \"TsdFrame\", \"TsdTensor\", \"TsGroup\", \"IntervalSet\"]\nif \"type\" in self.file.keys():\nif len(self.file[\"type\"]) == 1:\nif isinstance(self.file[\"type\"][0], np.str_):\nif self.file[\"type\"] in possible:\nself.type = self.file[\"type\"][0]\n# Second check manually\nif self.type == \"\":\nk = set(self.file.keys())\nif {\"t\", \"start\", \"end\", \"index\"}.issubset(k):\nself.type = \"TsGroup\"\nelif {\"t\", \"d\", \"start\", \"end\", \"columns\"}.issubset(k):\nself.type = \"TsdFrame\"\nelif {\"t\", \"d\", \"start\", \"end\"}.issubset(k):\nif self.file[\"d\"].ndim == 1:\nself.type = \"Tsd\"\nelse:\nself.type = \"TsdTensor\"\nelif {\"t\", \"start\", \"end\"}.issubset(k):\nself.type = \"Ts\"\nelif {\"start\", \"end\"}.issubset(k):\nself.type = \"IntervalSet\"\nelse:\nself.type = \"npz\"\ndef load(self):\n\"\"\"Load the NPZ file\n        Returns\n        -------\n        (Tsd, Ts, TsdFrame, TsdTensor, TsGroup, IntervalSet)\n            A pynapple object\n        \"\"\"\nif self.type == \"npz\":\nreturn self.file\nelse:\ntime_support = nap.IntervalSet(self.file[\"start\"], self.file[\"end\"])\nif self.type == \"TsGroup\":\ntsd = nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"index\"], time_support=time_support\n)\ntsgroup = tsd.to_tsgroup()\nif \"d\" in self.file.keys():\nprint(\"TODO\")\nmetainfo = {}\nfor k in set(self.file.keys()) - {\n\"start\",\n\"end\",\n\"t\",\n\"index\",\n\"d\",\n\"rate\",\n}:\ntmp = self.file[k]\nif len(tmp) == len(tsgroup):\nmetainfo[k] = tmp\ntsgroup.set_info(**metainfo)\nreturn tsgroup\nelif self.type == \"TsdFrame\":\nreturn nap.TsdFrame(\nt=self.file[\"t\"],\nd=self.file[\"d\"],\ntime_support=time_support,\ncolumns=self.file[\"columns\"],\n)\nelif self.type == \"TsdTensor\":\nreturn nap.TsdTensor(\nt=self.file[\"t\"], d=self.file[\"d\"], time_support=time_support\n)\nelif self.type == \"Tsd\":\nreturn nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"d\"], time_support=time_support\n)\nelif self.type == \"Ts\":\nreturn nap.Ts(t=self.file[\"t\"], time_support=time_support)\nelif self.type == \"IntervalSet\":\nreturn time_support\nelse:\nreturn self.file\n</code></pre>"},{"location":"reference/io/interface_npz/#pynapple.io.interface_npz.NPZFile.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Initialization of the NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Valid path to a NPZ file</p> required Source code in <code>pynapple/io/interface_npz.py</code> <pre><code>def __init__(self, path):\n\"\"\"Initialization of the NPZ file\n    Parameters\n    ----------\n    path : str\n        Valid path to a NPZ file\n    \"\"\"\nself.path = path\nself.name = os.path.basename(path)\nself.file = np.load(self.path, allow_pickle=True)\nself.type = \"\"\n# First check if type is explicitely defined\npossible = [\"Ts\", \"Tsd\", \"TsdFrame\", \"TsdTensor\", \"TsGroup\", \"IntervalSet\"]\nif \"type\" in self.file.keys():\nif len(self.file[\"type\"]) == 1:\nif isinstance(self.file[\"type\"][0], np.str_):\nif self.file[\"type\"] in possible:\nself.type = self.file[\"type\"][0]\n# Second check manually\nif self.type == \"\":\nk = set(self.file.keys())\nif {\"t\", \"start\", \"end\", \"index\"}.issubset(k):\nself.type = \"TsGroup\"\nelif {\"t\", \"d\", \"start\", \"end\", \"columns\"}.issubset(k):\nself.type = \"TsdFrame\"\nelif {\"t\", \"d\", \"start\", \"end\"}.issubset(k):\nif self.file[\"d\"].ndim == 1:\nself.type = \"Tsd\"\nelse:\nself.type = \"TsdTensor\"\nelif {\"t\", \"start\", \"end\"}.issubset(k):\nself.type = \"Ts\"\nelif {\"start\", \"end\"}.issubset(k):\nself.type = \"IntervalSet\"\nelse:\nself.type = \"npz\"\n</code></pre>"},{"location":"reference/io/interface_npz/#pynapple.io.interface_npz.NPZFile.load","title":"load","text":"<pre><code>load()\n</code></pre> <p>Load the NPZ file</p> <p>Returns:</p> Type Description <code>(Tsd, Ts, TsdFrame, TsdTensor, TsGroup, IntervalSet)</code> <p>A pynapple object</p> Source code in <code>pynapple/io/interface_npz.py</code> <pre><code>def load(self):\n\"\"\"Load the NPZ file\n    Returns\n    -------\n    (Tsd, Ts, TsdFrame, TsdTensor, TsGroup, IntervalSet)\n        A pynapple object\n    \"\"\"\nif self.type == \"npz\":\nreturn self.file\nelse:\ntime_support = nap.IntervalSet(self.file[\"start\"], self.file[\"end\"])\nif self.type == \"TsGroup\":\ntsd = nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"index\"], time_support=time_support\n)\ntsgroup = tsd.to_tsgroup()\nif \"d\" in self.file.keys():\nprint(\"TODO\")\nmetainfo = {}\nfor k in set(self.file.keys()) - {\n\"start\",\n\"end\",\n\"t\",\n\"index\",\n\"d\",\n\"rate\",\n}:\ntmp = self.file[k]\nif len(tmp) == len(tsgroup):\nmetainfo[k] = tmp\ntsgroup.set_info(**metainfo)\nreturn tsgroup\nelif self.type == \"TsdFrame\":\nreturn nap.TsdFrame(\nt=self.file[\"t\"],\nd=self.file[\"d\"],\ntime_support=time_support,\ncolumns=self.file[\"columns\"],\n)\nelif self.type == \"TsdTensor\":\nreturn nap.TsdTensor(\nt=self.file[\"t\"], d=self.file[\"d\"], time_support=time_support\n)\nelif self.type == \"Tsd\":\nreturn nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"d\"], time_support=time_support\n)\nelif self.type == \"Ts\":\nreturn nap.Ts(t=self.file[\"t\"], time_support=time_support)\nelif self.type == \"IntervalSet\":\nreturn time_support\nelse:\nreturn self.file\n</code></pre>"},{"location":"reference/io/interface_nwb/","title":"Interface nwb","text":""},{"location":"reference/io/interface_nwb/#pynapple.io.interface_nwb","title":"pynapple.io.interface_nwb","text":"<p>Pynapple class to interface with NWB files. Data are always lazy-loaded. Object behaves like dictionary.</p>"},{"location":"reference/io/interface_nwb/#pynapple.io.interface_nwb.NWBFile","title":"NWBFile","text":"<p>             Bases: <code>UserDict</code></p> <p>Class for reading NWB Files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; data = nap.load_file(\"my_file.nwb\")\n&gt;&gt;&gt; data[\"units\"]\n  Index    rate  location      group\n-------  ------  ----------  -------\n      0    1.0  brain        0\n      1    1.0  brain        0\n      2    1.0  brain        0\n</code></pre> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>class NWBFile(UserDict):\n\"\"\"Class for reading NWB Files.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; data = nap.load_file(\"my_file.nwb\")\n    &gt;&gt;&gt; data[\"units\"]\n      Index    rate  location      group\n    -------  ------  ----------  -------\n          0    1.0  brain        0\n          1    1.0  brain        0\n          2    1.0  brain        0\n    \"\"\"\n_f_eval = {\n\"IntervalSet\": _make_interval_set,\n\"Tsd\": _make_tsd,\n\"Ts\": _make_ts,\n\"TsdFrame\": _make_tsd_frame,\n\"TsdTensor\": _make_tsd_tensor,\n\"TsGroup\": _make_tsgroup,\n}\ndef __init__(self, file):\n\"\"\"\n        Parameters\n        ----------\n        file : str or pynwb.file.NWBFile\n            Valid file to a NWB file\n        Raises\n        ------\n        FileNotFoundError\n            If path is invalid\n        RuntimeError\n            If file is not an instance of NWBFile\n        \"\"\"\nif isinstance(file, str):\nif os.path.exists(file):\nself.path = file\nself.name = os.path.basename(file).split(\".\")[0]\nself.io = NWBHDF5IO(file, \"r\")\nself.nwb = self.io.read()\nelse:\nraise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), file)\nelif isinstance(file, pynwb.file.NWBFile):\nself.nwb = file\nself.name = self.nwb.session_id\nelse:\nraise RuntimeError(\n\"unrecognized argument. Please provide path to a valid NWB file or open NWB file.\"\n)\nself.data = _extract_compatible_data_from_nwbfile(self.nwb)\nself.key_to_id = {k: self.data[k][\"id\"] for k in self.data.keys()}\nself._view = [[k, self.data[k][\"type\"]] for k in self.data.keys()]\nUserDict.__init__(self, self.data)\ndef __str__(self):\ntitle = self.name if isinstance(self.name, str) else \"-\"\nheaders = [\"Keys\", \"Type\"]\nreturn (\ntitle\n+ \"\\n\"\n+ tabulate(self._view, headers=headers, tablefmt=\"mixed_outline\")\n)\n# self._view = Table(title=self.name)\n# self._view.add_column(\"Keys\", justify=\"left\", style=\"cyan\", no_wrap=True)\n# self._view.add_column(\"Type\", style=\"green\")\n# for k in self.data.keys():\n#     self._view.add_row(\n#         k,\n#         self.data[k][\"type\"],\n#     )\n# \"\"\"View of the object\"\"\"\n# with Console() as console:\n#     console.print(self._view)\n# return \"\"\ndef __repr__(self):\n\"\"\"View of the object\"\"\"\nreturn self.__str__()\ndef __getitem__(self, key):\n\"\"\"Get object from NWB\n        Parameters\n        ----------\n        key : str\n        Returns\n        -------\n        (Ts, Tsd, TsdFrame, TsGroup, IntervalSet or dict of IntervalSet)\n        Raises\n        ------\n        KeyError\n            If key is not in the dictionary\n        \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], dict) and \"id\" in self.data[key]:\nobj = self.nwb.objects[self.data[key][\"id\"]]\ntry:\ndata = self._f_eval[self.data[key][\"type\"]](obj)\nexcept Exception:\nwarnings.warn(\n\"Failed to build {}.\\n Returning the NWB object for manual inspection\".format(\nself.data[key][\"type\"]\n),\nstacklevel=2,\n)\ndata = obj\nself.data[key] = data\nreturn data\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n</code></pre>"},{"location":"reference/io/interface_nwb/#pynapple.io.interface_nwb.NWBFile.__init__","title":"__init__","text":"<pre><code>__init__(file)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str or NWBFile</code> <p>Valid file to a NWB file</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If path is invalid</p> <code>RuntimeError</code> <p>If file is not an instance of NWBFile</p> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>def __init__(self, file):\n\"\"\"\n    Parameters\n    ----------\n    file : str or pynwb.file.NWBFile\n        Valid file to a NWB file\n    Raises\n    ------\n    FileNotFoundError\n        If path is invalid\n    RuntimeError\n        If file is not an instance of NWBFile\n    \"\"\"\nif isinstance(file, str):\nif os.path.exists(file):\nself.path = file\nself.name = os.path.basename(file).split(\".\")[0]\nself.io = NWBHDF5IO(file, \"r\")\nself.nwb = self.io.read()\nelse:\nraise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), file)\nelif isinstance(file, pynwb.file.NWBFile):\nself.nwb = file\nself.name = self.nwb.session_id\nelse:\nraise RuntimeError(\n\"unrecognized argument. Please provide path to a valid NWB file or open NWB file.\"\n)\nself.data = _extract_compatible_data_from_nwbfile(self.nwb)\nself.key_to_id = {k: self.data[k][\"id\"] for k in self.data.keys()}\nself._view = [[k, self.data[k][\"type\"]] for k in self.data.keys()]\nUserDict.__init__(self, self.data)\n</code></pre>"},{"location":"reference/io/interface_nwb/#pynapple.io.interface_nwb.NWBFile.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>View of the object</p> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>def __repr__(self):\n\"\"\"View of the object\"\"\"\nreturn self.__str__()\n</code></pre>"},{"location":"reference/io/interface_nwb/#pynapple.io.interface_nwb.NWBFile.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key)\n</code></pre> <p>Get object from NWB</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required <p>Returns:</p> Type Description <code>(Ts, Tsd, TsdFrame, TsGroup, IntervalSet or dict of IntervalSet)</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If key is not in the dictionary</p> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>def __getitem__(self, key):\n\"\"\"Get object from NWB\n    Parameters\n    ----------\n    key : str\n    Returns\n    -------\n    (Ts, Tsd, TsdFrame, TsGroup, IntervalSet or dict of IntervalSet)\n    Raises\n    ------\n    KeyError\n        If key is not in the dictionary\n    \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], dict) and \"id\" in self.data[key]:\nobj = self.nwb.objects[self.data[key][\"id\"]]\ntry:\ndata = self._f_eval[self.data[key][\"type\"]](obj)\nexcept Exception:\nwarnings.warn(\n\"Failed to build {}.\\n Returning the NWB object for manual inspection\".format(\nself.data[key][\"type\"]\n),\nstacklevel=2,\n)\ndata = obj\nself.data[key] = data\nreturn data\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n</code></pre>"},{"location":"reference/io/loader/","title":"Loader","text":""},{"location":"reference/io/loader/#pynapple.io.loader","title":"pynapple.io.loader","text":"<p>BaseLoader is the general class for loading session with pynapple.</p> <p>@author: Guillaume Viejo</p>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader","title":"BaseLoader","text":"<p>             Bases: <code>object</code></p> <p>General loader for epochs and tracking data</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>class BaseLoader(object):\n\"\"\"\n    General loader for epochs and tracking data\n    \"\"\"\ndef __init__(self, path=None):\nself.path = path\nstart_gui = True\n# Check if a pynapplenwb folder exist to bypass GUI\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nstart_gui = False\nself.load_data(path)\n# Starting the GUI\nif start_gui:\nwarnings.warn(\nget_deprecation_text(), category=DeprecationWarning, stacklevel=2\n)\napp = App()\nwindow = BaseLoaderGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# Extracting all the information from gui loader\nif window.status:\nself.session_information = window.session_information\nself.subject_information = window.subject_information\nself.name = self.session_information[\"name\"]\nself.tracking_frequency = window.tracking_frequency\nself.position = self._make_position(\nwindow.tracking_parameters,\nwindow.tracking_method,\nwindow.tracking_frequency,\nwindow.epochs,\nwindow.time_units_epochs,\nwindow.tracking_alignment,\n)\nself.epochs = self._make_epochs(window.epochs, window.time_units_epochs)\nself.time_support = self._join_epochs(\nwindow.epochs, window.time_units_epochs\n)\n# Save the data\nself.create_nwb_file(path)\ndef load_default_csv(self, csv_file):\n\"\"\"\n        Load tracking data. The default csv should have the time index in the first column in seconds.\n        If no header is provided, the column names will be the column index.\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\ndef load_optitrack_csv(self, csv_file):\n\"\"\"\n        Load tracking data exported with Optitrack.\n        By default, the function reads rows 4 and 5 to build the column names.\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Raises\n        ------\n        RuntimeError\n            If header names are unknown. Should be 'Position' and 'Rotation'\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\ndef load_dlc_csv(self, csv_file):\n\"\"\"\n        Load tracking data exported with DeepLabCut\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\ndef load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n        Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n        Parameters\n        ----------\n        ttl_file : str\n            File name\n        n_channels : int, optional\n            The number of channels in the binary file.\n        channel : int, optional\n            Which channel contains the TTL\n        bytes_size : int, optional\n            Bytes size of the binary file.\n        fs : float, optional\n            Sampling frequency of the binary file\n        Returns\n        -------\n        pd.Series\n            A series containing the time index of the TTL.\n        \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\ndef _make_position(\nself, parameters, method, frequency, epochs, time_units, alignment\n):\n\"\"\"\n        Make the position TSDFrame with the parameters extracted from the GUI.\n        \"\"\"\nif len(parameters.index) == 0:\nreturn None\nelse:\nif len(epochs) == 0:\nepochs.loc[0, \"start\"] = 0.0\nframes = []\ntime_supports_starts = []\ntime_support_ends = []\nfor i in range(len(parameters)):\nif method.lower() == \"optitrack\":\nposition = self.load_optitrack_csv(parameters.loc[i, \"csv\"])\nelif method.lower() == \"deep lab cut\":\nposition = self.load_dlc_csv(parameters.loc[i, \"csv\"])\nelif method.lower() == \"default\":\nposition = self.load_default_csv(parameters.loc[i, \"csv\"])\nif alignment.lower() == \"local\":\nstart_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"start\"], time_units\n)\nend_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"end\"], time_units\n)\ntimestamps = (\nposition.index.values\n+ nap.return_timestamps(start_epoch, \"s\")[0]\n)\n# Make sure timestamps are within the epochs\nidx = np.where(timestamps &lt; end_epoch)[0]\nposition = position.iloc[idx]\nposition.index = pd.Index(timestamps[idx])\nif alignment.lower() == \"ttl\":\nttl = self.load_ttl_pulse(\nttl_file=parameters.loc[i, \"ttl\"],\ntracking_frequency=frequency,\nn_channels=int(parameters.loc[i, \"n_channels\"]),\nchannel=int(parameters.loc[i, \"tracking_channel\"]),\nbytes_size=int(parameters.loc[i, \"bytes_size\"]),\nfs=float(parameters.loc[i, \"fs\"]),\nthreshold=float(parameters.loc[i, \"threshold\"]),\n)\nif len(ttl):\nlength = np.minimum(len(ttl), len(position))\nttl = ttl.iloc[0:length]\nposition = position.iloc[0:length]\nelse:\nraise RuntimeError(\n\"No ttl detected for {}\".format(parameters.loc[i, \"ttl\"])\n)\n# Make sure start epochs in seconds\n# start_epoch = format_timestamp(\n#     epochs.loc[parameters.loc[f, \"epoch\"], \"start\"], time_units\n# )\nstart_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"start\"], time_units\n)\ntimestamps = start_epoch + ttl.index.values\nposition.index = pd.Index(timestamps)\nframes.append(position)\ntime_supports_starts.append(position.index[0])\ntime_support_ends.append(position.index[-1])\nposition = pd.concat(frames)\ntime_supports = nap.IntervalSet(\nstart=time_supports_starts, end=time_support_ends, time_units=\"s\"\n)\n# Specific to optitrACK\nif set([\"rx\", \"ry\", \"rz\"]).issubset(position.columns):\nposition[[\"ry\", \"rx\", \"rz\"]] *= np.pi / 180\nposition[[\"ry\", \"rx\", \"rz\"]] += 2 * np.pi\nposition[[\"ry\", \"rx\", \"rz\"]] %= 2 * np.pi\nposition = nap.TsdFrame(\nt=position.index.values,\nd=position.values,\ncolumns=position.columns.values,\ntime_support=time_supports,\ntime_units=\"s\",\n)\nreturn position\ndef _make_epochs(self, epochs, time_units=\"s\"):\n\"\"\"\n        Split GUI epochs into dict of epochs\n        \"\"\"\nlabels = epochs.groupby(\"label\").groups\nisets = {}\nfor lbs in labels.keys():\ntmp = epochs.loc[labels[lbs]]\nisets[lbs] = nap.IntervalSet(\nstart=tmp[\"start\"], end=tmp[\"end\"], time_units=time_units\n)\nreturn isets\ndef _join_epochs(self, epochs, time_units=\"s\"):\n\"\"\"\n        To create the global time support of the data\n        \"\"\"\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nisets = nap.IntervalSet(\nstart=epochs[\"start\"].sort_values(),\nend=epochs[\"end\"].sort_values(),\ntime_units=time_units,\n)\niset = isets.merge_close_intervals(1, time_units=\"us\")\nif len(iset):\nreturn iset\nelse:\nreturn None\ndef create_nwb_file(self, path):\n\"\"\"\n        Initialize the NWB file in the folder pynapplenwb within the data folder.\n        Parameters\n        ----------\n        path : str\n            The path to save the data\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\ndef load_data(self, path):\n\"\"\"\n        Load NWB data save with pynapple in the pynapplenwb folder\n        Parameters\n        ----------\n        path : str\n            Path to the session folder\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\ndef save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n        Add epochs to the NWB file (e.g. ripples epochs)\n        See pynwb.epoch.TimeIntervals\n        Parameters\n        ----------\n        iset : IntervalSet\n            The intervalSet to save\n        name : str\n            The name in the nwb file\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\ndef save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n        Save timestamps in the NWB file (e.g. ripples time) with the time support.\n        See pynwb.base.TimeSeries\n        Parameters\n        ----------\n        tsd : TsdFrame\n            _\n        name : str\n            _\n        description : str, optional\n            _\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_intervals(self, name):\n\"\"\"\n        Load epochs from the NWB file (e.g. 'ripples')\n        Parameters\n        ----------\n        name : str\n            The name in the nwb file\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\ndef load_nwb_timeseries(self, name):\n\"\"\"\n        Load timestamps in the NWB file (e.g. ripples time)\n        Parameters\n        ----------\n        name : str\n            _\n        Returns\n        -------\n        Tsd\n            _\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"reference/io/loader_gui/","title":"Loader gui","text":""},{"location":"reference/io/loader_gui/#pynapple.io.loader_gui","title":"pynapple.io.loader_gui","text":""},{"location":"reference/io/loader_gui/#pynapple.io.loader_gui.EntryPopup","title":"EntryPopup","text":"<p>             Bases: <code>Entry</code></p> Source code in <code>pynapple/io/loader_gui.py</code> <pre><code>class EntryPopup(ttk.Entry):\ndef __init__(self, parent, iid, column, text, table, **kw):\nttk.Style().configure(\"pad.TEntry\", padding=\"1 1 1 1\")\nsuper().__init__(parent, style=\"pad.TEntry\", **kw)\nself.tv = parent\nself.iid = iid\nself.column = column\nself.table = table\nself.insert(0, text)\nself[\"exportselection\"] = False\nself.focus_force()\nself.select_all()\nself.bind(\"&lt;Return&gt;\", self.on_return)\nself.bind(\"&lt;Control-a&gt;\", self.select_all)\nself.bind(\"&lt;Escape&gt;\", lambda *ignore: self.destroy())\ndef on_return(self, event):\nrowid = self.tv.focus()\nvals = self.tv.item(rowid, \"values\")\nvals = list(vals)\nvals[self.column] = self.get()\nself.tv.item(rowid, values=vals)\nself.destroy()\nself.table.loc[int(rowid), self.table.columns[int(self.column)]] = vals[\nself.column\n]\ndef select_all(self, *ignore):\n\"\"\"Set selection on the whole text\"\"\"\nself.selection_range(0, \"end\")\n# returns 'break' to interrupt default key-bindings\nreturn \"break\"\n</code></pre>"},{"location":"reference/io/loader_gui/#pynapple.io.loader_gui.EntryPopup.select_all","title":"select_all","text":"<pre><code>select_all(*ignore)\n</code></pre> <p>Set selection on the whole text</p> Source code in <code>pynapple/io/loader_gui.py</code> <pre><code>def select_all(self, *ignore):\n\"\"\"Set selection on the whole text\"\"\"\nself.selection_range(0, \"end\")\n# returns 'break' to interrupt default key-bindings\nreturn \"break\"\n</code></pre>"},{"location":"reference/io/misc/","title":"Misc","text":""},{"location":"reference/io/misc/#pynapple.io.misc","title":"pynapple.io.misc","text":"<p>Various io functions</p>"},{"location":"reference/io/misc/#pynapple.io.misc.load_file","title":"load_file","text":"<pre><code>load_file(path)\n</code></pre> <p>Load file. Current format supported is (npz,nwb,)</p> <p>.npz -&gt; If the file is compatible with a pynapple format, the function will return a pynapple object. Otherwise, the function will return the output of numpy.load</p> <p>.nwb -&gt; Return the pynapple.io.NWBFile class wrapping the NWBFile</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file</p> required <p>Returns:</p> Type Description <code>(Tsd, TsdFrame, Ts, IntervalSet, TsGroup, NWBFile)</code> <p>One of the 5 pynapple objects or pynapple.io.NWBFile</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file is missing</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_file(path):\n\"\"\"Load file. Current format supported is (npz,nwb,)\n    .npz -&gt; If the file is compatible with a pynapple format, the function will return a pynapple object.\n    Otherwise, the function will return the output of numpy.load\n    .nwb -&gt; Return the pynapple.io.NWBFile class wrapping the NWBFile\n    Parameters\n    ----------\n    path : str\n        Path to the file\n    Returns\n    -------\n    (Tsd, TsdFrame, Ts, IntervalSet, TsGroup, pynapple.io.NWBFile)\n        One of the 5 pynapple objects or pynapple.io.NWBFile\n    Raises\n    ------\n    FileNotFoundError\n        If file is missing\n    \"\"\"\nif os.path.isfile(path):\nif path.endswith(\".npz\"):\nreturn NPZFile(path).load()\nelif path.endswith(\".nwb\"):\nreturn NWBFile(path)\nelse:\nraise RuntimeError(\"File format not supported\")\nelse:\nraise FileNotFoundError(\"File {} does not exist\".format(path))\n</code></pre>"},{"location":"reference/io/misc/#pynapple.io.misc.load_folder","title":"load_folder","text":"<pre><code>load_folder(path)\n</code></pre> <p>Load folder containing files or other folder. Pynapple will walk throught the subfolders to detect compatible npz files or nwb files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the folder</p> required <p>Returns:</p> Type Description <code>Folder</code> <p>A dictionnary-like class containing all the sub-folders and compatible files (i.e. npz, nwb)</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If folder is missing</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_folder(path):\n\"\"\"Load folder containing files or other folder.\n    Pynapple will walk throught the subfolders to detect compatible npz files\n    or nwb files.\n    Parameters\n    ----------\n    path : str\n        Path to the folder\n    Returns\n    -------\n    Folder\n        A dictionnary-like class containing all the sub-folders and compatible files (i.e. npz, nwb)\n    Raises\n    ------\n    RuntimeError\n        If folder is missing\n    \"\"\"\nif os.path.isdir(path):\nreturn Folder(path)\nelse:\nraise RuntimeError(\"Folder {} does not exist\".format(path))\n</code></pre>"},{"location":"reference/io/misc/#pynapple.io.misc.load_session","title":"load_session","text":"<pre><code>load_session(path=None, session_type=None)\n</code></pre> <p>%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % WARNING : THIS FUNCTION IS DEPRECATED % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% General Loader for</p> <ul> <li> <p>Neurosuite</p> </li> <li> <p>Phy</p> </li> <li> <p>Minian</p> </li> <li> <p>Inscopix-cnmfe</p> </li> <li> <p>Matlab-cnmfe</p> </li> <li> <p>Suite2p</p> </li> <li>None for default session.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to load the data</p> <code>None</code> <code>session_type</code> <code>str</code> <p>Can be 'neurosuite', 'phy', 'minian', 'inscopix-cnmfe', 'cnmfe-matlab', 'suite2p' or None for default loader.</p> <code>None</code> <p>Returns:</p> Type Description <code>Session</code> <p>A class holding all the data from the session.</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_session(path=None, session_type=None):\n\"\"\"\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    % WARNING : THIS FUNCTION IS DEPRECATED %\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    General Loader for\n    - Neurosuite\\n\n    - Phy\\n\n    - Minian\\n\n    - Inscopix-cnmfe\\n\n    - Matlab-cnmfe\\n\n    - Suite2p\n    - None for default session.\n    Parameters\n    ----------\n    path : str, optional\n        The path to load the data\n    session_type : str, optional\n        Can be 'neurosuite', 'phy',\n        'minian', 'inscopix-cnmfe', 'cnmfe-matlab',\n        'suite2p' or None for default loader.\n    Returns\n    -------\n    Session\n        A class holding all the data from the session.\n    \"\"\"\nif path:\nif not os.path.isdir(path):\nraise RuntimeError(\"Path {} is not found.\".format(path))\nif isinstance(session_type, str):\nsession_type = session_type.lower()\nif session_type == \"neurosuite\":\nreturn NeuroSuite(path)\nelif session_type == \"phy\":\nreturn Phy(path)\nelif session_type == \"inscopix-cnmfe\":\nreturn InscopixCNMFE(path)\nelif session_type == \"minian\":\nreturn Minian(path)\nelif session_type == \"cnmfe-matlab\":\nreturn CNMF_E(path)\nelif session_type == \"suite2p\":\nreturn Suite2P(path)\nelse:\nreturn BaseLoader(path)\n</code></pre>"},{"location":"reference/io/misc/#pynapple.io.misc.load_eeg","title":"load_eeg","text":"<pre><code>load_eeg(\nfilepath,\nchannel=None,\nn_channels=None,\nfrequency=None,\nprecision=\"int16\",\nbytes_size=2,\n)\n</code></pre> <p>Standalone function to load eeg/lfp/dat file in binary format.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the eeg file</p> required <code>channel</code> <code>int or list of int</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>n_channels</code> <code>int</code> <p>Number of channels</p> <code>None</code> <code>frequency</code> <code>float</code> <p>Sampling rate of the file</p> <code>None</code> <code>precision</code> <code>str</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p>"},{"location":"reference/io/misc/#pynapple.io.misc.load_eeg--deleted-parameters","title":"Deleted Parameters","text":"<p>extension : str, optional     The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_eeg(\nfilepath,\nchannel=None,\nn_channels=None,\nfrequency=None,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Standalone function to load eeg/lfp/dat file in binary format.\n    Parameters\n    ----------\n    filepath : str\n        The path to the eeg file\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    n_channels : int, optional\n        Number of channels\n    frequency : float, optional\n        Sampling rate of the file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the binary file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    Deleted Parameters\n    ------------------\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    \"\"\"\n# Need to check if a xml file exists\npath = os.path.dirname(filepath)\nbasename = os.path.basename(filepath).split(\".\")[0]\nlistdir = os.listdir(path)\nif frequency is None or n_channels is None:\nif basename + \".xml\" in listdir:\nxmlpath = os.path.join(path, basename + \".xml\")\nxmldoc = minidom.parse(xmlpath)\nelse:\nraise RuntimeError(\n\"Can't find xml file; please specify sampling frequency or number of channels\"\n)\nif frequency is None:\nif filepath.endswith(\".dat\"):\nfs_dat = int(\nxmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nfrequency = fs_dat\nelif filepath.endswith((\".lfp\", \".eeg\")):\nfs_eeg = int(\nxmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nfrequency = fs_eeg\nif n_channels is None:\nn_channels = int(\nxmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn fp\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"reference/io/misc/#pynapple.io.misc.append_NWB_LFP","title":"append_NWB_LFP","text":"<pre><code>append_NWB_LFP(path, lfp, channel=None)\n</code></pre> <p>Standalone function for adding lfp/eeg to already existing nwb files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data. The function will looks for a nwb file in path or in path/pynapplenwb.</p> required <code>lfp</code> <code>Tsd or TsdFrame</code> <p>Description</p> required <code>channel</code> <code>None</code> <p>channel number in int ff lfp is a Tsd</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the nwb file </p> <p>If no channel is specify when passing a Tsd</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def append_NWB_LFP(path, lfp, channel=None):\n\"\"\"Standalone function for adding lfp/eeg to already existing nwb files.\n    Parameters\n    ----------\n    path : str\n        The path to the data. The function will looks for a nwb file in path\n        or in path/pynapplenwb.\n    lfp : Tsd or TsdFrame\n        Description\n    channel : None, optional\n        channel number in int ff lfp is a Tsd\n    Raises\n    ------\n    RuntimeError\n        If can't find the nwb file \\n\n        If no channel is specify when passing a Tsd\n    \"\"\"\nnew_path = os.path.join(path, \"pynapplenwb\")\nnwb_path = \"\"\nif os.path.exists(new_path):\nnwbfilename = [f for f in os.listdir(new_path) if f.endswith(\".nwb\")]\nif len(nwbfilename):\nnwb_path = os.path.join(path, \"pynapplenwb\", nwbfilename[0])\nelse:\nnwbfilename = [f for f in os.listdir(path) if f.endswith(\".nwb\")]\nif len(nwbfilename):\nnwb_path = os.path.join(path, \"pynapplenwb\", nwbfilename[0])\nif len(nwb_path) == 0:\nraise RuntimeError(\"Can't find nwb file in {}\".format(path))\nif isinstance(lfp, nap.TsdFrame):\nchannels = lfp.columns.values\nelif isinstance(lfp, nap.Tsd):\nif isinstance(channel, int):\nchannels = [channel]\nelse:\nraise RuntimeError(\"Please specify which channel it is.\")\nio = NWBHDF5IO(nwb_path, \"r+\")\nnwbfile = io.read()\nall_table_region = nwbfile.create_electrode_table_region(\nregion=channels, description=\"\", name=\"electrodes\"\n)\nlfp_electrical_series = ElectricalSeries(\nname=\"ElectricalSeries\",\ndata=lfp.values,\ntimestamps=lfp.index.values,\nelectrodes=all_table_region,\n)\nlfp = LFP(electrical_series=lfp_electrical_series)\necephys_module = nwbfile.create_processing_module(\nname=\"ecephys\", description=\"processed extracellular electrophysiology data\"\n)\necephys_module.add(lfp)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/","title":"Neurosuite","text":""},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite","title":"pynapple.io.neurosuite","text":"<p> DEPRECATED: This will be removed in version 1.0.0. Check nwbmatic or neuroconv instead.</p> <p>Class and functions for loading data processed with the Neurosuite (Klusters, Neuroscope, NDmanager)</p> <p>@author: Guillaume Viejo</p>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite","title":"NeuroSuite","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader for kluster data</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>class NeuroSuite(BaseLoader):\n\"\"\"\n    Loader for kluster data\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Instantiate the data class from a neurosuite folder.\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nself.time_support = None\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_neurosuite = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_nwb_spikes(path)\nif success:\nloading_neurosuite = False\n# Bypass if data have already been transfered to nwb\nif loading_neurosuite:\nself.load_neurosuite_xml(path)\n# print(\"XML loaded\")\n# To label the electrodes groups\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.group_to_channel)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# print(\"GUI DONE\")\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_neurosuite_spikes(path, self.basename, self.time_support)\nself.save_data(path)\ndef load_neurosuite_spikes(self, path, basename, time_support=None, fs=20000.0):\n\"\"\"\n        Read the clus and res files and convert to NWB.\n        Instantiate automatically a TsGroup object.\n        Parameters\n        ----------\n        path : str\n            The path to the data\n        basename : str\n            Basename of the clu and res files.\n        time_support : IntevalSet, optional\n            The time support of the data\n        fs : float, optional\n            Sampling rate of the recording.\n        Raises\n        ------\n        RuntimeError\n            If number of clu and res are not equal.\n        \"\"\"\nfiles = os.listdir(path)\nclu_files = np.sort([f for f in files if \".clu.\" in f and f[0] != \".\"])\nres_files = np.sort([f for f in files if \".res.\" in f and f[0] != \".\"])\nclu1 = np.sort([int(f.split(\".\")[-1]) for f in clu_files])\nclu2 = np.sort([int(f.split(\".\")[-1]) for f in res_files])\nif len(clu_files) != len(res_files) or not (clu1 == clu2).any():\nraise RuntimeError(\n\"Not the same number of clu and res files in \" + path + \"; Exiting ...\"\n)\ncount = 0\nspikes = {}\ngroup = pd.Series(dtype=np.int32)\nfor i, s in zip(range(len(clu_files)), clu1):\nclu = np.genfromtxt(\nos.path.join(path, basename + \".clu.\" + str(s)), dtype=np.int32\n)[1:]\nif np.max(clu) &gt; 1:  # getting rid of mua and noise\nres = np.genfromtxt(os.path.join(path, basename + \".res.\" + str(s)))\ntmp = np.unique(clu).astype(int)\nidx_clu = tmp[tmp &gt; 1]\nidx_out = np.arange(count, count + len(idx_clu))\nfor j, k in zip(idx_clu, idx_out):\nt = res[clu == j] / fs\nspikes[k] = nap.Ts(t=t, time_units=\"s\")\ngroup.loc[k] = s\ncount += len(idx_clu)\ngroup = group - 1  # better to start it a 0\nself.spikes = nap.TsGroup(\nspikes, time_support=time_support, time_units=\"s\", group=group\n)\n# adding some information to help parse the neurons\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\ndef load_neurosuite_xml(self, path):\n\"\"\"\n        path should be the folder session containing the XML file\n        Function reads :\n        1. the number of channels\n        2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder\n            eeg file first if both are present or both are absent\n        3. the mappings shanks to channels as a dict\n        Parameters\n        ----------\n        path: str\n            The path to the data\n        Raises\n        ------\n        RuntimeError\n            If path does not contain the xml file.\n        \"\"\"\nlistdir = os.listdir(path)\nxmlfiles = [f for f in listdir if f.endswith(\".xml\")]\nif not len(xmlfiles):\nraise RuntimeError(\"Path {} contains no xml files;\".format(path))\nsys.exit()\nnew_path = os.path.join(path, xmlfiles[0])\nself.xmldoc = minidom.parse(new_path)\nself.nChannels = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nself.fs_dat = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nself.fs_eeg = int(\nself.xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nself.group_to_channel = {}\ngroups = (\nself.xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n.getElementsByTagName(\"channelGroups\")[0]\n.getElementsByTagName(\"group\")\n)\nfor i in range(len(groups)):\nself.group_to_channel[i] = np.array(\n[\nint(child.firstChild.data)\nfor child in groups[i].getElementsByTagName(\"channel\")\n]\n)\nreturn\ndef save_data(self, path):\n\"\"\"\n        Save the data to NWB format.\n        Parameters\n        ----------\n        path : str\n            The path to save the data\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.group_to_channel:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.group_to_channel[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_spikes(self, path):\n\"\"\"\n        Read the NWB spikes to extract the spike times.\n        Parameters\n        ----------\n        path : str\n            The path to the data\n        Returns\n        -------\n        TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn False\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\ndef load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n        Load the LFP.\n        Parameters\n        ----------\n        filename : str, optional\n            The filename of the lfp file.\n            It can be useful it multiple dat files are present in the data directory\n        channel : int or list of int, optional\n            The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n        extension : str, optional\n            The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n        frequency : float, optional\n            Default 1250 Hz for the eeg file\n        precision : str, optional\n            The precision of the binary file\n        bytes_size : int, optional\n            Bytes size of the lfp file\n        Raises\n        ------\n        RuntimeError\n            If can't find the lfp/eeg/dat file\n        Returns\n        -------\n        Tsd or TsdFrame\n            The lfp in a time series format\n        \"\"\"\nif filename is not None:\nfilepath = os.path.join(self.path, filename)\nelse:\nlistdir = os.listdir(self.path)\neegfile = [f for f in listdir if f.endswith(extension)]\nif not len(eegfile):\nraise RuntimeError(\n\"Path {} contains no {} files;\".format(self.path, extension)\n)\nfilepath = os.path.join(self.path, eegfile[0])\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\ndef read_neuroscope_intervals(self, name=None, path2file=None):\n\"\"\"\n        This function reads .evt files in which odd raws indicate the beginning\n        of the time series and the even raws are the ends.\n        If the file is present in the nwb, provide the just the name. If the file\n        is not present in the nwb, it loads the events from the nwb directory.\n        If just the path is provided but not the name, it takes the name from the file.\n        Parameters\n        ----------\n        name: str\n            name of the epoch in the nwb file, e.g. \"rem\" or desired name save\n            the data in the nwb.\n        path2file: str\n            Path of the file you want to load.\n        Returns\n        -------\n        IntervalSet\n            Contains two columns corresponding to the start and end of the intervals.\n        \"\"\"\nif name:\nisets = self.load_nwb_intervals(name)\nif isinstance(isets, nap.IntervalSet):\nreturn isets\nif name is not None and path2file is None:\npath2file = os.path.join(self.path, self.basename + \".\" + name + \".evt\")\nif path2file is not None:\ntry:\n# df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None)\ntmp = np.genfromtxt(path2file)[:, 0]\ndf = tmp.reshape(len(tmp) // 2, 2)\nexcept ValueError:\nprint(\"specify a valid name\")\nisets = nap.IntervalSet(df[:, 0], df[:, 1], time_units=\"ms\")\nif name is None:\nname = path2file.split(\".\")[-2]\nprint(\"*** saving file in the nwb as\", name)\nself.save_nwb_intervals(isets, name)\nelse:\nraise ValueError(\"specify a valid path\")\nreturn isets\ndef write_neuroscope_intervals(self, extension, isets, name):\n\"\"\"Write events to load with neuroscope (e.g. ripples start and ends)\n        Parameters\n        ----------\n        extension : str\n            The extension of the file (e.g. basename.evt.py.rip)\n        isets : IntervalSet\n            The IntervalSet to write\n        name : str\n            The name of the events (e.g. Ripples)\n        \"\"\"\nstart = isets.as_units(\"ms\")[\"start\"].values\nends = isets.as_units(\"ms\")[\"end\"].values\ndatatowrite = np.vstack((start, ends)).T.flatten()\nn = len(isets)\ntexttowrite = np.vstack(\n(\n(np.repeat(np.array([name + \" start\"]), n)),\n(np.repeat(np.array([name + \" end\"]), n)),\n)\n).T.flatten()\nevt_file = os.path.join(self.path, self.basename + extension)\nf = open(evt_file, \"w\")\nfor t, n in zip(datatowrite, texttowrite):\nf.writelines(\"{:1.6f}\".format(t) + \"\\t\" + n + \"\\n\")\nf.close()\nreturn\ndef load_mean_waveforms(self, epoch=None, waveform_window=None, spike_count=1000):\n\"\"\"\n        Load the mean waveforms from a dat file.\n        Parameters\n        ----------\n        epoch : IntervalSet\n            default = None\n            Restrict spikes to an epoch.\n        waveform_window : IntervalSet\n            default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms')\n            Limit waveform extraction before and after spike time\n        spike_count : int\n            default = 1000\n            Number of spikes used per neuron for the calculation of waveforms\n        Returns\n        -------\n        dictionary\n            the waveforms for all neurons\n        pandas.Series\n            the channel with the maximum waveform for each neuron\n        \"\"\"\nif not isinstance(waveform_window, nap.IntervalSet):\nwaveform_window = nap.IntervalSet(start=-0.5, end=1, time_units=\"ms\")\nspikes = self.spikes\nif not os.path.exists(self.path):  # check if path exists\nprint(\"The path \" + self.path + \" doesn't exist; Exiting ...\")\nsys.exit()\n# Load XML INFO\nself.load_neurosuite_xml(self.path)\nn_channels = self.nChannels\nfs = self.fs_dat\ngroup_to_channel = self.group_to_channel\ngroup = spikes.get_info(\"group\")\n# Check if there is an epoch, restrict spike times to epoch\nif epoch is not None:\nif type(epoch) is not nap.IntervalSet:\nprint(\"Epoch must be an IntervalSet\")\nsys.exit()\nelse:\nprint(\"Restricting spikes to epoch\")\nspikes = spikes.restrict(epoch)\nepstart = int(epoch.as_units(\"s\")[\"start\"].values[0] * fs)\nepend = int(epoch.as_units(\"s\")[\"end\"].values[0] * fs)\n# Find dat file\nfiles = os.listdir(self.path)\ndat_files = np.sort([f for f in files if \"dat\" in f and f[0] != \".\"])\n# Need n_samples collected in the entire recording from dat file to load\nfile = os.path.join(self.path, dat_files[0])\nf = open(\nfile, \"rb\"\n)  # open file to get number of samples collected in the entire recording\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\n# map to memory all samples for all channels, channels are numbered according to neuroscope number\nfp = np.memmap(file, np.int16, \"r\", shape=(n_samples, n_channels))\n# convert spike times to spikes in sample number\nsample_spikes = {\nneuron: (spikes[neuron].as_units(\"s\").index.values * fs).astype(\"int\")\nfor neuron in spikes\n}\n# prep for waveforms\noverlap = int(\nwaveform_window.tot_length(time_units=\"s\")\n)  # one spike's worth of overlap between windows\nwaveform_window = abs(np.array(waveform_window.as_units(\"s\"))[0] * fs).astype(\nint\n)  # convert time to sample number\nneuron_waveforms = {\nn: np.zeros([np.sum(waveform_window), len(group_to_channel[group[n]])])\nfor n in sample_spikes\n}\n# divide dat file into batches that slightly overlap for faster loading\nbatch_size = 3000000\nwindows = np.arange(0, int(endoffile / n_channels / bytes_size), batch_size)\nif epoch is not None:\nprint(\"Restricting dat file to epoch\")\nwindows = windows[(windows &gt;= epstart) &amp; (windows &lt;= epend)]\nbatches = []\nfor (\ni\n) in windows:  # make overlapping batches from the beginning to end of recording\nif i == windows[-1]:  # the last batch cannot overlap with the next one\nbatches.append([i, n_samples])\nelse:\nbatches.append([i, i + batch_size + overlap])\nbatches = [np.int32(batch) for batch in batches]\nsample_counted_spikes = {}\nfor index, neuron in enumerate(sample_spikes):\nif len(sample_spikes[neuron]) &gt;= spike_count:\nsample_counted_spikes[neuron] = np.array(\nnp.random.choice(list(sample_spikes[neuron]), spike_count)\n)\nelif len(sample_spikes[neuron]) &lt; spike_count:\nprint(\n\"Not enough spikes in neuron \" + str(index) + \"... using all spikes\"\n)\nsample_counted_spikes[neuron] = sample_spikes[neuron]\n# Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file\nspike_check = np.array(\n[\nint(spikes_neuron)\nfor spikes_neuron in sample_counted_spikes[neuron]\nfor neuron in sample_counted_spikes\n]\n)\nfor index, timestep in enumerate(batches):\nprint(\nf\"Extracting waveforms from dat file: window {index+1} / {len(windows)}\",\nend=\"\\r\",\n)\nif (\nlen(\nspike_check[\n(timestep[0] &lt; spike_check) &amp; (timestep[1] &gt; spike_check)\n]\n)\n== 0\n):\ncontinue  # if there are no spikes for any neurons in this batch, skip and go to the next one\n# Load dat file for timestep\ntmp = pd.DataFrame(\ndata=fp[timestep[0] : timestep[1], :],\ncolumns=np.arange(n_channels),\nindex=range(timestep[0], timestep[1]),\n)  # load dat file\n# Check if any spikes are present\nfor neuron in sample_counted_spikes:\nneurontmp = sample_counted_spikes[neuron]\ntmp2 = neurontmp[(timestep[0] &lt; neurontmp) &amp; (timestep[1] &gt; neurontmp)]\nif len(neurontmp) == 0:\ncontinue  # skip neuron if it has no spikes in this batch\ntmpn = tmp[\ngroup_to_channel[group[neuron]]\n]  # restrict dat file to the channel group of the neuron\nfor time in tmp2:  # add each spike waveform to neuron_waveform\nspikewindow = tmpn.loc[\ntime - waveform_window[0] : time + waveform_window[1] - 1\n]  # waveform for this spike time\ntry:\nneuron_waveforms[neuron] += spikewindow.values\nexcept (\nException\n):  # ignore if full waveform is not present in this batch\npass\nmeanwf = {\nn: pd.DataFrame(\ndata=np.array(neuron_waveforms[n]) / spike_count,\ncolumns=np.arange(len(group_to_channel[group[n]])),\nindex=np.array(np.arange(-waveform_window[0], waveform_window[1])) / fs,\n)\nfor n in sample_counted_spikes\n}\n# find the max channel for each neuron\nmaxch = pd.Series(\ndata=[meanwf[n][meanwf[n].loc[0].idxmin()].name for n in meanwf],\nindex=spikes.keys(),\n)\nreturn meanwf, maxch\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Instantiate the data class from a neurosuite folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Instantiate the data class from a neurosuite folder.\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nself.time_support = None\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_neurosuite = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_nwb_spikes(path)\nif success:\nloading_neurosuite = False\n# Bypass if data have already been transfered to nwb\nif loading_neurosuite:\nself.load_neurosuite_xml(path)\n# print(\"XML loaded\")\n# To label the electrodes groups\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.group_to_channel)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# print(\"GUI DONE\")\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_neurosuite_spikes(path, self.basename, self.time_support)\nself.save_data(path)\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_spikes","title":"load_neurosuite_spikes","text":"<pre><code>load_neurosuite_spikes(\npath, basename, time_support=None, fs=20000.0\n)\n</code></pre> <p>Read the clus and res files and convert to NWB. Instantiate automatically a TsGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <code>basename</code> <code>str</code> <p>Basename of the clu and res files.</p> required <code>time_support</code> <code>IntevalSet</code> <p>The time support of the data</p> <code>None</code> <code>fs</code> <code>float</code> <p>Sampling rate of the recording.</p> <code>20000.0</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If number of clu and res are not equal.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_neurosuite_spikes(self, path, basename, time_support=None, fs=20000.0):\n\"\"\"\n    Read the clus and res files and convert to NWB.\n    Instantiate automatically a TsGroup object.\n    Parameters\n    ----------\n    path : str\n        The path to the data\n    basename : str\n        Basename of the clu and res files.\n    time_support : IntevalSet, optional\n        The time support of the data\n    fs : float, optional\n        Sampling rate of the recording.\n    Raises\n    ------\n    RuntimeError\n        If number of clu and res are not equal.\n    \"\"\"\nfiles = os.listdir(path)\nclu_files = np.sort([f for f in files if \".clu.\" in f and f[0] != \".\"])\nres_files = np.sort([f for f in files if \".res.\" in f and f[0] != \".\"])\nclu1 = np.sort([int(f.split(\".\")[-1]) for f in clu_files])\nclu2 = np.sort([int(f.split(\".\")[-1]) for f in res_files])\nif len(clu_files) != len(res_files) or not (clu1 == clu2).any():\nraise RuntimeError(\n\"Not the same number of clu and res files in \" + path + \"; Exiting ...\"\n)\ncount = 0\nspikes = {}\ngroup = pd.Series(dtype=np.int32)\nfor i, s in zip(range(len(clu_files)), clu1):\nclu = np.genfromtxt(\nos.path.join(path, basename + \".clu.\" + str(s)), dtype=np.int32\n)[1:]\nif np.max(clu) &gt; 1:  # getting rid of mua and noise\nres = np.genfromtxt(os.path.join(path, basename + \".res.\" + str(s)))\ntmp = np.unique(clu).astype(int)\nidx_clu = tmp[tmp &gt; 1]\nidx_out = np.arange(count, count + len(idx_clu))\nfor j, k in zip(idx_clu, idx_out):\nt = res[clu == j] / fs\nspikes[k] = nap.Ts(t=t, time_units=\"s\")\ngroup.loc[k] = s\ncount += len(idx_clu)\ngroup = group - 1  # better to start it a 0\nself.spikes = nap.TsGroup(\nspikes, time_support=time_support, time_units=\"s\", group=group\n)\n# adding some information to help parse the neurons\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_xml","title":"load_neurosuite_xml","text":"<pre><code>load_neurosuite_xml(path)\n</code></pre> <p>path should be the folder session containing the XML file</p> <p>Function reads : 1. the number of channels 2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder     eeg file first if both are present or both are absent 3. the mappings shanks to channels as a dict</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>The path to the data</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If path does not contain the xml file.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_neurosuite_xml(self, path):\n\"\"\"\n    path should be the folder session containing the XML file\n    Function reads :\n    1. the number of channels\n    2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder\n        eeg file first if both are present or both are absent\n    3. the mappings shanks to channels as a dict\n    Parameters\n    ----------\n    path: str\n        The path to the data\n    Raises\n    ------\n    RuntimeError\n        If path does not contain the xml file.\n    \"\"\"\nlistdir = os.listdir(path)\nxmlfiles = [f for f in listdir if f.endswith(\".xml\")]\nif not len(xmlfiles):\nraise RuntimeError(\"Path {} contains no xml files;\".format(path))\nsys.exit()\nnew_path = os.path.join(path, xmlfiles[0])\nself.xmldoc = minidom.parse(new_path)\nself.nChannels = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nself.fs_dat = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nself.fs_eeg = int(\nself.xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nself.group_to_channel = {}\ngroups = (\nself.xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n.getElementsByTagName(\"channelGroups\")[0]\n.getElementsByTagName(\"group\")\n)\nfor i in range(len(groups)):\nself.group_to_channel[i] = np.array(\n[\nint(child.firstChild.data)\nfor child in groups[i].getElementsByTagName(\"channel\")\n]\n)\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.save_data","title":"save_data","text":"<pre><code>save_data(path)\n</code></pre> <p>Save the data to NWB format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def save_data(self, path):\n\"\"\"\n    Save the data to NWB format.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.group_to_channel:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.group_to_channel[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_nwb_spikes","title":"load_nwb_spikes","text":"<pre><code>load_nwb_spikes(path)\n</code></pre> <p>Read the NWB spikes to extract the spike times.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_nwb_spikes(self, path):\n\"\"\"\n    Read the NWB spikes to extract the spike times.\n    Parameters\n    ----------\n    path : str\n        The path to the data\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn False\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_lfp","title":"load_lfp","text":"<pre><code>load_lfp(\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n)\n</code></pre> <p>Load the LFP.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename of the lfp file. It can be useful it multiple dat files are present in the data directory</p> <code>None</code> <code>channel</code> <code>int or list of int</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>extension</code> <code>str</code> <p>The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> <code>'.eeg'</code> <code>frequency</code> <code>float</code> <p>Default 1250 Hz for the eeg file</p> <code>1250.0</code> <code>precision</code> <code>str</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the lfp file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Load the LFP.\n    Parameters\n    ----------\n    filename : str, optional\n        The filename of the lfp file.\n        It can be useful it multiple dat files are present in the data directory\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    frequency : float, optional\n        Default 1250 Hz for the eeg file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the lfp file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    \"\"\"\nif filename is not None:\nfilepath = os.path.join(self.path, filename)\nelse:\nlistdir = os.listdir(self.path)\neegfile = [f for f in listdir if f.endswith(extension)]\nif not len(eegfile):\nraise RuntimeError(\n\"Path {} contains no {} files;\".format(self.path, extension)\n)\nfilepath = os.path.join(self.path, eegfile[0])\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.read_neuroscope_intervals","title":"read_neuroscope_intervals","text":"<pre><code>read_neuroscope_intervals(name=None, path2file=None)\n</code></pre> <p>This function reads .evt files in which odd raws indicate the beginning of the time series and the even raws are the ends. If the file is present in the nwb, provide the just the name. If the file is not present in the nwb, it loads the events from the nwb directory. If just the path is provided but not the name, it takes the name from the file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>name of the epoch in the nwb file, e.g. \"rem\" or desired name save the data in the nwb.</p> <code>None</code> <p>path2file: str     Path of the file you want to load.</p> <p>Returns:</p> Type Description <code>IntervalSet</code> <p>Contains two columns corresponding to the start and end of the intervals.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def read_neuroscope_intervals(self, name=None, path2file=None):\n\"\"\"\n    This function reads .evt files in which odd raws indicate the beginning\n    of the time series and the even raws are the ends.\n    If the file is present in the nwb, provide the just the name. If the file\n    is not present in the nwb, it loads the events from the nwb directory.\n    If just the path is provided but not the name, it takes the name from the file.\n    Parameters\n    ----------\n    name: str\n        name of the epoch in the nwb file, e.g. \"rem\" or desired name save\n        the data in the nwb.\n    path2file: str\n        Path of the file you want to load.\n    Returns\n    -------\n    IntervalSet\n        Contains two columns corresponding to the start and end of the intervals.\n    \"\"\"\nif name:\nisets = self.load_nwb_intervals(name)\nif isinstance(isets, nap.IntervalSet):\nreturn isets\nif name is not None and path2file is None:\npath2file = os.path.join(self.path, self.basename + \".\" + name + \".evt\")\nif path2file is not None:\ntry:\n# df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None)\ntmp = np.genfromtxt(path2file)[:, 0]\ndf = tmp.reshape(len(tmp) // 2, 2)\nexcept ValueError:\nprint(\"specify a valid name\")\nisets = nap.IntervalSet(df[:, 0], df[:, 1], time_units=\"ms\")\nif name is None:\nname = path2file.split(\".\")[-2]\nprint(\"*** saving file in the nwb as\", name)\nself.save_nwb_intervals(isets, name)\nelse:\nraise ValueError(\"specify a valid path\")\nreturn isets\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.write_neuroscope_intervals","title":"write_neuroscope_intervals","text":"<pre><code>write_neuroscope_intervals(extension, isets, name)\n</code></pre> <p>Write events to load with neuroscope (e.g. ripples start and ends)</p> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>The extension of the file (e.g. basename.evt.py.rip)</p> required <code>isets</code> <code>IntervalSet</code> <p>The IntervalSet to write</p> required <code>name</code> <code>str</code> <p>The name of the events (e.g. Ripples)</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def write_neuroscope_intervals(self, extension, isets, name):\n\"\"\"Write events to load with neuroscope (e.g. ripples start and ends)\n    Parameters\n    ----------\n    extension : str\n        The extension of the file (e.g. basename.evt.py.rip)\n    isets : IntervalSet\n        The IntervalSet to write\n    name : str\n        The name of the events (e.g. Ripples)\n    \"\"\"\nstart = isets.as_units(\"ms\")[\"start\"].values\nends = isets.as_units(\"ms\")[\"end\"].values\ndatatowrite = np.vstack((start, ends)).T.flatten()\nn = len(isets)\ntexttowrite = np.vstack(\n(\n(np.repeat(np.array([name + \" start\"]), n)),\n(np.repeat(np.array([name + \" end\"]), n)),\n)\n).T.flatten()\nevt_file = os.path.join(self.path, self.basename + extension)\nf = open(evt_file, \"w\")\nfor t, n in zip(datatowrite, texttowrite):\nf.writelines(\"{:1.6f}\".format(t) + \"\\t\" + n + \"\\n\")\nf.close()\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_mean_waveforms","title":"load_mean_waveforms","text":"<pre><code>load_mean_waveforms(\nepoch=None, waveform_window=None, spike_count=1000\n)\n</code></pre> <p>Load the mean waveforms from a dat file.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>IntervalSet</code> <p>default = None Restrict spikes to an epoch.</p> <code>None</code> <code>waveform_window</code> <code>IntervalSet</code> <p>default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms') Limit waveform extraction before and after spike time</p> <code>None</code> <code>spike_count</code> <code>int</code> <p>default = 1000 Number of spikes used per neuron for the calculation of waveforms</p> <code>1000</code> <p>Returns:</p> Type Description <code>dictionary</code> <p>the waveforms for all neurons</p> <code>Series</code> <p>the channel with the maximum waveform for each neuron</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_mean_waveforms(self, epoch=None, waveform_window=None, spike_count=1000):\n\"\"\"\n    Load the mean waveforms from a dat file.\n    Parameters\n    ----------\n    epoch : IntervalSet\n        default = None\n        Restrict spikes to an epoch.\n    waveform_window : IntervalSet\n        default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms')\n        Limit waveform extraction before and after spike time\n    spike_count : int\n        default = 1000\n        Number of spikes used per neuron for the calculation of waveforms\n    Returns\n    -------\n    dictionary\n        the waveforms for all neurons\n    pandas.Series\n        the channel with the maximum waveform for each neuron\n    \"\"\"\nif not isinstance(waveform_window, nap.IntervalSet):\nwaveform_window = nap.IntervalSet(start=-0.5, end=1, time_units=\"ms\")\nspikes = self.spikes\nif not os.path.exists(self.path):  # check if path exists\nprint(\"The path \" + self.path + \" doesn't exist; Exiting ...\")\nsys.exit()\n# Load XML INFO\nself.load_neurosuite_xml(self.path)\nn_channels = self.nChannels\nfs = self.fs_dat\ngroup_to_channel = self.group_to_channel\ngroup = spikes.get_info(\"group\")\n# Check if there is an epoch, restrict spike times to epoch\nif epoch is not None:\nif type(epoch) is not nap.IntervalSet:\nprint(\"Epoch must be an IntervalSet\")\nsys.exit()\nelse:\nprint(\"Restricting spikes to epoch\")\nspikes = spikes.restrict(epoch)\nepstart = int(epoch.as_units(\"s\")[\"start\"].values[0] * fs)\nepend = int(epoch.as_units(\"s\")[\"end\"].values[0] * fs)\n# Find dat file\nfiles = os.listdir(self.path)\ndat_files = np.sort([f for f in files if \"dat\" in f and f[0] != \".\"])\n# Need n_samples collected in the entire recording from dat file to load\nfile = os.path.join(self.path, dat_files[0])\nf = open(\nfile, \"rb\"\n)  # open file to get number of samples collected in the entire recording\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\n# map to memory all samples for all channels, channels are numbered according to neuroscope number\nfp = np.memmap(file, np.int16, \"r\", shape=(n_samples, n_channels))\n# convert spike times to spikes in sample number\nsample_spikes = {\nneuron: (spikes[neuron].as_units(\"s\").index.values * fs).astype(\"int\")\nfor neuron in spikes\n}\n# prep for waveforms\noverlap = int(\nwaveform_window.tot_length(time_units=\"s\")\n)  # one spike's worth of overlap between windows\nwaveform_window = abs(np.array(waveform_window.as_units(\"s\"))[0] * fs).astype(\nint\n)  # convert time to sample number\nneuron_waveforms = {\nn: np.zeros([np.sum(waveform_window), len(group_to_channel[group[n]])])\nfor n in sample_spikes\n}\n# divide dat file into batches that slightly overlap for faster loading\nbatch_size = 3000000\nwindows = np.arange(0, int(endoffile / n_channels / bytes_size), batch_size)\nif epoch is not None:\nprint(\"Restricting dat file to epoch\")\nwindows = windows[(windows &gt;= epstart) &amp; (windows &lt;= epend)]\nbatches = []\nfor (\ni\n) in windows:  # make overlapping batches from the beginning to end of recording\nif i == windows[-1]:  # the last batch cannot overlap with the next one\nbatches.append([i, n_samples])\nelse:\nbatches.append([i, i + batch_size + overlap])\nbatches = [np.int32(batch) for batch in batches]\nsample_counted_spikes = {}\nfor index, neuron in enumerate(sample_spikes):\nif len(sample_spikes[neuron]) &gt;= spike_count:\nsample_counted_spikes[neuron] = np.array(\nnp.random.choice(list(sample_spikes[neuron]), spike_count)\n)\nelif len(sample_spikes[neuron]) &lt; spike_count:\nprint(\n\"Not enough spikes in neuron \" + str(index) + \"... using all spikes\"\n)\nsample_counted_spikes[neuron] = sample_spikes[neuron]\n# Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file\nspike_check = np.array(\n[\nint(spikes_neuron)\nfor spikes_neuron in sample_counted_spikes[neuron]\nfor neuron in sample_counted_spikes\n]\n)\nfor index, timestep in enumerate(batches):\nprint(\nf\"Extracting waveforms from dat file: window {index+1} / {len(windows)}\",\nend=\"\\r\",\n)\nif (\nlen(\nspike_check[\n(timestep[0] &lt; spike_check) &amp; (timestep[1] &gt; spike_check)\n]\n)\n== 0\n):\ncontinue  # if there are no spikes for any neurons in this batch, skip and go to the next one\n# Load dat file for timestep\ntmp = pd.DataFrame(\ndata=fp[timestep[0] : timestep[1], :],\ncolumns=np.arange(n_channels),\nindex=range(timestep[0], timestep[1]),\n)  # load dat file\n# Check if any spikes are present\nfor neuron in sample_counted_spikes:\nneurontmp = sample_counted_spikes[neuron]\ntmp2 = neurontmp[(timestep[0] &lt; neurontmp) &amp; (timestep[1] &gt; neurontmp)]\nif len(neurontmp) == 0:\ncontinue  # skip neuron if it has no spikes in this batch\ntmpn = tmp[\ngroup_to_channel[group[neuron]]\n]  # restrict dat file to the channel group of the neuron\nfor time in tmp2:  # add each spike waveform to neuron_waveform\nspikewindow = tmpn.loc[\ntime - waveform_window[0] : time + waveform_window[1] - 1\n]  # waveform for this spike time\ntry:\nneuron_waveforms[neuron] += spikewindow.values\nexcept (\nException\n):  # ignore if full waveform is not present in this batch\npass\nmeanwf = {\nn: pd.DataFrame(\ndata=np.array(neuron_waveforms[n]) / spike_count,\ncolumns=np.arange(len(group_to_channel[group[n]])),\nindex=np.array(np.arange(-waveform_window[0], waveform_window[1])) / fs,\n)\nfor n in sample_counted_spikes\n}\n# find the max channel for each neuron\nmaxch = pd.Series(\ndata=[meanwf[n][meanwf[n].loc[0].idxmin()].name for n in meanwf],\nindex=spikes.keys(),\n)\nreturn meanwf, maxch\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"reference/io/ophys_gui/","title":"Ophys gui","text":""},{"location":"reference/io/ophys_gui/#pynapple.io.ophys_gui","title":"pynapple.io.ophys_gui","text":""},{"location":"reference/io/phy/","title":"Phy","text":""},{"location":"reference/io/phy/#pynapple.io.phy","title":"pynapple.io.phy","text":"<p> DEPRECATED: This will be removed in version 1.0.0. Check nwbmatic or neuroconv instead.</p> <p>Class and functions for loading data processed with Phy2</p> <p>@author: Sara Mahallati, Guillaume Viejo</p>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy","title":"Phy","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader for Phy data</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>class Phy(BaseLoader):\n\"\"\"\n    Loader for Phy data\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Instantiate the data class from a Phy folder.\n        Parameters\n        ----------\n        path : str or Path object\n            The path to the data.\n        \"\"\"\nself.time_support = None\nself.sample_rate = None\nself.n_channels_dat = None\nself.channel_map = None\nself.ch_to_sh = None\nself.spikes = None\nself.channel_positions = None\nsuper().__init__(path)\n# This path stuff should happen only once in the parent class\nself.path = Path(path)\nself.basename = self.path.name\nself.nwb_path = self.path / \"pynapplenwb\"\n# from what I can see in the loading function, only one nwb file per folder:\ntry:\nself.nwb_file = list(self.nwb_path.glob(\"*.nwb\"))[0]\nexcept IndexError:\nself.nwb_file = None\n# Need to check if nwb file exists and if data are there\n# if self.path is not None:  -&gt; are there any cases where this is None?\nif self.nwb_file is not None:\nloaded_spikes = self.load_nwb_spikes()\nif loaded_spikes is not None:\nreturn\n# Bypass if data have already been transferred to nwb\nself.load_phy_params()\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.channel_map)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_phy_spikes(self.time_support)\nself.save_data()\napp.quit()\ndef load_phy_params(self):\n\"\"\"\n        path should be the folder session containing the params.py file\n        Function reads :\n        1. the number of channels\n        2. the sampling frequency of the dat file\n        Raises\n        ------\n        AssertionError\n            If path does not contain the params file or channel_map.npy\n        \"\"\"\nassert (\nself.path / \"params.py\"\n).exists(), f\"Can't find params.py in {self.path}\"\n# It is strongly recommended not to conflate parameters and code! Also, there's a library called params.\n# I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py!\n# In this way we just read the file, and we don't have to add to sys to import...\n# TODO maybe remove this\nsys.path.append(str(self.path))\nimport params as params\nself.sample_rate = params.sample_rate\nself.n_channels_dat = params.n_channels_dat\nassert (\nself.path / \"channel_map.npy\"\n).exists(), f\"Can't find channel_map.npy in {self.path}\"\nchannel_map = np.load(self.path / \"channel_map.npy\")\nif (self.path / \"channel_shanks.npy\").exists():\nchannel_shank = np.load(self.path / \"channel_shanks.npy\")\nn_shanks = len(np.unique(channel_shank))\nself.channel_map = {\ni: channel_map[channel_shank == i] for i in range(n_shanks)\n}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=channel_shank.flatten(),\n)\nelse:\nself.channel_map = {i: channel_map[i] for i in range(len(channel_map))}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=np.hstack(\n[\nnp.ones(len(channel_map[i]), dtype=int) * i\nfor i in range(len(channel_map))\n]\n),\n)\nreturn\ndef load_phy_spikes(self, time_support=None):\n\"\"\"\n        Load Phy spike times and convert to NWB.\n        Instantiate automatically a TsGroup object.\n        The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv\n        Parameters\n        ----------\n        path : Path object\n            The path to the data\n        time_support : IntevalSet, optional\n            The time support of the data\n        Raises\n        ------\n        RuntimeError\n            If files are missing.\n            The function needs :\n            - cluster_info.tsv or cluster_group.tsv\n            - spike_times.npy\n            - spike_clusters.npy\n            - channel_positions.npy\n            - templates.npy\n        \"\"\"\n# Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used:\nhas_cluster_info = False\nif (self.path / \"cluster_info.tsv\").exists():\ncluster_info_file = self.path / \"cluster_info.tsv\"\nhas_cluster_info = True\nelif (self.path / \"cluster_group.tsv\").exists():\ncluster_info_file = self.path / \"cluster_group.tsv\"\nelse:\nraise RuntimeError(\n\"Can't find cluster_info.tsv or cluster_group.tsv in {};\".format(\nself.path\n)\n)\ncluster_info = pd.read_csv(cluster_info_file, sep=\"\\t\", index_col=\"cluster_id\")\n# In my processed data with KiloSort 3.0, the column is named KSLabel\nif \"group\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.group == \"good\"].index.values\nelif \"KSLabel\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.KSLabel == \"good\"].index.values\nelse:\nraise RuntimeError(\n\"Can't find column group or KSLabel in {};\".format(cluster_info_file)\n)\nspike_times = np.load(self.path / \"spike_times.npy\")\nspike_clusters = np.load(self.path / \"spike_clusters.npy\")\nspikes = {}\nfor n in cluster_id_good:\nspikes[n] = nap.Ts(\nt=spike_times[spike_clusters == n] / self.sample_rate,\ntime_support=time_support,\n)\nself.spikes = nap.TsGroup(spikes, time_support=time_support)\n# Adding the position of the electrodes in case\nself.channel_positions = np.load(self.path / \"channel_positions.npy\")\n# Adding shank group info from cluster_info if present\nif has_cluster_info:\ngroup = cluster_info.loc[cluster_id_good, \"sh\"]\nself.spikes.set_info(group=group)\nelse:\ntemplate = np.load(self.path / \"templates.npy\")\ntemplate = template[cluster_id_good]\nch = np.power(template, 2).max(1).argmax(1)\ngroup = pd.Series(index=cluster_id_good, data=self.ch_to_sh[ch].values)\nself.spikes.set_info(group=group)\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\ndef save_data(self):\n\"\"\"Save the data to NWB format.\"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.channel_map:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.channel_map[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_spikes(self):\n\"\"\"Read the NWB spikes to extract the spike times.\n        Returns\n        -------\n        TYPE\n            Description\n        \"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn None\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\ndef load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n        Load the LFP.\n        Parameters\n        ----------\n        filename : str, optional\n            The filename of the lfp file.\n            It can be useful it multiple dat files are present in the data directory\n        channel : int or list of int, optional\n            The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n        extension : str, optional\n            The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n        frequency : float, optional\n            Default 1250 Hz for the eeg file\n        precision : str, optional\n            The precision of the binary file\n        bytes_size : int, optional\n            Bytes size of the lfp file\n        Raises\n        ------\n        RuntimeError\n            If can't find the lfp/eeg/dat file\n        Returns\n        -------\n        Tsd or TsdFrame\n            The lfp in a time series format\n        \"\"\"\nif filename is not None:\nfilepath = self.path / filename\nelse:\ntry:\nfilepath = list(self.path.glob(f\"*{extension}\"))[0]\nexcept IndexError:\nraise RuntimeError(f\"Path {self.path} contains no {extension} files;\")\n# is it possible that this is a leftover from neurosuite data?\n# This is not implemented for this class.\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Instantiate the data class from a Phy folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path object</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/phy.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Instantiate the data class from a Phy folder.\n    Parameters\n    ----------\n    path : str or Path object\n        The path to the data.\n    \"\"\"\nself.time_support = None\nself.sample_rate = None\nself.n_channels_dat = None\nself.channel_map = None\nself.ch_to_sh = None\nself.spikes = None\nself.channel_positions = None\nsuper().__init__(path)\n# This path stuff should happen only once in the parent class\nself.path = Path(path)\nself.basename = self.path.name\nself.nwb_path = self.path / \"pynapplenwb\"\n# from what I can see in the loading function, only one nwb file per folder:\ntry:\nself.nwb_file = list(self.nwb_path.glob(\"*.nwb\"))[0]\nexcept IndexError:\nself.nwb_file = None\n# Need to check if nwb file exists and if data are there\n# if self.path is not None:  -&gt; are there any cases where this is None?\nif self.nwb_file is not None:\nloaded_spikes = self.load_nwb_spikes()\nif loaded_spikes is not None:\nreturn\n# Bypass if data have already been transferred to nwb\nself.load_phy_params()\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.channel_map)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_phy_spikes(self.time_support)\nself.save_data()\napp.quit()\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_phy_params","title":"load_phy_params","text":"<pre><code>load_phy_params()\n</code></pre> <p>path should be the folder session containing the params.py file</p> <p>Function reads : 1. the number of channels 2. the sampling frequency of the dat file</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If path does not contain the params file or channel_map.npy</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_phy_params(self):\n\"\"\"\n    path should be the folder session containing the params.py file\n    Function reads :\n    1. the number of channels\n    2. the sampling frequency of the dat file\n    Raises\n    ------\n    AssertionError\n        If path does not contain the params file or channel_map.npy\n    \"\"\"\nassert (\nself.path / \"params.py\"\n).exists(), f\"Can't find params.py in {self.path}\"\n# It is strongly recommended not to conflate parameters and code! Also, there's a library called params.\n# I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py!\n# In this way we just read the file, and we don't have to add to sys to import...\n# TODO maybe remove this\nsys.path.append(str(self.path))\nimport params as params\nself.sample_rate = params.sample_rate\nself.n_channels_dat = params.n_channels_dat\nassert (\nself.path / \"channel_map.npy\"\n).exists(), f\"Can't find channel_map.npy in {self.path}\"\nchannel_map = np.load(self.path / \"channel_map.npy\")\nif (self.path / \"channel_shanks.npy\").exists():\nchannel_shank = np.load(self.path / \"channel_shanks.npy\")\nn_shanks = len(np.unique(channel_shank))\nself.channel_map = {\ni: channel_map[channel_shank == i] for i in range(n_shanks)\n}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=channel_shank.flatten(),\n)\nelse:\nself.channel_map = {i: channel_map[i] for i in range(len(channel_map))}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=np.hstack(\n[\nnp.ones(len(channel_map[i]), dtype=int) * i\nfor i in range(len(channel_map))\n]\n),\n)\nreturn\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_phy_spikes","title":"load_phy_spikes","text":"<pre><code>load_phy_spikes(time_support=None)\n</code></pre> <p>Load Phy spike times and convert to NWB. Instantiate automatically a TsGroup object. The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path object</code> <p>The path to the data</p> required <code>time_support</code> <code>IntevalSet</code> <p>The time support of the data</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If files are missing. The function needs : - cluster_info.tsv or cluster_group.tsv - spike_times.npy - spike_clusters.npy - channel_positions.npy - templates.npy</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_phy_spikes(self, time_support=None):\n\"\"\"\n    Load Phy spike times and convert to NWB.\n    Instantiate automatically a TsGroup object.\n    The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv\n    Parameters\n    ----------\n    path : Path object\n        The path to the data\n    time_support : IntevalSet, optional\n        The time support of the data\n    Raises\n    ------\n    RuntimeError\n        If files are missing.\n        The function needs :\n        - cluster_info.tsv or cluster_group.tsv\n        - spike_times.npy\n        - spike_clusters.npy\n        - channel_positions.npy\n        - templates.npy\n    \"\"\"\n# Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used:\nhas_cluster_info = False\nif (self.path / \"cluster_info.tsv\").exists():\ncluster_info_file = self.path / \"cluster_info.tsv\"\nhas_cluster_info = True\nelif (self.path / \"cluster_group.tsv\").exists():\ncluster_info_file = self.path / \"cluster_group.tsv\"\nelse:\nraise RuntimeError(\n\"Can't find cluster_info.tsv or cluster_group.tsv in {};\".format(\nself.path\n)\n)\ncluster_info = pd.read_csv(cluster_info_file, sep=\"\\t\", index_col=\"cluster_id\")\n# In my processed data with KiloSort 3.0, the column is named KSLabel\nif \"group\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.group == \"good\"].index.values\nelif \"KSLabel\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.KSLabel == \"good\"].index.values\nelse:\nraise RuntimeError(\n\"Can't find column group or KSLabel in {};\".format(cluster_info_file)\n)\nspike_times = np.load(self.path / \"spike_times.npy\")\nspike_clusters = np.load(self.path / \"spike_clusters.npy\")\nspikes = {}\nfor n in cluster_id_good:\nspikes[n] = nap.Ts(\nt=spike_times[spike_clusters == n] / self.sample_rate,\ntime_support=time_support,\n)\nself.spikes = nap.TsGroup(spikes, time_support=time_support)\n# Adding the position of the electrodes in case\nself.channel_positions = np.load(self.path / \"channel_positions.npy\")\n# Adding shank group info from cluster_info if present\nif has_cluster_info:\ngroup = cluster_info.loc[cluster_id_good, \"sh\"]\nself.spikes.set_info(group=group)\nelse:\ntemplate = np.load(self.path / \"templates.npy\")\ntemplate = template[cluster_id_good]\nch = np.power(template, 2).max(1).argmax(1)\ngroup = pd.Series(index=cluster_id_good, data=self.ch_to_sh[ch].values)\nself.spikes.set_info(group=group)\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.save_data","title":"save_data","text":"<pre><code>save_data()\n</code></pre> <p>Save the data to NWB format.</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def save_data(self):\n\"\"\"Save the data to NWB format.\"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.channel_map:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.channel_map[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_nwb_spikes","title":"load_nwb_spikes","text":"<pre><code>load_nwb_spikes()\n</code></pre> <p>Read the NWB spikes to extract the spike times.</p> <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_nwb_spikes(self):\n\"\"\"Read the NWB spikes to extract the spike times.\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn None\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_lfp","title":"load_lfp","text":"<pre><code>load_lfp(\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n)\n</code></pre> <p>Load the LFP.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename of the lfp file. It can be useful it multiple dat files are present in the data directory</p> <code>None</code> <code>channel</code> <code>int or list of int</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>extension</code> <code>str</code> <p>The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> <code>'.eeg'</code> <code>frequency</code> <code>float</code> <p>Default 1250 Hz for the eeg file</p> <code>1250.0</code> <code>precision</code> <code>str</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the lfp file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Load the LFP.\n    Parameters\n    ----------\n    filename : str, optional\n        The filename of the lfp file.\n        It can be useful it multiple dat files are present in the data directory\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    frequency : float, optional\n        Default 1250 Hz for the eeg file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the lfp file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    \"\"\"\nif filename is not None:\nfilepath = self.path / filename\nelse:\ntry:\nfilepath = list(self.path.glob(f\"*{extension}\"))[0]\nexcept IndexError:\nraise RuntimeError(f\"Path {self.path} contains no {extension} files;\")\n# is it possible that this is a leftover from neurosuite data?\n# This is not implemented for this class.\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"reference/io/suite2p/","title":"Suite2p","text":""},{"location":"reference/io/suite2p/#pynapple.io.suite2p","title":"pynapple.io.suite2p","text":"<p> DEPRECATED: This will be removed in version 1.0.0. Check nwbmatic or neuroconv instead.</p> <p>Loader for Suite2P https://github.com/MouseLand/suite2p</p>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P","title":"Suite2P","text":"<p>             Bases: <code>BaseLoader</code></p> <p>Loader for data processed with Suite2P.</p> <p>Pynapple will try to look for data in this order :</p> <ol> <li> <p>pynapplenwb/session_name.nwb</p> </li> <li> <p>suite2p/plane/.npy</p> </li> </ol> <p>Attributes:</p> Name Type Description <code>F</code> <code>TsdFrame</code> <p>Fluorescence traces (timepoints x ROIs) for all planes</p> <code>Fneu</code> <code>TsdFrame</code> <p>Neuropil fluorescence traces (timepoints x ROIs) for all planes</p> <code>spks</code> <code>TsdFrame</code> <p>Deconvolved traces (timepoints x ROIS) for all planes</p> <code>plane_info</code> <code>DataFrame</code> <p>Contains plane identity of each cell</p> <code>stats</code> <code>dict</code> <p>dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells (Can be smaller when loading from the NWB file)</p> <code>ops</code> <code>dict</code> <p>Parameters from Suite2p. (Can be smaller when loading from the NWB file)</p> <code>iscell</code> <code>ndarray</code> <p>Cell classification</p> Source code in <code>pynapple/io/suite2p.py</code> <pre><code>class Suite2P(BaseLoader):\n\"\"\"Loader for data processed with Suite2P.\n    Pynapple will try to look for data in this order :\n    1. pynapplenwb/session_name.nwb\n    2. suite2p/plane*/*.npy\n    Attributes\n    ----------\n    F : TsdFrame\n        Fluorescence traces (timepoints x ROIs) for all planes\n    Fneu : TsdFrame\n        Neuropil fluorescence traces (timepoints x ROIs) for all planes\n    spks : TsdFrame\n        Deconvolved traces (timepoints x ROIS) for all planes\n    plane_info : pandas.DataFrame\n        Contains plane identity of each cell\n    stats : dict\n        dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells\n        (Can be smaller when loading from the NWB file)\n    ops : dict\n        Parameters from Suite2p. (Can be smaller when loading from the NWB file)\n    iscell : numpy.ndarray\n        Cell classification\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_suite2p_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_suite2p(path)\nself.save_suite2p_nwb(path)\ndef load_suite2p(self, path):\n\"\"\"\n        Looking for suite2/plane*\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.path_suite2p = os.path.join(path, \"suite2p\")\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ndata = {\n\"F\": [],\n\"Fneu\": [],\n\"spks\": [],\n}\nplane_info = []\nself.stats = {}\nself.pops = {}\nself.iscells = {}\nself.planes = []\nif os.path.exists(self.path_suite2p):\nplanes = glob.glob(os.path.join(self.path_suite2p, \"plane*\"))\nif len(planes):\n# count = 0\nfor plane_dir in planes:\nn = int(os.path.basename(plane_dir)[-1])\nself.planes.append(n)\n# Loading iscell.npy\ntry:\niscell = np.load(\nos.path.join(plane_dir, \"iscell.npy\"), allow_pickle=True\n)\nidx = np.where(iscell.astype(\"int\")[:, 0])[0]\nplane_info.append(np.ones(len(idx), dtype=\"int\") * n)\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading F.npy, Fneu.py and spks.npy\nfor obj in [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]:\ntry:\nname = obj.split(\".\")[0]\ntmp = np.load(\nos.path.join(plane_dir, obj), allow_pickle=True\n)\ndata[name].append(tmp[idx])\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading stat.npy and ops.npy\ntry:\nstat = np.load(\nos.path.join(plane_dir, \"stat.npy\"), allow_pickle=True\n)\nops = np.load(\nos.path.join(plane_dir, \"ops.npy\"), allow_pickle=True\n).item()\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Saving stat, ops and iscell\nself.stats[n] = stat\nself.pops[n] = ops\nself.iscells[n] = iscell\n# count += len(idx)\nelse:\nwarnings.warn(\n\"Couldn't find planes in %s\" % self.path_suite2p, stacklevel=2\n)\nsys.exit()\nelse:\nwarnings.warn(\"No suite2p folder in %s\" % path, stacklevel=2)\nsys.exit()\n# Calcium transients\ndata[\"F\"] = np.transpose(np.vstack(data[\"F\"]))\ndata[\"Fneu\"] = np.transpose(np.vstack(data[\"Fneu\"]))\ndata[\"spks\"] = np.transpose(np.vstack(data[\"spks\"]))\ntime_index = np.arange(0, len(data[\"F\"])) / self.sampling_rate\nself.F = nap.TsdFrame(t=time_index, d=data[\"F\"])\nself.Fneu = nap.TsdFrame(t=time_index, d=data[\"Fneu\"])\nself.spks = nap.TsdFrame(t=time_index, d=data[\"spks\"])\nself.ops = self.pops[0]\nself.iscell = np.vstack([self.iscells[k] for k in self.iscells.keys()])\n# Metadata\nself.plane_info = pd.DataFrame.from_dict({\"plane\": np.hstack(plane_info)})\nreturn\ndef save_suite2p_nwb(self, path):\n\"\"\"\n        Save the data to NWB. To ensure continuity, this function is based on :\n        https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nmultiplane = True if len(self.planes) &gt; 1 else False\nops = self.pops[list(self.pops.keys())[0]]\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice = nwbfile.create_device(\nname=self.ophys_information[\"device\"][\"name\"],\ndescription=self.ophys_information[\"device\"][\"description\"],\nmanufacturer=self.ophys_information[\"device\"][\"manufacturer\"],\n)\nimaging_plane = nwbfile.create_imaging_plane(\nname=self.ophys_information[\"ImagingPlane\"][\"name\"],\noptical_channel=OpticalChannel(\nname=self.ophys_information[\"OpticalChannel\"][\"name\"],\ndescription=self.ophys_information[\"OpticalChannel\"][\"description\"],\nemission_lambda=float(\nself.ophys_information[\"OpticalChannel\"][\"emission_lambda\"]\n),\n),\nimaging_rate=self.sampling_rate,\ndescription=self.ophys_information[\"ImagingPlane\"][\"description\"],\ndevice=device,\nexcitation_lambda=float(\nself.ophys_information[\"ImagingPlane\"][\"excitation_lambda\"]\n),\nindicator=self.ophys_information[\"ImagingPlane\"][\"indicator\"],\nlocation=self.ophys_information[\"ImagingPlane\"][\"location\"],\ngrid_spacing=([2.0, 2.0, 30.0] if multiplane else [2.0, 2.0]),\ngrid_spacing_unit=\"microns\",\n)\n# link to external data\nimage_series = TwoPhotonSeries(\nname=\"TwoPhotonSeries\",\ndimension=[ops[\"Ly\"], ops[\"Lx\"]],\nexternal_file=(ops[\"filelist\"] if \"filelist\" in ops else [\"\"]),\nimaging_plane=imaging_plane,\nstarting_frame=[0],\nformat=\"external\",\nstarting_time=0.0,\nrate=ops[\"fs\"] * ops[\"nplanes\"],\n)\nnwbfile.add_acquisition(image_series)\n# processing\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=self.ophys_information[\"PlaneSegmentation\"][\"name\"],\ndescription=self.ophys_information[\"PlaneSegmentation\"][\"description\"],\nimaging_plane=imaging_plane,\n# reference_images=image_series,\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nophys_module.add(img_seg)\nfile_strs = [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]\ntraces = []\nncells = np.zeros(len(self.pops), dtype=np.int_)\nNfr = np.array([self.pops[k][\"nframes\"] for k in self.pops.keys()]).max()\nfor iplane, ops in self.pops.items():\nif iplane == 0:\niscell = self.iscells[iplane]\nfor fstr in file_strs:\ntraces.append(np.load(os.path.join(ops[\"save_path\"], fstr)))\nPlaneCellsIdx = iplane * np.ones(len(iscell))\nelse:\niscell = np.append(\niscell,\nself.iscells[iplane],\naxis=0,\n)\nfor i, fstr in enumerate(file_strs):\ntrace = np.load(os.path.join(ops[\"save_path\"], fstr))\nif trace.shape[1] &lt; Nfr:\nfcat = np.zeros(\n(trace.shape[0], Nfr - trace.shape[1]), \"float32\"\n)\ntrace = np.concatenate((trace, fcat), axis=1)\ntraces[i] = np.append(traces[i], trace, axis=0)\nPlaneCellsIdx = np.append(\nPlaneCellsIdx, iplane * np.ones(len(iscell) - len(PlaneCellsIdx))\n)\nstat = self.stats[iplane]\nncells[iplane] = len(stat)\nfor n in range(ncells[iplane]):\nif multiplane:\npixel_mask = np.array(\n[\nstat[n][\"ypix\"],\nstat[n][\"xpix\"],\niplane * np.ones(stat[n][\"npix\"]),\nstat[n][\"lam\"],\n]\n)\nps.add_roi(voxel_mask=pixel_mask.T)\nelse:\npixel_mask = np.array(\n[stat[n][\"ypix\"], stat[n][\"xpix\"], stat[n][\"lam\"]]\n)\nps.add_roi(pixel_mask=pixel_mask.T)\nps.add_column(\"iscell\", \"two columns - iscell &amp; probcell\", iscell)\nrt_region = []\nfor iplane, ops in self.pops.items():\nif iplane == 0:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(0, ncells[iplane]),\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\nelse:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(\nnp.sum(ncells[:iplane]),\nncells[iplane] + np.sum(ncells[:iplane]),\n)\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\n# FLUORESCENCE (all are required)\nname_strs = [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\nfor i, (fstr, nstr) in enumerate(zip(file_strs, name_strs)):\nfor iplane, ops in self.pops.items():\nroi_resp_series = RoiResponseSeries(\nname=f\"plane{int(iplane)}\",\ndata=traces[i][PlaneCellsIdx == iplane],\nrois=rt_region[iplane],\nunit=\"lumens\",\nrate=ops[\"fs\"],\n)\nif iplane == 0:\nfl = Fluorescence(roi_response_series=roi_resp_series, name=nstr)\nelse:\nfl.add_roi_response_series(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_suite2p_nwb(self, path):\n\"\"\"\n        Load suite2p data from NWB\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\nophys = nwbfile.processing[\"ophys\"]\n#################################################################\n# STATS, OPS and ISCELL\n#################################################################\ndims = nwbfile.acquisition[\"TwoPhotonSeries\"].dimension[:]\nself.ops = {\"Ly\": dims[0], \"Lx\": dims[1]}\nself.rate = nwbfile.acquisition[\n\"TwoPhotonSeries\"\n].imaging_plane.imaging_rate\nself.stats = {0: {}}\nself.iscell = ophys[\"ImageSegmentation\"][\"PlaneSegmentation\"][\n\"iscell\"\n].data[:]\ninfo = pd.DataFrame(\ndata=self.iscell[:, 0].astype(\"int\"), columns=[\"iscell\"]\n)\n#################################################################\n# ROIS\n#################################################################\ntry:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"pixel_mask\"]\nmultiplane = False\nexcept Exception:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"voxel_mask\"]\nmultiplane = True\nidx = np.where(self.iscell[:, 0])[0]\ninfo[\"plane\"] = 0\nfor n in range(len(rois)):\nroi = pd.DataFrame(rois[n])\nif \"z\" in roi.columns:\npl = roi[\"z\"][0]\nelse:\npl = 0\ninfo.loc[n, \"plane\"] = pl\nif pl not in self.stats.keys():\nself.stats[pl] = {}\nif n in idx:\nself.stats[pl][n] = {\n\"xpix\": roi[\"y\"].values,\n\"ypix\": roi[\"x\"].values,\n\"lam\": roi[\"weight\"].values,\n}\n#################################################################\n# Time Series\n#################################################################\nfields = np.intersect1d(\n[\"Fluorescence\", \"Neuropil\", \"Deconvolved\"],\nlist(ophys.fields[\"data_interfaces\"].keys()),\n)\nif len(fields) == 0:\nprint(\n\"No \" + \" or \".join([\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]),\n\"found in nwb {}\".format(self.nwbfilepath),\n)\nreturn False\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\ndata = {}\nif multiplane:\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\nelse:\nplanes = [0]\nfor k, name in zip(\n[\"F\", \"Fneu\", \"spks\"], [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n):\ntmp = []\ntimestamps = []\nfor i, n in enumerate(planes):\nif multiplane:\npl = \"plane{}\".format(n)\nelse:\npl = name  # This doesn't make sense\ntokeep = info[\"iscell\"][info[\"plane\"] == n].values == 1\nd = np.transpose(ophys[name][pl].data[:][tokeep])\nif ophys[name][pl].timestamps is not None:\nt = ophys[name][pl].timestamps[:]\nelse:\nt = (np.arange(0, len(d)) / self.rate) + ophys[name][\npl\n].starting_time\ntmp.append(d)\ntimestamps.append(t)\ndata[k] = nap.TsdFrame(t=timestamps[0], d=np.hstack(tmp))\nif \"F\" in data.keys():\nself.F = data[\"F\"]\nif \"Fneu\" in data.keys():\nself.Fneu = data[\"Fneu\"]\nif \"spks\" in data.keys():\nself.spks = data[\"spks\"]\nself.plane_info = pd.DataFrame(\ndata=info[\"plane\"][info[\"iscell\"] == 1].values, columns=[\"plane\"]\n)\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.__init__","title":"__init__","text":"<pre><code>__init__(path)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_suite2p_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_suite2p(path)\nself.save_suite2p_nwb(path)\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p","title":"load_suite2p","text":"<pre><code>load_suite2p(path)\n</code></pre> <p>Looking for suite2/plane*</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def load_suite2p(self, path):\n\"\"\"\n    Looking for suite2/plane*\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.path_suite2p = os.path.join(path, \"suite2p\")\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ndata = {\n\"F\": [],\n\"Fneu\": [],\n\"spks\": [],\n}\nplane_info = []\nself.stats = {}\nself.pops = {}\nself.iscells = {}\nself.planes = []\nif os.path.exists(self.path_suite2p):\nplanes = glob.glob(os.path.join(self.path_suite2p, \"plane*\"))\nif len(planes):\n# count = 0\nfor plane_dir in planes:\nn = int(os.path.basename(plane_dir)[-1])\nself.planes.append(n)\n# Loading iscell.npy\ntry:\niscell = np.load(\nos.path.join(plane_dir, \"iscell.npy\"), allow_pickle=True\n)\nidx = np.where(iscell.astype(\"int\")[:, 0])[0]\nplane_info.append(np.ones(len(idx), dtype=\"int\") * n)\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading F.npy, Fneu.py and spks.npy\nfor obj in [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]:\ntry:\nname = obj.split(\".\")[0]\ntmp = np.load(\nos.path.join(plane_dir, obj), allow_pickle=True\n)\ndata[name].append(tmp[idx])\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading stat.npy and ops.npy\ntry:\nstat = np.load(\nos.path.join(plane_dir, \"stat.npy\"), allow_pickle=True\n)\nops = np.load(\nos.path.join(plane_dir, \"ops.npy\"), allow_pickle=True\n).item()\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Saving stat, ops and iscell\nself.stats[n] = stat\nself.pops[n] = ops\nself.iscells[n] = iscell\n# count += len(idx)\nelse:\nwarnings.warn(\n\"Couldn't find planes in %s\" % self.path_suite2p, stacklevel=2\n)\nsys.exit()\nelse:\nwarnings.warn(\"No suite2p folder in %s\" % path, stacklevel=2)\nsys.exit()\n# Calcium transients\ndata[\"F\"] = np.transpose(np.vstack(data[\"F\"]))\ndata[\"Fneu\"] = np.transpose(np.vstack(data[\"Fneu\"]))\ndata[\"spks\"] = np.transpose(np.vstack(data[\"spks\"]))\ntime_index = np.arange(0, len(data[\"F\"])) / self.sampling_rate\nself.F = nap.TsdFrame(t=time_index, d=data[\"F\"])\nself.Fneu = nap.TsdFrame(t=time_index, d=data[\"Fneu\"])\nself.spks = nap.TsdFrame(t=time_index, d=data[\"spks\"])\nself.ops = self.pops[0]\nself.iscell = np.vstack([self.iscells[k] for k in self.iscells.keys()])\n# Metadata\nself.plane_info = pd.DataFrame.from_dict({\"plane\": np.hstack(plane_info)})\nreturn\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.load_default_csv","title":"load_default_csv","text":"<pre><code>load_default_csv(csv_file)\n</code></pre> <p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.load_optitrack_csv","title":"load_optitrack_csv","text":"<pre><code>load_optitrack_csv(csv_file)\n</code></pre> <p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.load_dlc_csv","title":"load_dlc_csv","text":"<pre><code>load_dlc_csv(csv_file)\n</code></pre> <p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.load_ttl_pulse","title":"load_ttl_pulse","text":"<pre><code>load_ttl_pulse(\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n)\n</code></pre> <p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.save_suite2p_nwb","title":"save_suite2p_nwb","text":"<pre><code>save_suite2p_nwb(path)\n</code></pre> <p>Save the data to NWB. To ensure continuity, this function is based on : https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def save_suite2p_nwb(self, path):\n\"\"\"\n    Save the data to NWB. To ensure continuity, this function is based on :\n    https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nmultiplane = True if len(self.planes) &gt; 1 else False\nops = self.pops[list(self.pops.keys())[0]]\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice = nwbfile.create_device(\nname=self.ophys_information[\"device\"][\"name\"],\ndescription=self.ophys_information[\"device\"][\"description\"],\nmanufacturer=self.ophys_information[\"device\"][\"manufacturer\"],\n)\nimaging_plane = nwbfile.create_imaging_plane(\nname=self.ophys_information[\"ImagingPlane\"][\"name\"],\noptical_channel=OpticalChannel(\nname=self.ophys_information[\"OpticalChannel\"][\"name\"],\ndescription=self.ophys_information[\"OpticalChannel\"][\"description\"],\nemission_lambda=float(\nself.ophys_information[\"OpticalChannel\"][\"emission_lambda\"]\n),\n),\nimaging_rate=self.sampling_rate,\ndescription=self.ophys_information[\"ImagingPlane\"][\"description\"],\ndevice=device,\nexcitation_lambda=float(\nself.ophys_information[\"ImagingPlane\"][\"excitation_lambda\"]\n),\nindicator=self.ophys_information[\"ImagingPlane\"][\"indicator\"],\nlocation=self.ophys_information[\"ImagingPlane\"][\"location\"],\ngrid_spacing=([2.0, 2.0, 30.0] if multiplane else [2.0, 2.0]),\ngrid_spacing_unit=\"microns\",\n)\n# link to external data\nimage_series = TwoPhotonSeries(\nname=\"TwoPhotonSeries\",\ndimension=[ops[\"Ly\"], ops[\"Lx\"]],\nexternal_file=(ops[\"filelist\"] if \"filelist\" in ops else [\"\"]),\nimaging_plane=imaging_plane,\nstarting_frame=[0],\nformat=\"external\",\nstarting_time=0.0,\nrate=ops[\"fs\"] * ops[\"nplanes\"],\n)\nnwbfile.add_acquisition(image_series)\n# processing\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=self.ophys_information[\"PlaneSegmentation\"][\"name\"],\ndescription=self.ophys_information[\"PlaneSegmentation\"][\"description\"],\nimaging_plane=imaging_plane,\n# reference_images=image_series,\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nophys_module.add(img_seg)\nfile_strs = [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]\ntraces = []\nncells = np.zeros(len(self.pops), dtype=np.int_)\nNfr = np.array([self.pops[k][\"nframes\"] for k in self.pops.keys()]).max()\nfor iplane, ops in self.pops.items():\nif iplane == 0:\niscell = self.iscells[iplane]\nfor fstr in file_strs:\ntraces.append(np.load(os.path.join(ops[\"save_path\"], fstr)))\nPlaneCellsIdx = iplane * np.ones(len(iscell))\nelse:\niscell = np.append(\niscell,\nself.iscells[iplane],\naxis=0,\n)\nfor i, fstr in enumerate(file_strs):\ntrace = np.load(os.path.join(ops[\"save_path\"], fstr))\nif trace.shape[1] &lt; Nfr:\nfcat = np.zeros(\n(trace.shape[0], Nfr - trace.shape[1]), \"float32\"\n)\ntrace = np.concatenate((trace, fcat), axis=1)\ntraces[i] = np.append(traces[i], trace, axis=0)\nPlaneCellsIdx = np.append(\nPlaneCellsIdx, iplane * np.ones(len(iscell) - len(PlaneCellsIdx))\n)\nstat = self.stats[iplane]\nncells[iplane] = len(stat)\nfor n in range(ncells[iplane]):\nif multiplane:\npixel_mask = np.array(\n[\nstat[n][\"ypix\"],\nstat[n][\"xpix\"],\niplane * np.ones(stat[n][\"npix\"]),\nstat[n][\"lam\"],\n]\n)\nps.add_roi(voxel_mask=pixel_mask.T)\nelse:\npixel_mask = np.array(\n[stat[n][\"ypix\"], stat[n][\"xpix\"], stat[n][\"lam\"]]\n)\nps.add_roi(pixel_mask=pixel_mask.T)\nps.add_column(\"iscell\", \"two columns - iscell &amp; probcell\", iscell)\nrt_region = []\nfor iplane, ops in self.pops.items():\nif iplane == 0:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(0, ncells[iplane]),\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\nelse:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(\nnp.sum(ncells[:iplane]),\nncells[iplane] + np.sum(ncells[:iplane]),\n)\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\n# FLUORESCENCE (all are required)\nname_strs = [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\nfor i, (fstr, nstr) in enumerate(zip(file_strs, name_strs)):\nfor iplane, ops in self.pops.items():\nroi_resp_series = RoiResponseSeries(\nname=f\"plane{int(iplane)}\",\ndata=traces[i][PlaneCellsIdx == iplane],\nrois=rt_region[iplane],\nunit=\"lumens\",\nrate=ops[\"fs\"],\n)\nif iplane == 0:\nfl = Fluorescence(roi_response_series=roi_resp_series, name=nstr)\nelse:\nfl.add_roi_response_series(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.create_nwb_file","title":"create_nwb_file","text":"<pre><code>create_nwb_file(path)\n</code></pre> <p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p_nwb","title":"load_suite2p_nwb","text":"<pre><code>load_suite2p_nwb(path)\n</code></pre> <p>Load suite2p data from NWB</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def load_suite2p_nwb(self, path):\n\"\"\"\n    Load suite2p data from NWB\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\nophys = nwbfile.processing[\"ophys\"]\n#################################################################\n# STATS, OPS and ISCELL\n#################################################################\ndims = nwbfile.acquisition[\"TwoPhotonSeries\"].dimension[:]\nself.ops = {\"Ly\": dims[0], \"Lx\": dims[1]}\nself.rate = nwbfile.acquisition[\n\"TwoPhotonSeries\"\n].imaging_plane.imaging_rate\nself.stats = {0: {}}\nself.iscell = ophys[\"ImageSegmentation\"][\"PlaneSegmentation\"][\n\"iscell\"\n].data[:]\ninfo = pd.DataFrame(\ndata=self.iscell[:, 0].astype(\"int\"), columns=[\"iscell\"]\n)\n#################################################################\n# ROIS\n#################################################################\ntry:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"pixel_mask\"]\nmultiplane = False\nexcept Exception:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"voxel_mask\"]\nmultiplane = True\nidx = np.where(self.iscell[:, 0])[0]\ninfo[\"plane\"] = 0\nfor n in range(len(rois)):\nroi = pd.DataFrame(rois[n])\nif \"z\" in roi.columns:\npl = roi[\"z\"][0]\nelse:\npl = 0\ninfo.loc[n, \"plane\"] = pl\nif pl not in self.stats.keys():\nself.stats[pl] = {}\nif n in idx:\nself.stats[pl][n] = {\n\"xpix\": roi[\"y\"].values,\n\"ypix\": roi[\"x\"].values,\n\"lam\": roi[\"weight\"].values,\n}\n#################################################################\n# Time Series\n#################################################################\nfields = np.intersect1d(\n[\"Fluorescence\", \"Neuropil\", \"Deconvolved\"],\nlist(ophys.fields[\"data_interfaces\"].keys()),\n)\nif len(fields) == 0:\nprint(\n\"No \" + \" or \".join([\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]),\n\"found in nwb {}\".format(self.nwbfilepath),\n)\nreturn False\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\ndata = {}\nif multiplane:\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\nelse:\nplanes = [0]\nfor k, name in zip(\n[\"F\", \"Fneu\", \"spks\"], [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n):\ntmp = []\ntimestamps = []\nfor i, n in enumerate(planes):\nif multiplane:\npl = \"plane{}\".format(n)\nelse:\npl = name  # This doesn't make sense\ntokeep = info[\"iscell\"][info[\"plane\"] == n].values == 1\nd = np.transpose(ophys[name][pl].data[:][tokeep])\nif ophys[name][pl].timestamps is not None:\nt = ophys[name][pl].timestamps[:]\nelse:\nt = (np.arange(0, len(d)) / self.rate) + ophys[name][\npl\n].starting_time\ntmp.append(d)\ntimestamps.append(t)\ndata[k] = nap.TsdFrame(t=timestamps[0], d=np.hstack(tmp))\nif \"F\" in data.keys():\nself.F = data[\"F\"]\nif \"Fneu\" in data.keys():\nself.Fneu = data[\"Fneu\"]\nif \"spks\" in data.keys():\nself.spks = data[\"spks\"]\nself.plane_info = pd.DataFrame(\ndata=info[\"plane\"][info[\"iscell\"] == 1].values, columns=[\"plane\"]\n)\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.load_data","title":"load_data","text":"<pre><code>load_data(path)\n</code></pre> <p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.save_nwb_intervals","title":"save_nwb_intervals","text":"<pre><code>save_nwb_intervals(iset, name, description='')\n</code></pre> <p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.save_nwb_timeseries","title":"save_nwb_timeseries","text":"<pre><code>save_nwb_timeseries(tsd, name, description='')\n</code></pre> <p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.load_nwb_intervals","title":"load_nwb_intervals","text":"<pre><code>load_nwb_intervals(name)\n</code></pre> <p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.load_nwb_timeseries","title":"load_nwb_timeseries","text":"<pre><code>load_nwb_timeseries(name)\n</code></pre> <p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"reference/process/","title":"Process","text":""},{"location":"reference/process/#pynapple.process","title":"pynapple.process","text":""},{"location":"reference/process/correlograms/","title":"Correlograms","text":""},{"location":"reference/process/correlograms/#pynapple.process.correlograms","title":"pynapple.process.correlograms","text":""},{"location":"reference/process/correlograms/#pynapple.process.correlograms.cross_correlogram","title":"cross_correlogram","text":"<pre><code>cross_correlogram(t1, t2, binsize, windowsize)\n</code></pre> <p>Performs the discrete cross-correlogram of two time series. The units should be in s for all arguments. Return the firing rate of the series t2 relative to the timings of t1. See compute_crosscorrelogram, compute_autocorrelogram and compute_eventcorrelogram for wrappers of this function.</p> <p>Parameters:</p> Name Type Description Default <code>t1</code> <code>ndarray</code> <p>The timestamps of the reference time series (in seconds)</p> required <code>t2</code> <code>ndarray</code> <p>The timestamps of the target time series (in seconds)</p> required <code>binsize</code> <code>float</code> <p>The bin size (in seconds)</p> required <code>windowsize</code> <code>float</code> <p>The window size (in seconds)</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The cross-correlogram</p> <code>ndarray</code> <p>Center of the bins (in s)</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>@jit(nopython=True)\ndef cross_correlogram(t1, t2, binsize, windowsize):\n\"\"\"\n    Performs the discrete cross-correlogram of two time series.\n    The units should be in s for all arguments.\n    Return the firing rate of the series t2 relative to the timings of t1.\n    See compute_crosscorrelogram, compute_autocorrelogram and compute_eventcorrelogram\n    for wrappers of this function.\n    Parameters\n    ----------\n    t1 : numpy.ndarray\n        The timestamps of the reference time series (in seconds)\n    t2 : numpy.ndarray\n        The timestamps of the target time series (in seconds)\n    binsize : float\n        The bin size (in seconds)\n    windowsize : float\n        The window size (in seconds)\n    Returns\n    -------\n    numpy.ndarray\n        The cross-correlogram\n    numpy.ndarray\n        Center of the bins (in s)\n    \"\"\"\n# nbins = ((windowsize//binsize)*2)\nnt1 = len(t1)\nnt2 = len(t2)\nnbins = int((windowsize * 2) // binsize)\nif np.floor(nbins / 2) * 2 == nbins:\nnbins = nbins + 1\nw = (nbins / 2) * binsize\nC = np.zeros(nbins)\ni2 = 0\nfor i1 in range(nt1):\nlbound = t1[i1] - w\nwhile i2 &lt; nt2 and t2[i2] &lt; lbound:\ni2 = i2 + 1\nwhile i2 &gt; 0 and t2[i2 - 1] &gt; lbound:\ni2 = i2 - 1\nrbound = lbound\nleftb = i2\nfor j in range(nbins):\nk = 0\nrbound = rbound + binsize\nwhile leftb &lt; nt2 and t2[leftb] &lt; rbound:\nleftb = leftb + 1\nk = k + 1\nC[j] += k\nC = C / (nt1 * binsize)\nm = -w + binsize / 2\nB = np.zeros(nbins)\nfor j in range(nbins):\nB[j] = m + j * binsize\nreturn C, B\n</code></pre>"},{"location":"reference/process/correlograms/#pynapple.process.correlograms.compute_autocorrelogram","title":"compute_autocorrelogram","text":"<pre><code>compute_autocorrelogram(\ngroup,\nbinsize,\nwindowsize,\nep=None,\nnorm=True,\ntime_units=\"s\",\n)\n</code></pre> <p>Computes the autocorrelogram of a group of Ts/Tsd objects. The group can be passed directly as a TsGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects to auto-correlate</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which auto-corrs are computed. If None, the epoch is the time support of the group.</p> <code>None</code> <code>norm</code> <code>bool</code> <p>If True, autocorrelograms are normalized to baseline (i.e. divided by the average rate)  If False, autoorrelograms are returned as the rate (Hz) of the time series (relative to itself)</p> <code>True</code> <code>time_units</code> <code>str</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_autocorrelogram(\ngroup, binsize, windowsize, ep=None, norm=True, time_units=\"s\"\n):\n\"\"\"\n    Computes the autocorrelogram of a group of Ts/Tsd objects.\n    The group can be passed directly as a TsGroup object.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects to auto-correlate\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which auto-corrs are computed.\n        If None, the epoch is the time support of the group.\n    norm : bool, optional\n         If True, autocorrelograms are normalized to baseline (i.e. divided by the average rate)\n         If False, autoorrelograms are returned as the rate (Hz) of the time series (relative to itself)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup\n    \"\"\"\nif type(group) is nap.TsGroup:\nif isinstance(ep, nap.IntervalSet):\nnewgroup = group.restrict(ep)\nelse:\nnewgroup = group\nelse:\nraise RuntimeError(\"Unknown format for group\")\nautocorrs = {}\nbinsize = nap.TsIndex.format_timestamps(\nnp.array([binsize], dtype=np.float64), time_units\n)[0]\nwindowsize = nap.TsIndex.format_timestamps(\nnp.array([windowsize], dtype=np.float64), time_units\n)[0]\nfor n in newgroup.keys():\nspk_time = newgroup[n].index\nauc, times = cross_correlogram(spk_time, spk_time, binsize, windowsize)\nautocorrs[n] = pd.Series(index=np.round(times, 6), data=auc, dtype=\"float\")\nautocorrs = pd.DataFrame.from_dict(autocorrs)\nif norm:\nautocorrs = autocorrs / newgroup.get_info(\"rate\")\n# Bug here\nif 0 in autocorrs.index:\nautocorrs.loc[0] = 0.0\nreturn autocorrs.astype(\"float\")\n</code></pre>"},{"location":"reference/process/correlograms/#pynapple.process.correlograms.compute_crosscorrelogram","title":"compute_crosscorrelogram","text":"<pre><code>compute_crosscorrelogram(\ngroup,\nbinsize,\nwindowsize,\nep=None,\nnorm=True,\ntime_units=\"s\",\nreverse=False,\n)\n</code></pre> <p>Computes all the pairwise cross-correlograms for TsGroup or list/tuple of two TsGroup.</p> <p>If input is TsGroup only, the reference Ts/Tsd and target are chosen based on the builtin itertools.combinations function. For example if indexes are [0,1,2], the function computes cross-correlograms for the pairs (0,1), (0, 2), and (1, 2). The left index gives the reference time series. To reverse the order, set reverse=True.</p> <p>If input is tuple/list of TsGroup, for example group=(group1, group2), the reference for each pairs comes from group1.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup or tuple/list of two TsGroups</code> required <p>binsize : float     The bin size. Default is second.     If different, specify with the parameter time_units ('s' [default], 'ms', 'us'). windowsize : float     The window size. Default is second.     If different, specify with the parameter time_units ('s' [default], 'ms', 'us'). ep : IntervalSet     The epoch on which cross-corrs are computed.     If None, the epoch is the time support of the group. norm : bool, optional     If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)     If False, cross-orrelograms are returned as the rate (Hz) of the target time series ((relative to the reference time series) time_units : str, optional     The time units of the parameters. They have to be consistent for binsize and windowsize.     ('s' [default], 'ms', 'us'). reverse : bool, optional     To reverse the pair order if input is TsGroup</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup or tuple/list of two TsGroups</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_crosscorrelogram(\ngroup, binsize, windowsize, ep=None, norm=True, time_units=\"s\", reverse=False\n):\n\"\"\"\n    Computes all the pairwise cross-correlograms for TsGroup or list/tuple of two TsGroup.\n    If input is TsGroup only, the reference Ts/Tsd and target are chosen based on the builtin itertools.combinations function.\n    For example if indexes are [0,1,2], the function computes cross-correlograms\n    for the pairs (0,1), (0, 2), and (1, 2). The left index gives the reference time series.\n    To reverse the order, set reverse=True.\n    If input is tuple/list of TsGroup, for example group=(group1, group2), the reference for each pairs comes from group1.\n    Parameters\n    ----------\n    group : TsGroup or tuple/list of two TsGroups\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which cross-corrs are computed.\n        If None, the epoch is the time support of the group.\n    norm : bool, optional\n        If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)\n        If False, cross-orrelograms are returned as the rate (Hz) of the target time series ((relative to the reference time series)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    reverse : bool, optional\n        To reverse the pair order if input is TsGroup\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup or tuple/list of two TsGroups\n    \"\"\"\ncrosscorrs = {}\nbinsize = nap.TsIndex.format_timestamps(\nnp.array([binsize], dtype=np.float64), time_units\n)[0]\nwindowsize = nap.TsIndex.format_timestamps(\nnp.array([windowsize], dtype=np.float64), time_units\n)[0]\nif isinstance(group, nap.TsGroup):\nif isinstance(ep, nap.IntervalSet):\nnewgroup = group.restrict(ep)\nelse:\nnewgroup = group\nneurons = list(newgroup.keys())\npairs = list(combinations(neurons, 2))\nif reverse:\npairs = list(map(lambda n: (n[1], n[0]), pairs))\nfor i, j in pairs:\nspk1 = newgroup[i].index\nspk2 = newgroup[j].index\nauc, times = cross_correlogram(spk1, spk2, binsize, windowsize)\ncrosscorrs[(i, j)] = pd.Series(index=times, data=auc, dtype=\"float\")\ncrosscorrs = pd.DataFrame.from_dict(crosscorrs)\nif norm:\nfreq = newgroup.get_info(\"rate\")\nfreq2 = pd.Series(\nindex=pairs, data=list(map(lambda n: freq.loc[n[1]], pairs))\n)\ncrosscorrs = crosscorrs / freq2\nelif (\nisinstance(group, (tuple, list))\nand len(group) == 2\nand all(map(lambda g: isinstance(g, nap.TsGroup), group))\n):\nif isinstance(ep, nap.IntervalSet):\nnewgroup = [group[i].restrict(ep) for i in range(2)]\nelse:\nnewgroup = group\npairs = product(list(newgroup[0].keys()), list(newgroup[1].keys()))\nfor i, j in pairs:\nspk1 = newgroup[0][i].index\nspk2 = newgroup[1][j].index\nauc, times = cross_correlogram(spk1, spk2, binsize, windowsize)\nif norm:\nauc /= newgroup[1][j].rate\ncrosscorrs[(i, j)] = pd.Series(index=times, data=auc, dtype=\"float\")\ncrosscorrs = pd.DataFrame.from_dict(crosscorrs)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nreturn crosscorrs.astype(\"float\")\n</code></pre>"},{"location":"reference/process/correlograms/#pynapple.process.correlograms.compute_eventcorrelogram","title":"compute_eventcorrelogram","text":"<pre><code>compute_eventcorrelogram(\ngroup,\nevent,\nbinsize,\nwindowsize,\nep=None,\nnorm=True,\ntime_units=\"s\",\n)\n</code></pre> <p>Computes the correlograms of a group of Ts/Tsd objects with another single Ts/Tsd object The time of reference is the event times.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects to correlate with the event</p> required <code>event</code> <code>Ts / Tsd</code> <p>The event to correlate the each of the time series in the group with.</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which cross-corrs are computed. If None, the epoch is the time support of the event.</p> <code>None</code> <code>norm</code> <code>bool</code> <p>If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series) If False, cross-orrelograms are returned as the rate (Hz) of the target time series (relative to the event time series)</p> <code>True</code> <code>time_units</code> <code>str</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_eventcorrelogram(\ngroup, event, binsize, windowsize, ep=None, norm=True, time_units=\"s\"\n):\n\"\"\"\n    Computes the correlograms of a group of Ts/Tsd objects with another single Ts/Tsd object\n    The time of reference is the event times.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects to correlate with the event\n    event : Ts/Tsd\n        The event to correlate the each of the time series in the group with.\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which cross-corrs are computed.\n        If None, the epoch is the time support of the event.\n    norm : bool, optional\n        If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)\n        If False, cross-orrelograms are returned as the rate (Hz) of the target time series (relative to the event time series)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup\n    \"\"\"\nif ep is None:\nep = event.time_support\ntsd1 = event.index\nelse:\ntsd1 = event.restrict(ep).index\nif type(group) is nap.TsGroup:\nnewgroup = group.restrict(ep)\nelse:\nraise RuntimeError(\"Unknown format for group\")\ncrosscorrs = {}\nbinsize = nap.TsIndex.format_timestamps(\nnp.array([binsize], dtype=np.float64), time_units\n)[0]\nwindowsize = nap.TsIndex.format_timestamps(\nnp.array([windowsize], dtype=np.float64), time_units\n)[0]\nfor n in newgroup.keys():\nspk_time = newgroup[n].index\nauc, times = cross_correlogram(tsd1, spk_time, binsize, windowsize)\ncrosscorrs[n] = pd.Series(index=times, data=auc, dtype=\"float\")\ncrosscorrs = pd.DataFrame.from_dict(crosscorrs)\nif norm:\ncrosscorrs = crosscorrs / newgroup.get_info(\"rate\")\nreturn crosscorrs.astype(\"float\")\n</code></pre>"},{"location":"reference/process/decoding/","title":"Decoding","text":""},{"location":"reference/process/decoding/#pynapple.process.decoding","title":"pynapple.process.decoding","text":""},{"location":"reference/process/decoding/#pynapple.process.decoding.decode_1d","title":"decode_1d","text":"<pre><code>decode_1d(\ntuning_curves,\ngroup,\nep,\nbin_size,\ntime_units=\"s\",\nfeature=None,\n)\n</code></pre> <p>Performs Bayesian decoding over a one dimensional feature. See: Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.</p> <p>Parameters:</p> Name Type Description Default <code>tuning_curves</code> <code>DataFrame</code> <p>Each column is the tuning curve of one neuron relative to the feature. Index should be the center of the bin.</p> required <code>group</code> <code>TsGroup or dict of Ts/Tsd object.</code> <p>A group of neurons with the same index as tuning curves column names.</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which decoding is computed</p> required <code>bin_size</code> <code>float</code> <p>Bin size. Default is second. Use the parameter time_units to change it.</p> required <code>time_units</code> <code>str</code> <p>Time unit of the bin size ('s' [default], 'ms', 'us').</p> <code>'s'</code> <code>feature</code> <code>Tsd</code> <p>The 1d feature used to compute the tuning curves. Used to correct for occupancy. If feature is not passed, the occupancy is uniform.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tsd</code> <p>The decoded feature</p> <code>TsdFrame</code> <p>The probability distribution of the decoded feature for each time bin</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a dict of Ts/Tsd or TsGroup. If different size of neurons for tuning_curves and group. If indexes don't match between tuning_curves and group.</p> Source code in <code>pynapple/process/decoding.py</code> <pre><code>def decode_1d(tuning_curves, group, ep, bin_size, time_units=\"s\", feature=None):\n\"\"\"\n    Performs Bayesian decoding over a one dimensional feature.\n    See:\n    Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J.\n    (1998). Interpreting neuronal population activity by\n    reconstruction: unified framework with application to\n    hippocampal place cells. Journal of neurophysiology, 79(2),\n    1017-1044.\n    Parameters\n    ----------\n    tuning_curves : pandas.DataFrame\n        Each column is the tuning curve of one neuron relative to the feature.\n        Index should be the center of the bin.\n    group : TsGroup or dict of Ts/Tsd object.\n        A group of neurons with the same index as tuning curves column names.\n    ep : IntervalSet\n        The epoch on which decoding is computed\n    bin_size : float\n        Bin size. Default is second. Use the parameter time_units to change it.\n    time_units : str, optional\n        Time unit of the bin size ('s' [default], 'ms', 'us').\n    feature : Tsd, optional\n        The 1d feature used to compute the tuning curves. Used to correct for occupancy.\n        If feature is not passed, the occupancy is uniform.\n    Returns\n    -------\n    Tsd\n        The decoded feature\n    TsdFrame\n        The probability distribution of the decoded feature for each time bin\n    Raises\n    ------\n    RuntimeError\n        If group is not a dict of Ts/Tsd or TsGroup.\n        If different size of neurons for tuning_curves and group.\n        If indexes don't match between tuning_curves and group.\n    \"\"\"\nif isinstance(group, dict):\nnewgroup = nap.TsGroup(group, time_support=ep)\nelif isinstance(group, nap.TsGroup):\nnewgroup = group.restrict(ep)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nif tuning_curves.shape[1] != len(newgroup):\nraise RuntimeError(\"Different shapes for tuning_curves and group\")\nif not np.all(tuning_curves.columns.values == np.array(newgroup.keys())):\nraise RuntimeError(\"Difference indexes for tuning curves and group keys\")\n# Bin spikes\ncount = newgroup.count(bin_size, ep, time_units)\n# Occupancy\nif feature is None:\noccupancy = np.ones(tuning_curves.shape[0])\nelif isinstance(feature, nap.Tsd):\ndiff = np.diff(tuning_curves.index.values)\nbins = tuning_curves.index.values[:-1] - diff / 2\nbins = np.hstack(\n(bins, [bins[-1] + diff[-1], bins[-1] + 2 * diff[-1]])\n)  # assuming the size of the last 2 bins is equal\noccupancy, _ = np.histogram(feature.values, bins)\nelse:\nraise RuntimeError(\"Unknown format for feature in decode_1d\")\n# Transforming to pure numpy array\ntc = tuning_curves.values\nct = count.values\nbin_size_s = nap.TsIndex.format_timestamps(\nnp.array([bin_size], dtype=np.float64), time_units\n)[0]\np1 = np.exp(-bin_size_s * tc.sum(1))\np2 = occupancy / occupancy.sum()\nct2 = np.tile(ct[:, np.newaxis, :], (1, tc.shape[0], 1))\np3 = np.prod(tc**ct2, -1)\np = p1 * p2 * p3\np = p / p.sum(1)[:, np.newaxis]\nidxmax = np.argmax(p, 1)\np = nap.TsdFrame(\nt=count.index, d=p, time_support=ep, columns=tuning_curves.index.values\n)\ndecoded = nap.Tsd(\nt=count.index, d=tuning_curves.index.values[idxmax], time_support=ep\n)\nreturn decoded, p\n</code></pre>"},{"location":"reference/process/decoding/#pynapple.process.decoding.decode_2d","title":"decode_2d","text":"<pre><code>decode_2d(\ntuning_curves,\ngroup,\nep,\nbin_size,\nxy,\ntime_units=\"s\",\nfeatures=None,\n)\n</code></pre> <p>Performs Bayesian decoding over a two dimensional feature. See: Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.</p> <p>Parameters:</p> Name Type Description Default <code>tuning_curves</code> <code>dict</code> <p>Dictionnay of 2d tuning curves (one for each neuron).</p> required <code>group</code> <code>TsGroup or dict of Ts/Tsd object.</code> <p>A group of neurons with the same keys as tuning_curves dictionnary.</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which decoding is computed</p> required <code>bin_size</code> <code>float</code> <p>Bin size. Default is second. Use the parameter time_units to change it.</p> required <code>xy</code> <code>tuple</code> <p>A tuple of bin positions for the tuning curves i.e. xy=(x,y)</p> required <code>time_units</code> <code>str</code> <p>Time unit of the bin size ('s' [default], 'ms', 'us').</p> <code>'s'</code> <code>features</code> <code>TsdFrame</code> <p>The 2 columns features used to compute the tuning curves. Used to correct for occupancy. If feature is not passed, the occupancy is uniform.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tsd</code> <p>The decoded feature in 2d</p> <code>ndarray</code> <p>The probability distribution of the decoded trajectory for each time bin</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a dict of Ts/Tsd or TsGroup. If different size of neurons for tuning_curves and group. If indexes don't match between tuning_curves and group.</p> Source code in <code>pynapple/process/decoding.py</code> <pre><code>def decode_2d(tuning_curves, group, ep, bin_size, xy, time_units=\"s\", features=None):\n\"\"\"\n    Performs Bayesian decoding over a two dimensional feature.\n    See:\n    Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J.\n    (1998). Interpreting neuronal population activity by\n    reconstruction: unified framework with application to\n    hippocampal place cells. Journal of neurophysiology, 79(2),\n    1017-1044.\n    Parameters\n    ----------\n    tuning_curves : dict\n        Dictionnay of 2d tuning curves (one for each neuron).\n    group : TsGroup or dict of Ts/Tsd object.\n        A group of neurons with the same keys as tuning_curves dictionnary.\n    ep : IntervalSet\n        The epoch on which decoding is computed\n    bin_size : float\n        Bin size. Default is second. Use the parameter time_units to change it.\n    xy : tuple\n        A tuple of bin positions for the tuning curves i.e. xy=(x,y)\n    time_units : str, optional\n        Time unit of the bin size ('s' [default], 'ms', 'us').\n    features : TsdFrame\n        The 2 columns features used to compute the tuning curves. Used to correct for occupancy.\n        If feature is not passed, the occupancy is uniform.\n    Returns\n    -------\n    Tsd\n        The decoded feature in 2d\n    numpy.ndarray\n        The probability distribution of the decoded trajectory for each time bin\n    Raises\n    ------\n    RuntimeError\n        If group is not a dict of Ts/Tsd or TsGroup.\n        If different size of neurons for tuning_curves and group.\n        If indexes don't match between tuning_curves and group.\n    \"\"\"\nif type(group) is dict:\nnewgroup = nap.TsGroup(group, time_support=ep)\nnumcells = len(newgroup)\nelif type(group) is nap.TsGroup:\nnewgroup = group.restrict(ep)\nnumcells = len(newgroup)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nif len(tuning_curves) != numcells:\nraise RuntimeError(\"Different shapes for tuning_curves and group\")\nif not np.all(np.array(list(tuning_curves.keys())) == np.array(newgroup.keys())):\nraise RuntimeError(\"Difference indexes for tuning curves and group keys\")\n# Bin spikes\n# if type(newgroup) is not nap.TsdFrame:\ncount = newgroup.count(bin_size, ep, time_units)\n# else:\n#     #Spikes already \"binned\" with continuous TsdFrame input\n#     count = newgroup\nindexes = list(tuning_curves.keys())\n# Occupancy\nif features is None:\noccupancy = np.ones_like(tuning_curves[indexes[0]]).flatten()\nelse:\nbinsxy = []\nfor i in range(len(xy)):\ndiff = np.diff(xy[i])\nbins = xy[i][:-1] - diff / 2\nbins = np.hstack(\n(bins, [bins[-1] + diff[-1], bins[-1] + 2 * diff[-1]])\n)  # assuming the size of the last 2 bins is equal\nbinsxy.append(bins)\noccupancy, _, _ = np.histogram2d(\nfeatures[:, 0].values, features[:, 1].values, [binsxy[0], binsxy[1]]\n)\noccupancy = occupancy.flatten()\n# Transforming to pure numpy array\ntc = np.array([tuning_curves[i] for i in tuning_curves.keys()])\ntc = tc.reshape(tc.shape[0], np.prod(tc.shape[1:]))\ntc = tc.T\nct = count.values\nbin_size_s = nap.TsIndex.format_timestamps(\nnp.array([bin_size], dtype=np.float64), time_units\n)[0]\np1 = np.exp(-bin_size_s * np.nansum(tc, 1))\np2 = occupancy / occupancy.sum()\nct2 = np.tile(ct[:, np.newaxis, :], (1, tc.shape[0], 1))\np3 = np.nanprod(tc**ct2, -1)\np = p1 * p2 * p3\np = p / p.sum(1)[:, np.newaxis]\nidxmax = np.argmax(p, 1)\np = p.reshape(p.shape[0], len(xy[0]), len(xy[1]))\nidxmax2d = np.unravel_index(idxmax, (len(xy[0]), len(xy[1])))\nif features is not None:\ncols = features.columns\nelse:\ncols = np.arange(2)\ndecoded = nap.TsdFrame(\nt=count.index,\nd=np.vstack((xy[0][idxmax2d[0]], xy[1][idxmax2d[1]])).T,\ntime_support=ep,\ncolumns=cols,\n)\nreturn decoded, p\n</code></pre>"},{"location":"reference/process/perievent/","title":"Perievent","text":""},{"location":"reference/process/perievent/#pynapple.process.perievent","title":"pynapple.process.perievent","text":""},{"location":"reference/process/perievent/#pynapple.process.perievent.compute_perievent","title":"compute_perievent","text":"<pre><code>compute_perievent(data, tref, minmax, time_unit='s')\n</code></pre> <p>Center ts/tsd/tsgroup object around the timestamps given by the tref argument. minmax indicates the start and end of the window.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Ts / Tsd / TsGroup</code> <p>The data to align to tref. If Ts/Tsd, returns a TsGroup. If TsGroup, returns a dictionnary of TsGroup</p> required <code>tref</code> <code>Ts / Tsd</code> <p>The timestamps of the event to align to</p> required <code>minmax</code> <code>tuple or int or float</code> <p>The window size. Can be unequal on each side i.e. (-500, 1000).</p> required <code>time_unit</code> <code>str</code> <p>Time units of the minmax ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A TsGroup if data is a Ts/Tsd or a dictionnary of TsGroup if data is a TsGroup.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if tref is not a Ts/Tsd object or if data is not a Ts/Tsd or TsGroup</p> Source code in <code>pynapple/process/perievent.py</code> <pre><code>def compute_perievent(data, tref, minmax, time_unit=\"s\"):\n\"\"\"\n    Center ts/tsd/tsgroup object around the timestamps given by the tref argument.\n    minmax indicates the start and end of the window.\n    Parameters\n    ----------\n    data : Ts/Tsd/TsGroup\n        The data to align to tref.\n        If Ts/Tsd, returns a TsGroup.\n        If TsGroup, returns a dictionnary of TsGroup\n    tref : Ts/Tsd\n        The timestamps of the event to align to\n    minmax : tuple or int or float\n        The window size. Can be unequal on each side i.e. (-500, 1000).\n    time_unit : str, optional\n        Time units of the minmax ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    dict\n        A TsGroup if data is a Ts/Tsd or\n        a dictionnary of TsGroup if data is a TsGroup.\n    Raises\n    ------\n    RuntimeError\n        if tref is not a Ts/Tsd object or if data is not a Ts/Tsd or TsGroup\n    \"\"\"\nif not isinstance(tref, (nap.Ts, nap.Tsd)):\nraise RuntimeError(\"tref should be a Tsd object.\")\nif isinstance(minmax, float) or isinstance(minmax, int):\nminmax = np.array([minmax, minmax], dtype=np.float64)\nwindow = np.abs(nap.TsIndex.format_timestamps(np.array(minmax), time_unit))\ntime_support = nap.IntervalSet(start=-window[0], end=window[1])\nif isinstance(data, nap.TsGroup):\ntoreturn = {}\nfor n in data.index:\ntoreturn[n] = _align_tsd(data[n], tref, window, time_support)\nreturn toreturn\nelif isinstance(data, (nap.Ts, nap.Tsd)):\nreturn _align_tsd(data, tref, window, time_support)\nelse:\nraise RuntimeError(\"Unknown format for data\")\n</code></pre>"},{"location":"reference/process/perievent/#pynapple.process.perievent.compute_event_trigger_average","title":"compute_event_trigger_average","text":"<pre><code>compute_event_trigger_average(\ngroup, feature, binsize, windowsize, ep, time_units=\"s\"\n)\n</code></pre> <p>Bin the spike train in binsize and compute the Spike Trigger Average (STA) within windowsize. If C is the spike count matrix and feature is a Tsd array, the function computes the Hankel matrix H from windowsize=(-t1,+t2) by offseting the Tsd array.</p> <p>The STA is then defined as the dot product between H and C divided by the number of spikes.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects that hold the trigger time.</p> required <code>feature</code> <code>Tsd</code> <p>The 1-dimensional feature to average</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which STA are computed</p> required <code>time_units</code> <code>str</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>TsdFrame</code> <p>A TsdFrame of Spike-Trigger Average. Each column is an element from the group.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if group is not a Ts/Tsd or TsGroup</p> Source code in <code>pynapple/process/perievent.py</code> <pre><code>def compute_event_trigger_average(\ngroup, feature, binsize, windowsize, ep, time_units=\"s\"\n):\n\"\"\"\n    Bin the spike train in binsize and compute the Spike Trigger Average (STA) within windowsize.\n    If C is the spike count matrix and feature is a Tsd array, the function computes\n    the Hankel matrix H from windowsize=(-t1,+t2) by offseting the Tsd array.\n    The STA is then defined as the dot product between H and C divided by the number of spikes.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects that hold the trigger time.\n    feature : Tsd\n        The 1-dimensional feature to average\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which STA are computed\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    TsdFrame\n        A TsdFrame of Spike-Trigger Average. Each column is an element from the group.\n    Raises\n    ------\n    RuntimeError\n        if group is not a Ts/Tsd or TsGroup\n    \"\"\"\nif type(group) is not nap.TsGroup:\nraise RuntimeError(\"Unknown format for group\")\nbinsize = nap.TsIndex.format_timestamps(\nnp.array([binsize], dtype=np.float64), time_units\n)[0]\nstart = np.abs(\nnap.TsIndex.format_timestamps(\nnp.array([windowsize[0]], dtype=np.float64), time_units\n)[0]\n)\nend = np.abs(\nnap.TsIndex.format_timestamps(\nnp.array([windowsize[1]], dtype=np.float64), time_units\n)[0]\n)\nidx1 = -np.arange(0, start + binsize, binsize)[::-1][:-1]\nidx2 = np.arange(0, end + binsize, binsize)[1:]\ntime_idx = np.hstack((idx1, np.zeros(1), idx2))\ncount = group.count(binsize, ep)\ntmp = feature.bin_average(binsize, ep)\n# Build the Hankel matrix\nn_p = len(idx1)\nn_f = len(idx2)\npad_tmp = np.pad(tmp, (n_p, n_f))\noffset_tmp = hankel(pad_tmp, pad_tmp[-(n_p + n_f + 1) :])[0 : len(tmp)]\nsta = np.dot(offset_tmp.T, count.values)\nsta = sta / np.sum(count, 0)\nsta = nap.TsdFrame(t=time_idx, d=sta, columns=group.index)\nreturn sta\n</code></pre>"},{"location":"reference/process/randomize/","title":"Randomize","text":""},{"location":"reference/process/randomize/#pynapple.process.randomize","title":"pynapple.process.randomize","text":""},{"location":"reference/process/randomize/#pynapple.process.randomize.shift_timestamps","title":"shift_timestamps","text":"<pre><code>shift_timestamps(ts, min_shift=0.0, max_shift=None)\n</code></pre> <p>Shifts all the time stamps of a random amount between min_shift and max_shift, wrapping the end of the time support to the beginning.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to shift. If TsGroup, shifts all Ts in the group independently.</p> required <code>min_shift</code> <code>float</code> <p>minimum shift (default: 0 )</p> <code>0.0</code> <code>max_shift</code> <code>float</code> <p>maximum shift, (default: length of time support)</p> <code>None</code> <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The randomly shifted timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def shift_timestamps(ts, min_shift=0.0, max_shift=None):\n\"\"\"\n    Shifts all the time stamps of a random amount between min_shift and max_shift, wrapping the\n    end of the time support to the beginning.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to shift. If TsGroup, shifts all Ts in the group independently.\n    min_shift : float, optional\n        minimum shift (default: 0 )\n    max_shift : float, optional\n        maximum shift, (default: length of time support)\n    Returns\n    -------\n    Ts or TsGroup\n        The randomly shifted timestamps\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _shift_ts,\nnap.ts_group.TsGroup: _shift_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nstrategy = strategies[type(ts)]\nreturn strategy(ts, min_shift, max_shift)\n</code></pre>"},{"location":"reference/process/randomize/#pynapple.process.randomize.shuffle_ts_intervals","title":"shuffle_ts_intervals","text":"<pre><code>shuffle_ts_intervals(ts, min_shift=0.0, max_shift=None)\n</code></pre> <p>Randomizes the timestamps by shuffling the intervals between them.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to randomize. If TsGroup, randomizes all Ts in the group independently.</p> required <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The randomized timestamps, with shuffled intervals</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def shuffle_ts_intervals(ts, min_shift=0.0, max_shift=None):\n\"\"\"\n    Randomizes the timestamps by shuffling the intervals between them.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to randomize. If TsGroup, randomizes all Ts in the group independently.\n    Returns\n    -------\n    Ts or TsGroup\n        The randomized timestamps, with shuffled intervals\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _shuffle_intervals_ts,\nnap.ts_group.TsGroup: _shuffle_intervals_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nstrategy = strategies[type(ts)]\nreturn strategy(ts)\n</code></pre>"},{"location":"reference/process/randomize/#pynapple.process.randomize.jitter_timestamps","title":"jitter_timestamps","text":"<pre><code>jitter_timestamps(ts, max_jitter=None, keep_tsupport=False)\n</code></pre> <p>Jitters each time stamp independently of random amounts uniformly drawn between -max_jitter and max_jitter.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to jitter. If TsGroup, jitter is applied to each element of the group.</p> required <code>max_jitter</code> <code>float</code> <p>maximum jitter</p> <code>None</code> <code>keep_tsupport</code> <p>If True, keep time support of the input. The number of timestamps will not be conserved. If False, the time support is inferred from the jittered timestamps. The number of tmestamps is conserved. (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The jittered timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def jitter_timestamps(ts, max_jitter=None, keep_tsupport=False):\n\"\"\"\n    Jitters each time stamp independently of random amounts uniformly drawn between -max_jitter and max_jitter.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to jitter. If TsGroup, jitter is applied to each element of the group.\n    max_jitter : float\n        maximum jitter\n    keep_tsupport: bool, optional\n        If True, keep time support of the input. The number of timestamps will not be conserved.\n        If False, the time support is inferred from the jittered timestamps. The number of tmestamps\n        is conserved. (default: False)\n    Returns\n    -------\n    Ts or TsGroup\n        The jittered timestamps\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _jitter_ts,\nnap.ts_group.TsGroup: _jitter_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nif max_jitter is None:\nraise TypeError(\"missing required argument: max_jitter \")\nstrategy = strategies[type(ts)]\nreturn strategy(ts, max_jitter, keep_tsupport)\n</code></pre>"},{"location":"reference/process/randomize/#pynapple.process.randomize.resample_timestamps","title":"resample_timestamps","text":"<pre><code>resample_timestamps(ts)\n</code></pre> <p>Resamples the timestamps in the time support, with uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to resample. If TsGroup, each Ts object in the group is independently resampled, in the time support of the whole group.</p> required <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The resampled timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def resample_timestamps(ts):\n\"\"\"\n    Resamples the timestamps in the time support, with uniform distribution.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to resample. If TsGroup, each Ts object in the group is independently\n        resampled, in the time support of the whole group.\n    Returns\n    -------\n    Ts or TsGroup\n        The resampled timestamps\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _resample_ts,\nnap.ts_group.TsGroup: _resample_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nstrategy = strategies[type(ts)]\nreturn strategy(ts)\n</code></pre>"},{"location":"reference/process/tuning_curves/","title":"Tuning curves","text":""},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves","title":"pynapple.process.tuning_curves","text":"<p>Summary</p>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_discrete_tuning_curves","title":"compute_discrete_tuning_curves","text":"<pre><code>compute_discrete_tuning_curves(group, dict_ep)\n</code></pre> <pre><code>Compute discrete tuning curves of a TsGroup using a dictionnary of epochs.\n</code></pre> <p>The function returns a pandas DataFrame with each row being a key of the dictionnary of epochs and each column being a neurons.</p> <p>This function can typically being used for a set of stimulus being presented for multiple epochs. An example of the dictionnary is :</p> <pre><code>&gt;&gt;&gt; dict_ep =  {\n        \"stim0\": nap.IntervalSet(start=0, end=1),\n        \"stim1\":nap.IntervalSet(start=2, end=3)\n    }\n</code></pre> <p>In this case, the function will return a pandas DataFrame :</p> <pre><code>&gt;&gt;&gt; tc\n           neuron0    neuron1    neuron2\nstim0        0 Hz       1 Hz       2 Hz\nstim1        3 Hz       4 Hz       5 Hz\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>dict_ep</code> <code>dict</code> <p>Dictionary of IntervalSets</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Table of firing rate for each neuron and each IntervalSet</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_discrete_tuning_curves(group, dict_ep):\n\"\"\"\n        Compute discrete tuning curves of a TsGroup using a dictionnary of epochs.\n    The function returns a pandas DataFrame with each row being a key of the dictionnary of epochs\n    and each column being a neurons.\n       This function can typically being used for a set of stimulus being presented for multiple epochs.\n    An example of the dictionnary is :\n        &gt;&gt;&gt; dict_ep =  {\n                \"stim0\": nap.IntervalSet(start=0, end=1),\n                \"stim1\":nap.IntervalSet(start=2, end=3)\n            }\n    In this case, the function will return a pandas DataFrame :\n        &gt;&gt;&gt; tc\n                   neuron0    neuron1    neuron2\n        stim0        0 Hz       1 Hz       2 Hz\n        stim1        3 Hz       4 Hz       5 Hz\n    Parameters\n    ----------\n    group : nap.TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    dict_ep : dict\n        Dictionary of IntervalSets\n    Returns\n    -------\n    pandas.DataFrame\n        Table of firing rate for each neuron and each IntervalSet\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object.\n    \"\"\"\nif not isinstance(group, nap.TsGroup):\nraise RuntimeError(\"Unknown format for group\")\nidx = np.sort(list(dict_ep.keys()))\ntuning_curves = pd.DataFrame(index=idx, columns=list(group.keys()), data=0)\nfor k in dict_ep.keys():\nif not isinstance(dict_ep[k], nap.IntervalSet):\nraise RuntimeError(\"Key {} in dict_ep is not an IntervalSet\".format(k))\nfor n in group.keys():\ntuning_curves.loc[k, n] = float(len(group[n].restrict(dict_ep[k])))\ntuning_curves.loc[k] = tuning_curves.loc[k] / dict_ep[k].tot_length(\"s\")\nreturn tuning_curves\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_1d_tuning_curves","title":"compute_1d_tuning_curves","text":"<pre><code>compute_1d_tuning_curves(\ngroup, feature, nb_bins, ep=None, minmax=None\n)\n</code></pre> <p>Computes 1-dimensional tuning curves relative to a 1d feature.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>feature</code> <code>Tsd</code> <p>The 1-dimensional target feature (e.g. head-direction)</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curve</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame to hold the tuning curves</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None):\n\"\"\"\n    Computes 1-dimensional tuning curves relative to a 1d feature.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    feature : Tsd\n        The 1-dimensional target feature (e.g. head-direction)\n    nb_bins : int\n        Number of bins in the tuning curve\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame to hold the tuning curves\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object.\n    \"\"\"\nif not isinstance(group, nap.TsGroup):\nraise RuntimeError(\"Unknown format for group\")\nif minmax is None:\nbins = np.linspace(np.min(feature), np.max(feature), nb_bins + 1)\nelse:\nbins = np.linspace(minmax[0], minmax[1], nb_bins + 1)\nidx = bins[0:-1] + np.diff(bins) / 2\ntuning_curves = pd.DataFrame(index=idx, columns=list(group.keys()))\nif isinstance(ep, nap.IntervalSet):\ngroup_value = group.value_from(feature, ep)\noccupancy, _ = np.histogram(feature.restrict(ep).values, bins)\nelse:\ngroup_value = group.value_from(feature)\noccupancy, _ = np.histogram(feature.values, bins)\nfor k in group_value:\ncount, _ = np.histogram(group_value[k].values, bins)\ncount = count / occupancy\ncount[np.isnan(count)] = 0.0\ntuning_curves[k] = count\ntuning_curves[k] = count * feature.rate\nreturn tuning_curves\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_2d_tuning_curves","title":"compute_2d_tuning_curves","text":"<pre><code>compute_2d_tuning_curves(\ngroup, feature, nb_bins, ep=None, minmax=None\n)\n</code></pre> <p>Computes 2-dimensional tuning curves relative to a 2d feature</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>feature</code> <code>TsdFrame</code> <p>The 2d feature (i.e. 2 columns features).</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curves</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list</code> <p>The min and max boundaries of the tuning curves given as: (minx, maxx, miny, maxy) If None, the boundaries are inferred from the target variable</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: </p> <p>tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).</p> <p>xy (list): List of bins center in the two dimensions</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object or if feature is not 2 columns only.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None):\n\"\"\"\n    Computes 2-dimensional tuning curves relative to a 2d feature\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    feature : TsdFrame\n        The 2d feature (i.e. 2 columns features).\n    nb_bins : int\n        Number of bins in the tuning curves\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves given as:\n        (minx, maxx, miny, maxy)\n        If None, the boundaries are inferred from the target variable\n    Returns\n    -------\n    tuple\n        A tuple containing: \\n\n        tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).\\n\n        xy (list): List of bins center in the two dimensions\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object or if feature is not 2 columns only.\n    \"\"\"\nif feature.shape[1] != 2:\nraise RuntimeError(\"feature should have 2 columns only.\")\nif type(group) is not nap.TsGroup:\nraise RuntimeError(\"Unknown format for group\")\nif isinstance(ep, nap.IntervalSet):\nfeature = feature.restrict(ep)\nelse:\nep = feature.time_support\ncols = list(feature.columns)\ngroups_value = {}\nbinsxy = {}\nfor i, c in enumerate(cols):\ngroups_value[c] = group.value_from(feature.loc[c], ep)\nif minmax is None:\nbins = np.linspace(\nnp.min(feature.loc[c]), np.max(feature.loc[c]), nb_bins + 1\n)\nelse:\nbins = np.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins + 1)\nbinsxy[c] = bins\noccupancy, _, _ = np.histogram2d(\nfeature.loc[cols[0]].values.flatten(),\nfeature.loc[cols[1]].values.flatten(),\n[binsxy[cols[0]], binsxy[cols[1]]],\n)\ntc = {}\nfor n in group.keys():\ncount, _, _ = np.histogram2d(\ngroups_value[cols[0]][n].values.flatten(),\ngroups_value[cols[1]][n].values.flatten(),\n[binsxy[cols[0]], binsxy[cols[1]]],\n)\ncount = count / occupancy\n# count[np.isnan(count)] = 0.0\ntc[n] = count * feature.rate\nxy = [binsxy[c][0:-1] + np.diff(binsxy[c]) / 2 for c in binsxy.keys()]\nreturn tc, xy\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_1d_mutual_info","title":"compute_1d_mutual_info","text":"<pre><code>compute_1d_mutual_info(\ntc, feature, ep=None, minmax=None, bitssec=False\n)\n</code></pre> <p>Mutual information as defined in</p> <p>Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993). An information-theoretic approach to deciphering the hippocampal code. In Advances in neural information processing systems (pp. 1030-1037).</p> <p>Parameters:</p> Name Type Description Default <code>tc</code> <code>DataFrame or ndarray</code> <p>Tuning curves in columns</p> required <code>feature</code> <code>Tsd</code> <p>The feature that was used to compute the tuning curves</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch over which the tuning curves were computed If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <code>bitssec</code> <code>bool</code> <p>By default, the function return bits per spikes. Set to true for bits per seconds</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Spatial Information (default is bits/spikes)</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_mutual_info(tc, feature, ep=None, minmax=None, bitssec=False):\n\"\"\"\n    Mutual information as defined in\n    Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993).\n    An information-theoretic approach to deciphering the hippocampal code.\n    In Advances in neural information processing systems (pp. 1030-1037).\n    Parameters\n    ----------\n    tc : pandas.DataFrame or numpy.ndarray\n        Tuning curves in columns\n    feature : Tsd\n        The feature that was used to compute the tuning curves\n    ep : IntervalSet, optional\n        The epoch over which the tuning curves were computed\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n    bitssec : bool, optional\n        By default, the function return bits per spikes.\n        Set to true for bits per seconds\n    Returns\n    -------\n    pandas.DataFrame\n        Spatial Information (default is bits/spikes)\n    \"\"\"\nif isinstance(tc, pd.DataFrame):\ncolumns = tc.columns.values\nfx = np.atleast_2d(tc.values)\nelif isinstance(tc, np.ndarray):\nfx = np.atleast_2d(tc)\ncolumns = np.arange(tc.shape[1])\nnb_bins = tc.shape[0] + 1\nif minmax is None:\nbins = np.linspace(np.min(feature), np.max(feature), nb_bins)\nelse:\nbins = np.linspace(minmax[0], minmax[1], nb_bins)\nif isinstance(ep, nap.IntervalSet):\noccupancy, _ = np.histogram(feature.restrict(ep).values, bins)\nelse:\noccupancy, _ = np.histogram(feature.values, bins)\noccupancy = occupancy / occupancy.sum()\noccupancy = occupancy[:, np.newaxis]\nfr = np.sum(fx * occupancy, 0)\nfxfr = fx / fr\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nlogfx = np.log2(fxfr)\nlogfx[np.isinf(logfx)] = 0.0\nSI = np.sum(occupancy * fx * logfx, 0)\nif bitssec:\nSI = pd.DataFrame(index=columns, columns=[\"SI\"], data=SI)\nreturn SI\nelse:\nSI = SI / fr\nSI = pd.DataFrame(index=columns, columns=[\"SI\"], data=SI)\nreturn SI\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_2d_mutual_info","title":"compute_2d_mutual_info","text":"<pre><code>compute_2d_mutual_info(\ntc, features, ep=None, minmax=None, bitssec=False\n)\n</code></pre> <p>Mutual information as defined in</p> <p>Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993). An information-theoretic approach to deciphering the hippocampal code. In Advances in neural information processing systems (pp. 1030-1037).</p> <p>Parameters:</p> Name Type Description Default <code>tc</code> <code>dict or ndarray</code> <p>If array, first dimension should be the neuron</p> required <code>features</code> <code>TsdFrame</code> <p>The 2 columns features that were used to compute the tuning curves</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch over which the tuning curves were computed If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target features</p> <code>None</code> <code>bitssec</code> <code>bool</code> <p>By default, the function return bits per spikes. Set to true for bits per seconds</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Spatial Information (default is bits/spikes)</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_mutual_info(tc, features, ep=None, minmax=None, bitssec=False):\n\"\"\"\n    Mutual information as defined in\n    Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993).\n    An information-theoretic approach to deciphering the hippocampal code.\n    In Advances in neural information processing systems (pp. 1030-1037).\n    Parameters\n    ----------\n    tc : dict or numpy.ndarray\n        If array, first dimension should be the neuron\n    features : TsdFrame\n        The 2 columns features that were used to compute the tuning curves\n    ep : IntervalSet, optional\n        The epoch over which the tuning curves were computed\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target features\n    bitssec : bool, optional\n        By default, the function return bits per spikes.\n        Set to true for bits per seconds\n    Returns\n    -------\n    pandas.DataFrame\n        Spatial Information (default is bits/spikes)\n    \"\"\"\n# A bit tedious here\nif type(tc) is dict:\nfx = np.array([tc[i] for i in tc.keys()])\nidx = list(tc.keys())\nelif type(tc) is np.ndarray:\nfx = tc\nidx = np.arange(len(tc))\nnb_bins = (fx.shape[1] + 1, fx.shape[2] + 1)\ncols = features.columns\nbins = []\nfor i, c in enumerate(cols):\nif minmax is None:\nbins.append(\nnp.linspace(\nnp.min(features.loc[c]), np.max(features.loc[c]), nb_bins[i]\n)\n)\nelse:\nbins.append(\nnp.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins[i])\n)\nif isinstance(ep, nap.IntervalSet):\nfeatures = features.restrict(ep)\noccupancy, _, _ = np.histogram2d(\nfeatures.loc[cols[0]].values.flatten(),\nfeatures.loc[cols[1]].values.flatten(),\n[bins[0], bins[1]],\n)\noccupancy = occupancy / occupancy.sum()\nfr = np.nansum(fx * occupancy, (1, 2))\nfr = fr[:, np.newaxis, np.newaxis]\nfxfr = fx / fr\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nlogfx = np.log2(fxfr)\nlogfx[np.isinf(logfx)] = 0.0\nSI = np.nansum(occupancy * fx * logfx, (1, 2))\nif bitssec:\nSI = pd.DataFrame(index=idx, columns=[\"SI\"], data=SI)\nreturn SI\nelse:\nSI = SI / fr[:, 0, 0]\nSI = pd.DataFrame(index=idx, columns=[\"SI\"], data=SI)\nreturn SI\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_1d_tuning_curves_continous","title":"compute_1d_tuning_curves_continous","text":"<pre><code>compute_1d_tuning_curves_continous(\ntsdframe, feature, nb_bins, ep=None, minmax=None\n)\n</code></pre> <p>Computes 1-dimensional tuning curves relative to a feature with continous data.</p> <p>Parameters:</p> Name Type Description Default <code>tsdframe</code> <code>Tsd or TsdFrame</code> <p>Input data (e.g. continus calcium data where each column is the calcium activity of one neuron)</p> required <code>feature</code> <code>Tsd</code> <p>The feature (one column)</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curves</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame to hold the tuning curves</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tsdframe is not a Tsd or a TsdFrame object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_tuning_curves_continous(\ntsdframe, feature, nb_bins, ep=None, minmax=None\n):\n\"\"\"\n    Computes 1-dimensional tuning curves relative to a feature with continous data.\n    Parameters\n    ----------\n    tsdframe : Tsd or TsdFrame\n        Input data (e.g. continus calcium data\n        where each column is the calcium activity of one neuron)\n    feature : Tsd\n        The feature (one column)\n    nb_bins : int\n        Number of bins in the tuning curves\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame to hold the tuning curves\n    Raises\n    ------\n    RuntimeError\n        If tsdframe is not a Tsd or a TsdFrame object.\n    \"\"\"\nif not isinstance(tsdframe, (nap.Tsd, nap.TsdFrame)):\nraise RuntimeError(\"Unknown format for tsdframe.\")\nif isinstance(ep, nap.IntervalSet):\nfeature = feature.restrict(ep)\ntsdframe = tsdframe.restrict(ep)\nelse:\ntsdframe = tsdframe.restrict(feature.time_support)\nif minmax is None:\nbins = np.linspace(np.min(feature), np.max(feature), nb_bins + 1)\nelse:\nbins = np.linspace(minmax[0], minmax[1], nb_bins + 1)\nalign_times = tsdframe.value_from(feature)\nidx = np.digitize(align_times.values, bins) - 1\ntmp = tsdframe.as_dataframe().groupby(idx).mean()\ntmp = tmp.reindex(np.arange(0, len(bins) - 1))\ntmp.index = pd.Index(bins[0:-1] + np.diff(bins) / 2)\ntmp = tmp.fillna(0)\nreturn pd.DataFrame(tmp)\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_2d_tuning_curves_continuous","title":"compute_2d_tuning_curves_continuous","text":"<pre><code>compute_2d_tuning_curves_continuous(\ntsdframe, features, nb_bins, ep=None, minmax=None\n)\n</code></pre> <p>Computes 2-dimensional tuning curves relative to a 2d feature with continous data.</p> <p>Parameters:</p> Name Type Description Default <code>tsdframe</code> <code>Tsd or TsdFrame</code> <p>Input data (e.g. continuous calcium data where each column is the calcium activity of one neuron)</p> required <code>features</code> <code>TsdFrame</code> <p>The 2d feature (two columns)</p> required <code>nb_bins</code> <code>int or tuple</code> <p>Number of bins in the tuning curves (separate for 2 feature dimensions if tuple provided)</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list</code> <p>The min and max boundaries of the tuning curves. Should be a tuple of minx, maxx, miny, maxy If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: </p> <p>tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).</p> <p>xy (list): List of bins center in the two dimensions</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tsdframe is not a Tsd/TsdFrame or if features is not 2 columns</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_tuning_curves_continuous(\ntsdframe, features, nb_bins, ep=None, minmax=None\n):\n\"\"\"\n    Computes 2-dimensional tuning curves relative to a 2d feature with continous data.\n    Parameters\n    ----------\n    tsdframe : Tsd or TsdFrame\n        Input data (e.g. continuous calcium data\n        where each column is the calcium activity of one neuron)\n    features : TsdFrame\n        The 2d feature (two columns)\n    nb_bins : int or tuple\n        Number of bins in the tuning curves (separate for 2 feature dimensions if tuple provided)\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        Should be a tuple of minx, maxx, miny, maxy\n        If None, the boundaries are inferred from the target feature\n    Returns\n    -------\n    tuple\n        A tuple containing: \\n\n        tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).\\n\n        xy (list): List of bins center in the two dimensions\n    Raises\n    ------\n    RuntimeError\n        If tsdframe is not a Tsd/TsdFrame or if features is not 2 columns\n    \"\"\"\nif not isinstance(tsdframe, (nap.Tsd, nap.TsdFrame)):\nraise RuntimeError(\"Unknown format for tsdframe.\")\nif not isinstance(features, nap.TsdFrame):\nraise RuntimeError(\"Unknown format for features.\")\nif isinstance(ep, nap.IntervalSet):\nfeatures = features.restrict(ep)\ntsdframe = tsdframe.restrict(ep)\nelse:\ntsdframe = tsdframe.restrict(features.time_support)\nif features.shape[1] != 2:\nraise RuntimeError(\"features input is not 2 columns.\")\nif isinstance(nb_bins, int):\nnb_bins = (nb_bins, nb_bins)\nelif len(nb_bins) != 2:\nraise RuntimeError(\"nb_bins should be int or tuple of 2 ints\")\ncols = list(features.columns)\nbinsxy = {}\nidxs = {}\nfor i, c in enumerate(cols):\nif minmax is None:\nbins = np.linspace(\nnp.min(features.loc[c]), np.max(features.loc[c]), nb_bins[i] + 1\n)\nelse:\nbins = np.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins[i] + 1)\nalign_times = tsdframe.value_from(features.loc[c], ep)\nidxs[c] = np.digitize(align_times.values.flatten(), bins) - 1\nbinsxy[c] = bins\nidxs = pd.DataFrame(idxs)\ntc_np = np.zeros((tsdframe.shape[1], nb_bins[0], nb_bins[1])) * np.nan\nfor k, tmp in idxs.groupby(cols):\nif (0 &lt;= k[0] &lt; nb_bins[0]) and (0 &lt;= k[1] &lt; nb_bins[1]):\ntc_np[:, k[0], k[1]] = np.mean(tsdframe[tmp.index].values, 0)\ntc_np[np.isnan(tc_np)] = 0.0\nxy = [binsxy[c][0:-1] + np.diff(binsxy[c]) / 2 for c in binsxy.keys()]\ntc = {c: tc_np[i] for i, c in enumerate(tsdframe.columns)}\nreturn tc, xy\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_1d_poisson_glm","title":"compute_1d_poisson_glm","text":"<pre><code>compute_1d_poisson_glm(\ngroup,\nfeature,\nbinsize,\nwindowsize,\nep,\ntime_units=\"s\",\nniter=100,\ntolerance=1e-05,\n)\n</code></pre> <p>Poisson GLM</p> <p>Warning : this function is still experimental!</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>Spike trains</p> required <code>feature</code> <code>Tsd</code> <p>The regressors</p> required <code>binsize</code> <code>float</code> <p>Bin size</p> required <code>windowsize</code> <code>Float</code> <p>The window for offsetting the regressors</p> required <code>ep</code> <code>IntervalSet</code> <p>On which epoch to perfom the GLM</p> required <code>time_units</code> <code>str</code> <p>Time units of binsize and windowsize</p> <code>'s'</code> <code>niter</code> <code>int</code> <p>Number of iteration for fitting the GLM</p> <code>100</code> <code>tolerance</code> <code>float</code> <p>Tolerance for stopping the IRLS</p> <code>1e-05</code> <p>Returns:</p> Type Description <code>tuple</code> <p>regressors : TsdFrame</p> <p>offset : pandas.Series</p> <p>prediction : TsdFrame</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if group is not a TsGroup</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_poisson_glm(\ngroup, feature, binsize, windowsize, ep, time_units=\"s\", niter=100, tolerance=1e-5\n):\n\"\"\"\n    Poisson GLM\n    Warning : this function is still experimental!\n    Parameters\n    ----------\n    group : TsGroup\n        Spike trains\n    feature : Tsd\n        The regressors\n    binsize : float\n        Bin size\n    windowsize : Float\n        The window for offsetting the regressors\n    ep : IntervalSet, optional\n        On which epoch to perfom the GLM\n    time_units : str, optional\n        Time units of binsize and windowsize\n    niter : int, optional\n        Number of iteration for fitting the GLM\n    tolerance : float, optional\n        Tolerance for stopping the IRLS\n    Returns\n    -------\n    tuple\n        regressors : TsdFrame\\n\n        offset : pandas.Series\\n\n        prediction : TsdFrame\\n\n    Raises\n    ------\n    RuntimeError\n        if group is not a TsGroup\n    \"\"\"\nif type(group) is nap.TsGroup:\nnewgroup = group.restrict(ep)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nbinsize = nap.TsIndex.format_timestamps(binsize, time_units)[0]\nwindowsize = nap.TsIndex.format_timestamps(windowsize, time_units)[0]\n# Bin the spike train\ncount = newgroup.count(binsize)\n# Downsample the feature to binsize\ntidx = []\ndfeat = []\nfor i in ep.index:\nbins = np.arange(ep.start[i], ep.end[i] + binsize, binsize)\nidx = np.digitize(feature.index.values, bins) - 1\ntmp = feature.groupby(idx).mean()\ntidx.append(bins[0:-1] + np.diff(bins) / 2)\ndfeat.append(tmp)\ndfeat = nap.Tsd(t=np.hstack(tidx), d=np.hstack(dfeat), time_support=ep)\n# Build the Hankel matrix\nnt = np.abs(windowsize // binsize).astype(\"int\") + 1\nX = hankel(\nnp.hstack((np.zeros(nt - 1), dfeat.values))[: -nt + 1], dfeat.values[-nt:]\n)\nX = np.hstack((np.ones((len(dfeat), 1)), X))\n# Fitting GLM for each neuron\nregressors = []\nfor i, n in enumerate(group.keys()):\nprint(\"Fitting Poisson GLM for unit %i\" % n)\nb = nap.jitted_functions.jit_poisson_IRLS(\nX, count[n].values, niter=niter, tolerance=tolerance\n)\nregressors.append(b)\nregressors = np.array(regressors).T\noffset = regressors[0]\nregressors = regressors[1:]\nregressors = nap.TsdFrame(\nt=np.arange(-nt + 1, 1) * binsize, d=regressors, columns=list(group.keys())\n)\noffset = pd.Series(index=group.keys(), data=offset)\nprediction = nap.TsdFrame(\nt=dfeat.index.values,\nd=np.exp(np.dot(X[:, 1:], regressors.values) + offset.values) * binsize,\n)\nreturn (regressors, offset, prediction)\n</code></pre>"}]}