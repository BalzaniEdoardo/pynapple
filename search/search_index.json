{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>==========================</p> <p> </p> <p>PYthon Neural Analysis Package.</p> <p>pynapple is a light-weight python library for neurophysiological data analysis. The goal is to offer a versatile set of tools to study typical data in the field, i.e. time series (spike times, behavioral events, etc.) and time intervals (trials, brain states, etc.). It also provides users with generic functions for neuroscience such as tuning curves and cross-correlograms.</p> <ul> <li>Free software: GNU General Public License v3</li> <li>Documentation: https://pynapple-org.github.io/pynapple</li> <li>Notebooks and tutorials : https://pynapple-org.github.io/pynapple/notebooks/pynapple-quick-start/</li> <li>Collaborative repository: https://github.com/pynapple-org/pynacollada</li> </ul> <p>Note If you are using pynapple, please cite the following biorxiv paper</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<p>The best way to install pynapple is with pip within a new conda environment :</p> <pre><code>$ conda create --name pynapple pip python=3.8\n$ conda activate pynapple\n$ pip install pynapple\n</code></pre> <p>or directly from the source code:</p> <pre><code>$ conda create --name pynapple pip python=3.8\n$ conda activate pynapple\n$ # clone the repository\n$ git clone https://github.com/pynapple-org/pynapple.git\n$ cd pynapple\n$ # Install in editable mode with `-e` or, equivalently, `--editable`\n$ pip install -e .\n</code></pre> <p>Note The package is now using a pyproject.toml file for installation and dependencies management. If you want to run the tests, use pip install -e .[dev]</p> <p>This procedure will install all the dependencies including </p> <ul> <li>pandas</li> <li>numpy</li> <li>scipy</li> <li>numba</li> <li>pynwb 2.0</li> <li>tabulate</li> <li>h5py</li> </ul> <p>For spyder users, it is recommended to install spyder after installing pynapple with :</p> <pre><code>$ conda create --name pynapple pip python=3.8\n$ conda activate pynapple\n$ pip install pynapple\n$ pip install spyder\n$ spyder\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>After installation, you can now import the package: </p> <pre><code>$ python\n&gt;&gt;&gt; import pynapple as nap\n</code></pre> <p>You'll find an example of the package below. Click here to download the example dataset. The folder includes a NWB file containing the data (See this notebook for more information on the creation of the NWB file).</p> <pre><code>import numpy as np\nimport pandas as pd\nimport pynapple as nap\nfrom matplotlib.pyplot import *\n\ndata_directory = '/your/path/to/A2929-200711'\n\n# LOADING DATA\ndata = nap.load_session(data_directory, 'neurosuite')\n\n\nspikes = data.spikes\nposition = data.position\nwake_ep = data.epochs['wake']\n\n# COMPUTING TUNING CURVES\ntuning_curves = nap.compute_1d_tuning_curves(group = spikes, \n                                            feature = position['ry'], \n                                            ep = position['ry'].time_support, \n                                            nb_bins = 120,  \n                                            minmax=(0, 2*np.pi) )\n\n\n\n# PLOT\nfigure()\nfor i in spikes:\n    subplot(6,7,i+1, projection = 'polar')\n    plot(tuning_curves[i])\n\n\nshow()\n\n</code></pre> <p>Shown below, the final figure from the example code displays the firing rate of 15 neurons as a function of the direction of the head of the animal in the horizontal plane.</p> <p> </p>"},{"location":"#credits","title":"Credits","text":"<p>Special thanks to Francesco P. Battaglia (https://github.com/fpbattaglia) for the development of the original TSToolbox (https://github.com/PeyracheLab/TStoolbox) and neuroseries (https://github.com/NeuroNetMem/neuroseries) packages, the latter constituting the core of pynapple.</p> <p>This package was developped by Guillaume Viejo (https://github.com/gviejo) and other members of the Peyrache Lab.</p> <p>Logo: Sofia Skromne Carrasco, 2021.</p>"},{"location":"AUTHORS/","title":"Credits","text":""},{"location":"AUTHORS/#development-lead","title":"Development Lead","text":"<ul> <li>Guillaume Viejo guillaume.viejo@gmail.com</li> </ul>"},{"location":"AUTHORS/#contributors","title":"Contributors","text":"<ul> <li>Adrien Peyrache</li> <li>Dan Levenstein</li> <li>Sofia Skromne Carrasco</li> <li>Sara Mahallati</li> <li>Gilberto Vite</li> <li>Davide Spalla</li> <li>Luigi Petrucco</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/pynapple-org/pynapple/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in     troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \\\"bug\\\" and \\\"help wanted\\\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \\\"enhancement\\\" and \\\"help wanted\\\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>pynapple could always use more documentation, whether as part of the official pynapple docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/pynapple-org/pynapple/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to     implement.</li> <li>Remember that this is a volunteer-driven project, and that     contributions are welcome :)</li> </ul>"},{"location":"CONTRIBUTING/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up [pynapple]{https://github.com/pynapple-org/pynapple} for local development.</p> <ol> <li>Fork the [pynapple]{https://github.com/pynapple-org/pynapple} repo on GitHub.</li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/pynapple.git\n</code></pre> </li> <li> <p>Install your local copy with pip. </p> <pre><code>$ cd pynapple/\n$ pip install -e .\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> </ol> <ol> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.nd.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy.      </li> </ol>"},{"location":"HISTORY/","title":"History","text":"<p>This package somehow started about 20 years ago in Bruce McNaughton's lab. Dave Redish started the TSToolbox package in Matlab.  Another postdoc in the lab, Francesco Battaglia, then made major contributions to the package. Francesco passed it on to Adrien Peyrache and other trainees in Paris and The Netherlands. Around 2016-2017, Luke Sjulson started TSToolbox2, still in Matlab and which includes some important changes.</p> <p>In 2018, Francesco started neuroseries, a Python package built on Pandas. It was quickly adopted in Adrien's lab, especially by Guillaume Viejo, a postdoc in the lab. Gradually, the majority of the lab was using it and new functions were constantly added. In 2021, Guillaume and other trainees in Adrien's lab decided to fork from neuroseries and started pynapple. The core of pynapple is largely built upon neuroseries. Some of the original changes to TSToolbox made by Luke were included in this package, especially the time_support property of all ts/tsd objects.</p>"},{"location":"HISTORY/#034-coming","title":"0.3.4 (Coming)","text":"<ul> <li>TsGroup.to_tsd and Tsd.to_tsgroup transformations</li> <li>Count can take IntervalSet</li> <li>Saving to npz functions for all objects.</li> </ul>"},{"location":"HISTORY/#033-2023-04-17","title":"0.3.3 (2023-04-17)","text":"<ul> <li>Fixed minor bug with tkinter</li> </ul>"},{"location":"HISTORY/#032-2023-04-12","title":"0.3.2 (2023-04-12)","text":"<ul> <li>PyQt removed from the list of dependencies</li> </ul>"},{"location":"HISTORY/#031-2022-12-08","title":"0.3.1 (2022-12-08)","text":"<ul> <li>Core functions rewritten with Numba</li> </ul>"},{"location":"HISTORY/#024-2022-05-02","title":"0.2.4 (2022-05-02)","text":""},{"location":"HISTORY/#023-2022-04-05","title":"0.2.3 (2022-04-05)","text":"<ul> <li>Fixed minor bug when saving DLC in NWB.</li> </ul>"},{"location":"HISTORY/#023-2022-04-05_1","title":"0.2.3 (2022-04-05)","text":"<ul> <li>Alpha release</li> </ul>"},{"location":"HISTORY/#022-2022-04-05","title":"0.2.2 (2022-04-05)","text":"<ul> <li>Beta testing version for public</li> </ul>"},{"location":"HISTORY/#021-2022-02-07","title":"0.2.1 (2022-02-07)","text":"<ul> <li>Beta testing version for Peyrache Lab.</li> </ul>"},{"location":"HISTORY/#020-2022-01-10","title":"0.2.0 (2022-01-10)","text":"<ul> <li>First version for pynapple with main features in core, process and IO.</li> </ul>"},{"location":"HISTORY/#020-pre-release-2022-01-06","title":"0.2.0 Pre-release (2022-01-06)","text":"<ul> <li>Pre-release version for pynapple with main features in core and process.</li> </ul>"},{"location":"HISTORY/#011-2021-10-25","title":"0.1.1 (2021-10-25)","text":"<ul> <li>First release on PyPI.</li> <li>Firt minimal version</li> </ul>"},{"location":"core.interval_set/","title":"Interval Sets","text":""},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet","title":"<code>IntervalSet</code>","text":"<p>         Bases: <code>pd.DataFrame</code></p> <p>A subclass of pandas.DataFrame representing a (irregular) set of time intervals in elapsed time, with relative operations</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>class IntervalSet(pd.DataFrame):\n    # class IntervalSet():\n\"\"\"\n    A subclass of pandas.DataFrame representing a (irregular) set of time intervals in elapsed time, with relative operations\n    \"\"\"\n\n    def __init__(self, start, end=None, time_units=\"s\", **kwargs):\n\"\"\"\n        IntervalSet initializer\n\n        If start and end and not aligned, meaning that \\n\n        1. len(start) != len(end)\n        2. end[i] &gt; start[i]\n        3. start[i+1] &gt; end[i]\n        4. start and end are not sorted,\n\n        IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point\n\n        Parameters\n        ----------\n        start : numpy.ndarray or number or pandas.DataFrame\n            Beginning of intervals\n        end : numpy.ndarray or number, optional\n            Ends of intervals\n        time_units : str, optional\n            Time unit of the intervals ('us', 'ms', 's' [default])\n        **kwargs\n            Additional parameters passed ot pandas.DataFrame\n\n        Returns\n        -------\n        IntervalSet\n            _\n\n        Raises\n        ------\n        RuntimeError\n            Description\n        ValueError\n            If a pandas.DataFrame is passed, it should contains\n            a column 'start' and a column 'end'.\n\n        \"\"\"\n\n        if end is None:\n            df = pd.DataFrame(start)\n            if \"start\" not in df.columns or \"end\" not in df.columns:\n                raise ValueError(\"wrong columns name\")\n            start = df[\"start\"].values.astype(np.float64)\n            end = df[\"end\"].values.astype(np.float64)\n\n            start = sort_timestamps(format_timestamps(start.ravel(), time_units))\n            end = sort_timestamps(format_timestamps(end.ravel(), time_units))\n\n            data, to_warn = jitfix_iset(start, end)\n            if np.any(to_warn):\n                msg = \"\\n\".join(all_warnings[to_warn])\n                warnings.warn(msg, stacklevel=2)\n            super().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\n            self.r_cache = None\n            self._metadata = [\"nap_class\"]\n            self.nap_class = self.__class__.__name__\n            return\n\n        start = np.array(start).astype(np.float64)\n        end = np.array(end).astype(np.float64)\n\n        start = format_timestamps(np.array(start).ravel(), time_units)\n        end = format_timestamps(np.array(end).ravel(), time_units)\n\n        if len(start) != len(end):\n            raise RuntimeError(\"Starts end ends are not of the same length\")\n\n        if not (np.diff(start) &gt; 0).all():\n            warnings.warn(\"start is not sorted.\", stacklevel=2)\n            start = np.sort(start)\n\n        if not (np.diff(end) &gt; 0).all():\n            warnings.warn(\"end is not sorted.\", stacklevel=2)\n            end = np.sort(end)\n\n        data, to_warn = jitfix_iset(start, end)\n\n        if np.any(to_warn):\n            msg = \"\\n\".join(all_warnings[to_warn])\n            warnings.warn(msg, stacklevel=2)\n\n        super().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\n        self.r_cache = None\n        # self._metadata = [\"nap_class\"]\n        self.nap_class = self.__class__.__name__\n\n    def __repr__(self):\n        return self.as_units(\"s\").__repr__()\n\n    def __str__(self):\n        return self.__repr__()\n\n    def time_span(self):\n\"\"\"\n        Time span of the interval set.\n\n        Returns\n        -------\n        out: IntervalSet\n            an IntervalSet with a single interval encompassing the whole IntervalSet\n        \"\"\"\n        s = self[\"start\"][0]\n        e = self[\"end\"].iloc[-1]\n        return IntervalSet(s, e)\n\n    def tot_length(self, time_units=\"s\"):\n\"\"\"\n        Total elapsed time in the set.\n\n        Parameters\n        ----------\n        time_units : None, optional\n            The time units to return the result in ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        out: float\n            _\n        \"\"\"\n        tot_l = (self[\"end\"] - self[\"start\"]).sum()\n        return return_timestamps(np.array([tot_l]), time_units)[0]\n\n    def intersect(self, a):\n\"\"\"\n        set intersection of IntervalSet\n\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to intersect self with\n\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\n        start1 = self.values[:, 0]\n        end1 = self.values[:, 1]\n        start2 = a.values[:, 0]\n        end2 = a.values[:, 1]\n        s, e = jitintersect(start1, end1, start2, end2)\n        return IntervalSet(s, e)\n\n    def union(self, a):\n\"\"\"\n        set union of IntervalSet\n\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to union self with\n\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\n        start1 = self.values[:, 0]\n        end1 = self.values[:, 1]\n        start2 = a.values[:, 0]\n        end2 = a.values[:, 1]\n        s, e = jitunion(start1, end1, start2, end2)\n        return IntervalSet(s, e)\n\n    def set_diff(self, a):\n\"\"\"\n        set difference of IntervalSet\n\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to set-substract from self\n\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\n        start1 = self.values[:, 0]\n        end1 = self.values[:, 1]\n        start2 = a.values[:, 0]\n        end2 = a.values[:, 1]\n        s, e = jitdiff(start1, end1, start2, end2)\n        return IntervalSet(s, e)\n\n    def in_interval(self, tsd):\n\"\"\"\n        finds out in which element of the interval set each point in a time series fits.\n\n        NaNs for those that don't fit an interval\n\n        Parameters\n        ----------\n        tsd : Tsd\n            The tsd to be binned\n\n        Returns\n        -------\n        out: numpy.ndarray\n            an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet\n        \"\"\"\n        times = tsd.index.values\n        starts = self.values[:, 0]\n        ends = self.values[:, 1]\n\n        return jitin_interval(times, starts, ends)\n\n    def drop_short_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Drops the short intervals in the interval set.\n\n        Parameters\n        ----------\n        threshold : numeric\n            Time threshold for \"short\" intervals\n        time_units : None, optional\n            The time units for the treshold ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        out: IntervalSet\n            A copied IntervalSet with the dropped intervals\n        \"\"\"\n        threshold = format_timestamps(\n            np.array([threshold], dtype=np.float64), time_units\n        )[0]\n        return self.loc[(self[\"end\"] - self[\"start\"]) &gt; threshold].reset_index(\n            drop=True\n        )\n\n    def drop_long_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Drops the long intervals in the interval set.\n\n        Parameters\n        ----------\n        threshold : numeric\n            Time threshold for \"long\" intervals\n        time_units : None, optional\n            The time units for the treshold ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        out: IntervalSet\n            A copied IntervalSet with the dropped intervals\n        \"\"\"\n        threshold = format_timestamps(\n            np.array([threshold], dtype=np.float64), time_units\n        )[0]\n        return self.loc[(self[\"end\"] - self[\"start\"]) &lt; threshold].reset_index(\n            drop=True\n        )\n\n    def as_units(self, units=\"s\"):\n\"\"\"\n        returns a DataFrame with time expressed in the desired unit\n\n        Parameters\n        ----------\n        units : None, optional\n            'us', 'ms', or 's' [default]\n\n        Returns\n        -------\n        out: pandas.DataFrame\n            DataFrame with adjusted times\n        \"\"\"\n\n        data = self.values.copy()\n        data = return_timestamps(data, units)\n        if units == \"us\":\n            data = data.astype(np.int64)\n\n        df = pd.DataFrame(index=self.index.values, data=data, columns=self.columns)\n\n        return df\n\n    def merge_close_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Merges intervals that are very close.\n\n        Parameters\n        ----------\n        threshold : numeric\n            time threshold for the closeness of the intervals\n        time_units : None, optional\n            time units for the threshold ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        out: IntervalSet\n            a copied IntervalSet with merged intervals\n\n        \"\"\"\n        if len(self) == 0:\n            return IntervalSet(start=[], end=[])\n\n        threshold = format_timestamps(\n            np.array((threshold,), dtype=np.float64).ravel(), time_units\n        )[0]\n        start = self[\"start\"].values\n        end = self[\"end\"].values\n        tojoin = (start[1:] - end[0:-1]) &gt; threshold\n        start = np.hstack((start[0], start[1:][tojoin]))\n        end = np.hstack((end[0:-1][tojoin], end[-1]))\n\n        return IntervalSet(start=start, end=end)\n\n    def get_intervals_center(self, alpha=0.5):\n\"\"\"\n        Returns by default the centers of each intervals.\n\n        It is possible to bias the midpoint by changing the alpha parameter between [0, 1]\n        For each epoch:\n        t = start + (end-start)*alpha\n\n        Parameters\n        ----------\n        alpha : float, optional\n            The midpoint within each interval.\n\n        Returns\n        -------\n        Ts\n            Timestamps object\n        \"\"\"\n        time_series = importlib.import_module(\".time_series\", \"pynapple.core\")\n        starts = self.values[:, 0]\n        ends = self.values[:, 1]\n\n        if not isinstance(alpha, float):\n            raise RuntimeError(\"Parameter alpha should be float type\")\n\n        alpha = np.clip(alpha, 0, 1)\n        t = starts + (ends - starts) * alpha\n        return time_series.Ts(t=t, time_support=self)\n\n    def save(self, filename):\n\"\"\"\n        Save IntervalSet object in npz format. The file will contain the starts and ends.\n\n        The main purpose of this function is to save small/medium sized IntervalSet\n        objects. For example, you determined some epochs for one session that you want to save\n        to avoid recomputing them.\n\n        You can load the object with numpy.load. Keys are 'start' and 'end'.\n        See the example below.\n\n        Parameters\n        ----------\n        filename : str\n            The filename\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n        &gt;&gt;&gt; ep.save(\"my_ep.npz\")\n\n        Here I can retrieve my data with numpy directly:\n\n        &gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['start', 'end']\n        &gt;&gt;&gt; print(file['start'])\n        [0. 10. 20.]\n\n        It is then easy to recreate the IntervalSet object.\n        &gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n           start   end\n        0    0.0   5.0\n        1   10.0  12.0\n        2   20.0  33.0\n\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\n        if not isinstance(filename, str):\n            raise RuntimeError(\"Invalid type; please provide filename as string\")\n\n        if os.path.isdir(filename):\n            raise RuntimeError(\n                \"Invalid filename input. {} is directory.\".format(filename)\n            )\n\n        if not filename.lower().endswith(\".npz\"):\n            filename = filename + \".npz\"\n\n        dirname = os.path.dirname(filename)\n\n        if len(dirname) and not os.path.exists(dirname):\n            raise RuntimeError(\n                \"Path {} does not exist.\".format(os.path.dirname(filename))\n            )\n\n        np.savez(\n            filename,\n            start=self.start.values,\n            end=self.end.values,\n        )\n\n        return\n\n    @property\n    def _constructor(self):\n        return IntervalSet\n\n    @property\n    def starts(self):\n\"\"\"Return the starts of the IntervalSet as a Ts object\n\n        Returns\n        -------\n        Ts\n            The starts of the IntervalSet\n        \"\"\"\n        time_series = importlib.import_module(\".time_series\", \"pynapple.core\")\n        return time_series.Ts(t=self.values[:, 0], time_support=self)\n\n    @property\n    def ends(self):\n\"\"\"Return the ends of the IntervalSet as a Ts object\n\n        Returns\n        -------\n        Ts\n            The ends of the IntervalSet\n        \"\"\"\n        time_series = importlib.import_module(\".time_series\", \"pynapple.core\")\n        return time_series.Ts(t=self.values[:, 1], time_support=self)\n</code></pre>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.starts","title":"<code>starts</code>  <code>property</code>","text":"<p>Return the starts of the IntervalSet as a Ts object</p> <p>Returns:</p> Type Description <code>Ts</code> <p>The starts of the IntervalSet</p>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.ends","title":"<code>ends</code>  <code>property</code>","text":"<p>Return the ends of the IntervalSet as a Ts object</p> <p>Returns:</p> Type Description <code>Ts</code> <p>The ends of the IntervalSet</p>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.__init__","title":"<code>__init__(start, end=None, time_units='s', **kwargs)</code>","text":"<p>IntervalSet initializer</p> <p>If start and end and not aligned, meaning that </p> <ol> <li>len(start) != len(end)</li> <li>end[i] &gt; start[i]</li> <li>start[i+1] &gt; end[i]</li> <li>start and end are not sorted,</li> </ol> <p>IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>numpy.ndarray or number or pandas.DataFrame</code> <p>Beginning of intervals</p> required <code>end</code> <code>numpy.ndarray or number, optional</code> <p>Ends of intervals</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>Time unit of the intervals ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>**kwargs</code> <p>Additional parameters passed ot pandas.DataFrame</p> <code>{}</code> <p>Returns:</p> Type Description <code>IntervalSet</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Description</p> <code>ValueError</code> <p>If a pandas.DataFrame is passed, it should contains a column 'start' and a column 'end'.</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def __init__(self, start, end=None, time_units=\"s\", **kwargs):\n\"\"\"\n    IntervalSet initializer\n\n    If start and end and not aligned, meaning that \\n\n    1. len(start) != len(end)\n    2. end[i] &gt; start[i]\n    3. start[i+1] &gt; end[i]\n    4. start and end are not sorted,\n\n    IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point\n\n    Parameters\n    ----------\n    start : numpy.ndarray or number or pandas.DataFrame\n        Beginning of intervals\n    end : numpy.ndarray or number, optional\n        Ends of intervals\n    time_units : str, optional\n        Time unit of the intervals ('us', 'ms', 's' [default])\n    **kwargs\n        Additional parameters passed ot pandas.DataFrame\n\n    Returns\n    -------\n    IntervalSet\n        _\n\n    Raises\n    ------\n    RuntimeError\n        Description\n    ValueError\n        If a pandas.DataFrame is passed, it should contains\n        a column 'start' and a column 'end'.\n\n    \"\"\"\n\n    if end is None:\n        df = pd.DataFrame(start)\n        if \"start\" not in df.columns or \"end\" not in df.columns:\n            raise ValueError(\"wrong columns name\")\n        start = df[\"start\"].values.astype(np.float64)\n        end = df[\"end\"].values.astype(np.float64)\n\n        start = sort_timestamps(format_timestamps(start.ravel(), time_units))\n        end = sort_timestamps(format_timestamps(end.ravel(), time_units))\n\n        data, to_warn = jitfix_iset(start, end)\n        if np.any(to_warn):\n            msg = \"\\n\".join(all_warnings[to_warn])\n            warnings.warn(msg, stacklevel=2)\n        super().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\n        self.r_cache = None\n        self._metadata = [\"nap_class\"]\n        self.nap_class = self.__class__.__name__\n        return\n\n    start = np.array(start).astype(np.float64)\n    end = np.array(end).astype(np.float64)\n\n    start = format_timestamps(np.array(start).ravel(), time_units)\n    end = format_timestamps(np.array(end).ravel(), time_units)\n\n    if len(start) != len(end):\n        raise RuntimeError(\"Starts end ends are not of the same length\")\n\n    if not (np.diff(start) &gt; 0).all():\n        warnings.warn(\"start is not sorted.\", stacklevel=2)\n        start = np.sort(start)\n\n    if not (np.diff(end) &gt; 0).all():\n        warnings.warn(\"end is not sorted.\", stacklevel=2)\n        end = np.sort(end)\n\n    data, to_warn = jitfix_iset(start, end)\n\n    if np.any(to_warn):\n        msg = \"\\n\".join(all_warnings[to_warn])\n        warnings.warn(msg, stacklevel=2)\n\n    super().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\n    self.r_cache = None\n    # self._metadata = [\"nap_class\"]\n    self.nap_class = self.__class__.__name__\n</code></pre>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.time_span","title":"<code>time_span()</code>","text":"<p>Time span of the interval set.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>an IntervalSet with a single interval encompassing the whole IntervalSet</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def time_span(self):\n\"\"\"\n    Time span of the interval set.\n\n    Returns\n    -------\n    out: IntervalSet\n        an IntervalSet with a single interval encompassing the whole IntervalSet\n    \"\"\"\n    s = self[\"start\"][0]\n    e = self[\"end\"].iloc[-1]\n    return IntervalSet(s, e)\n</code></pre>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.tot_length","title":"<code>tot_length(time_units='s')</code>","text":"<p>Total elapsed time in the set.</p> <p>Parameters:</p> Name Type Description Default <code>time_units</code> <code>None, optional</code> <p>The time units to return the result in ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def tot_length(self, time_units=\"s\"):\n\"\"\"\n    Total elapsed time in the set.\n\n    Parameters\n    ----------\n    time_units : None, optional\n        The time units to return the result in ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    out: float\n        _\n    \"\"\"\n    tot_l = (self[\"end\"] - self[\"start\"]).sum()\n    return return_timestamps(np.array([tot_l]), time_units)[0]\n</code></pre>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.intersect","title":"<code>intersect(a)</code>","text":"<p>set intersection of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to intersect self with</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def intersect(self, a):\n\"\"\"\n    set intersection of IntervalSet\n\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to intersect self with\n\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\n    start1 = self.values[:, 0]\n    end1 = self.values[:, 1]\n    start2 = a.values[:, 0]\n    end2 = a.values[:, 1]\n    s, e = jitintersect(start1, end1, start2, end2)\n    return IntervalSet(s, e)\n</code></pre>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.union","title":"<code>union(a)</code>","text":"<p>set union of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to union self with</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def union(self, a):\n\"\"\"\n    set union of IntervalSet\n\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to union self with\n\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\n    start1 = self.values[:, 0]\n    end1 = self.values[:, 1]\n    start2 = a.values[:, 0]\n    end2 = a.values[:, 1]\n    s, e = jitunion(start1, end1, start2, end2)\n    return IntervalSet(s, e)\n</code></pre>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.set_diff","title":"<code>set_diff(a)</code>","text":"<p>set difference of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to set-substract from self</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def set_diff(self, a):\n\"\"\"\n    set difference of IntervalSet\n\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to set-substract from self\n\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\n    start1 = self.values[:, 0]\n    end1 = self.values[:, 1]\n    start2 = a.values[:, 0]\n    end2 = a.values[:, 1]\n    s, e = jitdiff(start1, end1, start2, end2)\n    return IntervalSet(s, e)\n</code></pre>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.in_interval","title":"<code>in_interval(tsd)</code>","text":"<p>finds out in which element of the interval set each point in a time series fits.</p> <p>NaNs for those that don't fit an interval</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>Tsd</code> <p>The tsd to be binned</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def in_interval(self, tsd):\n\"\"\"\n    finds out in which element of the interval set each point in a time series fits.\n\n    NaNs for those that don't fit an interval\n\n    Parameters\n    ----------\n    tsd : Tsd\n        The tsd to be binned\n\n    Returns\n    -------\n    out: numpy.ndarray\n        an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet\n    \"\"\"\n    times = tsd.index.values\n    starts = self.values[:, 0]\n    ends = self.values[:, 1]\n\n    return jitin_interval(times, starts, ends)\n</code></pre>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.drop_short_intervals","title":"<code>drop_short_intervals(threshold, time_units='s')</code>","text":"<p>Drops the short intervals in the interval set.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>Time threshold for \"short\" intervals</p> required <code>time_units</code> <code>None, optional</code> <p>The time units for the treshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>A copied IntervalSet with the dropped intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def drop_short_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Drops the short intervals in the interval set.\n\n    Parameters\n    ----------\n    threshold : numeric\n        Time threshold for \"short\" intervals\n    time_units : None, optional\n        The time units for the treshold ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    out: IntervalSet\n        A copied IntervalSet with the dropped intervals\n    \"\"\"\n    threshold = format_timestamps(\n        np.array([threshold], dtype=np.float64), time_units\n    )[0]\n    return self.loc[(self[\"end\"] - self[\"start\"]) &gt; threshold].reset_index(\n        drop=True\n    )\n</code></pre>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.drop_long_intervals","title":"<code>drop_long_intervals(threshold, time_units='s')</code>","text":"<p>Drops the long intervals in the interval set.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>Time threshold for \"long\" intervals</p> required <code>time_units</code> <code>None, optional</code> <p>The time units for the treshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>A copied IntervalSet with the dropped intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def drop_long_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Drops the long intervals in the interval set.\n\n    Parameters\n    ----------\n    threshold : numeric\n        Time threshold for \"long\" intervals\n    time_units : None, optional\n        The time units for the treshold ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    out: IntervalSet\n        A copied IntervalSet with the dropped intervals\n    \"\"\"\n    threshold = format_timestamps(\n        np.array([threshold], dtype=np.float64), time_units\n    )[0]\n    return self.loc[(self[\"end\"] - self[\"start\"]) &lt; threshold].reset_index(\n        drop=True\n    )\n</code></pre>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.as_units","title":"<code>as_units(units='s')</code>","text":"<p>returns a DataFrame with time expressed in the desired unit</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>None, optional</code> <p>'us', 'ms', or 's' [default]</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>pandas.DataFrame</code> <p>DataFrame with adjusted times</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    returns a DataFrame with time expressed in the desired unit\n\n    Parameters\n    ----------\n    units : None, optional\n        'us', 'ms', or 's' [default]\n\n    Returns\n    -------\n    out: pandas.DataFrame\n        DataFrame with adjusted times\n    \"\"\"\n\n    data = self.values.copy()\n    data = return_timestamps(data, units)\n    if units == \"us\":\n        data = data.astype(np.int64)\n\n    df = pd.DataFrame(index=self.index.values, data=data, columns=self.columns)\n\n    return df\n</code></pre>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.merge_close_intervals","title":"<code>merge_close_intervals(threshold, time_units='s')</code>","text":"<p>Merges intervals that are very close.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>time threshold for the closeness of the intervals</p> required <code>time_units</code> <code>None, optional</code> <p>time units for the threshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>a copied IntervalSet with merged intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def merge_close_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Merges intervals that are very close.\n\n    Parameters\n    ----------\n    threshold : numeric\n        time threshold for the closeness of the intervals\n    time_units : None, optional\n        time units for the threshold ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    out: IntervalSet\n        a copied IntervalSet with merged intervals\n\n    \"\"\"\n    if len(self) == 0:\n        return IntervalSet(start=[], end=[])\n\n    threshold = format_timestamps(\n        np.array((threshold,), dtype=np.float64).ravel(), time_units\n    )[0]\n    start = self[\"start\"].values\n    end = self[\"end\"].values\n    tojoin = (start[1:] - end[0:-1]) &gt; threshold\n    start = np.hstack((start[0], start[1:][tojoin]))\n    end = np.hstack((end[0:-1][tojoin], end[-1]))\n\n    return IntervalSet(start=start, end=end)\n</code></pre>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.get_intervals_center","title":"<code>get_intervals_center(alpha=0.5)</code>","text":"<p>Returns by default the centers of each intervals.</p> <p>It is possible to bias the midpoint by changing the alpha parameter between [0, 1] For each epoch: t = start + (end-start)*alpha</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float, optional</code> <p>The midpoint within each interval.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Ts</code> <p>Timestamps object</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def get_intervals_center(self, alpha=0.5):\n\"\"\"\n    Returns by default the centers of each intervals.\n\n    It is possible to bias the midpoint by changing the alpha parameter between [0, 1]\n    For each epoch:\n    t = start + (end-start)*alpha\n\n    Parameters\n    ----------\n    alpha : float, optional\n        The midpoint within each interval.\n\n    Returns\n    -------\n    Ts\n        Timestamps object\n    \"\"\"\n    time_series = importlib.import_module(\".time_series\", \"pynapple.core\")\n    starts = self.values[:, 0]\n    ends = self.values[:, 1]\n\n    if not isinstance(alpha, float):\n        raise RuntimeError(\"Parameter alpha should be float type\")\n\n    alpha = np.clip(alpha, 0, 1)\n    t = starts + (ends - starts) * alpha\n    return time_series.Ts(t=t, time_support=self)\n</code></pre>"},{"location":"core.interval_set/#pynapple.core.interval_set.IntervalSet.save","title":"<code>save(filename)</code>","text":"<p>Save IntervalSet object in npz format. The file will contain the starts and ends.</p> <p>The main purpose of this function is to save small/medium sized IntervalSet objects. For example, you determined some epochs for one session that you want to save to avoid recomputing them.</p> <p>You can load the object with numpy.load. Keys are 'start' and 'end'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n&gt;&gt;&gt; ep.save(\"my_ep.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['start', 'end']\n&gt;&gt;&gt; print(file['start'])\n[0. 10. 20.]\n</code></pre> <p>It is then easy to recreate the IntervalSet object.</p> <pre><code>&gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n   start   end\n0    0.0   5.0\n1   10.0  12.0\n2   20.0  33.0\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save IntervalSet object in npz format. The file will contain the starts and ends.\n\n    The main purpose of this function is to save small/medium sized IntervalSet\n    objects. For example, you determined some epochs for one session that you want to save\n    to avoid recomputing them.\n\n    You can load the object with numpy.load. Keys are 'start' and 'end'.\n    See the example below.\n\n    Parameters\n    ----------\n    filename : str\n        The filename\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n    &gt;&gt;&gt; ep.save(\"my_ep.npz\")\n\n    Here I can retrieve my data with numpy directly:\n\n    &gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['start', 'end']\n    &gt;&gt;&gt; print(file['start'])\n    [0. 10. 20.]\n\n    It is then easy to recreate the IntervalSet object.\n    &gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n       start   end\n    0    0.0   5.0\n    1   10.0  12.0\n    2   20.0  33.0\n\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\n    if not isinstance(filename, str):\n        raise RuntimeError(\"Invalid type; please provide filename as string\")\n\n    if os.path.isdir(filename):\n        raise RuntimeError(\n            \"Invalid filename input. {} is directory.\".format(filename)\n        )\n\n    if not filename.lower().endswith(\".npz\"):\n        filename = filename + \".npz\"\n\n    dirname = os.path.dirname(filename)\n\n    if len(dirname) and not os.path.exists(dirname):\n        raise RuntimeError(\n            \"Path {} does not exist.\".format(os.path.dirname(filename))\n        )\n\n    np.savez(\n        filename,\n        start=self.start.values,\n        end=self.end.values,\n    )\n\n    return\n</code></pre>"},{"location":"core.time_series/","title":"Time Series","text":""},{"location":"core.time_series/#pynapple.core.time_series.Tsd","title":"<code>Tsd</code>","text":"<p>         Bases: <code>pd.Series</code></p> <p>A subclass of pandas.Series specialized for neurophysiology time series.</p> <p>Tsd provides standardized time representation, plus various functions for manipulating times series.</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class Tsd(pd.Series):\n    # class Tsd():\n\"\"\"\n    A subclass of pandas.Series specialized for neurophysiology time series.\n\n    Tsd provides standardized time representation, plus various functions for manipulating times series.\n\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\n\n    def __init__(self, t, d=None, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n        Tsd Initializer.\n\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.Series\n            An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n        d : numpy.ndarray, optional\n            The data of the time series\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default])\n        time_support : IntervalSet, optional\n            The time support of the tsd object\n        **kwargs\n            Arguments that will be passed to the pandas.Series initializer.\n        \"\"\"\n        if isinstance(t, SingleBlockManager):\n            d = t.array\n            t = t.index.values\n            if \"index\" in kwargs:\n                kwargs.pop(\"index\")\n        elif isinstance(t, pd.Series):\n            d = t.values\n            t = t.index.values\n\n        t = t.astype(np.float64).flatten()\n        t = format_timestamps(t, time_units)\n        t = sort_timestamps(t)\n\n        if len(t):\n            if time_support is not None:\n                starts = time_support.start.values\n                ends = time_support.end.values\n                if d is not None:\n                    t, d = jitrestrict(t, d, starts, ends)\n                    super().__init__(index=t, data=d)\n                else:\n                    t = jittsrestrict(t, starts, ends)\n                    super().__init__(index=t, data=None, dtype=np.int8)\n            else:\n                time_support = IntervalSet(start=t[0], end=t[-1])\n                if d is not None:\n                    super().__init__(index=t, data=d)\n                else:\n                    super().__init__(index=t, data=d, dtype=np.float64)\n\n            self.time_support = time_support\n            self.rate = t.shape[0] / np.sum(\n                time_support.values[:, 1] - time_support.values[:, 0]\n            )\n\n        else:\n            time_support = IntervalSet(pd.DataFrame(columns=[\"start\", \"end\"]))\n            super().__init__(index=t, data=d, dtype=np.float64)\n\n            self.time_support = time_support\n            self.rate = 0.0\n\n        self.index.name = \"Time (s)\"\n        # self._metadata.append(\"nap_class\")\n        self.nap_class = self.__class__.__name__\n\n    def __add__(self, value):\n        ts = self.time_support\n        return Tsd(self.as_series().__add__(value), time_support=ts)\n\n    def __sub__(self, value):\n        ts = self.time_support\n        return Tsd(self.as_series().__sub__(value), time_support=ts)\n\n    def __truediv__(self, value):\n        ts = self.time_support\n        return Tsd(self.as_series().__truediv__(value), time_support=ts)\n\n    def __floordiv__(self, value):\n        ts = self.time_support\n        return Tsd(self.as_series().__floordiv__(value), time_support=ts)\n\n    def __mul__(self, value):\n        ts = self.time_support\n        return Tsd(self.as_series().__mul__(value), time_support=ts)\n\n    def __mod__(self, value):\n        ts = self.time_support\n        return Tsd(self.as_series().__mod__(value), time_support=ts)\n\n    def __pow__(self, value):\n        ts = self.time_support\n        return Tsd(self.as_series().__pow__(value), time_support=ts)\n\n    def __lt__(self, value):\n        return self.as_series().__lt__(value)\n\n    def __gt__(self, value):\n        return self.as_series().__gt__(value)\n\n    def __le__(self, value):\n        return self.as_series().__le__(value)\n\n    def __ge__(self, value):\n        return self.as_series().__ge__(value)\n\n    def __ne__(self, value):\n        return self.as_series().__ne__(value)\n\n    def __eq__(self, value):\n        return self.as_series().__eq__(value)\n\n    def __repr__(self):\n        return self.as_series().__repr__()\n\n    def __str__(self):\n        return self.__repr__()\n\n    def times(self, units=\"s\"):\n\"\"\"\n        The time index of the Tsd, returned as np.double in the desired time units.\n\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        out: numpy.ndarray\n            the time indexes\n        \"\"\"\n        return return_timestamps(self.index.values, units)\n\n    def as_series(self):\n\"\"\"\n        Convert the Ts/Tsd object to a pandas.Series object.\n\n        Returns\n        -------\n        out: pandas.Series\n            _\n        \"\"\"\n        return pd.Series(self, copy=True)\n\n    def as_units(self, units=\"s\"):\n\"\"\"\n        Returns a Series with time expressed in the desired unit.\n\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        pandas.Series\n            the series object with adjusted times\n        \"\"\"\n        ss = self.as_series()\n        t = self.index.values\n        t = return_timestamps(t, units)\n        if units == \"us\":\n            t = t.astype(np.int64)\n        ss.index = t\n        ss.index.name = \"Time (\" + str(units) + \")\"\n        return ss\n\n    def data(self):\n\"\"\"\n        The data in the Tsd object\n\n        Returns\n        -------\n        out: numpy.ndarray\n            _\n        \"\"\"\n        return self.values\n\n    def value_from(self, tsd, ep=None):\n\"\"\"\n        Replace the value with the closest value from tsd argument\n\n        Parameters\n        ----------\n        tsd : Tsd\n            The Tsd object holding the values to replace\n        ep : IntervalSet (optional)\n            The IntervalSet object to restrict the operation.\n            If None, the time support of the tsd input object is used.\n\n        Returns\n        -------\n        out : Tsd\n            Tsd object with the new values\n\n        Examples\n        --------\n        In this example, the ts object will receive the closest values in time from tsd.\n\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n        &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n\n        The variable ts is a time series object containing only nan.\n        The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n\n        &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n\n        newts is the same size as ts restrict to ep.\n\n        &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n            52 52\n        \"\"\"\n        if ep is None:\n            ep = tsd.time_support\n\n        time_array = self.index.values\n        time_target_array = tsd.index.values\n        data_target_array = tsd.values\n        starts = ep.start.values\n        ends = ep.end.values\n\n        t, d, ns, ne = jitvaluefrom(\n            time_array, time_target_array, data_target_array, starts, ends\n        )\n        time_support = IntervalSet(start=ns, end=ne)\n        return Tsd(t=t, d=d, time_support=time_support)\n\n    def restrict(self, ep):\n\"\"\"\n        Restricts a Tsd object to a set of time intervals delimited by an IntervalSet object\n\n        Parameters\n        ----------\n        ep : IntervalSet\n            the IntervalSet object\n\n        Returns\n        -------\n        out: Tsd\n            Tsd object restricted to ep\n\n        Examples\n        --------\n        The Ts object is restrict to the intervals defined by ep.\n\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n        &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n        &gt;&gt;&gt; newts = ts.restrict(ep)\n\n        The time support of newts automatically inherit the epochs defined by ep.\n\n        &gt;&gt;&gt; newts.time_support\n        &gt;&gt;&gt;    start    end\n        &gt;&gt;&gt; 0    0.0  500.0\n\n        \"\"\"\n        time_array = self.index.values\n        data_array = self.values\n        starts = ep.start.values\n        ends = ep.end.values\n        t, d = jitrestrict(time_array, data_array, starts, ends)\n        return Tsd(t=t, d=d, time_support=ep)\n\n    def count(self, *args, **kwargs):\n\"\"\"\n        Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n        You can call this function in multiple ways :\n\n        1. *tsd.count(bin_size=1, time_units = 'ms')*\n        -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n\n        2. *tsd.count(1, ep=my_epochs)*\n        -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n\n        3. *tsd.count(ep=my_bins)*\n        -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n\n        4. *tsd.count()*\n        -&gt; Count occurent of events within each epoch of the time support.\n\n        bin_size should be seconds unless specified.\n        If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n\n        Parameters\n        ----------\n        bin_size : None or float, optional\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        out: Tsd\n            A Tsd object indexed by the center of the bins.\n\n        Examples\n        --------\n        This example shows how to count events within bins of 0.1 second.\n\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n        &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n        &gt;&gt;&gt; bincount = ts.count(0.1)\n\n        An epoch can be specified:\n\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n        &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n\n        And bincount automatically inherit ep as time support:\n\n        &gt;&gt;&gt; bincount.time_support\n        &gt;&gt;&gt;    start    end\n        &gt;&gt;&gt; 0  100.0  800.0\n        \"\"\"\n        bin_size = None\n        if \"bin_size\" in kwargs:\n            bin_size = kwargs[\"bin_size\"]\n            if isinstance(bin_size, int):\n                bin_size = float(bin_size)\n            if not isinstance(bin_size, float):\n                raise ValueError(\"bin_size argument should be float.\")\n        else:\n            for a in args:\n                if isinstance(a, (float, int)):\n                    bin_size = float(a)\n\n        time_units = \"s\"\n        if \"time_units\" in kwargs:\n            time_units = kwargs[\"time_units\"]\n            if not isinstance(time_units, str):\n                raise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\n        else:\n            for a in args:\n                if isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\n                    time_units = a\n\n        ep = self.time_support\n        if \"ep\" in kwargs:\n            ep = kwargs[\"ep\"]\n            if not isinstance(ep, IntervalSet):\n                raise ValueError(\"ep argument should be IntervalSet\")\n        else:\n            for a in args:\n                if isinstance(a, IntervalSet):\n                    ep = a\n\n        time_array = self.index.values\n        starts = ep.start.values\n        ends = ep.end.values\n\n        if isinstance(bin_size, (float, int)):\n            bin_size = float(bin_size)\n            bin_size = format_timestamps(np.array([bin_size]), time_units)[0]\n            t, d = jitcount(time_array, starts, ends, bin_size)\n            time_support = IntervalSet(start=starts, end=ends)\n            return Tsd(t=t, d=d, time_support=time_support)\n        else:\n            _, countin = jittsrestrict_with_count(time_array, starts, ends)\n            t = starts + (ends - starts) / 2\n            return Tsd(t=t, d=countin, time_support=ep)\n\n    def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n        Bin the data by averaging points within bin_size\n        bin_size should be seconds unless specified.\n        If no epochs is passed, the data will be binned based on the time support.\n\n        Parameters\n        ----------\n        bin_size : float\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        out: Tsd\n            A Tsd object indexed by the center of the bins and holding the averaged data points.\n\n        Examples\n        --------\n        This example shows how to bin data within bins of 0.1 second.\n\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n        &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n\n        An epoch can be specified:\n\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n        &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n\n        And bintsd automatically inherit ep as time support:\n\n        &gt;&gt;&gt; bintsd.time_support\n        &gt;&gt;&gt;    start    end\n        &gt;&gt;&gt; 0  10.0     80.0\n        \"\"\"\n        if not isinstance(ep, IntervalSet):\n            ep = self.time_support\n\n        bin_size = format_timestamps(np.array([bin_size]), time_units)[0]\n\n        time_array = self.index.values\n        data_array = self.values\n        starts = ep.start.values\n        ends = ep.end.values\n        t, d = jitbin(time_array, data_array, starts, ends, bin_size)\n        time_support = IntervalSet(start=starts, end=ends)\n        return Tsd(t=t, d=d, time_support=time_support)\n\n    def threshold(self, thr, method=\"above\"):\n\"\"\"\n        Apply a threshold function to the tsd to return a new tsd\n        with the time support being the epochs above/below/&gt;=/&lt;= the threshold\n\n        Parameters\n        ----------\n        thr : float\n            The threshold value\n        method : str, optional\n            The threshold method (above/below/aboveequal/belowequal)\n\n        Returns\n        -------\n        out: Tsd\n            All the time points below/ above/greater than equal to/less than equal to the threshold\n\n        Raises\n        ------\n        ValueError\n            Raise an error if method is not 'below' or 'above'\n        RuntimeError\n            Raise an error if thr is too high/low and no epochs is found.\n\n        Examples\n        --------\n        This example finds all epoch above 0.5 within the tsd object.\n\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n        &gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n\n        The epochs with the times above/below the threshold can be accessed through the time support:\n\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n        &gt;&gt;&gt; tsd.threshold(50).time_support\n        &gt;&gt;&gt;    start   end\n        &gt;&gt;&gt; 0   50.5  99.0\n\n        \"\"\"\n        time_array = self.index.values\n        data_array = self.values\n        starts = self.time_support.start.values\n        ends = self.time_support.end.values\n        if method not in [\"above\", \"below\", \"aboveequal\", \"belowequal\"]:\n            raise ValueError(\n                \"Method {} for thresholding is not accepted.\".format(method)\n            )\n\n        t, d, ns, ne = jitthreshold(time_array, data_array, starts, ends, thr, method)\n        time_support = IntervalSet(start=ns, end=ne)\n        return Tsd(t=t, d=d, time_support=time_support)\n\n    def to_tsgroup(self):\n\"\"\"\n        Convert Tsd to a TsGroup by grouping timestamps with the same values.\n        By default, the values are converted to integers.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\n        Time (s)\n        0.0    0\n        1.0    2\n        2.0    0\n        3.0    1\n        dtype: int64\n\n        &gt;&gt;&gt; tsd.to_tsgroup()\n        Index    rate\n        -------  ------\n            0    0.67\n            1    0.33\n            2    0.33\n\n        The reverse operation can be done with the TsGroup.to_tsd function :\n\n        &gt;&gt;&gt; tsgroup.to_tsd()\n        Time (s)\n        0.0    0.0\n        1.0    2.0\n        2.0    0.0\n        3.0    1.0\n        dtype: float64\n\n        Returns\n        -------\n        TsGroup\n            Grouped timestamps\n\n        \"\"\"\n        ts_group = importlib.import_module(\".ts_group\", \"pynapple.core\")\n        t = self.index.values\n        d = self.values.astype(\"int\")\n        idx = np.unique(d)\n\n        group = {}\n        for k in idx:\n            group[k] = Ts(t=t[d == k], time_support=self.time_support)\n\n        return ts_group.TsGroup(group, time_support=self.time_support)\n\n    def save(self, filename):\n\"\"\"\n        Save Tsd object in npz format. The file will contain the timestamps, the\n        data and the time support.\n\n        The main purpose of this function is to save small/medium sized time series\n        objects. For example, you extracted one channel from your recording and\n        filtered it. You can save the filtered channel as a npz to avoid\n        reprocessing it.\n\n        You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end'.\n        See the example below.\n\n        Parameters\n        ----------\n        filename : str\n            The filename\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n        &gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n\n        Here I can retrieve my data with numpy directly:\n\n        &gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'd', 'start', 'end']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1.]\n\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\n        Time (s)\n        0.0    2\n        1.0    3\n        dtype: int64\n\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\n        if not isinstance(filename, str):\n            raise RuntimeError(\"Invalid type; please provide filename as string\")\n\n        if os.path.isdir(filename):\n            raise RuntimeError(\n                \"Invalid filename input. {} is directory.\".format(filename)\n            )\n\n        if not filename.lower().endswith(\".npz\"):\n            filename = filename + \".npz\"\n\n        dirname = os.path.dirname(filename)\n\n        if len(dirname) and not os.path.exists(dirname):\n            raise RuntimeError(\n                \"Path {} does not exist.\".format(os.path.dirname(filename))\n            )\n\n        np.savez(\n            filename,\n            t=self.index.values,\n            d=self.values,\n            start=self.time_support.start.values,\n            end=self.time_support.end.values,\n        )\n\n        return\n\n    # def find_gaps(self, min_gap, method=\"absolute\"):\n    #     \"\"\"\n    #     finds gaps in a tsd larger than min_gap\n\n    #     Parameters\n    #     ----------\n    #     min_gap : TYPE\n    #         Description\n    #     method : str, optional\n    #         Description\n    #     \"\"\"\n    #     print(\"TODO\")\n    #     return\n\n    # def find_support(self, min_gap, method=\"absolute\"):\n    #     \"\"\"\n    #     find the smallest (to a min_gap resolution) IntervalSet containing all the times in the Tsd\n\n    #     Parameters\n    #     ----------\n    #     min_gap : TYPE\n    #         Description\n    #     method : str, optional\n    #         Description\n\n    #     Returns\n    #     -------\n    #     TYPE\n    #         Description\n    #     \"\"\"\n    #     print(\"TODO\")\n    #     return\n\n    def start_time(self, units=\"s\"):\n\"\"\"\n        The first time index in the Ts/Tsd object\n\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        out: numpy.float64\n            _\n        \"\"\"\n        return self.times(units=units)[0]\n\n    def end_time(self, units=\"s\"):\n\"\"\"\n        The last time index in the Ts/Tsd object\n\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        out: numpy.float64\n            _\n        \"\"\"\n        return self.times(units=units)[-1]\n\n    @property\n    def _constructor(self):\n        return Tsd\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.__init__","title":"<code>__init__(t, d=None, time_units='s', time_support=None, **kwargs)</code>","text":"<p>Tsd Initializer.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>numpy.ndarray or pandas.Series</code> <p>An object transformable in a time series, or a pandas.Series equivalent (if d is None)</p> required <code>d</code> <code>numpy.ndarray, optional</code> <p>The data of the time series</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>The time units in which times are specified ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet, optional</code> <p>The time support of the tsd object</p> <code>None</code> <code>**kwargs</code> <p>Arguments that will be passed to the pandas.Series initializer.</p> <code>{}</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, d=None, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n    Tsd Initializer.\n\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.Series\n        An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n    d : numpy.ndarray, optional\n        The data of the time series\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default])\n    time_support : IntervalSet, optional\n        The time support of the tsd object\n    **kwargs\n        Arguments that will be passed to the pandas.Series initializer.\n    \"\"\"\n    if isinstance(t, SingleBlockManager):\n        d = t.array\n        t = t.index.values\n        if \"index\" in kwargs:\n            kwargs.pop(\"index\")\n    elif isinstance(t, pd.Series):\n        d = t.values\n        t = t.index.values\n\n    t = t.astype(np.float64).flatten()\n    t = format_timestamps(t, time_units)\n    t = sort_timestamps(t)\n\n    if len(t):\n        if time_support is not None:\n            starts = time_support.start.values\n            ends = time_support.end.values\n            if d is not None:\n                t, d = jitrestrict(t, d, starts, ends)\n                super().__init__(index=t, data=d)\n            else:\n                t = jittsrestrict(t, starts, ends)\n                super().__init__(index=t, data=None, dtype=np.int8)\n        else:\n            time_support = IntervalSet(start=t[0], end=t[-1])\n            if d is not None:\n                super().__init__(index=t, data=d)\n            else:\n                super().__init__(index=t, data=d, dtype=np.float64)\n\n        self.time_support = time_support\n        self.rate = t.shape[0] / np.sum(\n            time_support.values[:, 1] - time_support.values[:, 0]\n        )\n\n    else:\n        time_support = IntervalSet(pd.DataFrame(columns=[\"start\", \"end\"]))\n        super().__init__(index=t, data=d, dtype=np.float64)\n\n        self.time_support = time_support\n        self.rate = 0.0\n\n    self.index.name = \"Time (s)\"\n    # self._metadata.append(\"nap_class\")\n    self.nap_class = self.__class__.__name__\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.times","title":"<code>times(units='s')</code>","text":"<p>The time index of the Tsd, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>the time indexes</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the Tsd, returned as np.double in the desired time units.\n\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    out: numpy.ndarray\n        the time indexes\n    \"\"\"\n    return return_timestamps(self.index.values, units)\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.as_series","title":"<code>as_series()</code>","text":"<p>Convert the Ts/Tsd object to a pandas.Series object.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>pandas.Series</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_series(self):\n\"\"\"\n    Convert the Ts/Tsd object to a pandas.Series object.\n\n    Returns\n    -------\n    out: pandas.Series\n        _\n    \"\"\"\n    return pd.Series(self, copy=True)\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.as_units","title":"<code>as_units(units='s')</code>","text":"<p>Returns a Series with time expressed in the desired unit.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Type Description <code>pandas.Series</code> <p>the series object with adjusted times</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    Returns a Series with time expressed in the desired unit.\n\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    pandas.Series\n        the series object with adjusted times\n    \"\"\"\n    ss = self.as_series()\n    t = self.index.values\n    t = return_timestamps(t, units)\n    if units == \"us\":\n        t = t.astype(np.int64)\n    ss.index = t\n    ss.index.name = \"Time (\" + str(units) + \")\"\n    return ss\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.data","title":"<code>data()</code>","text":"<p>The data in the Tsd object</p> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    The data in the Tsd object\n\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\n    return self.values\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.value_from","title":"<code>value_from(tsd, ep=None)</code>","text":"<p>Replace the value with the closest value from tsd argument</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>Tsd</code> <p>The Tsd object holding the values to replace</p> required <code>ep</code> <code>IntervalSet(optional)</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>Tsd object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, tsd, ep=None):\n\"\"\"\n    Replace the value with the closest value from tsd argument\n\n    Parameters\n    ----------\n    tsd : Tsd\n        The Tsd object holding the values to replace\n    ep : IntervalSet (optional)\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n\n    Returns\n    -------\n    out : Tsd\n        Tsd object with the new values\n\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n\n    newts is the same size as ts restrict to ep.\n\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\n    if ep is None:\n        ep = tsd.time_support\n\n    time_array = self.index.values\n    time_target_array = tsd.index.values\n    data_target_array = tsd.values\n    starts = ep.start.values\n    ends = ep.end.values\n\n    t, d, ns, ne = jitvaluefrom(\n        time_array, time_target_array, data_target_array, starts, ends\n    )\n    time_support = IntervalSet(start=ns, end=ne)\n    return Tsd(t=t, d=d, time_support=time_support)\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.restrict","title":"<code>restrict(ep)</code>","text":"<p>Restricts a Tsd object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>ep</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>Tsd object restricted to ep</p> <p>Examples:</p> <p>The Ts object is restrict to the intervals defined by ep.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n&gt;&gt;&gt; newts = ts.restrict(ep)\n</code></pre> <p>The time support of newts automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newts.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0    0.0  500.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, ep):\n\"\"\"\n    Restricts a Tsd object to a set of time intervals delimited by an IntervalSet object\n\n    Parameters\n    ----------\n    ep : IntervalSet\n        the IntervalSet object\n\n    Returns\n    -------\n    out: Tsd\n        Tsd object restricted to ep\n\n    Examples\n    --------\n    The Ts object is restrict to the intervals defined by ep.\n\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n    &gt;&gt;&gt; newts = ts.restrict(ep)\n\n    The time support of newts automatically inherit the epochs defined by ep.\n\n    &gt;&gt;&gt; newts.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0    0.0  500.0\n\n    \"\"\"\n    time_array = self.index.values\n    data_array = self.values\n    starts = ep.start.values\n    ends = ep.end.values\n    t, d = jitrestrict(time_array, data_array, starts, ends)\n    return Tsd(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.count","title":"<code>count(*args, **kwargs)</code>","text":"<p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsd.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsd.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsd.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsd.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float, optional</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet, optional</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str, optional</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; bincount = ts.count(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n&gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n</code></pre> <p>And bincount automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bincount.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  100.0  800.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n\n    1. *tsd.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n\n    2. *tsd.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n\n    3. *tsd.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n\n    4. *tsd.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins.\n\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second.\n\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; bincount = ts.count(0.1)\n\n    An epoch can be specified:\n\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n    &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n\n    And bincount automatically inherit ep as time support:\n\n    &gt;&gt;&gt; bincount.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  100.0  800.0\n    \"\"\"\n    bin_size = None\n    if \"bin_size\" in kwargs:\n        bin_size = kwargs[\"bin_size\"]\n        if isinstance(bin_size, int):\n            bin_size = float(bin_size)\n        if not isinstance(bin_size, float):\n            raise ValueError(\"bin_size argument should be float.\")\n    else:\n        for a in args:\n            if isinstance(a, (float, int)):\n                bin_size = float(a)\n\n    time_units = \"s\"\n    if \"time_units\" in kwargs:\n        time_units = kwargs[\"time_units\"]\n        if not isinstance(time_units, str):\n            raise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\n    else:\n        for a in args:\n            if isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\n                time_units = a\n\n    ep = self.time_support\n    if \"ep\" in kwargs:\n        ep = kwargs[\"ep\"]\n        if not isinstance(ep, IntervalSet):\n            raise ValueError(\"ep argument should be IntervalSet\")\n    else:\n        for a in args:\n            if isinstance(a, IntervalSet):\n                ep = a\n\n    time_array = self.index.values\n    starts = ep.start.values\n    ends = ep.end.values\n\n    if isinstance(bin_size, (float, int)):\n        bin_size = float(bin_size)\n        bin_size = format_timestamps(np.array([bin_size]), time_units)[0]\n        t, d = jitcount(time_array, starts, ends, bin_size)\n        time_support = IntervalSet(start=starts, end=ends)\n        return Tsd(t=t, d=d, time_support=time_support)\n    else:\n        _, countin = jittsrestrict_with_count(time_array, starts, ends)\n        t = starts + (ends - starts) / 2\n        return Tsd(t=t, d=countin, time_support=ep)\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.bin_average","title":"<code>bin_average(bin_size, ep=None, time_units='s')</code>","text":"<p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet, optional</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsd automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsd.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins and holding the averaged data points.\n\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n\n    An epoch can be specified:\n\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n\n    And bintsd automatically inherit ep as time support:\n\n    &gt;&gt;&gt; bintsd.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\n    if not isinstance(ep, IntervalSet):\n        ep = self.time_support\n\n    bin_size = format_timestamps(np.array([bin_size]), time_units)[0]\n\n    time_array = self.index.values\n    data_array = self.values\n    starts = ep.start.values\n    ends = ep.end.values\n    t, d = jitbin(time_array, data_array, starts, ends, bin_size)\n    time_support = IntervalSet(start=starts, end=ends)\n    return Tsd(t=t, d=d, time_support=time_support)\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.threshold","title":"<code>threshold(thr, method='above')</code>","text":"<p>Apply a threshold function to the tsd to return a new tsd with the time support being the epochs above/below/&gt;=/&lt;= the threshold</p> <p>Parameters:</p> Name Type Description Default <code>thr</code> <code>float</code> <p>The threshold value</p> required <code>method</code> <code>str, optional</code> <p>The threshold method (above/below/aboveequal/belowequal)</p> <code>'above'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>All the time points below/ above/greater than equal to/less than equal to the threshold</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raise an error if method is not 'below' or 'above'</p> <code>RuntimeError</code> <p>Raise an error if thr is too high/low and no epochs is found.</p> <p>Examples:</p> <p>This example finds all epoch above 0.5 within the tsd object.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n</code></pre> <p>The epochs with the times above/below the threshold can be accessed through the time support:</p> <pre><code>&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n&gt;&gt;&gt; tsd.threshold(50).time_support\n&gt;&gt;&gt;    start   end\n&gt;&gt;&gt; 0   50.5  99.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def threshold(self, thr, method=\"above\"):\n\"\"\"\n    Apply a threshold function to the tsd to return a new tsd\n    with the time support being the epochs above/below/&gt;=/&lt;= the threshold\n\n    Parameters\n    ----------\n    thr : float\n        The threshold value\n    method : str, optional\n        The threshold method (above/below/aboveequal/belowequal)\n\n    Returns\n    -------\n    out: Tsd\n        All the time points below/ above/greater than equal to/less than equal to the threshold\n\n    Raises\n    ------\n    ValueError\n        Raise an error if method is not 'below' or 'above'\n    RuntimeError\n        Raise an error if thr is too high/low and no epochs is found.\n\n    Examples\n    --------\n    This example finds all epoch above 0.5 within the tsd object.\n\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n\n    The epochs with the times above/below the threshold can be accessed through the time support:\n\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n    &gt;&gt;&gt; tsd.threshold(50).time_support\n    &gt;&gt;&gt;    start   end\n    &gt;&gt;&gt; 0   50.5  99.0\n\n    \"\"\"\n    time_array = self.index.values\n    data_array = self.values\n    starts = self.time_support.start.values\n    ends = self.time_support.end.values\n    if method not in [\"above\", \"below\", \"aboveequal\", \"belowequal\"]:\n        raise ValueError(\n            \"Method {} for thresholding is not accepted.\".format(method)\n        )\n\n    t, d, ns, ne = jitthreshold(time_array, data_array, starts, ends, thr, method)\n    time_support = IntervalSet(start=ns, end=ne)\n    return Tsd(t=t, d=d, time_support=time_support)\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.to_tsgroup","title":"<code>to_tsgroup()</code>","text":"<p>Convert Tsd to a TsGroup by grouping timestamps with the same values. By default, the values are converted to integers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\nTime (s)\n0.0    0\n1.0    2\n2.0    0\n3.0    1\ndtype: int64\n</code></pre> <pre><code>&gt;&gt;&gt; tsd.to_tsgroup()\nIndex    rate\n-------  ------\n    0    0.67\n    1    0.33\n    2    0.33\n</code></pre> <p>The reverse operation can be done with the TsGroup.to_tsd function :</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd()\nTime (s)\n0.0    0.0\n1.0    2.0\n2.0    0.0\n3.0    1.0\ndtype: float64\n</code></pre> <p>Returns:</p> Type Description <code>TsGroup</code> <p>Grouped timestamps</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def to_tsgroup(self):\n\"\"\"\n    Convert Tsd to a TsGroup by grouping timestamps with the same values.\n    By default, the values are converted to integers.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\n    Time (s)\n    0.0    0\n    1.0    2\n    2.0    0\n    3.0    1\n    dtype: int64\n\n    &gt;&gt;&gt; tsd.to_tsgroup()\n    Index    rate\n    -------  ------\n        0    0.67\n        1    0.33\n        2    0.33\n\n    The reverse operation can be done with the TsGroup.to_tsd function :\n\n    &gt;&gt;&gt; tsgroup.to_tsd()\n    Time (s)\n    0.0    0.0\n    1.0    2.0\n    2.0    0.0\n    3.0    1.0\n    dtype: float64\n\n    Returns\n    -------\n    TsGroup\n        Grouped timestamps\n\n    \"\"\"\n    ts_group = importlib.import_module(\".ts_group\", \"pynapple.core\")\n    t = self.index.values\n    d = self.values.astype(\"int\")\n    idx = np.unique(d)\n\n    group = {}\n    for k in idx:\n        group[k] = Ts(t=t[d == k], time_support=self.time_support)\n\n    return ts_group.TsGroup(group, time_support=self.time_support)\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.save","title":"<code>save(filename)</code>","text":"<p>Save Tsd object in npz format. The file will contain the timestamps, the data and the time support.</p> <p>The main purpose of this function is to save small/medium sized time series objects. For example, you extracted one channel from your recording and filtered it. You can save the filtered channel as a npz to avoid reprocessing it.</p> <p>You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n&gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'd', 'start', 'end']\n&gt;&gt;&gt; print(file['t'])\n[0. 1.]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\nTime (s)\n0.0    2\n1.0    3\ndtype: int64\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save Tsd object in npz format. The file will contain the timestamps, the\n    data and the time support.\n\n    The main purpose of this function is to save small/medium sized time series\n    objects. For example, you extracted one channel from your recording and\n    filtered it. You can save the filtered channel as a npz to avoid\n    reprocessing it.\n\n    You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end'.\n    See the example below.\n\n    Parameters\n    ----------\n    filename : str\n        The filename\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n    &gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n\n    Here I can retrieve my data with numpy directly:\n\n    &gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'd', 'start', 'end']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1.]\n\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\n    Time (s)\n    0.0    2\n    1.0    3\n    dtype: int64\n\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\n    if not isinstance(filename, str):\n        raise RuntimeError(\"Invalid type; please provide filename as string\")\n\n    if os.path.isdir(filename):\n        raise RuntimeError(\n            \"Invalid filename input. {} is directory.\".format(filename)\n        )\n\n    if not filename.lower().endswith(\".npz\"):\n        filename = filename + \".npz\"\n\n    dirname = os.path.dirname(filename)\n\n    if len(dirname) and not os.path.exists(dirname):\n        raise RuntimeError(\n            \"Path {} does not exist.\".format(os.path.dirname(filename))\n        )\n\n    np.savez(\n        filename,\n        t=self.index.values,\n        d=self.values,\n        start=self.time_support.start.values,\n        end=self.time_support.end.values,\n    )\n\n    return\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.start_time","title":"<code>start_time(units='s')</code>","text":"<p>The first time index in the Ts/Tsd object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def start_time(self, units=\"s\"):\n\"\"\"\n    The first time index in the Ts/Tsd object\n\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\n    return self.times(units=units)[0]\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Tsd.end_time","title":"<code>end_time(units='s')</code>","text":"<p>The last time index in the Ts/Tsd object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def end_time(self, units=\"s\"):\n\"\"\"\n    The last time index in the Ts/Tsd object\n\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\n    return self.times(units=units)[-1]\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Ts","title":"<code>Ts</code>","text":"<p>         Bases: <code>Tsd</code></p> <p>A subclass of the Tsd object for a time series with only time index, By default, the values are set to nan. All the functions of a Tsd object are available in a Ts object.</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class Ts(Tsd):\n\"\"\"\n    A subclass of the Tsd object for a time series with only time index,\n    By default, the values are set to nan.\n    All the functions of a Tsd object are available in a Ts object.\n\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\n\n    def __init__(self, t, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n        Ts Initializer\n\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.Series\n            An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default])\n        time_support : IntervalSet, optional\n            The time support of the Ts object\n        **kwargs\n            Arguments that will be passed to the pandas.Series initializer.\n        \"\"\"\n        super().__init__(\n            t,\n            None,\n            time_units=time_units,\n            time_support=time_support,\n            dtype=np.float64,\n            **kwargs,\n        )\n        self.nts_class = self.__class__.__name__\n\n    def __repr__(self):\n        return self.as_series().fillna(\"\").__repr__()\n\n    def __str__(self):\n        return self.__repr__()\n\n    def save(self, filename):\n\"\"\"\n        Save Ts object in npz format. The file will contain the timestamps and\n        the time support.\n\n        The main purpose of this function is to save small/medium sized timestamps\n        object.\n\n        You can load the object with numpy.load. Keys are 't', 'start' and 'end'.\n        See the example below.\n\n        Parameters\n        ----------\n        filename : str\n            The filename\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n        &gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n\n        Here I can retrieve my data with numpy directly:\n\n        &gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'start', 'end']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1. 1.5]\n\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\n        Time (s)\n        0.0\n        1.0\n        1.5\n\n\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\n        if not isinstance(filename, str):\n            raise RuntimeError(\"Invalid type; please provide filename as string\")\n\n        if os.path.isdir(filename):\n            raise RuntimeError(\n                \"Invalid filename input. {} is directory.\".format(filename)\n            )\n\n        if not filename.lower().endswith(\".npz\"):\n            filename = filename + \".npz\"\n\n        dirname = os.path.dirname(filename)\n\n        if len(dirname) and not os.path.exists(dirname):\n            raise RuntimeError(\n                \"Path {} does not exist.\".format(os.path.dirname(filename))\n            )\n\n        np.savez(\n            filename,\n            t=self.index.values,\n            start=self.time_support.start.values,\n            end=self.time_support.end.values,\n        )\n\n        return\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Ts.__init__","title":"<code>__init__(t, time_units='s', time_support=None, **kwargs)</code>","text":"<p>Ts Initializer</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>numpy.ndarray or pandas.Series</code> <p>An object transformable in a time series, or a pandas.Series equivalent (if d is None)</p> required <code>time_units</code> <code>str, optional</code> <p>The time units in which times are specified ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet, optional</code> <p>The time support of the Ts object</p> <code>None</code> <code>**kwargs</code> <p>Arguments that will be passed to the pandas.Series initializer.</p> <code>{}</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n    Ts Initializer\n\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.Series\n        An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default])\n    time_support : IntervalSet, optional\n        The time support of the Ts object\n    **kwargs\n        Arguments that will be passed to the pandas.Series initializer.\n    \"\"\"\n    super().__init__(\n        t,\n        None,\n        time_units=time_units,\n        time_support=time_support,\n        dtype=np.float64,\n        **kwargs,\n    )\n    self.nts_class = self.__class__.__name__\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.Ts.save","title":"<code>save(filename)</code>","text":"<p>Save Ts object in npz format. The file will contain the timestamps and the time support.</p> <p>The main purpose of this function is to save small/medium sized timestamps object.</p> <p>You can load the object with numpy.load. Keys are 't', 'start' and 'end'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n&gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'start', 'end']\n&gt;&gt;&gt; print(file['t'])\n[0. 1. 1.5]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\nTime (s)\n0.0\n1.0\n1.5\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save Ts object in npz format. The file will contain the timestamps and\n    the time support.\n\n    The main purpose of this function is to save small/medium sized timestamps\n    object.\n\n    You can load the object with numpy.load. Keys are 't', 'start' and 'end'.\n    See the example below.\n\n    Parameters\n    ----------\n    filename : str\n        The filename\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n    &gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n\n    Here I can retrieve my data with numpy directly:\n\n    &gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'start', 'end']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1. 1.5]\n\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\n    Time (s)\n    0.0\n    1.0\n    1.5\n\n\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\n    if not isinstance(filename, str):\n        raise RuntimeError(\"Invalid type; please provide filename as string\")\n\n    if os.path.isdir(filename):\n        raise RuntimeError(\n            \"Invalid filename input. {} is directory.\".format(filename)\n        )\n\n    if not filename.lower().endswith(\".npz\"):\n        filename = filename + \".npz\"\n\n    dirname = os.path.dirname(filename)\n\n    if len(dirname) and not os.path.exists(dirname):\n        raise RuntimeError(\n            \"Path {} does not exist.\".format(os.path.dirname(filename))\n        )\n\n    np.savez(\n        filename,\n        t=self.index.values,\n        start=self.time_support.start.values,\n        end=self.time_support.end.values,\n    )\n\n    return\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.TsdFrame","title":"<code>TsdFrame</code>","text":"<p>         Bases: <code>pd.DataFrame</code></p> <p>A subclass of pandas.DataFrame specialized for neurophysiological time series.</p> <p>TsdFrame provides standardized time representation, plus various functions for manipulating times series with identical sampling frequency.</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class TsdFrame(pd.DataFrame):\n    # class TsdFrame():\n\"\"\"\n    A subclass of pandas.DataFrame specialized for neurophysiological time series.\n\n    TsdFrame provides standardized time representation, plus various functions for manipulating times series with identical sampling frequency.\n\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\n\n    def __init__(self, t, d=None, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n        TsdFrame initializer\n\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.DataFrame\n            the time index t,  or a pandas.DataFrame (if d is None)\n        d : numpy.ndarray\n            The data\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default]).\n        time_support : IntervalSet, optional\n            The time support of the TsdFrame object\n        **kwargs\n            Arguments that will be passed to the pandas.DataFrame initializer.\n        \"\"\"\n        if isinstance(t, BlockManager):\n            d = t.as_array()\n            c = t.axes[0].values\n            t = t.axes[1].values\n        elif isinstance(t, pd.DataFrame):\n            d = t.values\n            c = t.columns.values\n            t = t.index.values\n        else:\n            if \"columns\" in kwargs:\n                c = kwargs[\"columns\"]\n            else:\n                if isinstance(d, np.ndarray):\n                    if len(d.shape) == 2:\n                        c = np.arange(d.shape[1])\n                    elif len(d.shape) == 1:\n                        c = np.zeros(1)\n                    else:\n                        c = np.array([])\n                else:\n                    c = None\n\n        t = t.astype(np.float64).flatten()\n        t = format_timestamps(t, time_units)\n        t = sort_timestamps(t)\n\n        if len(t):\n            if time_support is not None:\n                starts = time_support.start.values\n                ends = time_support.end.values\n                if d is not None:\n                    t, d = jitrestrict(t, d, starts, ends)\n                    super().__init__(index=t, data=d, columns=c)\n                else:\n                    t = jittsrestrict(t, starts, ends)\n                    super().__init__(index=t, data=None, columns=c)\n            else:\n                time_support = IntervalSet(start=t[0], end=t[-1])\n                super().__init__(index=t, data=d, columns=c)\n\n            self.rate = t.shape[0] / np.sum(\n                time_support.values[:, 1] - time_support.values[:, 0]\n            )\n\n        else:\n            time_support = IntervalSet(pd.DataFrame(columns=[\"start\", \"end\"]))\n            super().__init__(index=np.array([]), dtype=np.float64)\n            self.rate = 0.0\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            self.time_support = time_support\n\n        self.index.name = \"Time (s)\"\n        # self._metadata.append(\"nap_class\")\n        self.nap_class = self.__class__.__name__\n\n    def __repr__(self):\n        return self.as_units(\"s\").__repr__()\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __getitem__(self, key):\n        result = super().__getitem__(key)\n        time_support = self.time_support\n        if isinstance(result, pd.Series):\n            return Tsd(result, time_support=time_support)\n        elif isinstance(result, pd.DataFrame):\n            return TsdFrame(result, time_support=time_support)\n\n    def __add__(self, value):\n        ts = self.time_support\n        return TsdFrame(self.as_dataframe().__add__(value), time_support=ts)\n\n    def __sub__(self, value):\n        ts = self.time_support\n        return TsdFrame(self.as_dataframe().__sub__(value), time_support=ts)\n\n    def __truediv__(self, value):\n        ts = self.time_support\n        return TsdFrame(self.as_dataframe().__truediv__(value), time_support=ts)\n\n    def __floordiv__(self, value):\n        ts = self.time_support\n        return TsdFrame(self.as_dataframe().__floordiv__(value), time_support=ts)\n\n    def __mul__(self, value):\n        ts = self.time_support\n        return TsdFrame(self.as_dataframe().__mul__(value), time_support=ts)\n\n    def __mod__(self, value):\n        ts = self.time_support\n        return TsdFrame(self.as_dataframe().__mod__(value), time_support=ts)\n\n    def __pow__(self, value):\n        ts = self.time_support\n        return TsdFrame(self.as_dataframe().__pow__(value), time_support=ts)\n\n    def __lt__(self, value):\n        return self.as_dataframe().__lt__(value)\n\n    def __gt__(self, value):\n        return self.as_dataframe().__gt__(value)\n\n    def __le__(self, value):\n        return self.as_dataframe().__le__(value)\n\n    def __ge__(self, value):\n        return self.as_dataframe().__ge__(value)\n\n    def __ne__(self, value):\n        return self.as_dataframe().__ne__(value)\n\n    def __eq__(self, value):\n        return self.as_dataframe().__eq__(value)\n\n    @property\n    def _constructor(self):\n        return TsdFrame\n\n    def times(self, units=\"s\"):\n\"\"\"\n        The time index of the TsdFrame, returned as np.double in the desired time units.\n\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        out: numpy.ndarray\n            _\n        \"\"\"\n        return return_timestamps(self.index.values, units)\n\n    def as_dataframe(self, copy=True):\n\"\"\"\n        Convert the TsdFrame object to a pandas.DataFrame object.\n\n        Returns\n        -------\n        out: pandas.DataFrame\n            _\n        \"\"\"\n        return pd.DataFrame(self, copy=copy)\n\n    def as_units(self, units=\"s\"):\n\"\"\"\n        Returns a DataFrame with time expressed in the desired unit.\n\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        pandas.DataFrame\n            the series object with adjusted times\n        \"\"\"\n        t = self.index.values.copy()\n        t = return_timestamps(t, units)\n        if units == \"us\":\n            t = t.astype(np.int64)\n\n        df = pd.DataFrame(index=t, data=self.values)\n        df.index.name = \"Time (\" + str(units) + \")\"\n        df.columns = self.columns.copy()\n        return df\n\n    def data(self):\n\"\"\"\n        The data in the TsdFrame object\n\n        Returns\n        -------\n        out: numpy.ndarray\n            _\n        \"\"\"\n        return self.values\n\n    def value_from(self, tsd, ep=None):\n\"\"\"\n        Replace the value with the closest value from tsd argument\n\n        Parameters\n        ----------\n        tsd : Tsd\n            The Tsd object holding the values to replace\n        ep : IntervalSet, optional\n            The IntervalSet object to restrict the operation.\n            If None, ep is taken from the tsd of the time support\n\n        Returns\n        -------\n        out: Tsd\n            Tsd object with the new values\n\n        Examples\n        --------\n        In this example, the ts object will receive the closest values in time from tsd.\n\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n        &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n        &gt;&gt;&gt; tsd = nap.TsdFrame(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n\n        The variable ts is a time series object containing only nan.\n        The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n\n        &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n\n        newts is the same size as ts restrict to ep.\n\n        &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n            52 52\n        \"\"\"\n        if ep is None:\n            ep = tsd.time_support\n\n        time_array = self.index.values\n        time_target_array = tsd.index.values\n        data_target_array = tsd.values\n        starts = ep.start.values\n        ends = ep.end.values\n\n        t, d, ns, ne = jitvaluefrom(\n            time_array, time_target_array, data_target_array, starts, ends\n        )\n        time_support = IntervalSet(start=ns, end=ne)\n        return Tsd(t=t, d=d, time_support=time_support)\n\n    def restrict(self, iset):\n\"\"\"\n        Restricts a TsdFrame object to a set of time intervals delimited by an IntervalSet object`\n\n        Parameters\n        ----------\n        iset : IntervalSet\n            the IntervalSet object\n\n        Returns\n        -------\n        TsdFrame\n            TsdFrame object restricted to ep\n\n        \"\"\"\n        c = self.columns.values\n        time_array = self.index.values\n        data_array = self.values\n        starts = iset.start.values\n        ends = iset.end.values\n        t, d = jitrestrict(time_array, data_array, starts, ends)\n        return TsdFrame(t=t, d=d, columns=c, time_support=iset)\n\n    def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n        Bin the data by averaging points within bin_size\n        bin_size should be seconds unless specified.\n        If no epochs is passed, the data will be binned based on the time support.\n\n        Parameters\n        ----------\n        bin_size : float\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        out: TsdFrame\n            A TsdFrame object indexed by the center of the bins and holding the averaged data points.\n\n        Examples\n        --------\n        This example shows how to bin data within bins of 0.1 second.\n\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.arange(100), d=np.random.rand(100, 3))\n        &gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1)\n\n        An epoch can be specified:\n\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n        &gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1, ep=ep)\n\n        And bintsdframe automatically inherit ep as time support:\n\n        &gt;&gt;&gt; bintsdframe.time_support\n        &gt;&gt;&gt;    start    end\n        &gt;&gt;&gt; 0  10.0     80.0\n        \"\"\"\n        if not isinstance(ep, IntervalSet):\n            ep = self.time_support\n\n        bin_size = format_timestamps(np.array([bin_size]), time_units)[0]\n\n        time_array = self.index.values\n        data_array = self.values\n        starts = ep.start.values\n        ends = ep.end.values\n        t, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\n        time_support = IntervalSet(start=starts, end=ends)\n        return TsdFrame(t=t, d=d, time_support=time_support)\n\n    def save(self, filename):\n\"\"\"\n        Save TsdFrame object in npz format. The file will contain the timestamps, the\n        data and the time support.\n\n        The main purpose of this function is to save small/medium sized time series\n        objects. For example, you extracted several channels from your recording and\n        filtered them. You can save the filtered channels as a npz to avoid\n        reprocessing it.\n\n        You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end'\n        and 'columns' for columns names.\n\n        Parameters\n        ----------\n        filename : str\n            The filename\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n        &gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n\n        Here I can retrieve my data with numpy directly:\n\n        &gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'd', 'start', 'end', 'columns'']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1.]\n\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n                  a  b\n        Time (s)\n        0.0       2  3\n        1.0       4  5\n\n\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\n        if not isinstance(filename, str):\n            raise RuntimeError(\"Invalid type; please provide filename as string\")\n\n        if os.path.isdir(filename):\n            raise RuntimeError(\n                \"Invalid filename input. {} is directory.\".format(filename)\n            )\n\n        if not filename.lower().endswith(\".npz\"):\n            filename = filename + \".npz\"\n\n        dirname = os.path.dirname(filename)\n\n        if len(dirname) and not os.path.exists(dirname):\n            raise RuntimeError(\n                \"Path {} does not exist.\".format(os.path.dirname(filename))\n            )\n\n        cols_name = self.columns.values\n        if cols_name.dtype == np.dtype(\"O\"):\n            cols_name = cols_name.astype(str)\n\n        np.savez(\n            filename,\n            t=self.index.values,\n            d=self.values,\n            start=self.time_support.start.values,\n            end=self.time_support.end.values,\n            columns=cols_name,\n        )\n\n        return\n\n    # def find_gaps(self, min_gap, time_units='s'):\n    #     \"\"\"\n    #     finds gaps in a tsd larger than min_gap. Return an IntervalSet.\n    #     Epochs are defined by adding and removing 1 microsecond to the time index.\n\n    #     Parameters\n    #     ----------\n    #     min_gap : float\n    #         The minimum interval size considered to be a gap (default is second).\n    #     time_units : str, optional\n    #         Time units of min_gap ('us', 'ms', 's' [default])\n    #     \"\"\"\n    #     min_gap = format_timestamps(np.array([min_gap]), time_units)[0]\n\n    #     time_array = self.index.values\n    #     starts = self.time_support.start.values\n    #     ends = self.time_support.end.values\n\n    #     s, e = jitfind_gaps(time_array, starts, ends, min_gap)\n\n    #     return nap.IntervalSet(s, e)\n\n    # def find_support(self, min_gap, method=\"absolute\"):\n    #     \"\"\"\n    #     find the smallest (to a min_gap resolution) IntervalSet containing all the times in the Tsd\n\n    #     Parameters\n    #     ----------\n    #     min_gap : float\n    #         Description\n    #     method : str, optional\n    #         Description\n\n    #     Returns\n    #     -------\n    #     TYPE\n    #         Description\n    #     \"\"\"\n    #     print(\"TODO\")\n    #     return\n\n    def start_time(self, units=\"s\"):\n        return self.times(units=units)[0]\n\n    def end_time(self, units=\"s\"):\n        return self.times(units=units)[-1]\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.TsdFrame.__init__","title":"<code>__init__(t, d=None, time_units='s', time_support=None, **kwargs)</code>","text":"<p>TsdFrame initializer</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>numpy.ndarray or pandas.DataFrame</code> <p>the time index t,  or a pandas.DataFrame (if d is None)</p> required <code>d</code> <code>numpy.ndarray</code> <p>The data</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>The time units in which times are specified ('us', 'ms', 's' [default]).</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet, optional</code> <p>The time support of the TsdFrame object</p> <code>None</code> <code>**kwargs</code> <p>Arguments that will be passed to the pandas.DataFrame initializer.</p> <code>{}</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, d=None, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n    TsdFrame initializer\n\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.DataFrame\n        the time index t,  or a pandas.DataFrame (if d is None)\n    d : numpy.ndarray\n        The data\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default]).\n    time_support : IntervalSet, optional\n        The time support of the TsdFrame object\n    **kwargs\n        Arguments that will be passed to the pandas.DataFrame initializer.\n    \"\"\"\n    if isinstance(t, BlockManager):\n        d = t.as_array()\n        c = t.axes[0].values\n        t = t.axes[1].values\n    elif isinstance(t, pd.DataFrame):\n        d = t.values\n        c = t.columns.values\n        t = t.index.values\n    else:\n        if \"columns\" in kwargs:\n            c = kwargs[\"columns\"]\n        else:\n            if isinstance(d, np.ndarray):\n                if len(d.shape) == 2:\n                    c = np.arange(d.shape[1])\n                elif len(d.shape) == 1:\n                    c = np.zeros(1)\n                else:\n                    c = np.array([])\n            else:\n                c = None\n\n    t = t.astype(np.float64).flatten()\n    t = format_timestamps(t, time_units)\n    t = sort_timestamps(t)\n\n    if len(t):\n        if time_support is not None:\n            starts = time_support.start.values\n            ends = time_support.end.values\n            if d is not None:\n                t, d = jitrestrict(t, d, starts, ends)\n                super().__init__(index=t, data=d, columns=c)\n            else:\n                t = jittsrestrict(t, starts, ends)\n                super().__init__(index=t, data=None, columns=c)\n        else:\n            time_support = IntervalSet(start=t[0], end=t[-1])\n            super().__init__(index=t, data=d, columns=c)\n\n        self.rate = t.shape[0] / np.sum(\n            time_support.values[:, 1] - time_support.values[:, 0]\n        )\n\n    else:\n        time_support = IntervalSet(pd.DataFrame(columns=[\"start\", \"end\"]))\n        super().__init__(index=np.array([]), dtype=np.float64)\n        self.rate = 0.0\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        self.time_support = time_support\n\n    self.index.name = \"Time (s)\"\n    # self._metadata.append(\"nap_class\")\n    self.nap_class = self.__class__.__name__\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.TsdFrame.times","title":"<code>times(units='s')</code>","text":"<p>The time index of the TsdFrame, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the TsdFrame, returned as np.double in the desired time units.\n\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\n    return return_timestamps(self.index.values, units)\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.TsdFrame.as_dataframe","title":"<code>as_dataframe(copy=True)</code>","text":"<p>Convert the TsdFrame object to a pandas.DataFrame object.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>pandas.DataFrame</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_dataframe(self, copy=True):\n\"\"\"\n    Convert the TsdFrame object to a pandas.DataFrame object.\n\n    Returns\n    -------\n    out: pandas.DataFrame\n        _\n    \"\"\"\n    return pd.DataFrame(self, copy=copy)\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.TsdFrame.as_units","title":"<code>as_units(units='s')</code>","text":"<p>Returns a DataFrame with time expressed in the desired unit.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>the series object with adjusted times</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    Returns a DataFrame with time expressed in the desired unit.\n\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    pandas.DataFrame\n        the series object with adjusted times\n    \"\"\"\n    t = self.index.values.copy()\n    t = return_timestamps(t, units)\n    if units == \"us\":\n        t = t.astype(np.int64)\n\n    df = pd.DataFrame(index=t, data=self.values)\n    df.index.name = \"Time (\" + str(units) + \")\"\n    df.columns = self.columns.copy()\n    return df\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.TsdFrame.data","title":"<code>data()</code>","text":"<p>The data in the TsdFrame object</p> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    The data in the TsdFrame object\n\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\n    return self.values\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.TsdFrame.value_from","title":"<code>value_from(tsd, ep=None)</code>","text":"<p>Replace the value with the closest value from tsd argument</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>Tsd</code> <p>The Tsd object holding the values to replace</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The IntervalSet object to restrict the operation. If None, ep is taken from the tsd of the time support</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>Tsd object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.TsdFrame(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, tsd, ep=None):\n\"\"\"\n    Replace the value with the closest value from tsd argument\n\n    Parameters\n    ----------\n    tsd : Tsd\n        The Tsd object holding the values to replace\n    ep : IntervalSet, optional\n        The IntervalSet object to restrict the operation.\n        If None, ep is taken from the tsd of the time support\n\n    Returns\n    -------\n    out: Tsd\n        Tsd object with the new values\n\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.TsdFrame(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n\n    newts is the same size as ts restrict to ep.\n\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\n    if ep is None:\n        ep = tsd.time_support\n\n    time_array = self.index.values\n    time_target_array = tsd.index.values\n    data_target_array = tsd.values\n    starts = ep.start.values\n    ends = ep.end.values\n\n    t, d, ns, ne = jitvaluefrom(\n        time_array, time_target_array, data_target_array, starts, ends\n    )\n    time_support = IntervalSet(start=ns, end=ne)\n    return Tsd(t=t, d=d, time_support=time_support)\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.TsdFrame.restrict","title":"<code>restrict(iset)</code>","text":"<p>Restricts a TsdFrame object to a set of time intervals delimited by an IntervalSet object`</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Type Description <code>TsdFrame</code> <p>TsdFrame object restricted to ep</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, iset):\n\"\"\"\n    Restricts a TsdFrame object to a set of time intervals delimited by an IntervalSet object`\n\n    Parameters\n    ----------\n    iset : IntervalSet\n        the IntervalSet object\n\n    Returns\n    -------\n    TsdFrame\n        TsdFrame object restricted to ep\n\n    \"\"\"\n    c = self.columns.values\n    time_array = self.index.values\n    data_array = self.values\n    starts = iset.start.values\n    ends = iset.end.values\n    t, d = jitrestrict(time_array, data_array, starts, ends)\n    return TsdFrame(t=t, d=d, columns=c, time_support=iset)\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.TsdFrame.bin_average","title":"<code>bin_average(bin_size, ep=None, time_units='s')</code>","text":"<p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet, optional</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>TsdFrame</code> <p>A TsdFrame object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.arange(100), d=np.random.rand(100, 3))\n&gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsdframe automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsdframe.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    out: TsdFrame\n        A TsdFrame object indexed by the center of the bins and holding the averaged data points.\n\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.arange(100), d=np.random.rand(100, 3))\n    &gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1)\n\n    An epoch can be specified:\n\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1, ep=ep)\n\n    And bintsdframe automatically inherit ep as time support:\n\n    &gt;&gt;&gt; bintsdframe.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\n    if not isinstance(ep, IntervalSet):\n        ep = self.time_support\n\n    bin_size = format_timestamps(np.array([bin_size]), time_units)[0]\n\n    time_array = self.index.values\n    data_array = self.values\n    starts = ep.start.values\n    ends = ep.end.values\n    t, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\n    time_support = IntervalSet(start=starts, end=ends)\n    return TsdFrame(t=t, d=d, time_support=time_support)\n</code></pre>"},{"location":"core.time_series/#pynapple.core.time_series.TsdFrame.save","title":"<code>save(filename)</code>","text":"<p>Save TsdFrame object in npz format. The file will contain the timestamps, the data and the time support.</p> <p>The main purpose of this function is to save small/medium sized time series objects. For example, you extracted several channels from your recording and filtered them. You can save the filtered channels as a npz to avoid reprocessing it.</p> <p>You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end' and 'columns' for columns names.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n&gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'd', 'start', 'end', 'columns'']\n&gt;&gt;&gt; print(file['t'])\n[0. 1.]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n          a  b\nTime (s)\n0.0       2  3\n1.0       4  5\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save TsdFrame object in npz format. The file will contain the timestamps, the\n    data and the time support.\n\n    The main purpose of this function is to save small/medium sized time series\n    objects. For example, you extracted several channels from your recording and\n    filtered them. You can save the filtered channels as a npz to avoid\n    reprocessing it.\n\n    You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end'\n    and 'columns' for columns names.\n\n    Parameters\n    ----------\n    filename : str\n        The filename\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n    &gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n\n    Here I can retrieve my data with numpy directly:\n\n    &gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'd', 'start', 'end', 'columns'']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1.]\n\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n              a  b\n    Time (s)\n    0.0       2  3\n    1.0       4  5\n\n\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\n    if not isinstance(filename, str):\n        raise RuntimeError(\"Invalid type; please provide filename as string\")\n\n    if os.path.isdir(filename):\n        raise RuntimeError(\n            \"Invalid filename input. {} is directory.\".format(filename)\n        )\n\n    if not filename.lower().endswith(\".npz\"):\n        filename = filename + \".npz\"\n\n    dirname = os.path.dirname(filename)\n\n    if len(dirname) and not os.path.exists(dirname):\n        raise RuntimeError(\n            \"Path {} does not exist.\".format(os.path.dirname(filename))\n        )\n\n    cols_name = self.columns.values\n    if cols_name.dtype == np.dtype(\"O\"):\n        cols_name = cols_name.astype(str)\n\n    np.savez(\n        filename,\n        t=self.index.values,\n        d=self.values,\n        start=self.time_support.start.values,\n        end=self.time_support.end.values,\n        columns=cols_name,\n    )\n\n    return\n</code></pre>"},{"location":"core.ts_group/","title":"Time Series Group","text":""},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup","title":"<code>TsGroup</code>","text":"<p>         Bases: <code>UserDict</code></p> <p>The TsGroup is a dictionnary-like object to hold multiple <code>Ts</code> or <code>Tsd</code> objects with different time index.</p> <p>Attributes:</p> Name Type Description <code>time_support</code> <code>IntervalSet</code> <p>The time support of the TsGroup</p> <code>rates</code> <code>pandas.Series</code> <p>The rate of each element of the TsGroup</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>class TsGroup(UserDict):\n\"\"\"\n    The TsGroup is a dictionnary-like object to hold multiple [`Ts`][pynapple.core.time_series.Ts] or [`Tsd`][pynapple.core.time_series.Tsd] objects with different time index.\n\n    Attributes\n    ----------\n    time_support: IntervalSet\n        The time support of the TsGroup\n    rates : pandas.Series\n        The rate of each element of the TsGroup\n    \"\"\"\n\n    def __init__(\n        self, data, time_support=None, time_units=\"s\", bypass_check=False, **kwargs\n    ):\n\"\"\"\n        TsGroup Initializer\n\n        Parameters\n        ----------\n        data : dict\n            Dictionnary containing Ts/Tsd objects\n        time_support : IntervalSet, optional\n            The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed.\n            If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.\n        time_units : str, optional\n            Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).\n        bypass_check: bool, optional\n            To avoid checking that each element is within time_support.\n            Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand\n        **kwargs\n            Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray.\n            Note that the index should match the index of the input dictionnary.\n\n        Raises\n        ------\n        RuntimeError\n            Raise error if the union of time support of Ts/Tsd object is empty.\n        \"\"\"\n        self._initialized = False\n\n        self.index = np.sort(list(data.keys()))\n\n        self._metadata = pd.DataFrame(index=self.index, columns=[\"rate\"], dtype=\"float\")\n\n        # Transform elements to Ts/Tsd objects\n        for k in self.index:\n            if isinstance(data[k], (np.ndarray, list)):\n                warnings.warn(\n                    \"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\n                    stacklevel=2,\n                )\n                data[k] = Ts(\n                    t=data[k], time_support=time_support, time_units=time_units\n                )\n\n        # If time_support is passed, all elements of data are restricted prior to init\n        if isinstance(time_support, IntervalSet):\n            self.time_support = time_support\n            if not bypass_check:\n                data = {k: data[k].restrict(self.time_support) for k in self.index}\n        else:\n            # Otherwise do the union of all time supports\n            time_support = union_intervals([data[k].time_support for k in self.index])\n            if len(time_support) == 0:\n                raise RuntimeError(\n                    \"Union of time supports is empty. Consider passing a time support as argument.\"\n                )\n            self.time_support = time_support\n            if not bypass_check:\n                data = {k: data[k].restrict(self.time_support) for k in self.index}\n\n        UserDict.__init__(self, data)\n\n        # Making the TsGroup non mutable\n        self._initialized = True\n\n        # Trying to add argument as metainfo\n        self.set_info(**kwargs)\n\n\"\"\"\n    Base functions\n    \"\"\"\n\n    def __setitem__(self, key, value):\n        if self._initialized:\n            raise RuntimeError(\"TsGroup object is not mutable.\")\n\n        self._metadata.loc[int(key), \"rate\"] = float(value.rate)\n        super().__setitem__(int(key), value)\n        # if self.__contains__(key):\n        #     raise KeyError(\"Key {} already in group index.\".format(key))\n        # else:\n        # if isinstance(value, (Ts, Tsd)):\n        #     self._metadata.loc[int(key), \"rate\"] = value.rate\n        #     super().__setitem__(int(key), value)\n        # elif isinstance(value, (np.ndarray, list)):\n        #     warnings.warn(\n        #         \"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\n        #         stacklevel=2,\n        #     )\n        #     tmp = Ts(t=value, time_units=\"s\")\n        #     self._metadata.loc[int(key), \"rate\"] = tmp.rate\n        #     super().__setitem__(int(key), tmp)\n        # else:\n        #     raise ValueError(\"Value with key {} is not an iterable.\".format(key))\n\n    def __getitem__(self, key):\n        if key.__hash__:\n            if self.__contains__(key):\n                return self.data[key]\n            else:\n                raise KeyError(\"Can't find key {} in group index.\".format(key))\n        else:\n            metadata = self._metadata.loc[key, self._metadata.columns.drop(\"rate\")]\n            return TsGroup(\n                {k: self[k] for k in key}, time_support=self.time_support, **metadata\n            )\n\n    def __repr__(self):\n        cols = self._metadata.columns.drop(\"rate\")\n        headers = [\"Index\", \"rate\"] + [c for c in cols]\n        lines = []\n\n        for i in self.data.keys():\n            lines.append(\n                [str(i), \"%.2f\" % self._metadata.loc[i, \"rate\"]]\n                + [self._metadata.loc[i, c] for c in cols]\n            )\n        return tabulate(lines, headers=headers)\n\n    def __str__(self):\n        return self.__repr__()\n\n    def keys(self):\n\"\"\"\n        Return index/keys of TsGroup\n\n        Returns\n        -------\n        list\n            List of keys\n        \"\"\"\n        return list(self.data.keys())\n\n    def items(self):\n\"\"\"\n        Return a list of key/object.\n\n        Returns\n        -------\n        list\n            List of tuples\n        \"\"\"\n        return list(self.data.items())\n\n    def values(self):\n\"\"\"\n        Return a list of all the Ts/Tsd objects in the TsGroup\n\n        Returns\n        -------\n        list\n            List of Ts/Tsd objects\n        \"\"\"\n        return list(self.data.values())\n\n    @property\n    def rates(self):\n\"\"\"\n        Return the rates of each element of the group in Hz\n        \"\"\"\n        return self._metadata[\"rate\"]\n\n    #######################\n    # Metadata\n    #######################\n    def set_info(self, *args, **kwargs):\n\"\"\"\n        Add metadata informations about the TsGroup.\n        Metadata are saved as a DataFrame.\n\n        Parameters\n        ----------\n        *args\n            pandas.Dataframe or list of pandas.DataFrame\n        **kwargs\n            Can be either pandas.Series or numpy.ndarray\n\n        Raises\n        ------\n        RuntimeError\n            Raise an error if\n                no column labels are found when passing simple arguments,\n                indexes are not equals for a pandas series,\n                not the same length when passing numpy array.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n\n        To add metadata with a pandas.DataFrame:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n        &gt;&gt;&gt; tsgroup.set_info(structs)\n        &gt;&gt;&gt; tsgroup\n          Index    Freq. (Hz)  struct\n        -------  ------------  --------\n              0             1  pfc\n              1             2  pfc\n              2             4  ca1\n\n        To add metadata with a pd.Series or numpy.ndarray:\n\n        &gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n        &gt;&gt;&gt; tsgroup.set_info(hd=hd)\n        &gt;&gt;&gt; tsgroup\n          Index    Freq. (Hz)  struct      hd\n        -------  ------------  --------  ----\n              0             1  pfc          0\n              1             2  pfc          1\n              2             4  ca1          1\n\n        \"\"\"\n        if len(args):\n            for arg in args:\n                if isinstance(arg, pd.DataFrame):\n                    if pd.Index.equals(self._metadata.index, arg.index):\n                        self._metadata = self._metadata.join(arg)\n                    else:\n                        raise RuntimeError(\"Index are not equals\")\n                elif isinstance(arg, (pd.Series, np.ndarray)):\n                    raise RuntimeError(\"Columns needs to be labelled for metadata\")\n        if len(kwargs):\n            for k, v in kwargs.items():\n                if isinstance(v, pd.Series):\n                    if pd.Index.equals(self._metadata.index, v.index):\n                        self._metadata[k] = v\n                    else:\n                        raise RuntimeError(\"Index are not equals\")\n                elif isinstance(v, np.ndarray):\n                    if len(self._metadata) == len(v):\n                        self._metadata[k] = v\n                    else:\n                        raise RuntimeError(\"Array is not the same length.\")\n        return\n\n    def get_info(self, key):\n\"\"\"\n        Returns the metainfo located in one column.\n        The key for the column frequency is \"rate\".\n\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n\n        Returns\n        -------\n        pandas.Series\n            The metainfo\n        \"\"\"\n        if key in [\"freq\", \"frequency\"]:\n            key = \"rate\"\n        return self._metadata[key]\n\n    #################################\n    # Generic functions of Tsd objects\n    #################################\n    def restrict(self, ep):\n\"\"\"\n        Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object\n\n        Parameters\n        ----------\n        ep : IntervalSet\n            the IntervalSet object\n\n        Returns\n        -------\n        TsGroup\n            TsGroup object restricted to ep\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        &gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n\n        All objects within the TsGroup automatically inherit the epochs defined by ep.\n\n        &gt;&gt;&gt; newtsgroup.time_support\n           start    end\n        0    0.0  100.0\n        &gt;&gt;&gt; newtsgroup[0].time_support\n           start    end\n        0    0.0  100.0\n        \"\"\"\n        newgr = {}\n        for k in self.index:\n            newgr[k] = self.data[k].restrict(ep)\n        cols = self._metadata.columns.drop(\"rate\")\n\n        return TsGroup(\n            newgr, time_support=ep, bypass_check=True, **self._metadata[cols]\n        )\n\n    def value_from(self, tsd, ep=None):\n\"\"\"\n        Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument\n\n        Parameters\n        ----------\n        tsd : Tsd\n            The Tsd object holding the values to replace\n        ep : IntervalSet\n            The IntervalSet object to restrict the operation.\n            If None, the time support of the tsd input object is used.\n\n        Returns\n        -------\n        TsGroup\n            TsGroup object with the new values\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n\n        The variable tsd is a time series object containing the values to assign, for example the tracking data:\n\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n        &gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n\n        \"\"\"\n        if ep is None:\n            ep = tsd.time_support\n\n        newgr = {}\n        for k in self.data:\n            newgr[k] = self.data[k].value_from(tsd, ep)\n\n        cols = self._metadata.columns.drop(\"rate\")\n        return TsGroup(newgr, time_support=ep, **self._metadata[cols])\n\n    def count(self, *args, **kwargs):\n\"\"\"\n        Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n        You can call this function in multiple ways :\n\n        1. *tsgroup.count(bin_size=1, time_units = 'ms')*\n        -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n\n        2. *tsgroup.count(1, ep=my_epochs)*\n        -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n\n        3. *tsgroup.count(ep=my_bins)*\n        -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n\n        4. *tsgroup.count()*\n        -&gt; Count occurent of events within each epoch of the time support.\n\n        bin_size should be seconds unless specified.\n        If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n\n        Parameters\n        ----------\n        bin_size : None or float, optional\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n\n        Returns\n        -------\n        out: TsdFrame\n            A TsdFrame with the columns being the index of each item in the TsGroup.\n\n        Examples\n        --------\n        This example shows how to count events within bins of 0.1 second for the first 100 seconds.\n\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        &gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n        &gt;&gt;&gt; bincount\n                  0  1  2\n        Time (s)\n        0.05      0  0  0\n        0.15      0  0  0\n        0.25      0  0  1\n        0.35      0  0  0\n        0.45      0  0  0\n        ...      .. .. ..\n        99.55     0  1  1\n        99.65     0  0  0\n        99.75     0  0  1\n        99.85     0  0  0\n        99.95     1  1  1\n        [1000 rows x 3 columns]\n\n        \"\"\"\n        bin_size = None\n        if \"bin_size\" in kwargs:\n            bin_size = kwargs[\"bin_size\"]\n            if isinstance(bin_size, int):\n                bin_size = float(bin_size)\n            if not isinstance(bin_size, float):\n                raise ValueError(\"bin_size argument should be float.\")\n        else:\n            for a in args:\n                if isinstance(a, (float, int)):\n                    bin_size = float(a)\n\n        time_units = \"s\"\n        if \"time_units\" in kwargs:\n            time_units = kwargs[\"time_units\"]\n            if not isinstance(time_units, str):\n                raise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\n        else:\n            for a in args:\n                if isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\n                    time_units = a\n\n        ep = self.time_support\n        if \"ep\" in kwargs:\n            ep = kwargs[\"ep\"]\n            if not isinstance(ep, IntervalSet):\n                raise ValueError(\"ep argument should be IntervalSet\")\n        else:\n            for a in args:\n                if isinstance(a, IntervalSet):\n                    ep = a\n\n        starts = ep.start.values\n        ends = ep.end.values\n\n        if isinstance(bin_size, (float, int)):\n            bin_size = float(bin_size)\n            bin_size = format_timestamps(np.array([bin_size]), time_units)[0]\n            time_index, _ = jitcount(np.array([]), starts, ends, bin_size)\n            n = len(self.index)\n            count = np.zeros((time_index.shape[0], n), dtype=np.int64)\n\n            for i in range(n):\n                count[:, i] = jitcount(\n                    self.data[self.index[i]].index.values, starts, ends, bin_size\n                )[1]\n\n        else:\n            time_index = starts + (ends - starts) / 2\n            n = len(self.index)\n            count = np.zeros((time_index.shape[0], n), dtype=np.int64)\n\n            for i in range(n):\n                count[:, i] = jittsrestrict_with_count(\n                    self.data[self.index[i]].index.values, starts, ends\n                )[1]\n\n        toreturn = TsdFrame(t=time_index, d=count, time_support=ep, columns=self.index)\n        return toreturn\n\n    def to_tsd(self, *args):\n\"\"\"\n        Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.\n\n        Parameters\n        ----------\n        *args\n            string, list, numpy.ndarray or pandas.Series\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\n        Index    rate\n        -------  ------\n        0       1\n        5       1\n\n        By default, the values of the Tsd is the index of the timestamp in the TsGroup:\n\n        &gt;&gt;&gt; tsgroup.to_tsd()\n        Time (s)\n        0.0    0.0\n        1.0    0.0\n        2.0    5.0\n        3.0    5.0\n        dtype: float64\n\n        Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.\n\n        &gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n        &gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\n        Time (s)\n        0.0    3.141593\n        1.0    3.141593\n        2.0    6.283185\n        3.0    6.283185\n        dtype: float64\n\n        Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :\n\n        &gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\n        Time (s)\n        0.0   -1.0\n        1.0   -1.0\n        2.0    1.0\n        3.0    1.0\n        dtype: float64\n\n        The reverse operation can be done with the Tsd.to_tsgroup function :\n\n        &gt;&gt;&gt; my_tsd\n        Time (s)\n        0.0    0.0\n        1.0    0.0\n        2.0    5.0\n        3.0    5.0\n        dtype: float64\n        &gt;&gt;&gt; my_tsd.to_tsgroup()\n          Index    rate\n        -------  ------\n              0       1\n              5       1\n\n        Returns\n        -------\n        Tsd\n\n        Raises\n        ------\n        RuntimeError\n            \"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes\n            \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object\n            \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata,\n            \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series\n\n        \"\"\"\n        if len(args):\n            if isinstance(args[0], pd.Series):\n                if pd.Index.equals(self._metadata.index, args[0].index):\n                    _values = args[0].values.flatten()\n                else:\n                    raise RuntimeError(\"Index are not equals\")\n            elif isinstance(args[0], (np.ndarray, list)):\n                if len(self._metadata) == len(args[0]):\n                    _values = np.array(args[0])\n                else:\n                    raise RuntimeError(\"Values is not the same length.\")\n            elif isinstance(args[0], str):\n                if args[0] in self._metadata.columns:\n                    _values = self._metadata[args[0]].values\n                else:\n                    raise RuntimeError(\n                        \"Key {} not in metadata of TsGroup\".format(args[0])\n                    )\n            else:\n                possible_keys = []\n                for k, d in self._metadata.dtypes.items():\n                    if \"int\" in str(d) or \"float\" in str(d):\n                        possible_keys.append(k)\n                raise RuntimeError(\n                    \"Unknown argument format. Must be pandas.Series, numpy.ndarray or a string from one of the following values : [{}]\".format(\n                        \", \".join(possible_keys)\n                    )\n                )\n        else:\n            _values = self.index\n\n        nt = 0\n        for n in self.index:\n            nt += self[n].shape[0]\n\n        times = np.zeros(nt)\n        data = np.zeros(nt)\n        k = 0\n        for n, v in zip(self.index, _values):\n            kl = self[n].shape[0]\n            times[k : k + kl] = self[n].index.values\n            data[k : k + kl] = v\n            k += kl\n\n        idx = np.argsort(times)\n        toreturn = Tsd(t=times[idx], d=data[idx], time_support=self.time_support)\n\n        return toreturn\n\n\"\"\"\n    Special slicing of metadata\n    \"\"\"\n\n    def getby_threshold(self, key, thr, op=\"&gt;\"):\n\"\"\"\n        Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.\n\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        thr : float\n            THe value for thresholding\n        op : str, optional\n            The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.\n\n        Returns\n        -------\n        TsGroup\n            The new TsGroup\n\n        Raises\n        ------\n        RuntimeError\n            Raise eror is operation is not recognized.\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n          Index    Freq. (Hz)\n        -------  ------------\n              0             1\n              1             2\n              2             4\n\n        This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.\n        &gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n          Index    Freq. (Hz)\n        -------  ------------\n              1             2\n              2             4\n\n        \"\"\"\n        if op == \"&gt;\":\n            ix = list(self._metadata.index[self._metadata[key] &gt; thr])\n            return self[ix]\n        elif op == \"&lt;\":\n            ix = list(self._metadata.index[self._metadata[key] &lt; thr])\n            return self[ix]\n        elif op == \"&gt;=\":\n            ix = list(self._metadata.index[self._metadata[key] &gt;= thr])\n            return self[ix]\n        elif op == \"&lt;=\":\n            ix = list(self._metadata.index[self._metadata[key] &lt;= thr])\n            return self[ix]\n        else:\n            raise RuntimeError(\"Operation {} not recognized.\".format(op))\n\n    def getby_intervals(self, key, bins):\n\"\"\"\n        Return a list of TsGroup binned.\n\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        bins : numpy.ndarray or list\n            The bin intervals\n\n        Returns\n        -------\n        list\n            A list of TsGroup\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n          Index    Freq. (Hz)    alpha\n        -------  ------------  -------\n              0             1        0\n              1             2        1\n              2             4        2\n\n        This exemple shows how to bin the TsGroup according to one metainfo key.\n        &gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n        &gt;&gt;&gt; newtsgroup\n        [  Index    Freq. (Hz)    alpha\n         -------  ------------  -------\n               0             1        0,\n           Index    Freq. (Hz)    alpha\n         -------  ------------  -------\n               1             2        1]\n\n        By default, the function returns the center of the bins.\n        &gt;&gt;&gt; bincenter\n        array([0.5, 1.5])\n        \"\"\"\n        idx = np.digitize(self._metadata[key], bins) - 1\n        groups = self._metadata.index.groupby(idx)\n        ix = np.unique(list(groups.keys()))\n        ix = ix[ix &gt;= 0]\n        ix = ix[ix &lt; len(bins) - 1]\n        xb = bins[0:-1] + np.diff(bins) / 2\n        sliced = [self[list(groups[i])] for i in ix]\n        return sliced, xb[ix]\n\n    def getby_category(self, key):\n\"\"\"\n        Return a list of TsGroup grouped by category.\n\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n\n        Returns\n        -------\n        dict\n            A dictionnary of TsGroup\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n          Index    Freq. (Hz)    group\n        -------  ------------  -------\n              0             1        0\n              1             2        1\n              2             4        1\n\n        This exemple shows how to group the TsGroup according to one metainfo key.\n        &gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n        &gt;&gt;&gt; newtsgroup\n        {0:   Index    Freq. (Hz)    group\n         -------  ------------  -------\n               0             1        0,\n         1:   Index    Freq. (Hz)    group\n         -------  ------------  -------\n               1             2        1\n               2             4        1}\n        \"\"\"\n        groups = self._metadata.groupby(key).groups\n        sliced = {k: self[list(groups[k])] for k in groups.keys()}\n        return sliced\n\n    def save(self, filename):\n\"\"\"\n        Save TsGroup object in npz format. The file will contain the timestamps,\n        the data (if group of Tsd), group index, the time support and the metadata\n\n        The main purpose of this function is to save small/medium sized TsGroup\n        objects.\n\n        The function will \"flatten\" the TsGroup by sorting all the timestamps\n        and assigning to each the corresponding index. Typically, a TsGroup like\n        this :\n\n            TsGroup({\n                0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n                1 : Tsd(t=[1, 5], d=[5, 6])\n            })\n\n        will be saved as npz with the following keys:\n\n            {\n                't' : [0, 1, 2, 4, 5],\n                'd' : [1, 5, 2, 3, 5],\n                'index' : [0, 1, 0, 0, 1],\n                'start' : [0],\n                'end' : [5]\n            }\n\n        Metadata are saved by columns with the column name as the npz key. To avoid\n        potential conflicts, make sure the columns name of the metadata are different\n        from ['t', 'd', 'start', 'end', 'index']\n\n        You can load the object with numpy.load. Default Keys are 't', 'd'(optional),\n        'start', 'end' and 'index'.\n        See the example below.\n\n        Parameters\n        ----------\n        filename : str\n            The filename\n\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsgroup = nap.TsGroup({\n            0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n            6 : nap.Ts(t=np.array([1.0, 5.0]))\n            },\n            group = np.array([0, 1]),\n            location = np.array(['right foot', 'left foot'])\n            )\n        &gt;&gt;&gt; tsgroup\n          Index    rate    group  location\n        -------  ------  -------  ----------\n              0     0.6        0  right foot\n              6     0.4        1  left foot\n        &gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n\n        Here I can retrieve my data with numpy directly:\n\n        &gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['rate', 'group', 'location', 't', 'index', 'start', 'end']\n        &gt;&gt;&gt; print(file['index'])\n        [0 6 0 0 6]\n\n        In the case where TsGroup is a set of Ts objects, it is very direct to\n        recreate the TsGroup by using the function to_tsgroup :\n\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n        &gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n        &gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n        &gt;&gt;&gt; tsgroup\n          Index    rate    group  location\n        -------  ------  -------  ----------\n              0     0.6        0  right foot\n              6     0.4        1  left foot\n\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\n        if not isinstance(filename, str):\n            raise RuntimeError(\"Invalid type; please provide filename as string\")\n\n        if os.path.isdir(filename):\n            raise RuntimeError(\n                \"Invalid filename input. {} is directory.\".format(filename)\n            )\n\n        if not filename.lower().endswith(\".npz\"):\n            filename = filename + \".npz\"\n\n        dirname = os.path.dirname(filename)\n\n        if len(dirname) and not os.path.exists(dirname):\n            raise RuntimeError(\n                \"Path {} does not exist.\".format(os.path.dirname(filename))\n            )\n\n        dicttosave = {}\n        for k in self._metadata.columns:\n            if k not in [\"t\", \"d\", \"start\", \"end\", \"index\"]:\n                tmp = self._metadata[k].values\n                if tmp.dtype == np.dtype(\"O\"):\n                    tmp = tmp.astype(np.str_)\n                dicttosave[k] = tmp\n\n        # We can't use to_tsd here in case tsgroup contains Tsd and not only Ts.\n        nt = 0\n        for n in self.index:\n            nt += self[n].shape[0]\n\n        times = np.zeros(nt)\n        data = np.zeros(nt)\n        index = np.zeros(nt, dtype=np.int64)\n        k = 0\n        for n in self.index:\n            kl = self[n].shape[0]\n            times[k : k + kl] = self[n].index.values\n            data[k : k + kl] = self[n].values\n            index[k : k + kl] = int(n)\n            k += kl\n\n        idx = np.argsort(times)\n        times = times[idx]\n        index = index[idx]\n\n        dicttosave[\"t\"] = times\n        dicttosave[\"index\"] = index\n        if not np.all(np.isnan(data)):\n            dicttosave[\"d\"] = data[idx]\n\n        dicttosave[\"start\"] = self.time_support.start.values\n        dicttosave[\"end\"] = self.time_support.end.values\n\n        np.savez(filename, **dicttosave)\n\n        return\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.rates","title":"<code>rates</code>  <code>property</code>","text":"<p>Return the rates of each element of the group in Hz</p>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.__init__","title":"<code>__init__(data, time_support=None, time_units='s', bypass_check=False, **kwargs)</code>","text":"<p>TsGroup Initializer</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionnary containing Ts/Tsd objects</p> required <code>time_support</code> <code>IntervalSet, optional</code> <p>The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed. If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).</p> <code>'s'</code> <code>bypass_check</code> <p>To avoid checking that each element is within time_support. Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand</p> <code>False</code> <code>**kwargs</code> <p>Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray. Note that the index should match the index of the input dictionnary.</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise error if the union of time support of Ts/Tsd object is empty.</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def __init__(\n    self, data, time_support=None, time_units=\"s\", bypass_check=False, **kwargs\n):\n\"\"\"\n    TsGroup Initializer\n\n    Parameters\n    ----------\n    data : dict\n        Dictionnary containing Ts/Tsd objects\n    time_support : IntervalSet, optional\n        The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed.\n        If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.\n    time_units : str, optional\n        Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).\n    bypass_check: bool, optional\n        To avoid checking that each element is within time_support.\n        Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand\n    **kwargs\n        Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray.\n        Note that the index should match the index of the input dictionnary.\n\n    Raises\n    ------\n    RuntimeError\n        Raise error if the union of time support of Ts/Tsd object is empty.\n    \"\"\"\n    self._initialized = False\n\n    self.index = np.sort(list(data.keys()))\n\n    self._metadata = pd.DataFrame(index=self.index, columns=[\"rate\"], dtype=\"float\")\n\n    # Transform elements to Ts/Tsd objects\n    for k in self.index:\n        if isinstance(data[k], (np.ndarray, list)):\n            warnings.warn(\n                \"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\n                stacklevel=2,\n            )\n            data[k] = Ts(\n                t=data[k], time_support=time_support, time_units=time_units\n            )\n\n    # If time_support is passed, all elements of data are restricted prior to init\n    if isinstance(time_support, IntervalSet):\n        self.time_support = time_support\n        if not bypass_check:\n            data = {k: data[k].restrict(self.time_support) for k in self.index}\n    else:\n        # Otherwise do the union of all time supports\n        time_support = union_intervals([data[k].time_support for k in self.index])\n        if len(time_support) == 0:\n            raise RuntimeError(\n                \"Union of time supports is empty. Consider passing a time support as argument.\"\n            )\n        self.time_support = time_support\n        if not bypass_check:\n            data = {k: data[k].restrict(self.time_support) for k in self.index}\n\n    UserDict.__init__(self, data)\n\n    # Making the TsGroup non mutable\n    self._initialized = True\n\n    # Trying to add argument as metainfo\n    self.set_info(**kwargs)\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.keys","title":"<code>keys()</code>","text":"<p>Return index/keys of TsGroup</p> <p>Returns:</p> Type Description <code>list</code> <p>List of keys</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def keys(self):\n\"\"\"\n    Return index/keys of TsGroup\n\n    Returns\n    -------\n    list\n        List of keys\n    \"\"\"\n    return list(self.data.keys())\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.items","title":"<code>items()</code>","text":"<p>Return a list of key/object.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of tuples</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def items(self):\n\"\"\"\n    Return a list of key/object.\n\n    Returns\n    -------\n    list\n        List of tuples\n    \"\"\"\n    return list(self.data.items())\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.values","title":"<code>values()</code>","text":"<p>Return a list of all the Ts/Tsd objects in the TsGroup</p> <p>Returns:</p> Type Description <code>list</code> <p>List of Ts/Tsd objects</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def values(self):\n\"\"\"\n    Return a list of all the Ts/Tsd objects in the TsGroup\n\n    Returns\n    -------\n    list\n        List of Ts/Tsd objects\n    \"\"\"\n    return list(self.data.values())\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.set_info","title":"<code>set_info(*args, **kwargs)</code>","text":"<p>Add metadata informations about the TsGroup. Metadata are saved as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>pandas.Dataframe or list of pandas.DataFrame</p> <code>()</code> <code>**kwargs</code> <p>Can be either pandas.Series or numpy.ndarray</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise an error if no column labels are found when passing simple arguments, indexes are not equals for a pandas series, not the same length when passing numpy array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n</code></pre> <p>To add metadata with a pandas.DataFrame:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n&gt;&gt;&gt; tsgroup.set_info(structs)\n&gt;&gt;&gt; tsgroup\n  Index    Freq. (Hz)  struct\n-------  ------------  --------\n      0             1  pfc\n      1             2  pfc\n      2             4  ca1\n</code></pre> <p>To add metadata with a pd.Series or numpy.ndarray:</p> <pre><code>&gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n&gt;&gt;&gt; tsgroup.set_info(hd=hd)\n&gt;&gt;&gt; tsgroup\n  Index    Freq. (Hz)  struct      hd\n-------  ------------  --------  ----\n      0             1  pfc          0\n      1             2  pfc          1\n      2             4  ca1          1\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def set_info(self, *args, **kwargs):\n\"\"\"\n    Add metadata informations about the TsGroup.\n    Metadata are saved as a DataFrame.\n\n    Parameters\n    ----------\n    *args\n        pandas.Dataframe or list of pandas.DataFrame\n    **kwargs\n        Can be either pandas.Series or numpy.ndarray\n\n    Raises\n    ------\n    RuntimeError\n        Raise an error if\n            no column labels are found when passing simple arguments,\n            indexes are not equals for a pandas series,\n            not the same length when passing numpy array.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n\n    To add metadata with a pandas.DataFrame:\n\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n    &gt;&gt;&gt; tsgroup.set_info(structs)\n    &gt;&gt;&gt; tsgroup\n      Index    Freq. (Hz)  struct\n    -------  ------------  --------\n          0             1  pfc\n          1             2  pfc\n          2             4  ca1\n\n    To add metadata with a pd.Series or numpy.ndarray:\n\n    &gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n    &gt;&gt;&gt; tsgroup.set_info(hd=hd)\n    &gt;&gt;&gt; tsgroup\n      Index    Freq. (Hz)  struct      hd\n    -------  ------------  --------  ----\n          0             1  pfc          0\n          1             2  pfc          1\n          2             4  ca1          1\n\n    \"\"\"\n    if len(args):\n        for arg in args:\n            if isinstance(arg, pd.DataFrame):\n                if pd.Index.equals(self._metadata.index, arg.index):\n                    self._metadata = self._metadata.join(arg)\n                else:\n                    raise RuntimeError(\"Index are not equals\")\n            elif isinstance(arg, (pd.Series, np.ndarray)):\n                raise RuntimeError(\"Columns needs to be labelled for metadata\")\n    if len(kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, pd.Series):\n                if pd.Index.equals(self._metadata.index, v.index):\n                    self._metadata[k] = v\n                else:\n                    raise RuntimeError(\"Index are not equals\")\n            elif isinstance(v, np.ndarray):\n                if len(self._metadata) == len(v):\n                    self._metadata[k] = v\n                else:\n                    raise RuntimeError(\"Array is not the same length.\")\n    return\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.get_info","title":"<code>get_info(key)</code>","text":"<p>Returns the metainfo located in one column. The key for the column frequency is \"rate\".</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <p>Returns:</p> Type Description <code>pandas.Series</code> <p>The metainfo</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def get_info(self, key):\n\"\"\"\n    Returns the metainfo located in one column.\n    The key for the column frequency is \"rate\".\n\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n\n    Returns\n    -------\n    pandas.Series\n        The metainfo\n    \"\"\"\n    if key in [\"freq\", \"frequency\"]:\n        key = \"rate\"\n    return self._metadata[key]\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.restrict","title":"<code>restrict(ep)</code>","text":"<p>Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>ep</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Type Description <code>TsGroup</code> <p>TsGroup object restricted to ep</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n&gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n</code></pre> <p>All objects within the TsGroup automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newtsgroup.time_support\n   start    end\n0    0.0  100.0\n&gt;&gt;&gt; newtsgroup[0].time_support\n   start    end\n0    0.0  100.0\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def restrict(self, ep):\n\"\"\"\n    Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object\n\n    Parameters\n    ----------\n    ep : IntervalSet\n        the IntervalSet object\n\n    Returns\n    -------\n    TsGroup\n        TsGroup object restricted to ep\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    &gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n\n    All objects within the TsGroup automatically inherit the epochs defined by ep.\n\n    &gt;&gt;&gt; newtsgroup.time_support\n       start    end\n    0    0.0  100.0\n    &gt;&gt;&gt; newtsgroup[0].time_support\n       start    end\n    0    0.0  100.0\n    \"\"\"\n    newgr = {}\n    for k in self.index:\n        newgr[k] = self.data[k].restrict(ep)\n    cols = self._metadata.columns.drop(\"rate\")\n\n    return TsGroup(\n        newgr, time_support=ep, bypass_check=True, **self._metadata[cols]\n    )\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.value_from","title":"<code>value_from(tsd, ep=None)</code>","text":"<p>Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>Tsd</code> <p>The Tsd object holding the values to replace</p> required <code>ep</code> <code>IntervalSet</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>TsGroup</code> <p>TsGroup object with the new values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n</code></pre> <p>The variable tsd is a time series object containing the values to assign, for example the tracking data:</p> <pre><code>&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n&gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def value_from(self, tsd, ep=None):\n\"\"\"\n    Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument\n\n    Parameters\n    ----------\n    tsd : Tsd\n        The Tsd object holding the values to replace\n    ep : IntervalSet\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n\n    Returns\n    -------\n    TsGroup\n        TsGroup object with the new values\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n\n    The variable tsd is a time series object containing the values to assign, for example the tracking data:\n\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n    &gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n\n    \"\"\"\n    if ep is None:\n        ep = tsd.time_support\n\n    newgr = {}\n    for k in self.data:\n        newgr[k] = self.data[k].value_from(tsd, ep)\n\n    cols = self._metadata.columns.drop(\"rate\")\n    return TsGroup(newgr, time_support=ep, **self._metadata[cols])\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.count","title":"<code>count(*args, **kwargs)</code>","text":"<p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsgroup.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsgroup.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsgroup.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsgroup.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float, optional</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet, optional</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str, optional</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>TsdFrame</code> <p>A TsdFrame with the columns being the index of each item in the TsGroup.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second for the first 100 seconds.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n&gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n&gt;&gt;&gt; bincount\n          0  1  2\nTime (s)\n0.05      0  0  0\n0.15      0  0  0\n0.25      0  0  1\n0.35      0  0  0\n0.45      0  0  0\n...      .. .. ..\n99.55     0  1  1\n99.65     0  0  0\n99.75     0  0  1\n99.85     0  0  0\n99.95     1  1  1\n[1000 rows x 3 columns]\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n\n    1. *tsgroup.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n\n    2. *tsgroup.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n\n    3. *tsgroup.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n\n    4. *tsgroup.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n\n    Returns\n    -------\n    out: TsdFrame\n        A TsdFrame with the columns being the index of each item in the TsGroup.\n\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second for the first 100 seconds.\n\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    &gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n    &gt;&gt;&gt; bincount\n              0  1  2\n    Time (s)\n    0.05      0  0  0\n    0.15      0  0  0\n    0.25      0  0  1\n    0.35      0  0  0\n    0.45      0  0  0\n    ...      .. .. ..\n    99.55     0  1  1\n    99.65     0  0  0\n    99.75     0  0  1\n    99.85     0  0  0\n    99.95     1  1  1\n    [1000 rows x 3 columns]\n\n    \"\"\"\n    bin_size = None\n    if \"bin_size\" in kwargs:\n        bin_size = kwargs[\"bin_size\"]\n        if isinstance(bin_size, int):\n            bin_size = float(bin_size)\n        if not isinstance(bin_size, float):\n            raise ValueError(\"bin_size argument should be float.\")\n    else:\n        for a in args:\n            if isinstance(a, (float, int)):\n                bin_size = float(a)\n\n    time_units = \"s\"\n    if \"time_units\" in kwargs:\n        time_units = kwargs[\"time_units\"]\n        if not isinstance(time_units, str):\n            raise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\n    else:\n        for a in args:\n            if isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\n                time_units = a\n\n    ep = self.time_support\n    if \"ep\" in kwargs:\n        ep = kwargs[\"ep\"]\n        if not isinstance(ep, IntervalSet):\n            raise ValueError(\"ep argument should be IntervalSet\")\n    else:\n        for a in args:\n            if isinstance(a, IntervalSet):\n                ep = a\n\n    starts = ep.start.values\n    ends = ep.end.values\n\n    if isinstance(bin_size, (float, int)):\n        bin_size = float(bin_size)\n        bin_size = format_timestamps(np.array([bin_size]), time_units)[0]\n        time_index, _ = jitcount(np.array([]), starts, ends, bin_size)\n        n = len(self.index)\n        count = np.zeros((time_index.shape[0], n), dtype=np.int64)\n\n        for i in range(n):\n            count[:, i] = jitcount(\n                self.data[self.index[i]].index.values, starts, ends, bin_size\n            )[1]\n\n    else:\n        time_index = starts + (ends - starts) / 2\n        n = len(self.index)\n        count = np.zeros((time_index.shape[0], n), dtype=np.int64)\n\n        for i in range(n):\n            count[:, i] = jittsrestrict_with_count(\n                self.data[self.index[i]].index.values, starts, ends\n            )[1]\n\n    toreturn = TsdFrame(t=time_index, d=count, time_support=ep, columns=self.index)\n    return toreturn\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.to_tsd","title":"<code>to_tsd(*args)</code>","text":"<p>Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>string, list, numpy.ndarray or pandas.Series</p> <code>()</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\nIndex    rate\n-------  ------\n0       1\n5       1\n</code></pre> <p>By default, the values of the Tsd is the index of the timestamp in the TsGroup:</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd()\nTime (s)\n0.0    0.0\n1.0    0.0\n2.0    5.0\n3.0    5.0\ndtype: float64\n</code></pre> <p>Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.</p> <pre><code>&gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n&gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\nTime (s)\n0.0    3.141593\n1.0    3.141593\n2.0    6.283185\n3.0    6.283185\ndtype: float64\n</code></pre> <p>Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\nTime (s)\n0.0   -1.0\n1.0   -1.0\n2.0    1.0\n3.0    1.0\ndtype: float64\n</code></pre> <p>The reverse operation can be done with the Tsd.to_tsgroup function :</p> <pre><code>&gt;&gt;&gt; my_tsd\nTime (s)\n0.0    0.0\n1.0    0.0\n2.0    5.0\n3.0    5.0\ndtype: float64\n&gt;&gt;&gt; my_tsd.to_tsgroup()\n  Index    rate\n-------  ------\n      0       1\n      5       1\n</code></pre> <p>Returns:</p> Type Description <code>Tsd</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>\"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata, \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def to_tsd(self, *args):\n\"\"\"\n    Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.\n\n    Parameters\n    ----------\n    *args\n        string, list, numpy.ndarray or pandas.Series\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\n    Index    rate\n    -------  ------\n    0       1\n    5       1\n\n    By default, the values of the Tsd is the index of the timestamp in the TsGroup:\n\n    &gt;&gt;&gt; tsgroup.to_tsd()\n    Time (s)\n    0.0    0.0\n    1.0    0.0\n    2.0    5.0\n    3.0    5.0\n    dtype: float64\n\n    Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.\n\n    &gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n    &gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\n    Time (s)\n    0.0    3.141593\n    1.0    3.141593\n    2.0    6.283185\n    3.0    6.283185\n    dtype: float64\n\n    Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :\n\n    &gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\n    Time (s)\n    0.0   -1.0\n    1.0   -1.0\n    2.0    1.0\n    3.0    1.0\n    dtype: float64\n\n    The reverse operation can be done with the Tsd.to_tsgroup function :\n\n    &gt;&gt;&gt; my_tsd\n    Time (s)\n    0.0    0.0\n    1.0    0.0\n    2.0    5.0\n    3.0    5.0\n    dtype: float64\n    &gt;&gt;&gt; my_tsd.to_tsgroup()\n      Index    rate\n    -------  ------\n          0       1\n          5       1\n\n    Returns\n    -------\n    Tsd\n\n    Raises\n    ------\n    RuntimeError\n        \"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes\n        \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object\n        \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata,\n        \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series\n\n    \"\"\"\n    if len(args):\n        if isinstance(args[0], pd.Series):\n            if pd.Index.equals(self._metadata.index, args[0].index):\n                _values = args[0].values.flatten()\n            else:\n                raise RuntimeError(\"Index are not equals\")\n        elif isinstance(args[0], (np.ndarray, list)):\n            if len(self._metadata) == len(args[0]):\n                _values = np.array(args[0])\n            else:\n                raise RuntimeError(\"Values is not the same length.\")\n        elif isinstance(args[0], str):\n            if args[0] in self._metadata.columns:\n                _values = self._metadata[args[0]].values\n            else:\n                raise RuntimeError(\n                    \"Key {} not in metadata of TsGroup\".format(args[0])\n                )\n        else:\n            possible_keys = []\n            for k, d in self._metadata.dtypes.items():\n                if \"int\" in str(d) or \"float\" in str(d):\n                    possible_keys.append(k)\n            raise RuntimeError(\n                \"Unknown argument format. Must be pandas.Series, numpy.ndarray or a string from one of the following values : [{}]\".format(\n                    \", \".join(possible_keys)\n                )\n            )\n    else:\n        _values = self.index\n\n    nt = 0\n    for n in self.index:\n        nt += self[n].shape[0]\n\n    times = np.zeros(nt)\n    data = np.zeros(nt)\n    k = 0\n    for n, v in zip(self.index, _values):\n        kl = self[n].shape[0]\n        times[k : k + kl] = self[n].index.values\n        data[k : k + kl] = v\n        k += kl\n\n    idx = np.argsort(times)\n    toreturn = Tsd(t=times[idx], d=data[idx], time_support=self.time_support)\n\n    return toreturn\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.getby_threshold","title":"<code>getby_threshold(key, thr, op='&gt;')</code>","text":"<p>Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <code>thr</code> <code>float</code> <p>THe value for thresholding</p> required <code>op</code> <code>str, optional</code> <p>The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.</p> <code>'&gt;'</code> <p>Returns:</p> Type Description <code>TsGroup</code> <p>The new TsGroup</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise eror is operation is not recognized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n  Index    Freq. (Hz)\n-------  ------------\n      0             1\n      1             2\n      2             4\n</code></pre> <p>This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.</p> <pre><code>&gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n  Index    Freq. (Hz)\n-------  ------------\n      1             2\n      2             4\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_threshold(self, key, thr, op=\"&gt;\"):\n\"\"\"\n    Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.\n\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    thr : float\n        THe value for thresholding\n    op : str, optional\n        The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.\n\n    Returns\n    -------\n    TsGroup\n        The new TsGroup\n\n    Raises\n    ------\n    RuntimeError\n        Raise eror is operation is not recognized.\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n      Index    Freq. (Hz)\n    -------  ------------\n          0             1\n          1             2\n          2             4\n\n    This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.\n    &gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n      Index    Freq. (Hz)\n    -------  ------------\n          1             2\n          2             4\n\n    \"\"\"\n    if op == \"&gt;\":\n        ix = list(self._metadata.index[self._metadata[key] &gt; thr])\n        return self[ix]\n    elif op == \"&lt;\":\n        ix = list(self._metadata.index[self._metadata[key] &lt; thr])\n        return self[ix]\n    elif op == \"&gt;=\":\n        ix = list(self._metadata.index[self._metadata[key] &gt;= thr])\n        return self[ix]\n    elif op == \"&lt;=\":\n        ix = list(self._metadata.index[self._metadata[key] &lt;= thr])\n        return self[ix]\n    else:\n        raise RuntimeError(\"Operation {} not recognized.\".format(op))\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.getby_intervals","title":"<code>getby_intervals(key, bins)</code>","text":"<p>Return a list of TsGroup binned.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <code>bins</code> <code>numpy.ndarray or list</code> <p>The bin intervals</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of TsGroup</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n  Index    Freq. (Hz)    alpha\n-------  ------------  -------\n      0             1        0\n      1             2        1\n      2             4        2\n</code></pre> <p>This exemple shows how to bin the TsGroup according to one metainfo key.</p> <pre><code>&gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n&gt;&gt;&gt; newtsgroup\n[  Index    Freq. (Hz)    alpha\n -------  ------------  -------\n       0             1        0,\n   Index    Freq. (Hz)    alpha\n -------  ------------  -------\n       1             2        1]\n</code></pre> <p>By default, the function returns the center of the bins.</p> <pre><code>&gt;&gt;&gt; bincenter\narray([0.5, 1.5])\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_intervals(self, key, bins):\n\"\"\"\n    Return a list of TsGroup binned.\n\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    bins : numpy.ndarray or list\n        The bin intervals\n\n    Returns\n    -------\n    list\n        A list of TsGroup\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n      Index    Freq. (Hz)    alpha\n    -------  ------------  -------\n          0             1        0\n          1             2        1\n          2             4        2\n\n    This exemple shows how to bin the TsGroup according to one metainfo key.\n    &gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n    &gt;&gt;&gt; newtsgroup\n    [  Index    Freq. (Hz)    alpha\n     -------  ------------  -------\n           0             1        0,\n       Index    Freq. (Hz)    alpha\n     -------  ------------  -------\n           1             2        1]\n\n    By default, the function returns the center of the bins.\n    &gt;&gt;&gt; bincenter\n    array([0.5, 1.5])\n    \"\"\"\n    idx = np.digitize(self._metadata[key], bins) - 1\n    groups = self._metadata.index.groupby(idx)\n    ix = np.unique(list(groups.keys()))\n    ix = ix[ix &gt;= 0]\n    ix = ix[ix &lt; len(bins) - 1]\n    xb = bins[0:-1] + np.diff(bins) / 2\n    sliced = [self[list(groups[i])] for i in ix]\n    return sliced, xb[ix]\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.getby_category","title":"<code>getby_category(key)</code>","text":"<p>Return a list of TsGroup grouped by category.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionnary of TsGroup</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n  Index    Freq. (Hz)    group\n-------  ------------  -------\n      0             1        0\n      1             2        1\n      2             4        1\n</code></pre> <p>This exemple shows how to group the TsGroup according to one metainfo key.</p> <pre><code>&gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n&gt;&gt;&gt; newtsgroup\n{0:   Index    Freq. (Hz)    group\n -------  ------------  -------\n       0             1        0,\n 1:   Index    Freq. (Hz)    group\n -------  ------------  -------\n       1             2        1\n       2             4        1}\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_category(self, key):\n\"\"\"\n    Return a list of TsGroup grouped by category.\n\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n\n    Returns\n    -------\n    dict\n        A dictionnary of TsGroup\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n      Index    Freq. (Hz)    group\n    -------  ------------  -------\n          0             1        0\n          1             2        1\n          2             4        1\n\n    This exemple shows how to group the TsGroup according to one metainfo key.\n    &gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n    &gt;&gt;&gt; newtsgroup\n    {0:   Index    Freq. (Hz)    group\n     -------  ------------  -------\n           0             1        0,\n     1:   Index    Freq. (Hz)    group\n     -------  ------------  -------\n           1             2        1\n           2             4        1}\n    \"\"\"\n    groups = self._metadata.groupby(key).groups\n    sliced = {k: self[list(groups[k])] for k in groups.keys()}\n    return sliced\n</code></pre>"},{"location":"core.ts_group/#pynapple.core.ts_group.TsGroup.save","title":"<code>save(filename)</code>","text":"<p>Save TsGroup object in npz format. The file will contain the timestamps, the data (if group of Tsd), group index, the time support and the metadata</p> <p>The main purpose of this function is to save small/medium sized TsGroup objects.</p> <p>The function will \"flatten\" the TsGroup by sorting all the timestamps and assigning to each the corresponding index. Typically, a TsGroup like this :</p> <pre><code>TsGroup({\n    0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n    1 : Tsd(t=[1, 5], d=[5, 6])\n})\n</code></pre> <p>will be saved as npz with the following keys:</p> <pre><code>{\n    't' : [0, 1, 2, 4, 5],\n    'd' : [1, 5, 2, 3, 5],\n    'index' : [0, 1, 0, 0, 1],\n    'start' : [0],\n    'end' : [5]\n}\n</code></pre> <p>Metadata are saved by columns with the column name as the npz key. To avoid potential conflicts, make sure the columns name of the metadata are different from ['t', 'd', 'start', 'end', 'index']</p> <p>You can load the object with numpy.load. Default Keys are 't', 'd'(optional), 'start', 'end' and 'index'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsgroup = nap.TsGroup({\n    0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n    6 : nap.Ts(t=np.array([1.0, 5.0]))\n    },\n    group = np.array([0, 1]),\n    location = np.array(['right foot', 'left foot'])\n    )\n&gt;&gt;&gt; tsgroup\n  Index    rate    group  location\n-------  ------  -------  ----------\n      0     0.6        0  right foot\n      6     0.4        1  left foot\n&gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['rate', 'group', 'location', 't', 'index', 'start', 'end']\n&gt;&gt;&gt; print(file['index'])\n[0 6 0 0 6]\n</code></pre> <p>In the case where TsGroup is a set of Ts objects, it is very direct to recreate the TsGroup by using the function to_tsgroup :</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n&gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n&gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n&gt;&gt;&gt; tsgroup\n  Index    rate    group  location\n-------  ------  -------  ----------\n      0     0.6        0  right foot\n      6     0.4        1  left foot\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save TsGroup object in npz format. The file will contain the timestamps,\n    the data (if group of Tsd), group index, the time support and the metadata\n\n    The main purpose of this function is to save small/medium sized TsGroup\n    objects.\n\n    The function will \"flatten\" the TsGroup by sorting all the timestamps\n    and assigning to each the corresponding index. Typically, a TsGroup like\n    this :\n\n        TsGroup({\n            0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n            1 : Tsd(t=[1, 5], d=[5, 6])\n        })\n\n    will be saved as npz with the following keys:\n\n        {\n            't' : [0, 1, 2, 4, 5],\n            'd' : [1, 5, 2, 3, 5],\n            'index' : [0, 1, 0, 0, 1],\n            'start' : [0],\n            'end' : [5]\n        }\n\n    Metadata are saved by columns with the column name as the npz key. To avoid\n    potential conflicts, make sure the columns name of the metadata are different\n    from ['t', 'd', 'start', 'end', 'index']\n\n    You can load the object with numpy.load. Default Keys are 't', 'd'(optional),\n    'start', 'end' and 'index'.\n    See the example below.\n\n    Parameters\n    ----------\n    filename : str\n        The filename\n\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsgroup = nap.TsGroup({\n        0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n        6 : nap.Ts(t=np.array([1.0, 5.0]))\n        },\n        group = np.array([0, 1]),\n        location = np.array(['right foot', 'left foot'])\n        )\n    &gt;&gt;&gt; tsgroup\n      Index    rate    group  location\n    -------  ------  -------  ----------\n          0     0.6        0  right foot\n          6     0.4        1  left foot\n    &gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n\n    Here I can retrieve my data with numpy directly:\n\n    &gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['rate', 'group', 'location', 't', 'index', 'start', 'end']\n    &gt;&gt;&gt; print(file['index'])\n    [0 6 0 0 6]\n\n    In the case where TsGroup is a set of Ts objects, it is very direct to\n    recreate the TsGroup by using the function to_tsgroup :\n\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n    &gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n    &gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n    &gt;&gt;&gt; tsgroup\n      Index    rate    group  location\n    -------  ------  -------  ----------\n          0     0.6        0  right foot\n          6     0.4        1  left foot\n\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\n    if not isinstance(filename, str):\n        raise RuntimeError(\"Invalid type; please provide filename as string\")\n\n    if os.path.isdir(filename):\n        raise RuntimeError(\n            \"Invalid filename input. {} is directory.\".format(filename)\n        )\n\n    if not filename.lower().endswith(\".npz\"):\n        filename = filename + \".npz\"\n\n    dirname = os.path.dirname(filename)\n\n    if len(dirname) and not os.path.exists(dirname):\n        raise RuntimeError(\n            \"Path {} does not exist.\".format(os.path.dirname(filename))\n        )\n\n    dicttosave = {}\n    for k in self._metadata.columns:\n        if k not in [\"t\", \"d\", \"start\", \"end\", \"index\"]:\n            tmp = self._metadata[k].values\n            if tmp.dtype == np.dtype(\"O\"):\n                tmp = tmp.astype(np.str_)\n            dicttosave[k] = tmp\n\n    # We can't use to_tsd here in case tsgroup contains Tsd and not only Ts.\n    nt = 0\n    for n in self.index:\n        nt += self[n].shape[0]\n\n    times = np.zeros(nt)\n    data = np.zeros(nt)\n    index = np.zeros(nt, dtype=np.int64)\n    k = 0\n    for n in self.index:\n        kl = self[n].shape[0]\n        times[k : k + kl] = self[n].index.values\n        data[k : k + kl] = self[n].values\n        index[k : k + kl] = int(n)\n        k += kl\n\n    idx = np.argsort(times)\n    times = times[idx]\n    index = index[idx]\n\n    dicttosave[\"t\"] = times\n    dicttosave[\"index\"] = index\n    if not np.all(np.isnan(data)):\n        dicttosave[\"d\"] = data[idx]\n\n    dicttosave[\"start\"] = self.time_support.start.values\n    dicttosave[\"end\"] = self.time_support.end.values\n\n    np.savez(filename, **dicttosave)\n\n    return\n</code></pre>"},{"location":"io.cnmfe/","title":"CNMF-E","text":"<p>Loaders for calcium imaging data with miniscope. Support CNMF-E in matlab, inscopix-cnmfe and minian.</p>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E","title":"<code>CNMF_E</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E). The path folder should contain a file ending in .mat when calling Source2d.save_neurons</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>numpy.ndarray</code> <p>Spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class CNMF_E(BaseLoader):\n\"\"\"Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E).\n    The path folder should contain a file ending in .mat\n    when calling Source2d.save_neurons\n\n    Attributes\n    ----------\n    A : numpy.ndarray\n        Spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n\n    \"\"\"\n\n    def __init__(self, path):\n\"\"\"\n\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\n        self.basename = os.path.basename(path)\n\n        super().__init__(path)\n\n        # Need to check if nwb file exists and if data are there\n        loading_my_data = True\n        if self.path is not None:\n            nwb_path = os.path.join(self.path, \"pynapplenwb\")\n            if os.path.exists(nwb_path):\n                files = os.listdir(nwb_path)\n                if len([f for f in files if f.endswith(\".nwb\")]):\n                    success = self.load_cnmfe_nwb(path)\n                    if success:\n                        loading_my_data = False\n\n        # Bypass if data have already been transfered to nwb\n        if loading_my_data:\n            app = App()\n            window = OphysGUI(app, path=path)\n            app.mainloop()\n            try:\n                app.update()\n            except Exception:\n                pass\n\n            if window.status:\n                self.ophys_information = window.ophys_information\n                self.load_cnmf_e(path)\n                self.save_cnmfe_nwb(path)\n\n    def load_cnmf_e(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\n        files = os.listdir(path)\n        matfiles = [f for f in files if f.endswith(\".mat\")]\n\n        if len(matfiles):\n            data = loadmat(os.path.join(path, matfiles[0]), struct_as_record=False)\n        else:\n            raise RuntimeError(\"No mat file found in {}\".format(path))\n\n        self.struct = data[\"neuron_results\"][0][0]\n\n        C = self.struct.C.T\n        self.A = self.struct.A.T\n\n        self.sampling_rate = float(\n            self.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n        )\n\n        time_index = np.arange(0, len(C)) / self.sampling_rate\n\n        self.C = nap.TsdFrame(t=time_index, d=C)\n\n        return None\n\n    def save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n        nwbfile = io.read()\n\n        device_info = self.ophys_information[\"device\"]\n        device = nwbfile.create_device(\n            name=device_info[\"name\"],\n            description=device_info[\"description\"],\n            manufacturer=device_info[\"manufacturer\"],\n        )\n        optical_info = self.ophys_information[\"OpticalChannel\"]\n        optical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\n        optical_channel = OpticalChannel(\n            name=optical_info[\"name\"],\n            description=optical_info[\"description\"],\n            emission_lambda=optical_info[\"emission_lambda\"],\n        )\n        imaging_info = self.ophys_information[\"ImagingPlane\"]\n        imaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\n        imaging_plane = nwbfile.create_imaging_plane(\n            name=imaging_info[\"name\"],\n            optical_channel=optical_channel,\n            imaging_rate=self.sampling_rate,\n            description=imaging_info[\"description\"],\n            device=device,\n            excitation_lambda=imaging_info[\"excitation_lambda\"],\n            indicator=imaging_info[\"indicator\"],\n            location=imaging_info[\"location\"],\n        )\n\n        ophys_module = nwbfile.create_processing_module(\n            name=\"ophys\", description=\"optical physiology processed data\"\n        )\n\n        seg_info = self.ophys_information[\"PlaneSegmentation\"]\n        img_seg = ImageSegmentation()\n        ps = img_seg.create_plane_segmentation(\n            name=seg_info[\"name\"],\n            description=seg_info[\"description\"],\n            imaging_plane=imaging_plane,\n        )\n\n        for i in range(self.C.shape[1]):\n            image_mask = np.atleast_2d(self.A[i])\n            # add image mask to plane segmentation\n            ps.add_roi(image_mask=image_mask)\n\n        ophys_module.add(img_seg)\n\n        rt_region = ps.create_roi_table_region(\n            region=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n        )\n\n        roi_resp_series = RoiResponseSeries(\n            name=\"RoiResponseSeries\",\n            data=self.C.values,\n            rois=rt_region,\n            unit=\"lumens\",\n            timestamps=self.C.index.values,\n        )\n\n        fl = Fluorescence(roi_response_series=roi_resp_series)\n        ophys_module.add(fl)\n\n        io.write(nwbfile)\n        io.close()\n\n        return\n\n    def load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r\")\n        nwbfile = io.read()\n\n        if \"ophys\" in nwbfile.processing.keys():\n            data = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n                \"RoiResponseSeries\"\n            ].data[:]\n            t = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n                \"RoiResponseSeries\"\n            ].timestamps[:]\n            self.C = nap.TsdFrame(t=t, d=data)\n            self.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n                \"PlaneSegmentation\"\n            ][\"image_mask\"].data[:]\n\n            io.close()\n            return True\n        else:\n            io.close()\n            return False\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\n    self.basename = os.path.basename(path)\n\n    super().__init__(path)\n\n    # Need to check if nwb file exists and if data are there\n    loading_my_data = True\n    if self.path is not None:\n        nwb_path = os.path.join(self.path, \"pynapplenwb\")\n        if os.path.exists(nwb_path):\n            files = os.listdir(nwb_path)\n            if len([f for f in files if f.endswith(\".nwb\")]):\n                success = self.load_cnmfe_nwb(path)\n                if success:\n                    loading_my_data = False\n\n    # Bypass if data have already been transfered to nwb\n    if loading_my_data:\n        app = App()\n        window = OphysGUI(app, path=path)\n        app.mainloop()\n        try:\n            app.update()\n        except Exception:\n            pass\n\n        if window.status:\n            self.ophys_information = window.ophys_information\n            self.load_cnmf_e(path)\n            self.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmf_e","title":"<code>load_cnmf_e(path)</code>","text":"<p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmf_e(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\n    files = os.listdir(path)\n    matfiles = [f for f in files if f.endswith(\".mat\")]\n\n    if len(matfiles):\n        data = loadmat(os.path.join(path, matfiles[0]), struct_as_record=False)\n    else:\n        raise RuntimeError(\"No mat file found in {}\".format(path))\n\n    self.struct = data[\"neuron_results\"][0][0]\n\n    C = self.struct.C.T\n    self.A = self.struct.A.T\n\n    self.sampling_rate = float(\n        self.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n    )\n\n    time_index = np.arange(0, len(C)) / self.sampling_rate\n\n    self.C = nap.TsdFrame(t=time_index, d=C)\n\n    return None\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E.save_cnmfe_nwb","title":"<code>save_cnmfe_nwb(path)</code>","text":"<p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n    self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n    self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n    io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n    nwbfile = io.read()\n\n    device_info = self.ophys_information[\"device\"]\n    device = nwbfile.create_device(\n        name=device_info[\"name\"],\n        description=device_info[\"description\"],\n        manufacturer=device_info[\"manufacturer\"],\n    )\n    optical_info = self.ophys_information[\"OpticalChannel\"]\n    optical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\n    optical_channel = OpticalChannel(\n        name=optical_info[\"name\"],\n        description=optical_info[\"description\"],\n        emission_lambda=optical_info[\"emission_lambda\"],\n    )\n    imaging_info = self.ophys_information[\"ImagingPlane\"]\n    imaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\n    imaging_plane = nwbfile.create_imaging_plane(\n        name=imaging_info[\"name\"],\n        optical_channel=optical_channel,\n        imaging_rate=self.sampling_rate,\n        description=imaging_info[\"description\"],\n        device=device,\n        excitation_lambda=imaging_info[\"excitation_lambda\"],\n        indicator=imaging_info[\"indicator\"],\n        location=imaging_info[\"location\"],\n    )\n\n    ophys_module = nwbfile.create_processing_module(\n        name=\"ophys\", description=\"optical physiology processed data\"\n    )\n\n    seg_info = self.ophys_information[\"PlaneSegmentation\"]\n    img_seg = ImageSegmentation()\n    ps = img_seg.create_plane_segmentation(\n        name=seg_info[\"name\"],\n        description=seg_info[\"description\"],\n        imaging_plane=imaging_plane,\n    )\n\n    for i in range(self.C.shape[1]):\n        image_mask = np.atleast_2d(self.A[i])\n        # add image mask to plane segmentation\n        ps.add_roi(image_mask=image_mask)\n\n    ophys_module.add(img_seg)\n\n    rt_region = ps.create_roi_table_region(\n        region=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n    )\n\n    roi_resp_series = RoiResponseSeries(\n        name=\"RoiResponseSeries\",\n        data=self.C.values,\n        rois=rt_region,\n        unit=\"lumens\",\n        timestamps=self.C.index.values,\n    )\n\n    fl = Fluorescence(roi_response_series=roi_resp_series)\n    ophys_module.add(fl)\n\n    io.write(nwbfile)\n    io.close()\n\n    return\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmfe_nwb","title":"<code>load_cnmfe_nwb(path)</code>","text":"<p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n\n    self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n    self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n    io = NWBHDF5IO(self.nwbfilepath, \"r\")\n    nwbfile = io.read()\n\n    if \"ophys\" in nwbfile.processing.keys():\n        data = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n            \"RoiResponseSeries\"\n        ].data[:]\n        t = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n            \"RoiResponseSeries\"\n        ].timestamps[:]\n        self.C = nap.TsdFrame(t=t, d=data)\n        self.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n            \"PlaneSegmentation\"\n        ][\"image_mask\"].data[:]\n\n        io.close()\n        return True\n    else:\n        io.close()\n        return False\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian","title":"<code>Minian</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for data processed with Minian (https://github.com/denisecailab/minian). The path folder should contain a subfolder name minian.</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>numpy.ndarray</code> <p>Spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class Minian(BaseLoader):\n\"\"\"Loader for data processed with Minian (https://github.com/denisecailab/minian).\n    The path folder should contain a subfolder name minian.\n\n    Attributes\n    ----------\n    A : numpy.ndarray\n        Spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n\n    \"\"\"\n\n    def __init__(self, path):\n\"\"\"\n\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\n        self.basename = os.path.basename(path)\n\n        super().__init__(path)\n\n        # Need to check if nwb file exists and if data are there\n        loading_my_data = True\n        if self.path is not None:\n            nwb_path = os.path.join(self.path, \"pynapplenwb\")\n            if os.path.exists(nwb_path):\n                files = os.listdir(nwb_path)\n                if len([f for f in files if f.endswith(\".nwb\")]):\n                    success = self.load_cnmfe_nwb(path)\n                    if success:\n                        loading_my_data = False\n\n        # Bypass if data have already been transfered to nwb\n        if loading_my_data:\n            app = App()\n            window = OphysGUI(app, path=path)\n            app.mainloop()\n            try:\n                app.update()\n            except Exception:\n                pass\n\n            if window.status:\n                self.ophys_information = window.ophys_information\n                self.load_minian(path)\n                self.save_cnmfe_nwb(path)\n\n    def load_minian(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\n        minian_folder = os.path.join(path, \"minian\")\n\n        if not os.path.exists(minian_folder):\n            raise RuntimeError(\"Path {} does not contain a minian folder\".format(path))\n\n        try:\n            import zarr\n        except ImportError as ie:\n            print(\"Please install module zarr for loading minian data\", ie)\n            sys.exit()\n        data = zarr.open(minian_folder, \"r\")\n\n        C = data[\"C.zarr\"][\"C\"][:]\n        C = C.T\n        self.sampling_rate = float(\n            self.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n        )\n        time_index = np.arange(0, len(C)) / self.sampling_rate\n\n        self.C = nap.TsdFrame(t=time_index, d=C)\n\n        self.A = data[\"A.zarr\"][\"A\"][:]\n\n        return None\n\n    def save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n        nwbfile = io.read()\n\n        device_info = self.ophys_information[\"device\"]\n        device = nwbfile.create_device(\n            name=device_info[\"name\"],\n            description=device_info[\"description\"],\n            manufacturer=device_info[\"manufacturer\"],\n        )\n        optical_info = self.ophys_information[\"OpticalChannel\"]\n        optical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\n        optical_channel = OpticalChannel(\n            name=optical_info[\"name\"],\n            description=optical_info[\"description\"],\n            emission_lambda=optical_info[\"emission_lambda\"],\n        )\n        imaging_info = self.ophys_information[\"ImagingPlane\"]\n        imaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\n        imaging_plane = nwbfile.create_imaging_plane(\n            name=imaging_info[\"name\"],\n            optical_channel=optical_channel,\n            imaging_rate=self.sampling_rate,\n            description=imaging_info[\"description\"],\n            device=device,\n            excitation_lambda=imaging_info[\"excitation_lambda\"],\n            indicator=imaging_info[\"indicator\"],\n            location=imaging_info[\"location\"],\n        )\n\n        ophys_module = nwbfile.create_processing_module(\n            name=\"ophys\", description=\"optical physiology processed data\"\n        )\n\n        seg_info = self.ophys_information[\"PlaneSegmentation\"]\n        img_seg = ImageSegmentation()\n        ps = img_seg.create_plane_segmentation(\n            name=seg_info[\"name\"],\n            description=seg_info[\"description\"],\n            imaging_plane=imaging_plane,\n        )\n\n        for i in range(self.C.shape[1]):\n            image_mask = self.A[i]\n            # add image mask to plane segmentation\n            ps.add_roi(image_mask=image_mask)\n\n        ophys_module.add(img_seg)\n\n        rt_region = ps.create_roi_table_region(\n            region=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n        )\n\n        roi_resp_series = RoiResponseSeries(\n            name=\"RoiResponseSeries\",\n            data=self.C.values,\n            rois=rt_region,\n            unit=\"lumens\",\n            timestamps=self.C.index.values,\n        )\n\n        fl = Fluorescence(roi_response_series=roi_resp_series)\n        ophys_module.add(fl)\n\n        io.write(nwbfile)\n        io.close()\n\n        return\n\n    def load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r\")\n        nwbfile = io.read()\n\n        if \"ophys\" in nwbfile.processing.keys():\n            data = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n                \"RoiResponseSeries\"\n            ].data[:]\n            t = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n                \"RoiResponseSeries\"\n            ].timestamps[:]\n            self.C = nap.TsdFrame(t=t, d=data)\n            self.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n                \"PlaneSegmentation\"\n            ][\"image_mask\"].data[:]\n\n            io.close()\n            return True\n        else:\n            io.close()\n            return False\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\n    self.basename = os.path.basename(path)\n\n    super().__init__(path)\n\n    # Need to check if nwb file exists and if data are there\n    loading_my_data = True\n    if self.path is not None:\n        nwb_path = os.path.join(self.path, \"pynapplenwb\")\n        if os.path.exists(nwb_path):\n            files = os.listdir(nwb_path)\n            if len([f for f in files if f.endswith(\".nwb\")]):\n                success = self.load_cnmfe_nwb(path)\n                if success:\n                    loading_my_data = False\n\n    # Bypass if data have already been transfered to nwb\n    if loading_my_data:\n        app = App()\n        window = OphysGUI(app, path=path)\n        app.mainloop()\n        try:\n            app.update()\n        except Exception:\n            pass\n\n        if window.status:\n            self.ophys_information = window.ophys_information\n            self.load_minian(path)\n            self.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian.load_minian","title":"<code>load_minian(path)</code>","text":"<p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_minian(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\n    minian_folder = os.path.join(path, \"minian\")\n\n    if not os.path.exists(minian_folder):\n        raise RuntimeError(\"Path {} does not contain a minian folder\".format(path))\n\n    try:\n        import zarr\n    except ImportError as ie:\n        print(\"Please install module zarr for loading minian data\", ie)\n        sys.exit()\n    data = zarr.open(minian_folder, \"r\")\n\n    C = data[\"C.zarr\"][\"C\"][:]\n    C = C.T\n    self.sampling_rate = float(\n        self.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n    )\n    time_index = np.arange(0, len(C)) / self.sampling_rate\n\n    self.C = nap.TsdFrame(t=time_index, d=C)\n\n    self.A = data[\"A.zarr\"][\"A\"][:]\n\n    return None\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian.save_cnmfe_nwb","title":"<code>save_cnmfe_nwb(path)</code>","text":"<p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n\n    self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n    self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n    io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n    nwbfile = io.read()\n\n    device_info = self.ophys_information[\"device\"]\n    device = nwbfile.create_device(\n        name=device_info[\"name\"],\n        description=device_info[\"description\"],\n        manufacturer=device_info[\"manufacturer\"],\n    )\n    optical_info = self.ophys_information[\"OpticalChannel\"]\n    optical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\n    optical_channel = OpticalChannel(\n        name=optical_info[\"name\"],\n        description=optical_info[\"description\"],\n        emission_lambda=optical_info[\"emission_lambda\"],\n    )\n    imaging_info = self.ophys_information[\"ImagingPlane\"]\n    imaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\n    imaging_plane = nwbfile.create_imaging_plane(\n        name=imaging_info[\"name\"],\n        optical_channel=optical_channel,\n        imaging_rate=self.sampling_rate,\n        description=imaging_info[\"description\"],\n        device=device,\n        excitation_lambda=imaging_info[\"excitation_lambda\"],\n        indicator=imaging_info[\"indicator\"],\n        location=imaging_info[\"location\"],\n    )\n\n    ophys_module = nwbfile.create_processing_module(\n        name=\"ophys\", description=\"optical physiology processed data\"\n    )\n\n    seg_info = self.ophys_information[\"PlaneSegmentation\"]\n    img_seg = ImageSegmentation()\n    ps = img_seg.create_plane_segmentation(\n        name=seg_info[\"name\"],\n        description=seg_info[\"description\"],\n        imaging_plane=imaging_plane,\n    )\n\n    for i in range(self.C.shape[1]):\n        image_mask = self.A[i]\n        # add image mask to plane segmentation\n        ps.add_roi(image_mask=image_mask)\n\n    ophys_module.add(img_seg)\n\n    rt_region = ps.create_roi_table_region(\n        region=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n    )\n\n    roi_resp_series = RoiResponseSeries(\n        name=\"RoiResponseSeries\",\n        data=self.C.values,\n        rois=rt_region,\n        unit=\"lumens\",\n        timestamps=self.C.index.values,\n    )\n\n    fl = Fluorescence(roi_response_series=roi_resp_series)\n    ophys_module.add(fl)\n\n    io.write(nwbfile)\n    io.close()\n\n    return\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.Minian.load_cnmfe_nwb","title":"<code>load_cnmfe_nwb(path)</code>","text":"<p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n\n    self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n    self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n    io = NWBHDF5IO(self.nwbfilepath, \"r\")\n    nwbfile = io.read()\n\n    if \"ophys\" in nwbfile.processing.keys():\n        data = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n            \"RoiResponseSeries\"\n        ].data[:]\n        t = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n            \"RoiResponseSeries\"\n        ].timestamps[:]\n        self.C = nap.TsdFrame(t=t, d=data)\n        self.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n            \"PlaneSegmentation\"\n        ][\"image_mask\"].data[:]\n\n        io.close()\n        return True\n    else:\n        io.close()\n        return False\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE","title":"<code>InscopixCNMFE</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe). The folder should contain a file ending with '_traces.csv' and a tiff file for spatial footprints.</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>np.ndarray</code> <p>The spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class InscopixCNMFE(BaseLoader):\n\"\"\"Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe).\n    The folder should contain a file ending with '_traces.csv'\n    and a tiff file for spatial footprints.\n\n    Attributes\n    ----------\n    A : np.ndarray\n        The spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n\n    \"\"\"\n\n    def __init__(self, path):\n\"\"\"\n\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\n        self.basename = os.path.basename(path)\n\n        super().__init__(path)\n\n        # Need to check if nwb file exists and if data are there\n        loading_my_data = True\n        if self.path is not None:\n            nwb_path = os.path.join(self.path, \"pynapplenwb\")\n            if os.path.exists(nwb_path):\n                files = os.listdir(nwb_path)\n                if len([f for f in files if f.endswith(\".nwb\")]):\n                    success = self.load_cnmfe_nwb(path)\n                    if success:\n                        loading_my_data = False\n\n        # Bypass if data have already been transfered to nwb\n        if loading_my_data:\n            app = App()\n            window = OphysGUI(app, path=path)\n            app.mainloop()\n            try:\n                app.update()\n            except Exception:\n                pass\n\n            if window.status:\n                self.ophys_information = window.ophys_information\n                self.load_inscopix_cnmfe(path)\n                self.save_cnmfe_nwb(path)\n\n    def load_inscopix_cnmfe(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\n        files = os.listdir(path)\n        tracefile = [f for f in files if f.endswith(\"_traces.csv\")]\n\n        if len(tracefile):\n            C = pd.read_csv(os.path.join(path, tracefile[0]), index_col=0)\n        else:\n            raise RuntimeError(\n                \"Path {} does not contain the file {}\".format(path, \"*_traces.csv\")\n            )\n\n        self.sampling_rate = float(\n            self.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n        )\n\n        time_index = np.arange(0, len(C)) / self.sampling_rate\n\n        self.C = nap.TsdFrame(t=time_index, d=C.values)\n        try:\n            import tifffile as tiff\n        except ImportError as ie:\n            print(\"Please install module tifffile for loading inscopix-cnmfe data\", ie)\n            sys.exit()\n\n        tifffile = [f for f in files if f.endswith(\".tiff\")]\n        if len(tifffile):\n            self.A = tiff.imread(os.path.join(path, tifffile[0]))\n        else:\n            raise RuntimeError(\n                \"Path {} does not contain the file {}\".format(path, \"*.tiff\")\n            )\n\n        return None\n\n    def save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n        nwbfile = io.read()\n\n        device_info = self.ophys_information[\"device\"]\n        device = nwbfile.create_device(\n            name=device_info[\"name\"],\n            description=device_info[\"description\"],\n            manufacturer=device_info[\"manufacturer\"],\n        )\n        optical_info = self.ophys_information[\"OpticalChannel\"]\n        optical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\n        optical_channel = OpticalChannel(\n            name=optical_info[\"name\"],\n            description=optical_info[\"description\"],\n            emission_lambda=optical_info[\"emission_lambda\"],\n        )\n        imaging_info = self.ophys_information[\"ImagingPlane\"]\n        imaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\n        imaging_plane = nwbfile.create_imaging_plane(\n            name=imaging_info[\"name\"],\n            optical_channel=optical_channel,\n            imaging_rate=self.sampling_rate,\n            description=imaging_info[\"description\"],\n            device=device,\n            excitation_lambda=imaging_info[\"excitation_lambda\"],\n            indicator=imaging_info[\"indicator\"],\n            location=imaging_info[\"location\"],\n        )\n\n        ophys_module = nwbfile.create_processing_module(\n            name=\"ophys\", description=\"optical physiology processed data\"\n        )\n\n        seg_info = self.ophys_information[\"PlaneSegmentation\"]\n        img_seg = ImageSegmentation()\n        ps = img_seg.create_plane_segmentation(\n            name=seg_info[\"name\"],\n            description=seg_info[\"description\"],\n            imaging_plane=imaging_plane,\n        )\n\n        for i in range(self.C.shape[1]):\n            image_mask = self.A[i]\n            # add image mask to plane segmentation\n            ps.add_roi(image_mask=image_mask)\n\n        ophys_module.add(img_seg)\n\n        rt_region = ps.create_roi_table_region(\n            region=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n        )\n\n        roi_resp_series = RoiResponseSeries(\n            name=\"RoiResponseSeries\",\n            data=self.C.values,\n            rois=rt_region,\n            unit=\"lumens\",\n            timestamps=self.C.index.values,\n        )\n\n        fl = Fluorescence(roi_response_series=roi_resp_series)\n        ophys_module.add(fl)\n\n        io.write(nwbfile)\n        io.close()\n\n        return\n\n    def load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r\")\n        nwbfile = io.read()\n\n        if \"ophys\" in nwbfile.processing.keys():\n            data = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n                \"RoiResponseSeries\"\n            ].data[:]\n            t = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n                \"RoiResponseSeries\"\n            ].timestamps[:]\n            self.C = nap.TsdFrame(t=t, d=data)\n            self.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n                \"PlaneSegmentation\"\n            ][\"image_mask\"].data[:]\n\n            io.close()\n            return True\n        else:\n            io.close()\n            return False\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\n    self.basename = os.path.basename(path)\n\n    super().__init__(path)\n\n    # Need to check if nwb file exists and if data are there\n    loading_my_data = True\n    if self.path is not None:\n        nwb_path = os.path.join(self.path, \"pynapplenwb\")\n        if os.path.exists(nwb_path):\n            files = os.listdir(nwb_path)\n            if len([f for f in files if f.endswith(\".nwb\")]):\n                success = self.load_cnmfe_nwb(path)\n                if success:\n                    loading_my_data = False\n\n    # Bypass if data have already been transfered to nwb\n    if loading_my_data:\n        app = App()\n        window = OphysGUI(app, path=path)\n        app.mainloop()\n        try:\n            app.update()\n        except Exception:\n            pass\n\n        if window.status:\n            self.ophys_information = window.ophys_information\n            self.load_inscopix_cnmfe(path)\n            self.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_inscopix_cnmfe","title":"<code>load_inscopix_cnmfe(path)</code>","text":"<p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_inscopix_cnmfe(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\n    files = os.listdir(path)\n    tracefile = [f for f in files if f.endswith(\"_traces.csv\")]\n\n    if len(tracefile):\n        C = pd.read_csv(os.path.join(path, tracefile[0]), index_col=0)\n    else:\n        raise RuntimeError(\n            \"Path {} does not contain the file {}\".format(path, \"*_traces.csv\")\n        )\n\n    self.sampling_rate = float(\n        self.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n    )\n\n    time_index = np.arange(0, len(C)) / self.sampling_rate\n\n    self.C = nap.TsdFrame(t=time_index, d=C.values)\n    try:\n        import tifffile as tiff\n    except ImportError as ie:\n        print(\"Please install module tifffile for loading inscopix-cnmfe data\", ie)\n        sys.exit()\n\n    tifffile = [f for f in files if f.endswith(\".tiff\")]\n    if len(tifffile):\n        self.A = tiff.imread(os.path.join(path, tifffile[0]))\n    else:\n        raise RuntimeError(\n            \"Path {} does not contain the file {}\".format(path, \"*.tiff\")\n        )\n\n    return None\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.save_cnmfe_nwb","title":"<code>save_cnmfe_nwb(path)</code>","text":"<p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n    self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n    self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n    io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n    nwbfile = io.read()\n\n    device_info = self.ophys_information[\"device\"]\n    device = nwbfile.create_device(\n        name=device_info[\"name\"],\n        description=device_info[\"description\"],\n        manufacturer=device_info[\"manufacturer\"],\n    )\n    optical_info = self.ophys_information[\"OpticalChannel\"]\n    optical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\n    optical_channel = OpticalChannel(\n        name=optical_info[\"name\"],\n        description=optical_info[\"description\"],\n        emission_lambda=optical_info[\"emission_lambda\"],\n    )\n    imaging_info = self.ophys_information[\"ImagingPlane\"]\n    imaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\n    imaging_plane = nwbfile.create_imaging_plane(\n        name=imaging_info[\"name\"],\n        optical_channel=optical_channel,\n        imaging_rate=self.sampling_rate,\n        description=imaging_info[\"description\"],\n        device=device,\n        excitation_lambda=imaging_info[\"excitation_lambda\"],\n        indicator=imaging_info[\"indicator\"],\n        location=imaging_info[\"location\"],\n    )\n\n    ophys_module = nwbfile.create_processing_module(\n        name=\"ophys\", description=\"optical physiology processed data\"\n    )\n\n    seg_info = self.ophys_information[\"PlaneSegmentation\"]\n    img_seg = ImageSegmentation()\n    ps = img_seg.create_plane_segmentation(\n        name=seg_info[\"name\"],\n        description=seg_info[\"description\"],\n        imaging_plane=imaging_plane,\n    )\n\n    for i in range(self.C.shape[1]):\n        image_mask = self.A[i]\n        # add image mask to plane segmentation\n        ps.add_roi(image_mask=image_mask)\n\n    ophys_module.add(img_seg)\n\n    rt_region = ps.create_roi_table_region(\n        region=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n    )\n\n    roi_resp_series = RoiResponseSeries(\n        name=\"RoiResponseSeries\",\n        data=self.C.values,\n        rois=rt_region,\n        unit=\"lumens\",\n        timestamps=self.C.index.values,\n    )\n\n    fl = Fluorescence(roi_response_series=roi_resp_series)\n    ophys_module.add(fl)\n\n    io.write(nwbfile)\n    io.close()\n\n    return\n</code></pre>"},{"location":"io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_cnmfe_nwb","title":"<code>load_cnmfe_nwb(path)</code>","text":"<p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n\n    self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n    self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n    io = NWBHDF5IO(self.nwbfilepath, \"r\")\n    nwbfile = io.read()\n\n    if \"ophys\" in nwbfile.processing.keys():\n        data = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n            \"RoiResponseSeries\"\n        ].data[:]\n        t = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n            \"RoiResponseSeries\"\n        ].timestamps[:]\n        self.C = nap.TsdFrame(t=t, d=data)\n        self.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n            \"PlaneSegmentation\"\n        ][\"image_mask\"].data[:]\n\n        io.close()\n        return True\n    else:\n        io.close()\n        return False\n</code></pre>"},{"location":"io.loader/","title":"Basic IO","text":"<p>BaseLoader is the general class for loading session with pynapple.</p> <p>@author: Guillaume Viejo</p>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader","title":"<code>BaseLoader</code>","text":"<p>         Bases: <code>object</code></p> <p>General loader for epochs and tracking data</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>class BaseLoader(object):\n\"\"\"\n    General loader for epochs and tracking data\n    \"\"\"\n\n    def __init__(self, path=None):\n        self.path = path\n\n        start_gui = True\n        # Check if a pynapplenwb folder exist to bypass GUI\n        if self.path is not None:\n            nwb_path = os.path.join(self.path, \"pynapplenwb\")\n            if os.path.exists(nwb_path):\n                files = os.listdir(nwb_path)\n                if len([f for f in files if f.endswith(\".nwb\")]):\n                    start_gui = False\n                    self.load_data(path)\n\n        # Starting the GUI\n        if start_gui:\n            app = App()\n            window = BaseLoaderGUI(app, path=path)\n            app.mainloop()\n            try:\n                app.update()\n            except Exception:\n                pass\n\n            # Extracting all the informations from gui loader\n            if window.status:\n                self.session_information = window.session_information\n                self.subject_information = window.subject_information\n                self.name = self.session_information[\"name\"]\n                self.tracking_frequency = window.tracking_frequency\n\n                self.position = self._make_position(\n                    window.tracking_parameters,\n                    window.tracking_method,\n                    window.tracking_frequency,\n                    window.epochs,\n                    window.time_units_epochs,\n                    window.tracking_alignment,\n                )\n                self.epochs = self._make_epochs(window.epochs, window.time_units_epochs)\n                self.time_support = self._join_epochs(\n                    window.epochs, window.time_units_epochs\n                )\n                # Save the data\n                self.create_nwb_file(path)\n\n    def load_default_csv(self, csv_file):\n\"\"\"\n        Load tracking data. The default csv should have the time index in the first column in seconds.\n        If no header is provided, the column names will be the column index.\n\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\n        position = pd.read_csv(csv_file, header=[0], index_col=0)\n        position = position[~position.index.duplicated(keep=\"first\")]\n        return position\n\n    def load_optitrack_csv(self, csv_file):\n\"\"\"\n        Load tracking data exported with Optitrack.\n        By default, the function reads rows 4 and 5 to build the column names.\n\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n\n        Raises\n        ------\n        RuntimeError\n            If header names are unknown. Should be 'Position' and 'Rotation'\n\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\n        position = pd.read_csv(csv_file, header=[4, 5], index_col=1)\n        if 1 in position.columns:\n            position = position.drop(labels=1, axis=1)\n        position = position[~position.index.duplicated(keep=\"first\")]\n        order = []\n        cols = []\n        for n in position.columns:\n            if n[0] == \"Rotation\":\n                order.append(\"r\" + n[1].lower())\n                cols.append(n)\n            elif n[0] == \"Position\":\n                order.append(n[1].lower())\n                cols.append(n)\n        if len(order) == 0:\n            raise RuntimeError(\n                \"Unknow tracking format for csv file {}\".format(csv_file)\n            )\n        position = position[cols]\n        position.columns = order\n        return position\n\n    def load_dlc_csv(self, csv_file):\n\"\"\"\n        Load tracking data exported with DeepLabCut\n\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\n        position = pd.read_csv(csv_file, header=[1, 2], index_col=0)\n        position = position[~position.index.duplicated(keep=\"first\")]\n        position.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\n        return position\n\n    def load_ttl_pulse(\n        self,\n        ttl_file,\n        tracking_frequency,\n        n_channels=1,\n        channel=0,\n        bytes_size=2,\n        fs=20000.0,\n        threshold=0.3,\n    ):\n\"\"\"\n        Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n\n        Parameters\n        ----------\n        ttl_file : str\n            File name\n        n_channels : int, optional\n            The number of channels in the binary file.\n        channel : int, optional\n            Which channel contains the TTL\n        bytes_size : int, optional\n            Bytes size of the binary file.\n        fs : float, optional\n            Sampling frequency of the binary file\n\n        Returns\n        -------\n        pd.Series\n            A series containing the time index of the TTL.\n        \"\"\"\n        f = open(ttl_file, \"rb\")\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n        f.close()\n        with open(ttl_file, \"rb\") as f:\n            data = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\n        if n_channels == 1:\n            data = data.flatten().astype(np.int32)\n        else:\n            data = data[:, channel].flatten().astype(np.int32)\n        data = data / data.max()\n        peaks, _ = scipy.signal.find_peaks(\n            np.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n        )\n        timestep = np.arange(0, len(data)) / fs\n        peaks += 1\n        ttl = pd.Series(index=timestep[peaks], data=data[peaks])\n        return ttl\n\n    def _make_position(\n        self, parameters, method, frequency, epochs, time_units, alignment\n    ):\n\"\"\"\n        Make the position TSDFrame with the parameters extracted from the GUI.\n        \"\"\"\n        if len(parameters.index) == 0:\n            return None\n        else:\n            if len(epochs) == 0:\n                epochs.loc[0, \"start\"] = 0.0\n            frames = []\n            time_supports_starts = []\n            time_support_ends = []\n\n            for i in range(len(parameters)):\n                if method.lower() == \"optitrack\":\n                    position = self.load_optitrack_csv(parameters.loc[i, \"csv\"])\n                elif method.lower() == \"deep lab cut\":\n                    position = self.load_dlc_csv(parameters.loc[i, \"csv\"])\n                elif method.lower() == \"default\":\n                    position = self.load_default_csv(parameters.loc[i, \"csv\"])\n\n                if alignment.lower() == \"local\":\n                    start_epoch = nap.format_timestamps(\n                        epochs.loc[int(parameters.loc[i, \"epoch\"]), \"start\"], time_units\n                    )\n                    end_epoch = nap.format_timestamps(\n                        epochs.loc[int(parameters.loc[i, \"epoch\"]), \"end\"], time_units\n                    )\n                    timestamps = (\n                        position.index.values\n                        + nap.return_timestamps(start_epoch, \"s\")[0]\n                    )\n                    # Make sure timestamps are within the epochs\n                    idx = np.where(timestamps &lt; end_epoch)[0]\n                    position = position.iloc[idx]\n                    position.index = pd.Index(timestamps[idx])\n\n                if alignment.lower() == \"ttl\":\n                    ttl = self.load_ttl_pulse(\n                        ttl_file=parameters.loc[i, \"ttl\"],\n                        tracking_frequency=frequency,\n                        n_channels=int(parameters.loc[i, \"n_channels\"]),\n                        channel=int(parameters.loc[i, \"tracking_channel\"]),\n                        bytes_size=int(parameters.loc[i, \"bytes_size\"]),\n                        fs=float(parameters.loc[i, \"fs\"]),\n                        threshold=float(parameters.loc[i, \"threshold\"]),\n                    )\n\n                    if len(ttl):\n                        length = np.minimum(len(ttl), len(position))\n                        ttl = ttl.iloc[0:length]\n                        position = position.iloc[0:length]\n                    else:\n                        raise RuntimeError(\n                            \"No ttl detected for {}\".format(parameters.loc[i, \"ttl\"])\n                        )\n\n                    # Make sure start epochs in seconds\n                    # start_epoch = format_timestamp(\n                    #     epochs.loc[parameters.loc[f, \"epoch\"], \"start\"], time_units\n                    # )\n                    start_epoch = nap.format_timestamps(\n                        epochs.loc[int(parameters.loc[i, \"epoch\"]), \"start\"], time_units\n                    )\n                    timestamps = start_epoch + ttl.index.values\n                    position.index = pd.Index(timestamps)\n\n                frames.append(position)\n                time_supports_starts.append(position.index[0])\n                time_support_ends.append(position.index[-1])\n\n            position = pd.concat(frames)\n\n            time_supports = nap.IntervalSet(\n                start=time_supports_starts, end=time_support_ends, time_units=\"s\"\n            )\n\n            # Specific to optitrACK\n            if set([\"rx\", \"ry\", \"rz\"]).issubset(position.columns):\n                position[[\"ry\", \"rx\", \"rz\"]] *= np.pi / 180\n                position[[\"ry\", \"rx\", \"rz\"]] += 2 * np.pi\n                position[[\"ry\", \"rx\", \"rz\"]] %= 2 * np.pi\n\n            position = nap.TsdFrame(\n                t=position.index.values,\n                d=position.values,\n                columns=position.columns.values,\n                time_support=time_supports,\n                time_units=\"s\",\n            )\n\n            return position\n\n    def _make_epochs(self, epochs, time_units=\"s\"):\n\"\"\"\n        Split GUI epochs into dict of epochs\n        \"\"\"\n        labels = epochs.groupby(\"label\").groups\n        isets = {}\n        for lbs in labels.keys():\n            tmp = epochs.loc[labels[lbs]]\n            isets[lbs] = nap.IntervalSet(\n                start=tmp[\"start\"], end=tmp[\"end\"], time_units=time_units\n            )\n        return isets\n\n    def _join_epochs(self, epochs, time_units=\"s\"):\n\"\"\"\n        To create the global time support of the data\n        \"\"\"\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            isets = nap.IntervalSet(\n                start=epochs[\"start\"].sort_values(),\n                end=epochs[\"end\"].sort_values(),\n                time_units=time_units,\n            )\n            iset = isets.merge_close_intervals(1, time_units=\"us\")\n        if len(iset):\n            return iset\n        else:\n            return None\n\n    def create_nwb_file(self, path):\n\"\"\"\n        Initialize the NWB file in the folder pynapplenwb within the data folder.\n\n        Parameters\n        ----------\n        path : str\n            The path to save the data\n\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            os.makedirs(self.nwb_path)\n        self.nwbfilepath = os.path.join(\n            self.nwb_path, self.session_information[\"name\"] + \".nwb\"\n        )\n\n        self.subject_information[\"date_of_birth\"] = None\n\n        nwbfile = NWBFile(\n            session_description=self.session_information[\"description\"],\n            identifier=self.session_information[\"name\"],\n            session_start_time=datetime.datetime.now(datetime.timezone.utc),\n            experimenter=self.session_information[\"experimenter\"],\n            lab=self.session_information[\"lab\"],\n            institution=self.session_information[\"institution\"],\n            subject=Subject(**self.subject_information),\n        )\n\n        # Tracking\n        if self.position is not None:\n            data = self.position.as_units(\"s\")\n            # specific to optitrack\n            if set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\n                position = Position()\n                for c in [\"x\", \"y\", \"z\"]:\n                    tmp = SpatialSeries(\n                        name=c,\n                        data=data[c].values,\n                        timestamps=data.index.values,\n                        unit=\"\",\n                        reference_frame=\"\",\n                    )\n                    position.add_spatial_series(tmp)\n                direction = CompassDirection()\n                for c in [\"rx\", \"ry\", \"rz\"]:\n                    tmp = SpatialSeries(\n                        name=c,\n                        data=data[c].values,\n                        timestamps=data.index.values,\n                        unit=\"radian\",\n                        reference_frame=\"\",\n                    )\n                    direction.add_spatial_series(tmp)\n\n                nwbfile.add_acquisition(position)\n                nwbfile.add_acquisition(direction)\n\n            # Other types\n            else:\n                position = Position()\n                for c in data.columns:\n                    tmp = SpatialSeries(\n                        name=c,\n                        data=data[c].values,\n                        timestamps=data.index.values,\n                        unit=\"\",\n                        reference_frame=\"\",\n                    )\n                    position.add_spatial_series(tmp)\n                nwbfile.add_acquisition(position)\n\n            # Adding time support of position as TimeIntervals\n            epochs = self.position.time_support.as_units(\"s\")\n            position_time_support = TimeIntervals(\n                name=\"position_time_support\",\n                description=\"The time support of the position i.e the real start and end of the tracking\",\n            )\n            for i in self.position.time_support.index:\n                position_time_support.add_interval(\n                    start_time=epochs.loc[i, \"start\"],\n                    stop_time=epochs.loc[i, \"end\"],\n                    tags=str(i),\n                )\n\n            nwbfile.add_time_intervals(position_time_support)\n\n        # Epochs\n        for ep in self.epochs.keys():\n            epochs = self.epochs[ep].as_units(\"s\")\n            for i in self.epochs[ep].index:\n                nwbfile.add_epoch(\n                    start_time=epochs.loc[i, \"start\"],\n                    stop_time=epochs.loc[i, \"end\"],\n                    tags=[ep],  # This is stupid nwb who tries to parse the string\n                )\n\n        with NWBHDF5IO(self.nwbfilepath, \"w\") as io:\n            io.write(nwbfile)\n\n        return\n\n    def load_data(self, path):\n\"\"\"\n        Load NWB data save with pynapple in the pynapplenwb folder\n\n        Parameters\n        ----------\n        path : str\n            Path to the session folder\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n        nwbfile = io.read()\n\n        position = {}\n        acq_keys = nwbfile.acquisition.keys()\n        if \"CompassDirection\" in acq_keys:\n            compass = nwbfile.acquisition[\"CompassDirection\"]\n            for k in compass.spatial_series.keys():\n                position[k] = pd.Series(\n                    index=compass.get_spatial_series(k).timestamps[:],\n                    data=compass.get_spatial_series(k).data[:],\n                )\n        if \"Position\" in acq_keys:\n            tracking = nwbfile.acquisition[\"Position\"]\n            for k in tracking.spatial_series.keys():\n                position[k] = pd.Series(\n                    index=tracking.get_spatial_series(k).timestamps[:],\n                    data=tracking.get_spatial_series(k).data[:],\n                )\n        if len(position):\n            position = pd.DataFrame.from_dict(position)\n\n            # retrieveing time support position if in epochs\n            if \"position_time_support\" in nwbfile.intervals.keys():\n                epochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\n                time_support = nap.IntervalSet(\n                    start=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n                )\n\n            self.position = nap.TsdFrame(\n                position, time_units=\"s\", time_support=time_support\n            )\n\n        if nwbfile.epochs is not None:\n            epochs = nwbfile.epochs.to_dataframe()\n            # NWB is dumb and cannot take a single string for labels\n            epochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\n            epochs = epochs.drop(labels=\"tags\", axis=1)\n            epochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\n            self.epochs = self._make_epochs(epochs)\n\n            self.time_support = self._join_epochs(epochs, \"s\")\n\n        io.close()\n\n        return\n\n    def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n        Add epochs to the NWB file (e.g. ripples epochs)\n        See pynwb.epoch.TimeIntervals\n\n        Parameters\n        ----------\n        iset : IntervalSet\n            The intervalSet to save\n        name : str\n            The name in the nwb file\n        \"\"\"\n        io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n        nwbfile = io.read()\n\n        epochs = iset.as_units(\"s\")\n        time_intervals = TimeIntervals(name=name, description=description)\n        for i in epochs.index:\n            time_intervals.add_interval(\n                start_time=epochs.loc[i, \"start\"],\n                stop_time=epochs.loc[i, \"end\"],\n                tags=str(i),\n            )\n\n        nwbfile.add_time_intervals(time_intervals)\n        io.write(nwbfile)\n        io.close()\n\n        return\n\n    def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n        Save timestamps in the NWB file (e.g. ripples time) with the time support.\n        See pynwb.base.TimeSeries\n\n\n        Parameters\n        ----------\n        tsd : TsdFrame\n            _\n        name : str\n            _\n        description : str, optional\n            _\n        \"\"\"\n        io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n        nwbfile = io.read()\n\n        ts = TimeSeries(\n            name=name,\n            unit=\"s\",\n            data=tsd.values,\n            timestamps=tsd.as_units(\"s\").index.values,\n        )\n\n        time_support = TimeIntervals(\n            name=name + \"_timesupport\", description=\"The time support of the object\"\n        )\n\n        epochs = tsd.time_support.as_units(\"s\")\n        for i in epochs.index:\n            time_support.add_interval(\n                start_time=epochs.loc[i, \"start\"],\n                stop_time=epochs.loc[i, \"end\"],\n                tags=str(i),\n            )\n        nwbfile.add_time_intervals(time_support)\n        nwbfile.add_acquisition(ts)\n        io.write(nwbfile)\n        io.close()\n\n        return\n\n    def load_nwb_intervals(self, name):\n\"\"\"\n        Load epochs from the NWB file (e.g. 'ripples')\n\n        Parameters\n        ----------\n        name : str\n            The name in the nwb file\n        \"\"\"\n        io = NWBHDF5IO(self.nwbfilepath, \"r\")\n        nwbfile = io.read()\n\n        if name in nwbfile.intervals.keys():\n            epochs = nwbfile.intervals[name].to_dataframe()\n            isets = nap.IntervalSet(\n                start=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n            )\n            io.close()\n            return isets\n        else:\n            io.close()\n        return\n\n    def load_nwb_timeseries(self, name):\n\"\"\"\n        Load timestamps in the NWB file (e.g. ripples time)\n\n        Parameters\n        ----------\n        name : str\n            _\n\n        Returns\n        -------\n        Tsd\n            _\n        \"\"\"\n        io = NWBHDF5IO(self.nwbfilepath, \"r\")\n        nwbfile = io.read()\n\n        ts = nwbfile.acquisition[name]\n\n        time_support = self.load_nwb_intervals(name + \"_timesupport\")\n\n        tsd = nap.Tsd(\n            t=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n        )\n\n        io.close()\n\n        return tsd\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_default_csv","title":"<code>load_default_csv(csv_file)</code>","text":"<p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\n    position = pd.read_csv(csv_file, header=[0], index_col=0)\n    position = position[~position.index.duplicated(keep=\"first\")]\n    return position\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_optitrack_csv","title":"<code>load_optitrack_csv(csv_file)</code>","text":"<p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\n    position = pd.read_csv(csv_file, header=[4, 5], index_col=1)\n    if 1 in position.columns:\n        position = position.drop(labels=1, axis=1)\n    position = position[~position.index.duplicated(keep=\"first\")]\n    order = []\n    cols = []\n    for n in position.columns:\n        if n[0] == \"Rotation\":\n            order.append(\"r\" + n[1].lower())\n            cols.append(n)\n        elif n[0] == \"Position\":\n            order.append(n[1].lower())\n            cols.append(n)\n    if len(order) == 0:\n        raise RuntimeError(\n            \"Unknow tracking format for csv file {}\".format(csv_file)\n        )\n    position = position[cols]\n    position.columns = order\n    return position\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_dlc_csv","title":"<code>load_dlc_csv(csv_file)</code>","text":"<p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\n    position = pd.read_csv(csv_file, header=[1, 2], index_col=0)\n    position = position[~position.index.duplicated(keep=\"first\")]\n    position.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\n    return position\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_ttl_pulse","title":"<code>load_ttl_pulse(ttl_file, tracking_frequency, n_channels=1, channel=0, bytes_size=2, fs=20000.0, threshold=0.3)</code>","text":"<p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int, optional</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int, optional</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float, optional</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>pd.Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\n    self,\n    ttl_file,\n    tracking_frequency,\n    n_channels=1,\n    channel=0,\n    bytes_size=2,\n    fs=20000.0,\n    threshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\n    f = open(ttl_file, \"rb\")\n    startoffile = f.seek(0, 0)\n    endoffile = f.seek(0, 2)\n    n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n    f.close()\n    with open(ttl_file, \"rb\") as f:\n        data = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\n    if n_channels == 1:\n        data = data.flatten().astype(np.int32)\n    else:\n        data = data[:, channel].flatten().astype(np.int32)\n    data = data / data.max()\n    peaks, _ = scipy.signal.find_peaks(\n        np.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n    )\n    timestep = np.arange(0, len(data)) / fs\n    peaks += 1\n    ttl = pd.Series(index=timestep[peaks], data=data[peaks])\n    return ttl\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.create_nwb_file","title":"<code>create_nwb_file(path)</code>","text":"<p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        os.makedirs(self.nwb_path)\n    self.nwbfilepath = os.path.join(\n        self.nwb_path, self.session_information[\"name\"] + \".nwb\"\n    )\n\n    self.subject_information[\"date_of_birth\"] = None\n\n    nwbfile = NWBFile(\n        session_description=self.session_information[\"description\"],\n        identifier=self.session_information[\"name\"],\n        session_start_time=datetime.datetime.now(datetime.timezone.utc),\n        experimenter=self.session_information[\"experimenter\"],\n        lab=self.session_information[\"lab\"],\n        institution=self.session_information[\"institution\"],\n        subject=Subject(**self.subject_information),\n    )\n\n    # Tracking\n    if self.position is not None:\n        data = self.position.as_units(\"s\")\n        # specific to optitrack\n        if set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\n            position = Position()\n            for c in [\"x\", \"y\", \"z\"]:\n                tmp = SpatialSeries(\n                    name=c,\n                    data=data[c].values,\n                    timestamps=data.index.values,\n                    unit=\"\",\n                    reference_frame=\"\",\n                )\n                position.add_spatial_series(tmp)\n            direction = CompassDirection()\n            for c in [\"rx\", \"ry\", \"rz\"]:\n                tmp = SpatialSeries(\n                    name=c,\n                    data=data[c].values,\n                    timestamps=data.index.values,\n                    unit=\"radian\",\n                    reference_frame=\"\",\n                )\n                direction.add_spatial_series(tmp)\n\n            nwbfile.add_acquisition(position)\n            nwbfile.add_acquisition(direction)\n\n        # Other types\n        else:\n            position = Position()\n            for c in data.columns:\n                tmp = SpatialSeries(\n                    name=c,\n                    data=data[c].values,\n                    timestamps=data.index.values,\n                    unit=\"\",\n                    reference_frame=\"\",\n                )\n                position.add_spatial_series(tmp)\n            nwbfile.add_acquisition(position)\n\n        # Adding time support of position as TimeIntervals\n        epochs = self.position.time_support.as_units(\"s\")\n        position_time_support = TimeIntervals(\n            name=\"position_time_support\",\n            description=\"The time support of the position i.e the real start and end of the tracking\",\n        )\n        for i in self.position.time_support.index:\n            position_time_support.add_interval(\n                start_time=epochs.loc[i, \"start\"],\n                stop_time=epochs.loc[i, \"end\"],\n                tags=str(i),\n            )\n\n        nwbfile.add_time_intervals(position_time_support)\n\n    # Epochs\n    for ep in self.epochs.keys():\n        epochs = self.epochs[ep].as_units(\"s\")\n        for i in self.epochs[ep].index:\n            nwbfile.add_epoch(\n                start_time=epochs.loc[i, \"start\"],\n                stop_time=epochs.loc[i, \"end\"],\n                tags=[ep],  # This is stupid nwb who tries to parse the string\n            )\n\n    with NWBHDF5IO(self.nwbfilepath, \"w\") as io:\n        io.write(nwbfile)\n\n    return\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_data","title":"<code>load_data(path)</code>","text":"<p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n    self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n    self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n    io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n    nwbfile = io.read()\n\n    position = {}\n    acq_keys = nwbfile.acquisition.keys()\n    if \"CompassDirection\" in acq_keys:\n        compass = nwbfile.acquisition[\"CompassDirection\"]\n        for k in compass.spatial_series.keys():\n            position[k] = pd.Series(\n                index=compass.get_spatial_series(k).timestamps[:],\n                data=compass.get_spatial_series(k).data[:],\n            )\n    if \"Position\" in acq_keys:\n        tracking = nwbfile.acquisition[\"Position\"]\n        for k in tracking.spatial_series.keys():\n            position[k] = pd.Series(\n                index=tracking.get_spatial_series(k).timestamps[:],\n                data=tracking.get_spatial_series(k).data[:],\n            )\n    if len(position):\n        position = pd.DataFrame.from_dict(position)\n\n        # retrieveing time support position if in epochs\n        if \"position_time_support\" in nwbfile.intervals.keys():\n            epochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\n            time_support = nap.IntervalSet(\n                start=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n            )\n\n        self.position = nap.TsdFrame(\n            position, time_units=\"s\", time_support=time_support\n        )\n\n    if nwbfile.epochs is not None:\n        epochs = nwbfile.epochs.to_dataframe()\n        # NWB is dumb and cannot take a single string for labels\n        epochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\n        epochs = epochs.drop(labels=\"tags\", axis=1)\n        epochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\n        self.epochs = self._make_epochs(epochs)\n\n        self.time_support = self._join_epochs(epochs, \"s\")\n\n    io.close()\n\n    return\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.save_nwb_intervals","title":"<code>save_nwb_intervals(iset, name, description='')</code>","text":"<p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\n    io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n    nwbfile = io.read()\n\n    epochs = iset.as_units(\"s\")\n    time_intervals = TimeIntervals(name=name, description=description)\n    for i in epochs.index:\n        time_intervals.add_interval(\n            start_time=epochs.loc[i, \"start\"],\n            stop_time=epochs.loc[i, \"end\"],\n            tags=str(i),\n        )\n\n    nwbfile.add_time_intervals(time_intervals)\n    io.write(nwbfile)\n    io.close()\n\n    return\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.save_nwb_timeseries","title":"<code>save_nwb_timeseries(tsd, name, description='')</code>","text":"<p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str, optional</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n\n\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\n    io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n    nwbfile = io.read()\n\n    ts = TimeSeries(\n        name=name,\n        unit=\"s\",\n        data=tsd.values,\n        timestamps=tsd.as_units(\"s\").index.values,\n    )\n\n    time_support = TimeIntervals(\n        name=name + \"_timesupport\", description=\"The time support of the object\"\n    )\n\n    epochs = tsd.time_support.as_units(\"s\")\n    for i in epochs.index:\n        time_support.add_interval(\n            start_time=epochs.loc[i, \"start\"],\n            stop_time=epochs.loc[i, \"end\"],\n            tags=str(i),\n        )\n    nwbfile.add_time_intervals(time_support)\n    nwbfile.add_acquisition(ts)\n    io.write(nwbfile)\n    io.close()\n\n    return\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_nwb_intervals","title":"<code>load_nwb_intervals(name)</code>","text":"<p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\n    io = NWBHDF5IO(self.nwbfilepath, \"r\")\n    nwbfile = io.read()\n\n    if name in nwbfile.intervals.keys():\n        epochs = nwbfile.intervals[name].to_dataframe()\n        isets = nap.IntervalSet(\n            start=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n        )\n        io.close()\n        return isets\n    else:\n        io.close()\n    return\n</code></pre>"},{"location":"io.loader/#pynapple.io.loader.BaseLoader.load_nwb_timeseries","title":"<code>load_nwb_timeseries(name)</code>","text":"<p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n\n    Parameters\n    ----------\n    name : str\n        _\n\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\n    io = NWBHDF5IO(self.nwbfilepath, \"r\")\n    nwbfile = io.read()\n\n    ts = nwbfile.acquisition[name]\n\n    time_support = self.load_nwb_intervals(name + \"_timesupport\")\n\n    tsd = nap.Tsd(\n        t=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n    )\n\n    io.close()\n\n    return tsd\n</code></pre>"},{"location":"io/","title":"Miscellaneous","text":"<p>Various io functions</p> <p>@author: Guillaume Viejo</p>"},{"location":"io/#pynapple.io.misc.load_session","title":"<code>load_session(path=None, session_type=None)</code>","text":"<p>General Loader for</p> <ul> <li> <p>Neurosuite</p> </li> <li> <p>Phy</p> </li> <li> <p>Minian</p> </li> <li> <p>Inscopix-cnmfe</p> </li> <li> <p>Matlab-cnmfe</p> </li> <li> <p>Suite2p</p> </li> <li>None for default session.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str, optional</code> <p>The path to load the data</p> <code>None</code> <code>session_type</code> <code>str, optional</code> <p>Can be 'neurosuite', 'phy', 'minian', 'inscopix-cnmfe', 'cnmfe-matlab', 'suite2p' or None for default loader.</p> <code>None</code> <p>Returns:</p> Type Description <code>Session</code> <p>A class holding all the data from the session.</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_session(path=None, session_type=None):\n\"\"\"\n    General Loader for\n\n    - Neurosuite\\n\n    - Phy\\n\n    - Minian\\n\n    - Inscopix-cnmfe\\n\n    - Matlab-cnmfe\\n\n    - Suite2p\n    - None for default session.\n\n    Parameters\n    ----------\n    path : str, optional\n        The path to load the data\n    session_type : str, optional\n        Can be 'neurosuite', 'phy',\n        'minian', 'inscopix-cnmfe', 'cnmfe-matlab',\n        'suite2p' or None for default loader.\n\n    Returns\n    -------\n    Session\n        A class holding all the data from the session.\n\n    \"\"\"\n    if path:\n        if not os.path.isdir(path):\n            raise RuntimeError(\"Path {} is not found.\".format(path))\n\n    if isinstance(session_type, str):\n        session_type = session_type.lower()\n\n    if session_type == \"neurosuite\":\n        return NeuroSuite(path)\n\n    elif session_type == \"phy\":\n        return Phy(path)\n\n    elif session_type == \"inscopix-cnmfe\":\n        return InscopixCNMFE(path)\n\n    elif session_type == \"minian\":\n        return Minian(path)\n\n    elif session_type == \"cnmfe-matlab\":\n        return CNMF_E(path)\n\n    elif session_type == \"suite2p\":\n        return Suite2P(path)\n\n    else:\n        return BaseLoader(path)\n</code></pre>"},{"location":"io/#pynapple.io.misc.load_eeg","title":"<code>load_eeg(filepath, channel=None, n_channels=None, frequency=None, precision='int16', bytes_size=2)</code>","text":"<p>Standalone function to load eeg/lfp/dat file in binary format.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the eeg file</p> required <code>channel</code> <code>int or list of int, optional</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>n_channels</code> <code>int, optional</code> <p>Number of channels</p> <code>None</code> <code>frequency</code> <code>float, optional</code> <p>Sampling rate of the file</p> <code>None</code> <code>precision</code> <code>str, optional</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the binary file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p>"},{"location":"io/#pynapple.io.misc.load_eeg--deleted-parameters","title":"Deleted Parameters","text":"<p>extension : str, optional     The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_eeg(\n    filepath,\n    channel=None,\n    n_channels=None,\n    frequency=None,\n    precision=\"int16\",\n    bytes_size=2,\n):\n\"\"\"\n    Standalone function to load eeg/lfp/dat file in binary format.\n\n    Parameters\n    ----------\n    filepath : str\n        The path to the eeg file\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    n_channels : int, optional\n        Number of channels\n    frequency : float, optional\n        Sampling rate of the file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the binary file\n\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n\n    Deleted Parameters\n    ------------------\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n\n    \"\"\"\n    # Need to check if a xml file exists\n    path = os.path.dirname(filepath)\n    basename = os.path.basename(filepath).split(\".\")[0]\n    listdir = os.listdir(path)\n\n    if frequency is None or n_channels is None:\n        if basename + \".xml\" in listdir:\n            xmlpath = os.path.join(path, basename + \".xml\")\n            xmldoc = minidom.parse(xmlpath)\n        else:\n            raise RuntimeError(\n                \"Can't find xml file; please specify sampling frequency or number of channels\"\n            )\n\n        if frequency is None:\n            if filepath.endswith(\".dat\"):\n                fs_dat = int(\n                    xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n                    .getElementsByTagName(\"samplingRate\")[0]\n                    .firstChild.data\n                )\n                frequency = fs_dat\n            elif filepath.endswith((\".lfp\", \".eeg\")):\n                fs_eeg = int(\n                    xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n                    .getElementsByTagName(\"lfpSamplingRate\")[0]\n                    .firstChild.data\n                )\n                frequency = fs_eeg\n\n        if n_channels is None:\n            n_channels = int(\n                xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n                .getElementsByTagName(\"nChannels\")[0]\n                .firstChild.data\n            )\n\n    f = open(filepath, \"rb\")\n    startoffile = f.seek(0, 0)\n    endoffile = f.seek(0, 2)\n    bytes_size = 2\n    n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n    duration = n_samples / frequency\n    f.close()\n    fp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\n    timestep = np.arange(0, n_samples) / frequency\n\n    time_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\n\n    if channel is None:\n        return fp\n    elif type(channel) is int:\n        return nap.Tsd(\n            t=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n        )\n    elif type(channel) is list:\n        return nap.TsdFrame(\n            t=timestep,\n            d=fp[:, channel],\n            time_units=\"s\",\n            time_support=time_support,\n            columns=channel,\n        )\n</code></pre>"},{"location":"io/#pynapple.io.misc.append_NWB_LFP","title":"<code>append_NWB_LFP(path, lfp, channel=None)</code>","text":"<p>Standalone function for adding lfp/eeg to already existing nwb files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data. The function will looks for a nwb file in path or in path/pynapplenwb.</p> required <code>lfp</code> <code>Tsd or TsdFrame</code> <p>Description</p> required <code>channel</code> <code>None, optional</code> <p>channel number in int ff lfp is a Tsd</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the nwb file </p> <p>If no channel is specify when passing a Tsd</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def append_NWB_LFP(path, lfp, channel=None):\n\"\"\"Standalone function for adding lfp/eeg to already existing nwb files.\n\n    Parameters\n    ----------\n    path : str\n        The path to the data. The function will looks for a nwb file in path\n        or in path/pynapplenwb.\n    lfp : Tsd or TsdFrame\n        Description\n    channel : None, optional\n        channel number in int ff lfp is a Tsd\n\n    Raises\n    ------\n    RuntimeError\n        If can't find the nwb file \\n\n        If no channel is specify when passing a Tsd\n\n    \"\"\"\n    new_path = os.path.join(path, \"pynapplenwb\")\n    nwb_path = \"\"\n    if os.path.exists(new_path):\n        nwbfilename = [f for f in os.listdir(new_path) if f.endswith(\".nwb\")]\n        if len(nwbfilename):\n            nwb_path = os.path.join(path, \"pynapplenwb\", nwbfilename[0])\n    else:\n        nwbfilename = [f for f in os.listdir(path) if f.endswith(\".nwb\")]\n        if len(nwbfilename):\n            nwb_path = os.path.join(path, \"pynapplenwb\", nwbfilename[0])\n\n    if len(nwb_path) == 0:\n        raise RuntimeError(\"Can't find nwb file in {}\".format(path))\n\n    if isinstance(lfp, nap.TsdFrame):\n        channels = lfp.columns.values\n    elif isinstance(lfp, nap.Tsd):\n        if isinstance(channel, int):\n            channels = [channel]\n        else:\n            raise RuntimeError(\"Please specify which channel it is.\")\n\n    io = NWBHDF5IO(nwb_path, \"r+\")\n    nwbfile = io.read()\n\n    all_table_region = nwbfile.create_electrode_table_region(\n        region=channels, description=\"\", name=\"electrodes\"\n    )\n\n    lfp_electrical_series = ElectricalSeries(\n        name=\"ElectricalSeries\",\n        data=lfp.values,\n        timestamps=lfp.index.values,\n        electrodes=all_table_region,\n    )\n\n    lfp = LFP(electrical_series=lfp_electrical_series)\n\n    ecephys_module = nwbfile.create_processing_module(\n        name=\"ecephys\", description=\"processed extracellular electrophysiology data\"\n    )\n    ecephys_module.add(lfp)\n\n    io.write(nwbfile)\n    io.close()\n\n    return\n</code></pre>"},{"location":"io.neurosuite/","title":"Neurosuite","text":"<p>Class and functions for loading data processed with the Neurosuite (Klusters, Neuroscope, NDmanager)</p> <p>@author: Guillaume Viejo</p>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite","title":"<code>NeuroSuite</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for kluster data</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>class NeuroSuite(BaseLoader):\n\"\"\"\n    Loader for kluster data\n    \"\"\"\n\n    def __init__(self, path):\n\"\"\"\n        Instantiate the data class from a neurosuite folder.\n\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\n        self.basename = os.path.basename(path)\n        self.time_support = None\n\n        super().__init__(path)\n\n        # Need to check if nwb file exists and if data are there\n        loading_neurosuite = True\n        if self.path is not None:\n            nwb_path = os.path.join(self.path, \"pynapplenwb\")\n            if os.path.exists(nwb_path):\n                files = os.listdir(nwb_path)\n                if len([f for f in files if f.endswith(\".nwb\")]):\n                    success = self.load_nwb_spikes(path)\n                    if success:\n                        loading_neurosuite = False\n\n        # Bypass if data have already been transfered to nwb\n        if loading_neurosuite:\n            self.load_neurosuite_xml(path)\n            # print(\"XML loaded\")\n            # To label the electrodes groups\n            app = App()\n            window = EphysGUI(app, path=path, groups=self.group_to_channel)\n            app.mainloop()\n            try:\n                app.update()\n            except Exception:\n                pass\n\n            # print(\"GUI DONE\")\n            if window.status:\n                self.ephys_information = window.ephys_information\n                self.load_neurosuite_spikes(path, self.basename, self.time_support)\n                self.save_data(path)\n\n    def load_neurosuite_spikes(self, path, basename, time_support=None, fs=20000.0):\n\"\"\"\n        Read the clus and res files and convert to NWB.\n        Instantiate automatically a TsGroup object.\n\n        Parameters\n        ----------\n        path : str\n            The path to the data\n        basename : str\n            Basename of the clu and res files.\n        time_support : IntevalSet, optional\n            The time support of the data\n        fs : float, optional\n            Sampling rate of the recording.\n\n        Raises\n        ------\n        RuntimeError\n            If number of clu and res are not equal.\n\n        \"\"\"\n        files = os.listdir(path)\n        clu_files = np.sort([f for f in files if \".clu.\" in f and f[0] != \".\"])\n        res_files = np.sort([f for f in files if \".res.\" in f and f[0] != \".\"])\n        clu1 = np.sort([int(f.split(\".\")[-1]) for f in clu_files])\n        clu2 = np.sort([int(f.split(\".\")[-1]) for f in res_files])\n        if len(clu_files) != len(res_files) or not (clu1 == clu2).any():\n            raise RuntimeError(\n                \"Not the same number of clu and res files in \" + path + \"; Exiting ...\"\n            )\n\n        count = 0\n        spikes = {}\n        group = pd.Series(dtype=np.int32)\n        for i, s in zip(range(len(clu_files)), clu1):\n            clu = np.genfromtxt(\n                os.path.join(path, basename + \".clu.\" + str(s)), dtype=np.int32\n            )[1:]\n            if np.max(clu) &gt; 1:  # getting rid of mua and noise\n                res = np.genfromtxt(os.path.join(path, basename + \".res.\" + str(s)))\n                tmp = np.unique(clu).astype(int)\n                idx_clu = tmp[tmp &gt; 1]\n                idx_out = np.arange(count, count + len(idx_clu))\n\n                for j, k in zip(idx_clu, idx_out):\n                    t = res[clu == j] / fs\n                    spikes[k] = nap.Ts(t=t, time_units=\"s\")\n                    group.loc[k] = s\n\n                count += len(idx_clu)\n\n        group = group - 1  # better to start it a 0\n\n        self.spikes = nap.TsGroup(\n            spikes, time_support=time_support, time_units=\"s\", group=group\n        )\n\n        # adding some information to help parse the neurons\n        names = pd.Series(\n            index=group.index,\n            data=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n        )\n        if ~np.all(names.values == \"\"):\n            self.spikes.set_info(name=names)\n        locations = pd.Series(\n            index=group.index,\n            data=[\n                self.ephys_information[group.loc[i]][\"location\"] for i in group.index\n            ],\n        )\n        if ~np.all(locations.values == \"\"):\n            self.spikes.set_info(location=locations)\n\n        return\n\n    def load_neurosuite_xml(self, path):\n\"\"\"\n        path should be the folder session containing the XML file\n\n        Function reads :\n        1. the number of channels\n        2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder\n            eeg file first if both are present or both are absent\n        3. the mappings shanks to channels as a dict\n\n        Parameters\n        ----------\n        path: str\n            The path to the data\n\n        Raises\n        ------\n        RuntimeError\n            If path does not contain the xml file.\n        \"\"\"\n        listdir = os.listdir(path)\n        xmlfiles = [f for f in listdir if f.endswith(\".xml\")]\n        if not len(xmlfiles):\n            raise RuntimeError(\"Path {} contains no xml files;\".format(path))\n            sys.exit()\n        new_path = os.path.join(path, xmlfiles[0])\n\n        self.xmldoc = minidom.parse(new_path)\n        self.nChannels = int(\n            self.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n            .getElementsByTagName(\"nChannels\")[0]\n            .firstChild.data\n        )\n        self.fs_dat = int(\n            self.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n            .getElementsByTagName(\"samplingRate\")[0]\n            .firstChild.data\n        )\n        self.fs_eeg = int(\n            self.xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n            .getElementsByTagName(\"lfpSamplingRate\")[0]\n            .firstChild.data\n        )\n\n        self.group_to_channel = {}\n        groups = (\n            self.xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n            .getElementsByTagName(\"channelGroups\")[0]\n            .getElementsByTagName(\"group\")\n        )\n        for i in range(len(groups)):\n            self.group_to_channel[i] = np.array(\n                [\n                    int(child.firstChild.data)\n                    for child in groups[i].getElementsByTagName(\"channel\")\n                ]\n            )\n\n        return\n\n    def save_data(self, path):\n\"\"\"\n        Save the data to NWB format.\n\n        Parameters\n        ----------\n        path : str\n            The path to save the data\n\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n        nwbfile = io.read()\n\n        electrode_groups = {}\n\n        for g in self.group_to_channel:\n            device = nwbfile.create_device(\n                name=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\n                description=self.ephys_information[g][\"device\"][\"description\"],\n                manufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n            )\n\n            if (\n                len(self.ephys_information[g][\"position\"])\n                and type(self.ephys_information[g][\"position\"]) is str\n            ):\n                self.ephys_information[g][\"position\"] = re.split(\n                    \";|,| \", self.ephys_information[g][\"position\"]\n                )\n            elif self.ephys_information[g][\"position\"] == \"\":\n                self.ephys_information[g][\"position\"] = None\n\n            electrode_groups[g] = nwbfile.create_electrode_group(\n                name=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\n                description=self.ephys_information[g][\"description\"],\n                position=self.ephys_information[g][\"position\"],\n                location=self.ephys_information[g][\"location\"],\n                device=device,\n            )\n\n            for idx in self.group_to_channel[g]:\n                nwbfile.add_electrode(\n                    id=idx,\n                    x=0.0,\n                    y=0.0,\n                    z=0.0,\n                    imp=0.0,\n                    location=self.ephys_information[g][\"location\"],\n                    filtering=\"none\",\n                    group=electrode_groups[g],\n                )\n\n        # Adding units\n        nwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\n        nwbfile.add_unit_column(\"group\", \"the group of the unit\")\n        for u in self.spikes.keys():\n            nwbfile.add_unit(\n                id=u,\n                spike_times=self.spikes[u].as_units(\"s\").index.values,\n                electrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\n                location=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n                    \"location\"\n                ],\n                group=self.spikes.get_info(\"group\").loc[u],\n            )\n\n        io.write(nwbfile)\n        io.close()\n\n        return\n\n    def load_nwb_spikes(self, path):\n\"\"\"\n        Read the NWB spikes to extract the spike times.\n\n        Parameters\n        ----------\n        path : str\n            The path to the data\n\n        Returns\n        -------\n        TYPE\n            Description\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r\")\n        nwbfile = io.read()\n\n        if nwbfile.units is None:\n            io.close()\n            return False\n        else:\n            units = nwbfile.units.to_dataframe()\n            spikes = {\n                n: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\n                for n in units.index\n            }\n\n            self.spikes = nap.TsGroup(\n                spikes,\n                time_support=self.time_support,\n                time_units=\"s\",\n                group=units[\"group\"],\n            )\n\n            if ~np.all(units[\"location\"] == \"\"):\n                self.spikes.set_info(location=units[\"location\"])\n\n            io.close()\n            return True\n\n    def load_lfp(\n        self,\n        filename=None,\n        channel=None,\n        extension=\".eeg\",\n        frequency=1250.0,\n        precision=\"int16\",\n        bytes_size=2,\n    ):\n\"\"\"\n        Load the LFP.\n\n        Parameters\n        ----------\n        filename : str, optional\n            The filename of the lfp file.\n            It can be useful it multiple dat files are present in the data directory\n        channel : int or list of int, optional\n            The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n        extension : str, optional\n            The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n        frequency : float, optional\n            Default 1250 Hz for the eeg file\n        precision : str, optional\n            The precision of the binary file\n        bytes_size : int, optional\n            Bytes size of the lfp file\n\n        Raises\n        ------\n        RuntimeError\n            If can't find the lfp/eeg/dat file\n\n        Returns\n        -------\n        Tsd or TsdFrame\n            The lfp in a time series format\n        \"\"\"\n        if filename is not None:\n            filepath = os.path.join(self.path, filename)\n        else:\n            listdir = os.listdir(self.path)\n            eegfile = [f for f in listdir if f.endswith(extension)]\n            if not len(eegfile):\n                raise RuntimeError(\n                    \"Path {} contains no {} files;\".format(self.path, extension)\n                )\n\n            filepath = os.path.join(self.path, eegfile[0])\n\n        self.load_neurosuite_xml(self.path)\n\n        n_channels = int(self.nChannels)\n\n        f = open(filepath, \"rb\")\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        bytes_size = 2\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n        duration = n_samples / frequency\n        f.close()\n        fp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\n        timestep = np.arange(0, n_samples) / frequency\n\n        time_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\n\n        if channel is None:\n            return nap.TsdFrame(\n                t=timestep, d=fp, time_units=\"s\", time_support=time_support\n            )\n        elif type(channel) is int:\n            return nap.Tsd(\n                t=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n            )\n        elif type(channel) is list:\n            return nap.TsdFrame(\n                t=timestep,\n                d=fp[:, channel],\n                time_units=\"s\",\n                time_support=time_support,\n                columns=channel,\n            )\n\n    def read_neuroscope_intervals(self, name=None, path2file=None):\n\"\"\"\n        This function reads .evt files in which odd raws indicate the beginning\n        of the time series and the even raws are the ends.\n        If the file is present in the nwb, provide the just the name. If the file\n        is not present in the nwb, it loads the events from the nwb directory.\n        If just the path is provided but not the name, it takes the name from the file.\n\n        Parameters\n        ----------\n        name: str\n            name of the epoch in the nwb file, e.g. \"rem\" or desired name save\n            the data in the nwb.\n\n        path2file: str\n            Path of the file you want to load.\n\n        Returns\n        -------\n        IntervalSet\n            Contains two columns corresponding to the start and end of the intervals.\n\n        \"\"\"\n        if name:\n            isets = self.load_nwb_intervals(name)\n            if isinstance(isets, nap.IntervalSet):\n                return isets\n        if name is not None and path2file is None:\n            path2file = os.path.join(self.path, self.basename + \".\" + name + \".evt\")\n        if path2file is not None:\n            try:\n                # df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None)\n                tmp = np.genfromtxt(path2file)[:, 0]\n                df = tmp.reshape(len(tmp) // 2, 2)\n            except ValueError:\n                print(\"specify a valid name\")\n            isets = nap.IntervalSet(df[:, 0], df[:, 1], time_units=\"ms\")\n            if name is None:\n                name = path2file.split(\".\")[-2]\n                print(\"*** saving file in the nwb as\", name)\n            self.save_nwb_intervals(isets, name)\n        else:\n            raise ValueError(\"specify a valid path\")\n        return isets\n\n    def write_neuroscope_intervals(self, extension, isets, name):\n\"\"\"Write events to load with neuroscope (e.g. ripples start and ends)\n\n        Parameters\n        ----------\n        extension : str\n            The extension of the file (e.g. basename.evt.py.rip)\n        isets : IntervalSet\n            The IntervalSet to write\n        name : str\n            The name of the events (e.g. Ripples)\n        \"\"\"\n        start = isets.as_units(\"ms\")[\"start\"].values\n        ends = isets.as_units(\"ms\")[\"end\"].values\n\n        datatowrite = np.vstack((start, ends)).T.flatten()\n\n        n = len(isets)\n\n        texttowrite = np.vstack(\n            (\n                (np.repeat(np.array([name + \" start\"]), n)),\n                (np.repeat(np.array([name + \" end\"]), n)),\n            )\n        ).T.flatten()\n\n        evt_file = os.path.join(self.path, self.basename + extension)\n\n        f = open(evt_file, \"w\")\n        for t, n in zip(datatowrite, texttowrite):\n            f.writelines(\"{:1.6f}\".format(t) + \"\\t\" + n + \"\\n\")\n        f.close()\n\n        return\n\n    def load_mean_waveforms(self, epoch=None, waveform_window=None, spike_count=1000):\n\"\"\"\n        Load the mean waveforms from a dat file.\n\n        Parameters\n        ----------\n        epoch : IntervalSet\n            default = None\n            Restrict spikes to an epoch.\n        waveform_window : IntervalSet\n            default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms')\n            Limit waveform extraction before and after spike time\n        spike_count : int\n            default = 1000\n            Number of spikes used per neuron for the calculation of waveforms\n\n        Returns\n        -------\n        dictionary\n            the waveforms for all neurons\n        pandas.Series\n            the channel with the maximum waveform for each neuron\n\n        \"\"\"\n        if not isinstance(waveform_window, nap.IntervalSet):\n            waveform_window = nap.IntervalSet(start=-0.5, end=1, time_units=\"ms\")\n\n        spikes = self.spikes\n        if not os.path.exists(self.path):  # check if path exists\n            print(\"The path \" + self.path + \" doesn't exist; Exiting ...\")\n            sys.exit()\n\n        # Load XML INFO\n        self.load_neurosuite_xml(self.path)\n        n_channels = self.nChannels\n        fs = self.fs_dat\n        group_to_channel = self.group_to_channel\n        group = spikes.get_info(\"group\")\n\n        # Check if there is an epoch, restrict spike times to epoch\n        if epoch is not None:\n            if type(epoch) is not nap.IntervalSet:\n                print(\"Epoch must be an IntervalSet\")\n                sys.exit()\n            else:\n                print(\"Restricting spikes to epoch\")\n                spikes = spikes.restrict(epoch)\n                epstart = int(epoch.as_units(\"s\")[\"start\"].values[0] * fs)\n                epend = int(epoch.as_units(\"s\")[\"end\"].values[0] * fs)\n\n        # Find dat file\n        files = os.listdir(self.path)\n        dat_files = np.sort([f for f in files if \"dat\" in f and f[0] != \".\"])\n\n        # Need n_samples collected in the entire recording from dat file to load\n        file = os.path.join(self.path, dat_files[0])\n        f = open(\n            file, \"rb\"\n        )  # open file to get number of samples collected in the entire recording\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        bytes_size = 2\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n        f.close()\n        # map to memory all samples for all channels, channels are numbered according to neuroscope number\n        fp = np.memmap(file, np.int16, \"r\", shape=(n_samples, n_channels))\n\n        # convert spike times to spikes in sample number\n        sample_spikes = {\n            neuron: (spikes[neuron].as_units(\"s\").index.values * fs).astype(\"int\")\n            for neuron in spikes\n        }\n\n        # prep for waveforms\n        overlap = int(\n            waveform_window.tot_length(time_units=\"s\")\n        )  # one spike's worth of overlap between windows\n        waveform_window = abs(np.array(waveform_window.as_units(\"s\"))[0] * fs).astype(\n            int\n        )  # convert time to sample number\n        neuron_waveforms = {\n            n: np.zeros([np.sum(waveform_window), len(group_to_channel[group[n]])])\n            for n in sample_spikes\n        }\n\n        # divide dat file into batches that slightly overlap for faster loading\n        batch_size = 3000000\n        windows = np.arange(0, int(endoffile / n_channels / bytes_size), batch_size)\n        if epoch is not None:\n            print(\"Restricting dat file to epoch\")\n            windows = windows[(windows &gt;= epstart) &amp; (windows &lt;= epend)]\n        batches = []\n        for (\n            i\n        ) in windows:  # make overlapping batches from the beginning to end of recording\n            if i == windows[-1]:  # the last batch cannot overlap with the next one\n                batches.append([i, n_samples])\n            else:\n                batches.append([i, i + batch_size + overlap])\n        batches = [np.int32(batch) for batch in batches]\n\n        sample_counted_spikes = {}\n        for index, neuron in enumerate(sample_spikes):\n            if len(sample_spikes[neuron]) &gt;= spike_count:\n                sample_counted_spikes[neuron] = np.array(\n                    np.random.choice(list(sample_spikes[neuron]), spike_count)\n                )\n            elif len(sample_spikes[neuron]) &lt; spike_count:\n                print(\n                    \"Not enough spikes in neuron \" + str(index) + \"... using all spikes\"\n                )\n                sample_counted_spikes[neuron] = sample_spikes[neuron]\n\n        # Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file\n        spike_check = np.array(\n            [\n                int(spikes_neuron)\n                for spikes_neuron in sample_counted_spikes[neuron]\n                for neuron in sample_counted_spikes\n            ]\n        )\n\n        for index, timestep in enumerate(batches):\n            print(\n                f\"Extracting waveforms from dat file: window {index+1} / {len(windows)}\",\n                end=\"\\r\",\n            )\n\n            if (\n                len(\n                    spike_check[\n                        (timestep[0] &lt; spike_check) &amp; (timestep[1] &gt; spike_check)\n                    ]\n                )\n                == 0\n            ):\n                continue  # if there are no spikes for any neurons in this batch, skip and go to the next one\n\n            # Load dat file for timestep\n            tmp = pd.DataFrame(\n                data=fp[timestep[0] : timestep[1], :],\n                columns=np.arange(n_channels),\n                index=range(timestep[0], timestep[1]),\n            )  # load dat file\n\n            # Check if any spikes are present\n            for neuron in sample_counted_spikes:\n                neurontmp = sample_counted_spikes[neuron]\n                tmp2 = neurontmp[(timestep[0] &lt; neurontmp) &amp; (timestep[1] &gt; neurontmp)]\n                if len(neurontmp) == 0:\n                    continue  # skip neuron if it has no spikes in this batch\n                tmpn = tmp[\n                    group_to_channel[group[neuron]]\n                ]  # restrict dat file to the channel group of the neuron\n\n                for time in tmp2:  # add each spike waveform to neuron_waveform\n                    spikewindow = tmpn.loc[\n                        time - waveform_window[0] : time + waveform_window[1] - 1\n                    ]  # waveform for this spike time\n                    try:\n                        neuron_waveforms[neuron] += spikewindow.values\n                    except (\n                        Exception\n                    ):  # ignore if full waveform is not present in this batch\n                        pass\n\n        meanwf = {\n            n: pd.DataFrame(\n                data=np.array(neuron_waveforms[n]) / spike_count,\n                columns=np.arange(len(group_to_channel[group[n]])),\n                index=np.array(np.arange(-waveform_window[0], waveform_window[1])) / fs,\n            )\n            for n in sample_counted_spikes\n        }\n\n        # find the max channel for each neuron\n        maxch = pd.Series(\n            data=[meanwf[n][meanwf[n].loc[0].idxmin()].name for n in meanwf],\n            index=spikes.keys(),\n        )\n\n        return meanwf, maxch\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.__init__","title":"<code>__init__(path)</code>","text":"<p>Instantiate the data class from a neurosuite folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Instantiate the data class from a neurosuite folder.\n\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\n    self.basename = os.path.basename(path)\n    self.time_support = None\n\n    super().__init__(path)\n\n    # Need to check if nwb file exists and if data are there\n    loading_neurosuite = True\n    if self.path is not None:\n        nwb_path = os.path.join(self.path, \"pynapplenwb\")\n        if os.path.exists(nwb_path):\n            files = os.listdir(nwb_path)\n            if len([f for f in files if f.endswith(\".nwb\")]):\n                success = self.load_nwb_spikes(path)\n                if success:\n                    loading_neurosuite = False\n\n    # Bypass if data have already been transfered to nwb\n    if loading_neurosuite:\n        self.load_neurosuite_xml(path)\n        # print(\"XML loaded\")\n        # To label the electrodes groups\n        app = App()\n        window = EphysGUI(app, path=path, groups=self.group_to_channel)\n        app.mainloop()\n        try:\n            app.update()\n        except Exception:\n            pass\n\n        # print(\"GUI DONE\")\n        if window.status:\n            self.ephys_information = window.ephys_information\n            self.load_neurosuite_spikes(path, self.basename, self.time_support)\n            self.save_data(path)\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_spikes","title":"<code>load_neurosuite_spikes(path, basename, time_support=None, fs=20000.0)</code>","text":"<p>Read the clus and res files and convert to NWB. Instantiate automatically a TsGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <code>basename</code> <code>str</code> <p>Basename of the clu and res files.</p> required <code>time_support</code> <code>IntevalSet, optional</code> <p>The time support of the data</p> <code>None</code> <code>fs</code> <code>float, optional</code> <p>Sampling rate of the recording.</p> <code>20000.0</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If number of clu and res are not equal.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_neurosuite_spikes(self, path, basename, time_support=None, fs=20000.0):\n\"\"\"\n    Read the clus and res files and convert to NWB.\n    Instantiate automatically a TsGroup object.\n\n    Parameters\n    ----------\n    path : str\n        The path to the data\n    basename : str\n        Basename of the clu and res files.\n    time_support : IntevalSet, optional\n        The time support of the data\n    fs : float, optional\n        Sampling rate of the recording.\n\n    Raises\n    ------\n    RuntimeError\n        If number of clu and res are not equal.\n\n    \"\"\"\n    files = os.listdir(path)\n    clu_files = np.sort([f for f in files if \".clu.\" in f and f[0] != \".\"])\n    res_files = np.sort([f for f in files if \".res.\" in f and f[0] != \".\"])\n    clu1 = np.sort([int(f.split(\".\")[-1]) for f in clu_files])\n    clu2 = np.sort([int(f.split(\".\")[-1]) for f in res_files])\n    if len(clu_files) != len(res_files) or not (clu1 == clu2).any():\n        raise RuntimeError(\n            \"Not the same number of clu and res files in \" + path + \"; Exiting ...\"\n        )\n\n    count = 0\n    spikes = {}\n    group = pd.Series(dtype=np.int32)\n    for i, s in zip(range(len(clu_files)), clu1):\n        clu = np.genfromtxt(\n            os.path.join(path, basename + \".clu.\" + str(s)), dtype=np.int32\n        )[1:]\n        if np.max(clu) &gt; 1:  # getting rid of mua and noise\n            res = np.genfromtxt(os.path.join(path, basename + \".res.\" + str(s)))\n            tmp = np.unique(clu).astype(int)\n            idx_clu = tmp[tmp &gt; 1]\n            idx_out = np.arange(count, count + len(idx_clu))\n\n            for j, k in zip(idx_clu, idx_out):\n                t = res[clu == j] / fs\n                spikes[k] = nap.Ts(t=t, time_units=\"s\")\n                group.loc[k] = s\n\n            count += len(idx_clu)\n\n    group = group - 1  # better to start it a 0\n\n    self.spikes = nap.TsGroup(\n        spikes, time_support=time_support, time_units=\"s\", group=group\n    )\n\n    # adding some information to help parse the neurons\n    names = pd.Series(\n        index=group.index,\n        data=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n    )\n    if ~np.all(names.values == \"\"):\n        self.spikes.set_info(name=names)\n    locations = pd.Series(\n        index=group.index,\n        data=[\n            self.ephys_information[group.loc[i]][\"location\"] for i in group.index\n        ],\n    )\n    if ~np.all(locations.values == \"\"):\n        self.spikes.set_info(location=locations)\n\n    return\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_xml","title":"<code>load_neurosuite_xml(path)</code>","text":"<p>path should be the folder session containing the XML file</p> <p>Function reads : 1. the number of channels 2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder     eeg file first if both are present or both are absent 3. the mappings shanks to channels as a dict</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>The path to the data</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If path does not contain the xml file.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_neurosuite_xml(self, path):\n\"\"\"\n    path should be the folder session containing the XML file\n\n    Function reads :\n    1. the number of channels\n    2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder\n        eeg file first if both are present or both are absent\n    3. the mappings shanks to channels as a dict\n\n    Parameters\n    ----------\n    path: str\n        The path to the data\n\n    Raises\n    ------\n    RuntimeError\n        If path does not contain the xml file.\n    \"\"\"\n    listdir = os.listdir(path)\n    xmlfiles = [f for f in listdir if f.endswith(\".xml\")]\n    if not len(xmlfiles):\n        raise RuntimeError(\"Path {} contains no xml files;\".format(path))\n        sys.exit()\n    new_path = os.path.join(path, xmlfiles[0])\n\n    self.xmldoc = minidom.parse(new_path)\n    self.nChannels = int(\n        self.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n        .getElementsByTagName(\"nChannels\")[0]\n        .firstChild.data\n    )\n    self.fs_dat = int(\n        self.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n        .getElementsByTagName(\"samplingRate\")[0]\n        .firstChild.data\n    )\n    self.fs_eeg = int(\n        self.xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n        .getElementsByTagName(\"lfpSamplingRate\")[0]\n        .firstChild.data\n    )\n\n    self.group_to_channel = {}\n    groups = (\n        self.xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n        .getElementsByTagName(\"channelGroups\")[0]\n        .getElementsByTagName(\"group\")\n    )\n    for i in range(len(groups)):\n        self.group_to_channel[i] = np.array(\n            [\n                int(child.firstChild.data)\n                for child in groups[i].getElementsByTagName(\"channel\")\n            ]\n        )\n\n    return\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.save_data","title":"<code>save_data(path)</code>","text":"<p>Save the data to NWB format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def save_data(self, path):\n\"\"\"\n    Save the data to NWB format.\n\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n    self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n    self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n    io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n    nwbfile = io.read()\n\n    electrode_groups = {}\n\n    for g in self.group_to_channel:\n        device = nwbfile.create_device(\n            name=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\n            description=self.ephys_information[g][\"device\"][\"description\"],\n            manufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n        )\n\n        if (\n            len(self.ephys_information[g][\"position\"])\n            and type(self.ephys_information[g][\"position\"]) is str\n        ):\n            self.ephys_information[g][\"position\"] = re.split(\n                \";|,| \", self.ephys_information[g][\"position\"]\n            )\n        elif self.ephys_information[g][\"position\"] == \"\":\n            self.ephys_information[g][\"position\"] = None\n\n        electrode_groups[g] = nwbfile.create_electrode_group(\n            name=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\n            description=self.ephys_information[g][\"description\"],\n            position=self.ephys_information[g][\"position\"],\n            location=self.ephys_information[g][\"location\"],\n            device=device,\n        )\n\n        for idx in self.group_to_channel[g]:\n            nwbfile.add_electrode(\n                id=idx,\n                x=0.0,\n                y=0.0,\n                z=0.0,\n                imp=0.0,\n                location=self.ephys_information[g][\"location\"],\n                filtering=\"none\",\n                group=electrode_groups[g],\n            )\n\n    # Adding units\n    nwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\n    nwbfile.add_unit_column(\"group\", \"the group of the unit\")\n    for u in self.spikes.keys():\n        nwbfile.add_unit(\n            id=u,\n            spike_times=self.spikes[u].as_units(\"s\").index.values,\n            electrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\n            location=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n                \"location\"\n            ],\n            group=self.spikes.get_info(\"group\").loc[u],\n        )\n\n    io.write(nwbfile)\n    io.close()\n\n    return\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_nwb_spikes","title":"<code>load_nwb_spikes(path)</code>","text":"<p>Read the NWB spikes to extract the spike times.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_nwb_spikes(self, path):\n\"\"\"\n    Read the NWB spikes to extract the spike times.\n\n    Parameters\n    ----------\n    path : str\n        The path to the data\n\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n    self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n    self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n    io = NWBHDF5IO(self.nwbfilepath, \"r\")\n    nwbfile = io.read()\n\n    if nwbfile.units is None:\n        io.close()\n        return False\n    else:\n        units = nwbfile.units.to_dataframe()\n        spikes = {\n            n: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\n            for n in units.index\n        }\n\n        self.spikes = nap.TsGroup(\n            spikes,\n            time_support=self.time_support,\n            time_units=\"s\",\n            group=units[\"group\"],\n        )\n\n        if ~np.all(units[\"location\"] == \"\"):\n            self.spikes.set_info(location=units[\"location\"])\n\n        io.close()\n        return True\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_lfp","title":"<code>load_lfp(filename=None, channel=None, extension='.eeg', frequency=1250.0, precision='int16', bytes_size=2)</code>","text":"<p>Load the LFP.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str, optional</code> <p>The filename of the lfp file. It can be useful it multiple dat files are present in the data directory</p> <code>None</code> <code>channel</code> <code>int or list of int, optional</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>extension</code> <code>str, optional</code> <p>The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> <code>'.eeg'</code> <code>frequency</code> <code>float, optional</code> <p>Default 1250 Hz for the eeg file</p> <code>1250.0</code> <code>precision</code> <code>str, optional</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the lfp file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_lfp(\n    self,\n    filename=None,\n    channel=None,\n    extension=\".eeg\",\n    frequency=1250.0,\n    precision=\"int16\",\n    bytes_size=2,\n):\n\"\"\"\n    Load the LFP.\n\n    Parameters\n    ----------\n    filename : str, optional\n        The filename of the lfp file.\n        It can be useful it multiple dat files are present in the data directory\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    frequency : float, optional\n        Default 1250 Hz for the eeg file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the lfp file\n\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    \"\"\"\n    if filename is not None:\n        filepath = os.path.join(self.path, filename)\n    else:\n        listdir = os.listdir(self.path)\n        eegfile = [f for f in listdir if f.endswith(extension)]\n        if not len(eegfile):\n            raise RuntimeError(\n                \"Path {} contains no {} files;\".format(self.path, extension)\n            )\n\n        filepath = os.path.join(self.path, eegfile[0])\n\n    self.load_neurosuite_xml(self.path)\n\n    n_channels = int(self.nChannels)\n\n    f = open(filepath, \"rb\")\n    startoffile = f.seek(0, 0)\n    endoffile = f.seek(0, 2)\n    bytes_size = 2\n    n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n    duration = n_samples / frequency\n    f.close()\n    fp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\n    timestep = np.arange(0, n_samples) / frequency\n\n    time_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\n\n    if channel is None:\n        return nap.TsdFrame(\n            t=timestep, d=fp, time_units=\"s\", time_support=time_support\n        )\n    elif type(channel) is int:\n        return nap.Tsd(\n            t=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n        )\n    elif type(channel) is list:\n        return nap.TsdFrame(\n            t=timestep,\n            d=fp[:, channel],\n            time_units=\"s\",\n            time_support=time_support,\n            columns=channel,\n        )\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.read_neuroscope_intervals","title":"<code>read_neuroscope_intervals(name=None, path2file=None)</code>","text":"<p>This function reads .evt files in which odd raws indicate the beginning of the time series and the even raws are the ends. If the file is present in the nwb, provide the just the name. If the file is not present in the nwb, it loads the events from the nwb directory. If just the path is provided but not the name, it takes the name from the file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>name of the epoch in the nwb file, e.g. \"rem\" or desired name save the data in the nwb.</p> <code>None</code> <p>path2file: str     Path of the file you want to load.</p> <p>Returns:</p> Type Description <code>IntervalSet</code> <p>Contains two columns corresponding to the start and end of the intervals.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def read_neuroscope_intervals(self, name=None, path2file=None):\n\"\"\"\n    This function reads .evt files in which odd raws indicate the beginning\n    of the time series and the even raws are the ends.\n    If the file is present in the nwb, provide the just the name. If the file\n    is not present in the nwb, it loads the events from the nwb directory.\n    If just the path is provided but not the name, it takes the name from the file.\n\n    Parameters\n    ----------\n    name: str\n        name of the epoch in the nwb file, e.g. \"rem\" or desired name save\n        the data in the nwb.\n\n    path2file: str\n        Path of the file you want to load.\n\n    Returns\n    -------\n    IntervalSet\n        Contains two columns corresponding to the start and end of the intervals.\n\n    \"\"\"\n    if name:\n        isets = self.load_nwb_intervals(name)\n        if isinstance(isets, nap.IntervalSet):\n            return isets\n    if name is not None and path2file is None:\n        path2file = os.path.join(self.path, self.basename + \".\" + name + \".evt\")\n    if path2file is not None:\n        try:\n            # df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None)\n            tmp = np.genfromtxt(path2file)[:, 0]\n            df = tmp.reshape(len(tmp) // 2, 2)\n        except ValueError:\n            print(\"specify a valid name\")\n        isets = nap.IntervalSet(df[:, 0], df[:, 1], time_units=\"ms\")\n        if name is None:\n            name = path2file.split(\".\")[-2]\n            print(\"*** saving file in the nwb as\", name)\n        self.save_nwb_intervals(isets, name)\n    else:\n        raise ValueError(\"specify a valid path\")\n    return isets\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.write_neuroscope_intervals","title":"<code>write_neuroscope_intervals(extension, isets, name)</code>","text":"<p>Write events to load with neuroscope (e.g. ripples start and ends)</p> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>The extension of the file (e.g. basename.evt.py.rip)</p> required <code>isets</code> <code>IntervalSet</code> <p>The IntervalSet to write</p> required <code>name</code> <code>str</code> <p>The name of the events (e.g. Ripples)</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def write_neuroscope_intervals(self, extension, isets, name):\n\"\"\"Write events to load with neuroscope (e.g. ripples start and ends)\n\n    Parameters\n    ----------\n    extension : str\n        The extension of the file (e.g. basename.evt.py.rip)\n    isets : IntervalSet\n        The IntervalSet to write\n    name : str\n        The name of the events (e.g. Ripples)\n    \"\"\"\n    start = isets.as_units(\"ms\")[\"start\"].values\n    ends = isets.as_units(\"ms\")[\"end\"].values\n\n    datatowrite = np.vstack((start, ends)).T.flatten()\n\n    n = len(isets)\n\n    texttowrite = np.vstack(\n        (\n            (np.repeat(np.array([name + \" start\"]), n)),\n            (np.repeat(np.array([name + \" end\"]), n)),\n        )\n    ).T.flatten()\n\n    evt_file = os.path.join(self.path, self.basename + extension)\n\n    f = open(evt_file, \"w\")\n    for t, n in zip(datatowrite, texttowrite):\n        f.writelines(\"{:1.6f}\".format(t) + \"\\t\" + n + \"\\n\")\n    f.close()\n\n    return\n</code></pre>"},{"location":"io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_mean_waveforms","title":"<code>load_mean_waveforms(epoch=None, waveform_window=None, spike_count=1000)</code>","text":"<p>Load the mean waveforms from a dat file.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>IntervalSet</code> <p>default = None Restrict spikes to an epoch.</p> <code>None</code> <code>waveform_window</code> <code>IntervalSet</code> <p>default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms') Limit waveform extraction before and after spike time</p> <code>None</code> <code>spike_count</code> <code>int</code> <p>default = 1000 Number of spikes used per neuron for the calculation of waveforms</p> <code>1000</code> <p>Returns:</p> Type Description <code>dictionary</code> <p>the waveforms for all neurons</p> <code>pandas.Series</code> <p>the channel with the maximum waveform for each neuron</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_mean_waveforms(self, epoch=None, waveform_window=None, spike_count=1000):\n\"\"\"\n    Load the mean waveforms from a dat file.\n\n    Parameters\n    ----------\n    epoch : IntervalSet\n        default = None\n        Restrict spikes to an epoch.\n    waveform_window : IntervalSet\n        default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms')\n        Limit waveform extraction before and after spike time\n    spike_count : int\n        default = 1000\n        Number of spikes used per neuron for the calculation of waveforms\n\n    Returns\n    -------\n    dictionary\n        the waveforms for all neurons\n    pandas.Series\n        the channel with the maximum waveform for each neuron\n\n    \"\"\"\n    if not isinstance(waveform_window, nap.IntervalSet):\n        waveform_window = nap.IntervalSet(start=-0.5, end=1, time_units=\"ms\")\n\n    spikes = self.spikes\n    if not os.path.exists(self.path):  # check if path exists\n        print(\"The path \" + self.path + \" doesn't exist; Exiting ...\")\n        sys.exit()\n\n    # Load XML INFO\n    self.load_neurosuite_xml(self.path)\n    n_channels = self.nChannels\n    fs = self.fs_dat\n    group_to_channel = self.group_to_channel\n    group = spikes.get_info(\"group\")\n\n    # Check if there is an epoch, restrict spike times to epoch\n    if epoch is not None:\n        if type(epoch) is not nap.IntervalSet:\n            print(\"Epoch must be an IntervalSet\")\n            sys.exit()\n        else:\n            print(\"Restricting spikes to epoch\")\n            spikes = spikes.restrict(epoch)\n            epstart = int(epoch.as_units(\"s\")[\"start\"].values[0] * fs)\n            epend = int(epoch.as_units(\"s\")[\"end\"].values[0] * fs)\n\n    # Find dat file\n    files = os.listdir(self.path)\n    dat_files = np.sort([f for f in files if \"dat\" in f and f[0] != \".\"])\n\n    # Need n_samples collected in the entire recording from dat file to load\n    file = os.path.join(self.path, dat_files[0])\n    f = open(\n        file, \"rb\"\n    )  # open file to get number of samples collected in the entire recording\n    startoffile = f.seek(0, 0)\n    endoffile = f.seek(0, 2)\n    bytes_size = 2\n    n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n    f.close()\n    # map to memory all samples for all channels, channels are numbered according to neuroscope number\n    fp = np.memmap(file, np.int16, \"r\", shape=(n_samples, n_channels))\n\n    # convert spike times to spikes in sample number\n    sample_spikes = {\n        neuron: (spikes[neuron].as_units(\"s\").index.values * fs).astype(\"int\")\n        for neuron in spikes\n    }\n\n    # prep for waveforms\n    overlap = int(\n        waveform_window.tot_length(time_units=\"s\")\n    )  # one spike's worth of overlap between windows\n    waveform_window = abs(np.array(waveform_window.as_units(\"s\"))[0] * fs).astype(\n        int\n    )  # convert time to sample number\n    neuron_waveforms = {\n        n: np.zeros([np.sum(waveform_window), len(group_to_channel[group[n]])])\n        for n in sample_spikes\n    }\n\n    # divide dat file into batches that slightly overlap for faster loading\n    batch_size = 3000000\n    windows = np.arange(0, int(endoffile / n_channels / bytes_size), batch_size)\n    if epoch is not None:\n        print(\"Restricting dat file to epoch\")\n        windows = windows[(windows &gt;= epstart) &amp; (windows &lt;= epend)]\n    batches = []\n    for (\n        i\n    ) in windows:  # make overlapping batches from the beginning to end of recording\n        if i == windows[-1]:  # the last batch cannot overlap with the next one\n            batches.append([i, n_samples])\n        else:\n            batches.append([i, i + batch_size + overlap])\n    batches = [np.int32(batch) for batch in batches]\n\n    sample_counted_spikes = {}\n    for index, neuron in enumerate(sample_spikes):\n        if len(sample_spikes[neuron]) &gt;= spike_count:\n            sample_counted_spikes[neuron] = np.array(\n                np.random.choice(list(sample_spikes[neuron]), spike_count)\n            )\n        elif len(sample_spikes[neuron]) &lt; spike_count:\n            print(\n                \"Not enough spikes in neuron \" + str(index) + \"... using all spikes\"\n            )\n            sample_counted_spikes[neuron] = sample_spikes[neuron]\n\n    # Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file\n    spike_check = np.array(\n        [\n            int(spikes_neuron)\n            for spikes_neuron in sample_counted_spikes[neuron]\n            for neuron in sample_counted_spikes\n        ]\n    )\n\n    for index, timestep in enumerate(batches):\n        print(\n            f\"Extracting waveforms from dat file: window {index+1} / {len(windows)}\",\n            end=\"\\r\",\n        )\n\n        if (\n            len(\n                spike_check[\n                    (timestep[0] &lt; spike_check) &amp; (timestep[1] &gt; spike_check)\n                ]\n            )\n            == 0\n        ):\n            continue  # if there are no spikes for any neurons in this batch, skip and go to the next one\n\n        # Load dat file for timestep\n        tmp = pd.DataFrame(\n            data=fp[timestep[0] : timestep[1], :],\n            columns=np.arange(n_channels),\n            index=range(timestep[0], timestep[1]),\n        )  # load dat file\n\n        # Check if any spikes are present\n        for neuron in sample_counted_spikes:\n            neurontmp = sample_counted_spikes[neuron]\n            tmp2 = neurontmp[(timestep[0] &lt; neurontmp) &amp; (timestep[1] &gt; neurontmp)]\n            if len(neurontmp) == 0:\n                continue  # skip neuron if it has no spikes in this batch\n            tmpn = tmp[\n                group_to_channel[group[neuron]]\n            ]  # restrict dat file to the channel group of the neuron\n\n            for time in tmp2:  # add each spike waveform to neuron_waveform\n                spikewindow = tmpn.loc[\n                    time - waveform_window[0] : time + waveform_window[1] - 1\n                ]  # waveform for this spike time\n                try:\n                    neuron_waveforms[neuron] += spikewindow.values\n                except (\n                    Exception\n                ):  # ignore if full waveform is not present in this batch\n                    pass\n\n    meanwf = {\n        n: pd.DataFrame(\n            data=np.array(neuron_waveforms[n]) / spike_count,\n            columns=np.arange(len(group_to_channel[group[n]])),\n            index=np.array(np.arange(-waveform_window[0], waveform_window[1])) / fs,\n        )\n        for n in sample_counted_spikes\n    }\n\n    # find the max channel for each neuron\n    maxch = pd.Series(\n        data=[meanwf[n][meanwf[n].loc[0].idxmin()].name for n in meanwf],\n        index=spikes.keys(),\n    )\n\n    return meanwf, maxch\n</code></pre>"},{"location":"io.phy/","title":"Phy","text":"<p>Class and functions for loading data processed with Phy2</p> <p>@author: Sara Mahallati, Guillaume Viejo</p>"},{"location":"io.phy/#pynapple.io.phy.Phy","title":"<code>Phy</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for Phy data</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>class Phy(BaseLoader):\n\"\"\"\n    Loader for Phy data\n    \"\"\"\n\n    def __init__(self, path):\n\"\"\"\n        Instantiate the data class from a Phy folder.\n\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\n        self.basename = os.path.basename(path)\n        self.time_support = None\n\n        super().__init__(path)\n\n        # Need to check if nwb file exists and if data are there\n        loading_phy = True\n        if self.path is not None:\n            nwb_path = os.path.join(self.path, \"pynapplenwb\")\n            if os.path.exists(nwb_path):\n                files = os.listdir(nwb_path)\n                if len([f for f in files if f.endswith(\".nwb\")]):\n                    success = self.load_nwb_spikes(path)\n                    if success:\n                        loading_phy = False\n\n        # Bypass if data have already been transfered to nwb\n        if loading_phy:\n            self.load_phy_params(path)\n\n            app = App()\n            window = EphysGUI(app, path=path, groups=self.channel_map)\n            app.mainloop()\n            try:\n                app.update()\n            except Exception:\n                pass\n\n            if window.status:\n                self.ephys_information = window.ephys_information\n                self.load_phy_spikes(path, self.time_support)\n                self.save_data(path)\n            app.quit()\n\n    def load_phy_params(self, path):\n\"\"\"\n        path should be the folder session containing the params.py file\n\n        Function reads :\n        1. the number of channels\n        2. the sampling frequency of the dat file\n\n        Parameters\n        ----------\n        path: str\n            The path to the data\n\n        Raises\n        ------\n        RuntimeError\n            If path does not contain the params file or channel_map.npy\n        \"\"\"\n        if os.path.isfile(os.path.join(path, \"params.py\")):\n            sys.path.append(path)\n            import params as params\n\n            self.sample_rate = params.sample_rate\n            self.n_channels_dat = params.n_channels_dat\n        else:\n            raise RuntimeError(\"Can't find params.py in path {};\".format(path))\n\n        if os.path.isfile(os.path.join(path, \"channel_map.npy\")):\n            channel_map = np.load(os.path.join(path, \"channel_map.npy\"))\n            self.channel_map = {i: channel_map[i] for i in range(len(channel_map))}\n            self.ch_to_sh = pd.Series(\n                index=channel_map.flatten(),\n                data=np.hstack(\n                    [\n                        np.ones(len(channel_map[i]), dtype=int) * i\n                        for i in range(len(channel_map))\n                    ]\n                ),\n            )\n        else:\n            raise RuntimeError(\"Can't find channel_map.npy in path {};\".format(path))\n\n        return\n\n    def load_phy_spikes(self, path, time_support=None):\n\"\"\"\n        Load Phy spike times and convert to NWB.\n        Instantiate automatically a TsGroup object.\n        The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv\n\n        Parameters\n        ----------\n        path : str\n            The path to the data\n        time_support : IntevalSet, optional\n            The time support of the data\n\n        Raises\n        ------\n        RuntimeError\n            If files are missing.\n            The function needs :\n            - cluster_info.tsv or cluster_group.tsv\n            - spike_times.npy\n            - spike_clusters.npy\n            - channel_positions.npy\n            - templates.npy\n\n        \"\"\"\n        files = os.listdir(path)\n\n        has_cluster_info = False\n        if \"cluster_info.tsv\" in files:\n            self.cluster_info = pd.read_csv(\n                os.path.join(path, \"cluster_info.tsv\"), sep=\"\\t\", index_col=\"cluster_id\"\n            )\n            cluster_id_good = self.cluster_info[\n                self.cluster_info.group == \"good\"\n            ].index.values\n            has_cluster_info = True\n        elif \"cluster_group.tsv\" in files:\n            self.cluster_group = pd.read_csv(\n                os.path.join(path, \"cluster_group.tsv\"),\n                sep=\"\\t\",\n                index_col=\"cluster_id\",\n            )\n            cluster_id_good = self.cluster_group[\n                self.cluster_group.group == \"good\"\n            ].index.values\n        else:\n            raise RuntimeError(\n                \"Can't find cluster_info.tsv or cluster_group.tsv in {};\".format(path)\n            )\n\n        spike_times = np.load(os.path.join(path, \"spike_times.npy\"))\n        spike_clusters = np.load(os.path.join(path, \"spike_clusters.npy\"))\n\n        spikes = {}\n        for n in cluster_id_good:\n            spikes[n] = nap.Ts(\n                t=spike_times[spike_clusters == n] / self.sample_rate,\n                time_support=time_support,\n            )\n\n        self.spikes = nap.TsGroup(spikes, time_support=time_support)\n\n        # Adding the position of the electrodes in case\n        self.channel_positions = np.load(os.path.join(path, \"channel_positions.npy\"))\n\n        # Adding shank group info from cluster_info if present\n        if has_cluster_info:\n            group = self.cluster_info.loc[cluster_id_good, \"sh\"]\n            self.spikes.set_info(group=group)\n        else:\n            template = np.load(os.path.join(path, \"templates.npy\"))\n            template = template[cluster_id_good]\n            ch = np.power(template, 2).max(1).argmax(1)\n            group = pd.Series(index=cluster_id_good, data=self.ch_to_sh[ch].values)\n            self.spikes.set_info(group=group)\n\n        names = pd.Series(\n            index=group.index,\n            data=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n        )\n        if ~np.all(names.values == \"\"):\n            self.spikes.set_info(name=names)\n\n        locations = pd.Series(\n            index=group.index,\n            data=[\n                self.ephys_information[group.loc[i]][\"location\"] for i in group.index\n            ],\n        )\n        if ~np.all(locations.values == \"\"):\n            self.spikes.set_info(location=locations)\n\n        return\n\n    def save_data(self, path):\n\"\"\"\n        Save the data to NWB format.\n\n        Parameters\n        ----------\n        path : str\n            The path to save the data\n\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n        nwbfile = io.read()\n\n        electrode_groups = {}\n\n        for g in self.channel_map:\n            device = nwbfile.create_device(\n                name=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\n                description=self.ephys_information[g][\"device\"][\"description\"],\n                manufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n            )\n\n            if (\n                len(self.ephys_information[g][\"position\"])\n                and type(self.ephys_information[g][\"position\"]) is str\n            ):\n                self.ephys_information[g][\"position\"] = re.split(\n                    \";|,| \", self.ephys_information[g][\"position\"]\n                )\n            elif self.ephys_information[g][\"position\"] == \"\":\n                self.ephys_information[g][\"position\"] = None\n\n            electrode_groups[g] = nwbfile.create_electrode_group(\n                name=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\n                description=self.ephys_information[g][\"description\"],\n                position=self.ephys_information[g][\"position\"],\n                location=self.ephys_information[g][\"location\"],\n                device=device,\n            )\n\n            for idx in self.channel_map[g]:\n                nwbfile.add_electrode(\n                    id=idx,\n                    x=0.0,\n                    y=0.0,\n                    z=0.0,\n                    imp=0.0,\n                    location=self.ephys_information[g][\"location\"],\n                    filtering=\"none\",\n                    group=electrode_groups[g],\n                )\n\n        # Adding units\n        nwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\n        nwbfile.add_unit_column(\"group\", \"the group of the unit\")\n        for u in self.spikes.keys():\n            nwbfile.add_unit(\n                id=u,\n                spike_times=self.spikes[u].as_units(\"s\").index.values,\n                electrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\n                location=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n                    \"location\"\n                ],\n                group=self.spikes.get_info(\"group\").loc[u],\n            )\n\n        io.write(nwbfile)\n        io.close()\n\n        return\n\n    def load_nwb_spikes(self, path):\n\"\"\"\n        Read the NWB spikes to extract the spike times.\n\n        Parameters\n        ----------\n        path : str\n            The path to the data\n\n        Returns\n        -------\n        TYPE\n            Description\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r\")\n        nwbfile = io.read()\n\n        if nwbfile.units is None:\n            io.close()\n            return False\n        else:\n            units = nwbfile.units.to_dataframe()\n            spikes = {\n                n: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\n                for n in units.index\n            }\n\n            self.spikes = nap.TsGroup(\n                spikes,\n                time_support=self.time_support,\n                time_units=\"s\",\n                group=units[\"group\"],\n            )\n\n            if ~np.all(units[\"location\"] == \"\"):\n                self.spikes.set_info(location=units[\"location\"])\n\n            io.close()\n            return True\n\n    def load_lfp(\n        self,\n        filename=None,\n        channel=None,\n        extension=\".eeg\",\n        frequency=1250.0,\n        precision=\"int16\",\n        bytes_size=2,\n    ):\n\"\"\"\n        Load the LFP.\n\n        Parameters\n        ----------\n        filename : str, optional\n            The filename of the lfp file.\n            It can be useful it multiple dat files are present in the data directory\n        channel : int or list of int, optional\n            The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n        extension : str, optional\n            The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n        frequency : float, optional\n            Default 1250 Hz for the eeg file\n        precision : str, optional\n            The precision of the binary file\n        bytes_size : int, optional\n            Bytes size of the lfp file\n\n        Raises\n        ------\n        RuntimeError\n            If can't find the lfp/eeg/dat file\n\n        Returns\n        -------\n        Tsd or TsdFrame\n            The lfp in a time series format\n        \"\"\"\n        if filename is not None:\n            filepath = os.path.join(self.path, filename)\n        else:\n            listdir = os.listdir(self.path)\n            eegfile = [f for f in listdir if f.endswith(extension)]\n            if not len(eegfile):\n                raise RuntimeError(\n                    \"Path {} contains no {} files;\".format(self.path, extension)\n                )\n\n            filepath = os.path.join(self.path, eegfile[0])\n\n        self.load_neurosuite_xml(self.path)\n\n        n_channels = int(self.nChannels)\n\n        f = open(filepath, \"rb\")\n        startoffile = f.seek(0, 0)\n        endoffile = f.seek(0, 2)\n        bytes_size = 2\n        n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n        duration = n_samples / frequency\n        f.close()\n        fp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\n        timestep = np.arange(0, n_samples) / frequency\n\n        time_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\n\n        if channel is None:\n            return nap.TsdFrame(\n                t=timestep, d=fp, time_units=\"s\", time_support=time_support\n            )\n        elif type(channel) is int:\n            return nap.Tsd(\n                t=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n            )\n        elif type(channel) is list:\n            return nap.TsdFrame(\n                t=timestep,\n                d=fp[:, channel],\n                time_units=\"s\",\n                time_support=time_support,\n                columns=channel,\n            )\n</code></pre>"},{"location":"io.phy/#pynapple.io.phy.Phy.__init__","title":"<code>__init__(path)</code>","text":"<p>Instantiate the data class from a Phy folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/phy.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Instantiate the data class from a Phy folder.\n\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\n    self.basename = os.path.basename(path)\n    self.time_support = None\n\n    super().__init__(path)\n\n    # Need to check if nwb file exists and if data are there\n    loading_phy = True\n    if self.path is not None:\n        nwb_path = os.path.join(self.path, \"pynapplenwb\")\n        if os.path.exists(nwb_path):\n            files = os.listdir(nwb_path)\n            if len([f for f in files if f.endswith(\".nwb\")]):\n                success = self.load_nwb_spikes(path)\n                if success:\n                    loading_phy = False\n\n    # Bypass if data have already been transfered to nwb\n    if loading_phy:\n        self.load_phy_params(path)\n\n        app = App()\n        window = EphysGUI(app, path=path, groups=self.channel_map)\n        app.mainloop()\n        try:\n            app.update()\n        except Exception:\n            pass\n\n        if window.status:\n            self.ephys_information = window.ephys_information\n            self.load_phy_spikes(path, self.time_support)\n            self.save_data(path)\n        app.quit()\n</code></pre>"},{"location":"io.phy/#pynapple.io.phy.Phy.load_phy_params","title":"<code>load_phy_params(path)</code>","text":"<p>path should be the folder session containing the params.py file</p> <p>Function reads : 1. the number of channels 2. the sampling frequency of the dat file</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>The path to the data</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If path does not contain the params file or channel_map.npy</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_phy_params(self, path):\n\"\"\"\n    path should be the folder session containing the params.py file\n\n    Function reads :\n    1. the number of channels\n    2. the sampling frequency of the dat file\n\n    Parameters\n    ----------\n    path: str\n        The path to the data\n\n    Raises\n    ------\n    RuntimeError\n        If path does not contain the params file or channel_map.npy\n    \"\"\"\n    if os.path.isfile(os.path.join(path, \"params.py\")):\n        sys.path.append(path)\n        import params as params\n\n        self.sample_rate = params.sample_rate\n        self.n_channels_dat = params.n_channels_dat\n    else:\n        raise RuntimeError(\"Can't find params.py in path {};\".format(path))\n\n    if os.path.isfile(os.path.join(path, \"channel_map.npy\")):\n        channel_map = np.load(os.path.join(path, \"channel_map.npy\"))\n        self.channel_map = {i: channel_map[i] for i in range(len(channel_map))}\n        self.ch_to_sh = pd.Series(\n            index=channel_map.flatten(),\n            data=np.hstack(\n                [\n                    np.ones(len(channel_map[i]), dtype=int) * i\n                    for i in range(len(channel_map))\n                ]\n            ),\n        )\n    else:\n        raise RuntimeError(\"Can't find channel_map.npy in path {};\".format(path))\n\n    return\n</code></pre>"},{"location":"io.phy/#pynapple.io.phy.Phy.load_phy_spikes","title":"<code>load_phy_spikes(path, time_support=None)</code>","text":"<p>Load Phy spike times and convert to NWB. Instantiate automatically a TsGroup object. The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <code>time_support</code> <code>IntevalSet, optional</code> <p>The time support of the data</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If files are missing. The function needs : - cluster_info.tsv or cluster_group.tsv - spike_times.npy - spike_clusters.npy - channel_positions.npy - templates.npy</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_phy_spikes(self, path, time_support=None):\n\"\"\"\n    Load Phy spike times and convert to NWB.\n    Instantiate automatically a TsGroup object.\n    The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv\n\n    Parameters\n    ----------\n    path : str\n        The path to the data\n    time_support : IntevalSet, optional\n        The time support of the data\n\n    Raises\n    ------\n    RuntimeError\n        If files are missing.\n        The function needs :\n        - cluster_info.tsv or cluster_group.tsv\n        - spike_times.npy\n        - spike_clusters.npy\n        - channel_positions.npy\n        - templates.npy\n\n    \"\"\"\n    files = os.listdir(path)\n\n    has_cluster_info = False\n    if \"cluster_info.tsv\" in files:\n        self.cluster_info = pd.read_csv(\n            os.path.join(path, \"cluster_info.tsv\"), sep=\"\\t\", index_col=\"cluster_id\"\n        )\n        cluster_id_good = self.cluster_info[\n            self.cluster_info.group == \"good\"\n        ].index.values\n        has_cluster_info = True\n    elif \"cluster_group.tsv\" in files:\n        self.cluster_group = pd.read_csv(\n            os.path.join(path, \"cluster_group.tsv\"),\n            sep=\"\\t\",\n            index_col=\"cluster_id\",\n        )\n        cluster_id_good = self.cluster_group[\n            self.cluster_group.group == \"good\"\n        ].index.values\n    else:\n        raise RuntimeError(\n            \"Can't find cluster_info.tsv or cluster_group.tsv in {};\".format(path)\n        )\n\n    spike_times = np.load(os.path.join(path, \"spike_times.npy\"))\n    spike_clusters = np.load(os.path.join(path, \"spike_clusters.npy\"))\n\n    spikes = {}\n    for n in cluster_id_good:\n        spikes[n] = nap.Ts(\n            t=spike_times[spike_clusters == n] / self.sample_rate,\n            time_support=time_support,\n        )\n\n    self.spikes = nap.TsGroup(spikes, time_support=time_support)\n\n    # Adding the position of the electrodes in case\n    self.channel_positions = np.load(os.path.join(path, \"channel_positions.npy\"))\n\n    # Adding shank group info from cluster_info if present\n    if has_cluster_info:\n        group = self.cluster_info.loc[cluster_id_good, \"sh\"]\n        self.spikes.set_info(group=group)\n    else:\n        template = np.load(os.path.join(path, \"templates.npy\"))\n        template = template[cluster_id_good]\n        ch = np.power(template, 2).max(1).argmax(1)\n        group = pd.Series(index=cluster_id_good, data=self.ch_to_sh[ch].values)\n        self.spikes.set_info(group=group)\n\n    names = pd.Series(\n        index=group.index,\n        data=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n    )\n    if ~np.all(names.values == \"\"):\n        self.spikes.set_info(name=names)\n\n    locations = pd.Series(\n        index=group.index,\n        data=[\n            self.ephys_information[group.loc[i]][\"location\"] for i in group.index\n        ],\n    )\n    if ~np.all(locations.values == \"\"):\n        self.spikes.set_info(location=locations)\n\n    return\n</code></pre>"},{"location":"io.phy/#pynapple.io.phy.Phy.save_data","title":"<code>save_data(path)</code>","text":"<p>Save the data to NWB format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/phy.py</code> <pre><code>def save_data(self, path):\n\"\"\"\n    Save the data to NWB format.\n\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n\n    self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n    self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n    io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n    nwbfile = io.read()\n\n    electrode_groups = {}\n\n    for g in self.channel_map:\n        device = nwbfile.create_device(\n            name=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\n            description=self.ephys_information[g][\"device\"][\"description\"],\n            manufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n        )\n\n        if (\n            len(self.ephys_information[g][\"position\"])\n            and type(self.ephys_information[g][\"position\"]) is str\n        ):\n            self.ephys_information[g][\"position\"] = re.split(\n                \";|,| \", self.ephys_information[g][\"position\"]\n            )\n        elif self.ephys_information[g][\"position\"] == \"\":\n            self.ephys_information[g][\"position\"] = None\n\n        electrode_groups[g] = nwbfile.create_electrode_group(\n            name=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\n            description=self.ephys_information[g][\"description\"],\n            position=self.ephys_information[g][\"position\"],\n            location=self.ephys_information[g][\"location\"],\n            device=device,\n        )\n\n        for idx in self.channel_map[g]:\n            nwbfile.add_electrode(\n                id=idx,\n                x=0.0,\n                y=0.0,\n                z=0.0,\n                imp=0.0,\n                location=self.ephys_information[g][\"location\"],\n                filtering=\"none\",\n                group=electrode_groups[g],\n            )\n\n    # Adding units\n    nwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\n    nwbfile.add_unit_column(\"group\", \"the group of the unit\")\n    for u in self.spikes.keys():\n        nwbfile.add_unit(\n            id=u,\n            spike_times=self.spikes[u].as_units(\"s\").index.values,\n            electrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\n            location=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n                \"location\"\n            ],\n            group=self.spikes.get_info(\"group\").loc[u],\n        )\n\n    io.write(nwbfile)\n    io.close()\n\n    return\n</code></pre>"},{"location":"io.phy/#pynapple.io.phy.Phy.load_nwb_spikes","title":"<code>load_nwb_spikes(path)</code>","text":"<p>Read the NWB spikes to extract the spike times.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_nwb_spikes(self, path):\n\"\"\"\n    Read the NWB spikes to extract the spike times.\n\n    Parameters\n    ----------\n    path : str\n        The path to the data\n\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n    self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n    self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n    io = NWBHDF5IO(self.nwbfilepath, \"r\")\n    nwbfile = io.read()\n\n    if nwbfile.units is None:\n        io.close()\n        return False\n    else:\n        units = nwbfile.units.to_dataframe()\n        spikes = {\n            n: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\n            for n in units.index\n        }\n\n        self.spikes = nap.TsGroup(\n            spikes,\n            time_support=self.time_support,\n            time_units=\"s\",\n            group=units[\"group\"],\n        )\n\n        if ~np.all(units[\"location\"] == \"\"):\n            self.spikes.set_info(location=units[\"location\"])\n\n        io.close()\n        return True\n</code></pre>"},{"location":"io.phy/#pynapple.io.phy.Phy.load_lfp","title":"<code>load_lfp(filename=None, channel=None, extension='.eeg', frequency=1250.0, precision='int16', bytes_size=2)</code>","text":"<p>Load the LFP.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str, optional</code> <p>The filename of the lfp file. It can be useful it multiple dat files are present in the data directory</p> <code>None</code> <code>channel</code> <code>int or list of int, optional</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>extension</code> <code>str, optional</code> <p>The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> <code>'.eeg'</code> <code>frequency</code> <code>float, optional</code> <p>Default 1250 Hz for the eeg file</p> <code>1250.0</code> <code>precision</code> <code>str, optional</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the lfp file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_lfp(\n    self,\n    filename=None,\n    channel=None,\n    extension=\".eeg\",\n    frequency=1250.0,\n    precision=\"int16\",\n    bytes_size=2,\n):\n\"\"\"\n    Load the LFP.\n\n    Parameters\n    ----------\n    filename : str, optional\n        The filename of the lfp file.\n        It can be useful it multiple dat files are present in the data directory\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    frequency : float, optional\n        Default 1250 Hz for the eeg file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the lfp file\n\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    \"\"\"\n    if filename is not None:\n        filepath = os.path.join(self.path, filename)\n    else:\n        listdir = os.listdir(self.path)\n        eegfile = [f for f in listdir if f.endswith(extension)]\n        if not len(eegfile):\n            raise RuntimeError(\n                \"Path {} contains no {} files;\".format(self.path, extension)\n            )\n\n        filepath = os.path.join(self.path, eegfile[0])\n\n    self.load_neurosuite_xml(self.path)\n\n    n_channels = int(self.nChannels)\n\n    f = open(filepath, \"rb\")\n    startoffile = f.seek(0, 0)\n    endoffile = f.seek(0, 2)\n    bytes_size = 2\n    n_samples = int((endoffile - startoffile) / n_channels / bytes_size)\n    duration = n_samples / frequency\n    f.close()\n    fp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\n    timestep = np.arange(0, n_samples) / frequency\n\n    time_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\n\n    if channel is None:\n        return nap.TsdFrame(\n            t=timestep, d=fp, time_units=\"s\", time_support=time_support\n        )\n    elif type(channel) is int:\n        return nap.Tsd(\n            t=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n        )\n    elif type(channel) is list:\n        return nap.TsdFrame(\n            t=timestep,\n            d=fp[:, channel],\n            time_units=\"s\",\n            time_support=time_support,\n            columns=channel,\n        )\n</code></pre>"},{"location":"io.suite2p/","title":"Suite2p","text":"<p>Loader for Suite2P https://github.com/MouseLand/suite2p</p>"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P","title":"<code>Suite2P</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for data processed with Suite2P.</p> <p>Pynapple will try to look for data in this order :</p> <ol> <li> <p>pynapplenwb/session_name.nwb</p> </li> <li> <p>suite2p/plane/.npy</p> </li> </ol> <p>Attributes:</p> Name Type Description <code>F</code> <code>TsdFrame</code> <p>Fluorescence traces (timepoints x ROIs) for all planes</p> <code>Fneu</code> <code>TsdFrame</code> <p>Neuropil fluorescence traces (timepoints x ROIs) for all planes</p> <code>spks</code> <code>TsdFrame</code> <p>Deconvolved traces (timepoints x ROIS) for all planes</p> <code>plane_info</code> <code>pandas.DataFrame</code> <p>Contains plane identity of each cell</p> <code>stats</code> <code>dict</code> <p>dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells (Can be smaller when loading from the NWB file)</p> <code>ops</code> <code>dict</code> <p>Parameters from Suite2p. (Can be smaller when loading from the NWB file)</p> <code>iscell</code> <code>numpy.ndarray</code> <p>Cell classification</p> Source code in <code>pynapple/io/suite2p.py</code> <pre><code>class Suite2P(BaseLoader):\n\"\"\"Loader for data processed with Suite2P.\n\n    Pynapple will try to look for data in this order :\n\n    1. pynapplenwb/session_name.nwb\n\n    2. suite2p/plane*/*.npy\n\n\n    Attributes\n    ----------\n    F : TsdFrame\n        Fluorescence traces (timepoints x ROIs) for all planes\n    Fneu : TsdFrame\n        Neuropil fluorescence traces (timepoints x ROIs) for all planes\n    spks : TsdFrame\n        Deconvolved traces (timepoints x ROIS) for all planes\n    plane_info : pandas.DataFrame\n        Contains plane identity of each cell\n    stats : dict\n        dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells\n        (Can be smaller when loading from the NWB file)\n    ops : dict\n        Parameters from Suite2p. (Can be smaller when loading from the NWB file)\n    iscell : numpy.ndarray\n        Cell classification\n    \"\"\"\n\n    def __init__(self, path):\n\"\"\"\n\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\n        self.basename = os.path.basename(path)\n\n        super().__init__(path)\n\n        # Need to check if nwb file exists and if data are there\n        loading_my_data = True\n        if self.path is not None:\n            nwb_path = os.path.join(self.path, \"pynapplenwb\")\n            if os.path.exists(nwb_path):\n                files = os.listdir(nwb_path)\n                if len([f for f in files if f.endswith(\".nwb\")]):\n                    success = self.load_suite2p_nwb(path)\n                    if success:\n                        loading_my_data = False\n\n        # Bypass if data have already been transfered to nwb\n        if loading_my_data:\n            app = App()\n            window = OphysGUI(app, path=path)\n            app.mainloop()\n            try:\n                app.update()\n            except Exception:\n                pass\n\n            if window.status:\n                self.ophys_information = window.ophys_information\n                self.load_suite2p(path)\n                self.save_suite2p_nwb(path)\n\n    def load_suite2p(self, path):\n\"\"\"\n        Looking for suite2/plane*\n\n        Parameters\n        ----------\n        path : str\n            The path of the session\n\n        \"\"\"\n        self.path_suite2p = os.path.join(path, \"suite2p\")\n\n        self.sampling_rate = float(\n            self.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n        )\n\n        data = {\n            \"F\": [],\n            \"Fneu\": [],\n            \"spks\": [],\n        }\n        plane_info = []\n\n        self.stats = {}\n        self.pops = {}\n        self.iscells = {}\n\n        self.planes = []\n\n        if os.path.exists(self.path_suite2p):\n            planes = glob.glob(os.path.join(self.path_suite2p, \"plane*\"))\n\n            if len(planes):\n                # count = 0\n                for plane_dir in planes:\n                    n = int(os.path.basename(plane_dir)[-1])\n                    self.planes.append(n)\n                    # Loading iscell.npy\n                    try:\n                        iscell = np.load(\n                            os.path.join(plane_dir, \"iscell.npy\"), allow_pickle=True\n                        )\n                        idx = np.where(iscell.astype(\"int\")[:, 0])[0]\n                        plane_info.append(np.ones(len(idx), dtype=\"int\") * n)\n\n                    except OSError as e:\n                        print(e)\n                        sys.exit()\n\n                    # Loading F.npy, Fneu.py and spks.npy\n                    for obj in [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]:\n                        try:\n                            name = obj.split(\".\")[0]\n                            tmp = np.load(\n                                os.path.join(plane_dir, obj), allow_pickle=True\n                            )\n                            data[name].append(tmp[idx])\n\n                        except OSError as e:\n                            print(e)\n                            sys.exit()\n\n                    # Loading stat.npy and ops.npy\n                    try:\n                        stat = np.load(\n                            os.path.join(plane_dir, \"stat.npy\"), allow_pickle=True\n                        )\n                        ops = np.load(\n                            os.path.join(plane_dir, \"ops.npy\"), allow_pickle=True\n                        ).item()\n                    except OSError as e:\n                        print(e)\n                        sys.exit()\n\n                    # Saving stat, ops and iscell\n                    self.stats[n] = stat\n                    self.pops[n] = ops\n                    self.iscells[n] = iscell\n\n                    # count += len(idx)\n\n            else:\n                warnings.warn(\n                    \"Couldn't find planes in %s\" % self.path_suite2p, stacklevel=2\n                )\n                sys.exit()\n        else:\n            warnings.warn(\"No suite2p folder in %s\" % path, stacklevel=2)\n            sys.exit()\n\n        # Calcium transients\n        data[\"F\"] = np.transpose(np.vstack(data[\"F\"]))\n        data[\"Fneu\"] = np.transpose(np.vstack(data[\"Fneu\"]))\n        data[\"spks\"] = np.transpose(np.vstack(data[\"spks\"]))\n\n        time_index = np.arange(0, len(data[\"F\"])) / self.sampling_rate\n\n        self.F = nap.TsdFrame(t=time_index, d=data[\"F\"])\n        self.Fneu = nap.TsdFrame(t=time_index, d=data[\"Fneu\"])\n        self.spks = nap.TsdFrame(t=time_index, d=data[\"spks\"])\n\n        self.ops = self.pops[0]\n        self.iscell = np.vstack([self.iscells[k] for k in self.iscells.keys()])\n\n        # Metadata\n        self.plane_info = pd.DataFrame.from_dict({\"plane\": np.hstack(plane_info)})\n        return\n\n    def save_suite2p_nwb(self, path):\n\"\"\"\n        Save the data to NWB. To ensure continuity, this function is based on :\n        https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.\n\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        multiplane = True if len(self.planes) &gt; 1 else False\n\n        ops = self.pops[list(self.pops.keys())[0]]\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n        nwbfile = io.read()\n\n        device = nwbfile.create_device(\n            name=self.ophys_information[\"device\"][\"name\"],\n            description=self.ophys_information[\"device\"][\"description\"],\n            manufacturer=self.ophys_information[\"device\"][\"manufacturer\"],\n        )\n        imaging_plane = nwbfile.create_imaging_plane(\n            name=self.ophys_information[\"ImagingPlane\"][\"name\"],\n            optical_channel=OpticalChannel(\n                name=self.ophys_information[\"OpticalChannel\"][\"name\"],\n                description=self.ophys_information[\"OpticalChannel\"][\"description\"],\n                emission_lambda=float(\n                    self.ophys_information[\"OpticalChannel\"][\"emission_lambda\"]\n                ),\n            ),\n            imaging_rate=self.sampling_rate,\n            description=self.ophys_information[\"ImagingPlane\"][\"description\"],\n            device=device,\n            excitation_lambda=float(\n                self.ophys_information[\"ImagingPlane\"][\"excitation_lambda\"]\n            ),\n            indicator=self.ophys_information[\"ImagingPlane\"][\"indicator\"],\n            location=self.ophys_information[\"ImagingPlane\"][\"location\"],\n            grid_spacing=([2.0, 2.0, 30.0] if multiplane else [2.0, 2.0]),\n            grid_spacing_unit=\"microns\",\n        )\n\n        # link to external data\n        image_series = TwoPhotonSeries(\n            name=\"TwoPhotonSeries\",\n            dimension=[ops[\"Ly\"], ops[\"Lx\"]],\n            external_file=(ops[\"filelist\"] if \"filelist\" in ops else [\"\"]),\n            imaging_plane=imaging_plane,\n            starting_frame=[0],\n            format=\"external\",\n            starting_time=0.0,\n            rate=ops[\"fs\"] * ops[\"nplanes\"],\n        )\n        nwbfile.add_acquisition(image_series)\n\n        # processing\n        img_seg = ImageSegmentation()\n        ps = img_seg.create_plane_segmentation(\n            name=self.ophys_information[\"PlaneSegmentation\"][\"name\"],\n            description=self.ophys_information[\"PlaneSegmentation\"][\"description\"],\n            imaging_plane=imaging_plane,\n            # reference_images=image_series,\n        )\n        ophys_module = nwbfile.create_processing_module(\n            name=\"ophys\", description=\"optical physiology processed data\"\n        )\n        ophys_module.add(img_seg)\n\n        file_strs = [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]\n        traces = []\n        ncells = np.zeros(len(self.pops), dtype=np.int_)\n        Nfr = np.array([self.pops[k][\"nframes\"] for k in self.pops.keys()]).max()\n\n        for iplane, ops in self.pops.items():\n            if iplane == 0:\n                iscell = self.iscells[iplane]\n                for fstr in file_strs:\n                    traces.append(np.load(os.path.join(ops[\"save_path\"], fstr)))\n                PlaneCellsIdx = iplane * np.ones(len(iscell))\n            else:\n                iscell = np.append(\n                    iscell,\n                    self.iscells[iplane],\n                    axis=0,\n                )\n                for i, fstr in enumerate(file_strs):\n                    trace = np.load(os.path.join(ops[\"save_path\"], fstr))\n                    if trace.shape[1] &lt; Nfr:\n                        fcat = np.zeros(\n                            (trace.shape[0], Nfr - trace.shape[1]), \"float32\"\n                        )\n                        trace = np.concatenate((trace, fcat), axis=1)\n                    traces[i] = np.append(traces[i], trace, axis=0)\n                PlaneCellsIdx = np.append(\n                    PlaneCellsIdx, iplane * np.ones(len(iscell) - len(PlaneCellsIdx))\n                )\n\n            stat = self.stats[iplane]\n            ncells[iplane] = len(stat)\n\n            for n in range(ncells[iplane]):\n                if multiplane:\n                    pixel_mask = np.array(\n                        [\n                            stat[n][\"ypix\"],\n                            stat[n][\"xpix\"],\n                            iplane * np.ones(stat[n][\"npix\"]),\n                            stat[n][\"lam\"],\n                        ]\n                    )\n                    ps.add_roi(voxel_mask=pixel_mask.T)\n                else:\n                    pixel_mask = np.array(\n                        [stat[n][\"ypix\"], stat[n][\"xpix\"], stat[n][\"lam\"]]\n                    )\n                    ps.add_roi(pixel_mask=pixel_mask.T)\n\n        ps.add_column(\"iscell\", \"two columns - iscell &amp; probcell\", iscell)\n\n        rt_region = []\n        for iplane, ops in self.pops.items():\n            if iplane == 0:\n                rt_region.append(\n                    ps.create_roi_table_region(\n                        region=list(\n                            np.arange(0, ncells[iplane]),\n                        ),\n                        description=f\"ROIs for plane{int(iplane)}\",\n                    )\n                )\n            else:\n                rt_region.append(\n                    ps.create_roi_table_region(\n                        region=list(\n                            np.arange(\n                                np.sum(ncells[:iplane]),\n                                ncells[iplane] + np.sum(ncells[:iplane]),\n                            )\n                        ),\n                        description=f\"ROIs for plane{int(iplane)}\",\n                    )\n                )\n\n        # FLUORESCENCE (all are required)\n        name_strs = [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n\n        for i, (fstr, nstr) in enumerate(zip(file_strs, name_strs)):\n            for iplane, ops in self.pops.items():\n                roi_resp_series = RoiResponseSeries(\n                    name=f\"plane{int(iplane)}\",\n                    data=traces[i][PlaneCellsIdx == iplane],\n                    rois=rt_region[iplane],\n                    unit=\"lumens\",\n                    rate=ops[\"fs\"],\n                )\n                if iplane == 0:\n                    fl = Fluorescence(roi_response_series=roi_resp_series, name=nstr)\n                else:\n                    fl.add_roi_response_series(roi_response_series=roi_resp_series)\n            ophys_module.add(fl)\n\n        io.write(nwbfile)\n        io.close()\n        return\n\n    def load_suite2p_nwb(self, path):\n\"\"\"\n        Load suite2p data from NWB\n\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\n        self.nwb_path = os.path.join(path, \"pynapplenwb\")\n        if not os.path.exists(self.nwb_path):\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, \"r\")\n        nwbfile = io.read()\n\n        if \"ophys\" in nwbfile.processing.keys():\n            ophys = nwbfile.processing[\"ophys\"]\n\n            #################################################################\n            # STATS, OPS and ISCELL\n            #################################################################\n            dims = nwbfile.acquisition[\"TwoPhotonSeries\"].dimension[:]\n            self.ops = {\"Ly\": dims[0], \"Lx\": dims[1]}\n            self.rate = nwbfile.acquisition[\n                \"TwoPhotonSeries\"\n            ].imaging_plane.imaging_rate\n\n            self.stats = {0: {}}\n            self.iscell = ophys[\"ImageSegmentation\"][\"PlaneSegmentation\"][\n                \"iscell\"\n            ].data[:]\n\n            info = pd.DataFrame(\n                data=self.iscell[:, 0].astype(\"int\"), columns=[\"iscell\"]\n            )\n\n            #################################################################\n            # ROIS\n            #################################################################\n            try:\n                rois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n                    \"PlaneSegmentation\"\n                ][\"pixel_mask\"]\n                multiplane = False\n            except Exception:\n                rois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n                    \"PlaneSegmentation\"\n                ][\"voxel_mask\"]\n                multiplane = True\n\n            idx = np.where(self.iscell[:, 0])[0]\n            info[\"plane\"] = 0\n\n            for n in range(len(rois)):\n                roi = pd.DataFrame(rois[n])\n                if \"z\" in roi.columns:\n                    pl = roi[\"z\"][0]\n                else:\n                    pl = 0\n\n                info.loc[n, \"plane\"] = pl\n\n                if pl not in self.stats.keys():\n                    self.stats[pl] = {}\n\n                if n in idx:\n                    self.stats[pl][n] = {\n                        \"xpix\": roi[\"y\"].values,\n                        \"ypix\": roi[\"x\"].values,\n                        \"lam\": roi[\"weight\"].values,\n                    }\n\n            #################################################################\n            # Time Series\n            #################################################################\n            fields = np.intersect1d(\n                [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"],\n                list(ophys.fields[\"data_interfaces\"].keys()),\n            )\n\n            if len(fields) == 0:\n                print(\n                    \"No \" + \" or \".join([\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]),\n                    \"found in nwb {}\".format(self.nwbfilepath),\n                )\n                return False\n\n            keys = ophys[fields[0]].roi_response_series.keys()\n\n            planes = [int(k[-1]) for k in keys if \"plane\" in k]\n\n            data = {}\n\n            if multiplane:\n                keys = ophys[fields[0]].roi_response_series.keys()\n                planes = [int(k[-1]) for k in keys if \"plane\" in k]\n            else:\n                planes = [0]\n\n            for k, name in zip(\n                [\"F\", \"Fneu\", \"spks\"], [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n            ):\n                tmp = []\n                timestamps = []\n\n                for i, n in enumerate(planes):\n                    if multiplane:\n                        pl = \"plane{}\".format(n)\n                    else:\n                        pl = name  # This doesn't make sense\n\n                    tokeep = info[\"iscell\"][info[\"plane\"] == n].values == 1\n\n                    d = np.transpose(ophys[name][pl].data[:][tokeep])\n\n                    if ophys[name][pl].timestamps is not None:\n                        t = ophys[name][pl].timestamps[:]\n                    else:\n                        t = (np.arange(0, len(d)) / self.rate) + ophys[name][\n                            pl\n                        ].starting_time\n\n                    tmp.append(d)\n                    timestamps.append(t)\n\n                data[k] = nap.TsdFrame(t=timestamps[0], d=np.hstack(tmp))\n\n            if \"F\" in data.keys():\n                self.F = data[\"F\"]\n            if \"Fneu\" in data.keys():\n                self.Fneu = data[\"Fneu\"]\n            if \"spks\" in data.keys():\n                self.spks = data[\"spks\"]\n\n            self.plane_info = pd.DataFrame(\n                data=info[\"plane\"][info[\"iscell\"] == 1].values, columns=[\"plane\"]\n            )\n\n            io.close()\n            return True\n        else:\n            io.close()\n            return False\n</code></pre>"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\n    self.basename = os.path.basename(path)\n\n    super().__init__(path)\n\n    # Need to check if nwb file exists and if data are there\n    loading_my_data = True\n    if self.path is not None:\n        nwb_path = os.path.join(self.path, \"pynapplenwb\")\n        if os.path.exists(nwb_path):\n            files = os.listdir(nwb_path)\n            if len([f for f in files if f.endswith(\".nwb\")]):\n                success = self.load_suite2p_nwb(path)\n                if success:\n                    loading_my_data = False\n\n    # Bypass if data have already been transfered to nwb\n    if loading_my_data:\n        app = App()\n        window = OphysGUI(app, path=path)\n        app.mainloop()\n        try:\n            app.update()\n        except Exception:\n            pass\n\n        if window.status:\n            self.ophys_information = window.ophys_information\n            self.load_suite2p(path)\n            self.save_suite2p_nwb(path)\n</code></pre>"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p","title":"<code>load_suite2p(path)</code>","text":"<p>Looking for suite2/plane*</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def load_suite2p(self, path):\n\"\"\"\n    Looking for suite2/plane*\n\n    Parameters\n    ----------\n    path : str\n        The path of the session\n\n    \"\"\"\n    self.path_suite2p = os.path.join(path, \"suite2p\")\n\n    self.sampling_rate = float(\n        self.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n    )\n\n    data = {\n        \"F\": [],\n        \"Fneu\": [],\n        \"spks\": [],\n    }\n    plane_info = []\n\n    self.stats = {}\n    self.pops = {}\n    self.iscells = {}\n\n    self.planes = []\n\n    if os.path.exists(self.path_suite2p):\n        planes = glob.glob(os.path.join(self.path_suite2p, \"plane*\"))\n\n        if len(planes):\n            # count = 0\n            for plane_dir in planes:\n                n = int(os.path.basename(plane_dir)[-1])\n                self.planes.append(n)\n                # Loading iscell.npy\n                try:\n                    iscell = np.load(\n                        os.path.join(plane_dir, \"iscell.npy\"), allow_pickle=True\n                    )\n                    idx = np.where(iscell.astype(\"int\")[:, 0])[0]\n                    plane_info.append(np.ones(len(idx), dtype=\"int\") * n)\n\n                except OSError as e:\n                    print(e)\n                    sys.exit()\n\n                # Loading F.npy, Fneu.py and spks.npy\n                for obj in [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]:\n                    try:\n                        name = obj.split(\".\")[0]\n                        tmp = np.load(\n                            os.path.join(plane_dir, obj), allow_pickle=True\n                        )\n                        data[name].append(tmp[idx])\n\n                    except OSError as e:\n                        print(e)\n                        sys.exit()\n\n                # Loading stat.npy and ops.npy\n                try:\n                    stat = np.load(\n                        os.path.join(plane_dir, \"stat.npy\"), allow_pickle=True\n                    )\n                    ops = np.load(\n                        os.path.join(plane_dir, \"ops.npy\"), allow_pickle=True\n                    ).item()\n                except OSError as e:\n                    print(e)\n                    sys.exit()\n\n                # Saving stat, ops and iscell\n                self.stats[n] = stat\n                self.pops[n] = ops\n                self.iscells[n] = iscell\n\n                # count += len(idx)\n\n        else:\n            warnings.warn(\n                \"Couldn't find planes in %s\" % self.path_suite2p, stacklevel=2\n            )\n            sys.exit()\n    else:\n        warnings.warn(\"No suite2p folder in %s\" % path, stacklevel=2)\n        sys.exit()\n\n    # Calcium transients\n    data[\"F\"] = np.transpose(np.vstack(data[\"F\"]))\n    data[\"Fneu\"] = np.transpose(np.vstack(data[\"Fneu\"]))\n    data[\"spks\"] = np.transpose(np.vstack(data[\"spks\"]))\n\n    time_index = np.arange(0, len(data[\"F\"])) / self.sampling_rate\n\n    self.F = nap.TsdFrame(t=time_index, d=data[\"F\"])\n    self.Fneu = nap.TsdFrame(t=time_index, d=data[\"Fneu\"])\n    self.spks = nap.TsdFrame(t=time_index, d=data[\"spks\"])\n\n    self.ops = self.pops[0]\n    self.iscell = np.vstack([self.iscells[k] for k in self.iscells.keys()])\n\n    # Metadata\n    self.plane_info = pd.DataFrame.from_dict({\"plane\": np.hstack(plane_info)})\n    return\n</code></pre>"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P.save_suite2p_nwb","title":"<code>save_suite2p_nwb(path)</code>","text":"<p>Save the data to NWB. To ensure continuity, this function is based on : https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def save_suite2p_nwb(self, path):\n\"\"\"\n    Save the data to NWB. To ensure continuity, this function is based on :\n    https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.\n\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n    self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n    self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n    multiplane = True if len(self.planes) &gt; 1 else False\n\n    ops = self.pops[list(self.pops.keys())[0]]\n\n    io = NWBHDF5IO(self.nwbfilepath, \"r+\")\n    nwbfile = io.read()\n\n    device = nwbfile.create_device(\n        name=self.ophys_information[\"device\"][\"name\"],\n        description=self.ophys_information[\"device\"][\"description\"],\n        manufacturer=self.ophys_information[\"device\"][\"manufacturer\"],\n    )\n    imaging_plane = nwbfile.create_imaging_plane(\n        name=self.ophys_information[\"ImagingPlane\"][\"name\"],\n        optical_channel=OpticalChannel(\n            name=self.ophys_information[\"OpticalChannel\"][\"name\"],\n            description=self.ophys_information[\"OpticalChannel\"][\"description\"],\n            emission_lambda=float(\n                self.ophys_information[\"OpticalChannel\"][\"emission_lambda\"]\n            ),\n        ),\n        imaging_rate=self.sampling_rate,\n        description=self.ophys_information[\"ImagingPlane\"][\"description\"],\n        device=device,\n        excitation_lambda=float(\n            self.ophys_information[\"ImagingPlane\"][\"excitation_lambda\"]\n        ),\n        indicator=self.ophys_information[\"ImagingPlane\"][\"indicator\"],\n        location=self.ophys_information[\"ImagingPlane\"][\"location\"],\n        grid_spacing=([2.0, 2.0, 30.0] if multiplane else [2.0, 2.0]),\n        grid_spacing_unit=\"microns\",\n    )\n\n    # link to external data\n    image_series = TwoPhotonSeries(\n        name=\"TwoPhotonSeries\",\n        dimension=[ops[\"Ly\"], ops[\"Lx\"]],\n        external_file=(ops[\"filelist\"] if \"filelist\" in ops else [\"\"]),\n        imaging_plane=imaging_plane,\n        starting_frame=[0],\n        format=\"external\",\n        starting_time=0.0,\n        rate=ops[\"fs\"] * ops[\"nplanes\"],\n    )\n    nwbfile.add_acquisition(image_series)\n\n    # processing\n    img_seg = ImageSegmentation()\n    ps = img_seg.create_plane_segmentation(\n        name=self.ophys_information[\"PlaneSegmentation\"][\"name\"],\n        description=self.ophys_information[\"PlaneSegmentation\"][\"description\"],\n        imaging_plane=imaging_plane,\n        # reference_images=image_series,\n    )\n    ophys_module = nwbfile.create_processing_module(\n        name=\"ophys\", description=\"optical physiology processed data\"\n    )\n    ophys_module.add(img_seg)\n\n    file_strs = [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]\n    traces = []\n    ncells = np.zeros(len(self.pops), dtype=np.int_)\n    Nfr = np.array([self.pops[k][\"nframes\"] for k in self.pops.keys()]).max()\n\n    for iplane, ops in self.pops.items():\n        if iplane == 0:\n            iscell = self.iscells[iplane]\n            for fstr in file_strs:\n                traces.append(np.load(os.path.join(ops[\"save_path\"], fstr)))\n            PlaneCellsIdx = iplane * np.ones(len(iscell))\n        else:\n            iscell = np.append(\n                iscell,\n                self.iscells[iplane],\n                axis=0,\n            )\n            for i, fstr in enumerate(file_strs):\n                trace = np.load(os.path.join(ops[\"save_path\"], fstr))\n                if trace.shape[1] &lt; Nfr:\n                    fcat = np.zeros(\n                        (trace.shape[0], Nfr - trace.shape[1]), \"float32\"\n                    )\n                    trace = np.concatenate((trace, fcat), axis=1)\n                traces[i] = np.append(traces[i], trace, axis=0)\n            PlaneCellsIdx = np.append(\n                PlaneCellsIdx, iplane * np.ones(len(iscell) - len(PlaneCellsIdx))\n            )\n\n        stat = self.stats[iplane]\n        ncells[iplane] = len(stat)\n\n        for n in range(ncells[iplane]):\n            if multiplane:\n                pixel_mask = np.array(\n                    [\n                        stat[n][\"ypix\"],\n                        stat[n][\"xpix\"],\n                        iplane * np.ones(stat[n][\"npix\"]),\n                        stat[n][\"lam\"],\n                    ]\n                )\n                ps.add_roi(voxel_mask=pixel_mask.T)\n            else:\n                pixel_mask = np.array(\n                    [stat[n][\"ypix\"], stat[n][\"xpix\"], stat[n][\"lam\"]]\n                )\n                ps.add_roi(pixel_mask=pixel_mask.T)\n\n    ps.add_column(\"iscell\", \"two columns - iscell &amp; probcell\", iscell)\n\n    rt_region = []\n    for iplane, ops in self.pops.items():\n        if iplane == 0:\n            rt_region.append(\n                ps.create_roi_table_region(\n                    region=list(\n                        np.arange(0, ncells[iplane]),\n                    ),\n                    description=f\"ROIs for plane{int(iplane)}\",\n                )\n            )\n        else:\n            rt_region.append(\n                ps.create_roi_table_region(\n                    region=list(\n                        np.arange(\n                            np.sum(ncells[:iplane]),\n                            ncells[iplane] + np.sum(ncells[:iplane]),\n                        )\n                    ),\n                    description=f\"ROIs for plane{int(iplane)}\",\n                )\n            )\n\n    # FLUORESCENCE (all are required)\n    name_strs = [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n\n    for i, (fstr, nstr) in enumerate(zip(file_strs, name_strs)):\n        for iplane, ops in self.pops.items():\n            roi_resp_series = RoiResponseSeries(\n                name=f\"plane{int(iplane)}\",\n                data=traces[i][PlaneCellsIdx == iplane],\n                rois=rt_region[iplane],\n                unit=\"lumens\",\n                rate=ops[\"fs\"],\n            )\n            if iplane == 0:\n                fl = Fluorescence(roi_response_series=roi_resp_series, name=nstr)\n            else:\n                fl.add_roi_response_series(roi_response_series=roi_resp_series)\n        ophys_module.add(fl)\n\n    io.write(nwbfile)\n    io.close()\n    return\n</code></pre>"},{"location":"io.suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p_nwb","title":"<code>load_suite2p_nwb(path)</code>","text":"<p>Load suite2p data from NWB</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def load_suite2p_nwb(self, path):\n\"\"\"\n    Load suite2p data from NWB\n\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\n    self.nwb_path = os.path.join(path, \"pynapplenwb\")\n    if not os.path.exists(self.nwb_path):\n        raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n\n    self.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\n    self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n    io = NWBHDF5IO(self.nwbfilepath, \"r\")\n    nwbfile = io.read()\n\n    if \"ophys\" in nwbfile.processing.keys():\n        ophys = nwbfile.processing[\"ophys\"]\n\n        #################################################################\n        # STATS, OPS and ISCELL\n        #################################################################\n        dims = nwbfile.acquisition[\"TwoPhotonSeries\"].dimension[:]\n        self.ops = {\"Ly\": dims[0], \"Lx\": dims[1]}\n        self.rate = nwbfile.acquisition[\n            \"TwoPhotonSeries\"\n        ].imaging_plane.imaging_rate\n\n        self.stats = {0: {}}\n        self.iscell = ophys[\"ImageSegmentation\"][\"PlaneSegmentation\"][\n            \"iscell\"\n        ].data[:]\n\n        info = pd.DataFrame(\n            data=self.iscell[:, 0].astype(\"int\"), columns=[\"iscell\"]\n        )\n\n        #################################################################\n        # ROIS\n        #################################################################\n        try:\n            rois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n                \"PlaneSegmentation\"\n            ][\"pixel_mask\"]\n            multiplane = False\n        except Exception:\n            rois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n                \"PlaneSegmentation\"\n            ][\"voxel_mask\"]\n            multiplane = True\n\n        idx = np.where(self.iscell[:, 0])[0]\n        info[\"plane\"] = 0\n\n        for n in range(len(rois)):\n            roi = pd.DataFrame(rois[n])\n            if \"z\" in roi.columns:\n                pl = roi[\"z\"][0]\n            else:\n                pl = 0\n\n            info.loc[n, \"plane\"] = pl\n\n            if pl not in self.stats.keys():\n                self.stats[pl] = {}\n\n            if n in idx:\n                self.stats[pl][n] = {\n                    \"xpix\": roi[\"y\"].values,\n                    \"ypix\": roi[\"x\"].values,\n                    \"lam\": roi[\"weight\"].values,\n                }\n\n        #################################################################\n        # Time Series\n        #################################################################\n        fields = np.intersect1d(\n            [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"],\n            list(ophys.fields[\"data_interfaces\"].keys()),\n        )\n\n        if len(fields) == 0:\n            print(\n                \"No \" + \" or \".join([\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]),\n                \"found in nwb {}\".format(self.nwbfilepath),\n            )\n            return False\n\n        keys = ophys[fields[0]].roi_response_series.keys()\n\n        planes = [int(k[-1]) for k in keys if \"plane\" in k]\n\n        data = {}\n\n        if multiplane:\n            keys = ophys[fields[0]].roi_response_series.keys()\n            planes = [int(k[-1]) for k in keys if \"plane\" in k]\n        else:\n            planes = [0]\n\n        for k, name in zip(\n            [\"F\", \"Fneu\", \"spks\"], [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n        ):\n            tmp = []\n            timestamps = []\n\n            for i, n in enumerate(planes):\n                if multiplane:\n                    pl = \"plane{}\".format(n)\n                else:\n                    pl = name  # This doesn't make sense\n\n                tokeep = info[\"iscell\"][info[\"plane\"] == n].values == 1\n\n                d = np.transpose(ophys[name][pl].data[:][tokeep])\n\n                if ophys[name][pl].timestamps is not None:\n                    t = ophys[name][pl].timestamps[:]\n                else:\n                    t = (np.arange(0, len(d)) / self.rate) + ophys[name][\n                        pl\n                    ].starting_time\n\n                tmp.append(d)\n                timestamps.append(t)\n\n            data[k] = nap.TsdFrame(t=timestamps[0], d=np.hstack(tmp))\n\n        if \"F\" in data.keys():\n            self.F = data[\"F\"]\n        if \"Fneu\" in data.keys():\n            self.Fneu = data[\"Fneu\"]\n        if \"spks\" in data.keys():\n            self.spks = data[\"spks\"]\n\n        self.plane_info = pd.DataFrame(\n            data=info[\"plane\"][info[\"iscell\"] == 1].values, columns=[\"plane\"]\n        )\n\n        io.close()\n        return True\n    else:\n        io.close()\n        return False\n</code></pre>"},{"location":"process.correlograms/","title":"Correlograms","text":""},{"location":"process.correlograms/#pynapple.process.correlograms.cross_correlogram","title":"<code>cross_correlogram(t1, t2, binsize, windowsize)</code>","text":"<p>Performs the discrete cross-correlogram of two time series. The units should be in s for all arguments. Return the firing rate of the series t2 relative to the timings of t1. See compute_crosscorrelogram, compute_autocorrelogram and compute_eventcorrelogram for wrappers of this function.</p> <p>Parameters:</p> Name Type Description Default <code>t1</code> <code>numpy.ndarray</code> <p>The timestamps of the reference time series (in seconds)</p> required <code>t2</code> <code>numpy.ndarray</code> <p>The timestamps of the target time series (in seconds)</p> required <code>binsize</code> <code>float</code> <p>The bin size (in seconds)</p> required <code>windowsize</code> <code>float</code> <p>The window size (in seconds)</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>The cross-correlogram</p> <code>numpy.ndarray</code> <p>Center of the bins (in s)</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>@jit(nopython=True)\ndef cross_correlogram(t1, t2, binsize, windowsize):\n\"\"\"\n    Performs the discrete cross-correlogram of two time series.\n    The units should be in s for all arguments.\n    Return the firing rate of the series t2 relative to the timings of t1.\n    See compute_crosscorrelogram, compute_autocorrelogram and compute_eventcorrelogram\n    for wrappers of this function.\n\n    Parameters\n    ----------\n    t1 : numpy.ndarray\n        The timestamps of the reference time series (in seconds)\n    t2 : numpy.ndarray\n        The timestamps of the target time series (in seconds)\n    binsize : float\n        The bin size (in seconds)\n    windowsize : float\n        The window size (in seconds)\n\n    Returns\n    -------\n    numpy.ndarray\n        The cross-correlogram\n    numpy.ndarray\n        Center of the bins (in s)\n\n    \"\"\"\n    # nbins = ((windowsize//binsize)*2)\n\n    nt1 = len(t1)\n    nt2 = len(t2)\n\n    nbins = int((windowsize * 2) // binsize)\n    if np.floor(nbins / 2) * 2 == nbins:\n        nbins = nbins + 1\n\n    w = (nbins / 2) * binsize\n    C = np.zeros(nbins)\n    i2 = 0\n\n    for i1 in range(nt1):\n        lbound = t1[i1] - w\n        while i2 &lt; nt2 and t2[i2] &lt; lbound:\n            i2 = i2 + 1\n        while i2 &gt; 0 and t2[i2 - 1] &gt; lbound:\n            i2 = i2 - 1\n\n        rbound = lbound\n        leftb = i2\n        for j in range(nbins):\n            k = 0\n            rbound = rbound + binsize\n            while leftb &lt; nt2 and t2[leftb] &lt; rbound:\n                leftb = leftb + 1\n                k = k + 1\n\n            C[j] += k\n\n    C = C / (nt1 * binsize)\n\n    m = -w + binsize / 2\n    B = np.zeros(nbins)\n    for j in range(nbins):\n        B[j] = m + j * binsize\n\n    return C, B\n</code></pre>"},{"location":"process.correlograms/#pynapple.process.correlograms.compute_autocorrelogram","title":"<code>compute_autocorrelogram(group, binsize, windowsize, ep=None, norm=True, time_units='s')</code>","text":"<p>Computes the autocorrelogram of a group of Ts/Tsd objects. The group can be passed directly as a TsGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects to auto-correlate</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which auto-corrs are computed. If None, the epoch is the time support of the group.</p> <code>None</code> <code>norm</code> <code>bool, optional</code> <p>If True, autocorrelograms are normalized to baseline (i.e. divided by the average rate) If False, autoorrelograms are returned as the rate (Hz) of the time series (relative to itself)</p> <code>True</code> <code>time_units</code> <code>str, optional</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_autocorrelogram(\n    group, binsize, windowsize, ep=None, norm=True, time_units=\"s\"\n):\n\"\"\"\n    Computes the autocorrelogram of a group of Ts/Tsd objects.\n    The group can be passed directly as a TsGroup object.\n\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects to auto-correlate\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which auto-corrs are computed.\n        If None, the epoch is the time support of the group.\n    norm : bool, optional\n         If True, autocorrelograms are normalized to baseline (i.e. divided by the average rate)\n         If False, autoorrelograms are returned as the rate (Hz) of the time series (relative to itself)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n\n    Returns\n    -------\n    pandas.DataFrame\n        _\n\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup\n    \"\"\"\n    if type(group) is nap.TsGroup:\n        if isinstance(ep, nap.IntervalSet):\n            newgroup = group.restrict(ep)\n        else:\n            newgroup = group\n    else:\n        raise RuntimeError(\"Unknown format for group\")\n\n    autocorrs = {}\n\n    binsize = nap.format_timestamps(np.array([binsize], dtype=np.float64), time_units)[\n        0\n    ]\n    windowsize = nap.format_timestamps(\n        np.array([windowsize], dtype=np.float64), time_units\n    )[0]\n\n    for n in newgroup.keys():\n        spk_time = newgroup[n].index.values\n        auc, times = cross_correlogram(spk_time, spk_time, binsize, windowsize)\n        autocorrs[n] = pd.Series(index=np.round(times, 6), data=auc, dtype=\"float\")\n\n    autocorrs = pd.DataFrame.from_dict(autocorrs)\n\n    if norm:\n        autocorrs = autocorrs / newgroup.get_info(\"rate\")\n\n    # Bug here\n    if 0 in autocorrs.index.values:\n        autocorrs.loc[0] = 0.0\n\n    return autocorrs.astype(\"float\")\n</code></pre>"},{"location":"process.correlograms/#pynapple.process.correlograms.compute_crosscorrelogram","title":"<code>compute_crosscorrelogram(group, binsize, windowsize, ep=None, norm=True, time_units='s', reverse=False)</code>","text":"<p>Computes all the pairwise cross-correlograms from a group of Ts/Tsd objects. The group can be passed directly as a TsGroup object. The reference Ts/Tsd and target are chosen based on the builtin itertools.combinations function. For example if indexes are [0,1,2], the function computes cross-correlograms for the pairs (0,1), (0, 2), and (1, 2). The left index gives the reference time series. To reverse the order, set reverse=True.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects to cross-correlate</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which cross-corrs are computed. If None, the epoch is the time support of the group.</p> <code>None</code> <code>norm</code> <code>bool, optional</code> <p>If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series) If False, cross-orrelograms are returned as the rate (Hz) of the target time series ((relative to the reference time series)</p> <code>True</code> <code>time_units</code> <code>str, optional</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <code>reverse</code> <code>bool, optional</code> <p>To reverse the pair order</p> <code>False</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_crosscorrelogram(\n    group, binsize, windowsize, ep=None, norm=True, time_units=\"s\", reverse=False\n):\n\"\"\"\n    Computes all the pairwise cross-correlograms from a group of Ts/Tsd objects.\n    The group can be passed directly as a TsGroup object.\n    The reference Ts/Tsd and target are chosen based on the builtin itertools.combinations function.\n    For example if indexes are [0,1,2], the function computes cross-correlograms\n    for the pairs (0,1), (0, 2), and (1, 2). The left index gives the reference time series.\n    To reverse the order, set reverse=True.\n\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects to cross-correlate\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which cross-corrs are computed.\n        If None, the epoch is the time support of the group.\n    norm : bool, optional\n        If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)\n        If False, cross-orrelograms are returned as the rate (Hz) of the target time series ((relative to the reference time series)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    reverse : bool, optional\n        To reverse the pair order\n\n    Returns\n    -------\n    pandas.DataFrame\n        _\n\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup\n\n    \"\"\"\n    if type(group) is nap.TsGroup:\n        if isinstance(ep, nap.IntervalSet):\n            newgroup = group.restrict(ep)\n        else:\n            newgroup = group\n    else:\n        raise RuntimeError(\"Unknown format for group\")\n\n    neurons = list(newgroup.keys())\n    pairs = list(combinations(neurons, 2))\n    if reverse:\n        pairs = list(map(lambda n: (n[1], n[0]), pairs))\n    crosscorrs = {}\n\n    binsize = nap.format_timestamps(np.array([binsize], dtype=np.float64), time_units)[\n        0\n    ]\n    windowsize = nap.format_timestamps(\n        np.array([windowsize], dtype=np.float64), time_units\n    )[0]\n\n    for i, j in pairs:\n        spk1 = newgroup[i].index.values\n        spk2 = newgroup[j].index.values\n        auc, times = cross_correlogram(spk1, spk2, binsize, windowsize)\n        crosscorrs[(i, j)] = pd.Series(index=times, data=auc, dtype=\"float\")\n\n    crosscorrs = pd.DataFrame.from_dict(crosscorrs)\n\n    if norm:\n        freq = newgroup.get_info(\"rate\")\n        freq2 = pd.Series(index=pairs, data=list(map(lambda n: freq.loc[n[1]], pairs)))\n        crosscorrs = crosscorrs / freq2\n\n    return crosscorrs.astype(\"float\")\n</code></pre>"},{"location":"process.correlograms/#pynapple.process.correlograms.compute_eventcorrelogram","title":"<code>compute_eventcorrelogram(group, event, binsize, windowsize, ep=None, norm=True, time_units='s')</code>","text":"<p>Computes the correlograms of a group of Ts/Tsd objects with another single Ts/Tsd object The time of reference is the event times.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects to correlate with the event</p> required <code>event</code> <code>Ts</code> <p>The event to correlate the each of the time series in the group with.</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which cross-corrs are computed. If None, the epoch is the time support of the event.</p> <code>None</code> <code>norm</code> <code>bool, optional</code> <p>If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series) If False, cross-orrelograms are returned as the rate (Hz) of the target time series (relative to the event time series)</p> <code>True</code> <code>time_units</code> <code>str, optional</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_eventcorrelogram(\n    group, event, binsize, windowsize, ep=None, norm=True, time_units=\"s\"\n):\n\"\"\"\n    Computes the correlograms of a group of Ts/Tsd objects with another single Ts/Tsd object\n    The time of reference is the event times.\n\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects to correlate with the event\n    event : Ts/Tsd\n        The event to correlate the each of the time series in the group with.\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which cross-corrs are computed.\n        If None, the epoch is the time support of the event.\n    norm : bool, optional\n        If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)\n        If False, cross-orrelograms are returned as the rate (Hz) of the target time series (relative to the event time series)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n\n    Returns\n    -------\n    pandas.DataFrame\n        _\n\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup\n\n    \"\"\"\n    if ep is None:\n        ep = event.time_support\n        tsd1 = event.index.values\n    else:\n        tsd1 = event.restrict(ep).index.values\n\n    if type(group) is nap.TsGroup:\n        newgroup = group.restrict(ep)\n    else:\n        raise RuntimeError(\"Unknown format for group\")\n\n    crosscorrs = {}\n\n    binsize = nap.format_timestamps(np.array([binsize], dtype=np.float64), time_units)[\n        0\n    ]\n    windowsize = nap.format_timestamps(\n        np.array([windowsize], dtype=np.float64), time_units\n    )[0]\n\n    for n in newgroup.keys():\n        spk_time = newgroup[n].index.values\n        auc, times = cross_correlogram(tsd1, spk_time, binsize, windowsize)\n        crosscorrs[n] = pd.Series(index=times, data=auc, dtype=\"float\")\n\n    crosscorrs = pd.DataFrame.from_dict(crosscorrs)\n\n    if norm:\n        crosscorrs = crosscorrs / newgroup.get_info(\"rate\")\n\n    return crosscorrs.astype(\"float\")\n</code></pre>"},{"location":"process.decoding/","title":"Decoding","text":""},{"location":"process.decoding/#pynapple.process.decoding.decode_1d","title":"<code>decode_1d(tuning_curves, group, ep, bin_size, time_units='s', feature=None)</code>","text":"<p>Performs Bayesian decoding over a one dimensional feature. See: Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.</p> <p>Parameters:</p> Name Type Description Default <code>tuning_curves</code> <code>pandas.DataFrame</code> <p>Each column is the tuning curve of one neuron relative to the feature. Index should be the center of the bin.</p> required <code>group</code> <code>TsGroup or dict of Ts</code> <p>A group of neurons with the same index as tuning curves column names.</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which decoding is computed</p> required <code>bin_size</code> <code>float</code> <p>Bin size. Default is second. Use the parameter time_units to change it.</p> required <code>time_units</code> <code>str, optional</code> <p>Time unit of the bin size ('s' [default], 'ms', 'us').</p> <code>'s'</code> <code>feature</code> <code>Tsd, optional</code> <p>The 1d feature used to compute the tuning curves. Used to correct for occupancy. If feature is not passed, the occupancy is uniform.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tsd</code> <p>The decoded feature</p> <code>TsdFrame</code> <p>The probability distribution of the decoded feature for each time bin</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a dict of Ts/Tsd or TsGroup. If different size of neurons for tuning_curves and group. If indexes don't match between tuning_curves and group.</p> Source code in <code>pynapple/process/decoding.py</code> <pre><code>def decode_1d(tuning_curves, group, ep, bin_size, time_units=\"s\", feature=None):\n\"\"\"\n    Performs Bayesian decoding over a one dimensional feature.\n    See:\n    Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J.\n    (1998). Interpreting neuronal population activity by\n    reconstruction: unified framework with application to\n    hippocampal place cells. Journal of neurophysiology, 79(2),\n    1017-1044.\n\n    Parameters\n    ----------\n    tuning_curves : pandas.DataFrame\n        Each column is the tuning curve of one neuron relative to the feature.\n        Index should be the center of the bin.\n    group : TsGroup or dict of Ts/Tsd object.\n        A group of neurons with the same index as tuning curves column names.\n    ep : IntervalSet\n        The epoch on which decoding is computed\n    bin_size : float\n        Bin size. Default is second. Use the parameter time_units to change it.\n    time_units : str, optional\n        Time unit of the bin size ('s' [default], 'ms', 'us').\n    feature : Tsd, optional\n        The 1d feature used to compute the tuning curves. Used to correct for occupancy.\n        If feature is not passed, the occupancy is uniform.\n\n    Returns\n    -------\n    Tsd\n        The decoded feature\n    TsdFrame\n        The probability distribution of the decoded feature for each time bin\n\n    Raises\n    ------\n    RuntimeError\n        If group is not a dict of Ts/Tsd or TsGroup.\n        If different size of neurons for tuning_curves and group.\n        If indexes don't match between tuning_curves and group.\n    \"\"\"\n    if isinstance(group, dict):\n        newgroup = nap.TsGroup(group, time_support=ep)\n    elif isinstance(group, nap.TsGroup):\n        newgroup = group.restrict(ep)\n    else:\n        raise RuntimeError(\"Unknown format for group\")\n\n    if tuning_curves.shape[1] != len(newgroup):\n        raise RuntimeError(\"Different shapes for tuning_curves and group\")\n\n    if not np.all(tuning_curves.columns.values == np.array(newgroup.keys())):\n        raise RuntimeError(\"Difference indexes for tuning curves and group keys\")\n\n    # Bin spikes\n    count = newgroup.count(bin_size, ep, time_units)\n\n    # Occupancy\n    if feature is None:\n        occupancy = np.ones(tuning_curves.shape[0])\n    elif isinstance(feature, nap.Tsd):\n        diff = np.diff(tuning_curves.index.values)\n        bins = tuning_curves.index.values[:-1] - diff / 2\n        bins = np.hstack(\n            (bins, [bins[-1] + diff[-1], bins[-1] + 2 * diff[-1]])\n        )  # assuming the size of the last 2 bins is equal\n        occupancy, _ = np.histogram(feature, bins)\n    else:\n        raise RuntimeError(\"Unknown format for feature in decode_1d\")\n\n    # Transforming to pure numpy array\n    tc = tuning_curves.values\n    ct = count.values\n\n    bin_size_s = nap.format_timestamps(\n        np.array([bin_size], dtype=np.float64), time_units\n    )[0]\n\n    p1 = np.exp(-bin_size_s * tc.sum(1))\n    p2 = occupancy / occupancy.sum()\n\n    ct2 = np.tile(ct[:, np.newaxis, :], (1, tc.shape[0], 1))\n\n    p3 = np.prod(tc**ct2, -1)\n\n    p = p1 * p2 * p3\n    p = p / p.sum(1)[:, np.newaxis]\n\n    idxmax = np.argmax(p, 1)\n\n    p = nap.TsdFrame(\n        t=count.index.values, d=p, time_support=ep, columns=tuning_curves.index.values\n    )\n\n    decoded = nap.Tsd(\n        t=count.index.values, d=tuning_curves.index.values[idxmax], time_support=ep\n    )\n\n    return decoded, p\n</code></pre>"},{"location":"process.decoding/#pynapple.process.decoding.decode_2d","title":"<code>decode_2d(tuning_curves, group, ep, bin_size, xy, time_units='s', features=None)</code>","text":"<p>Performs Bayesian decoding over a two dimensional feature. See: Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.</p> <p>Parameters:</p> Name Type Description Default <code>tuning_curves</code> <code>dict</code> <p>Dictionnay of 2d tuning curves (one for each neuron).</p> required <code>group</code> <code>TsGroup or dict of Ts</code> <p>A group of neurons with the same keys as tuning_curves dictionnary.</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which decoding is computed</p> required <code>bin_size</code> <code>float</code> <p>Bin size. Default is second. Use the parameter time_units to change it.</p> required <code>xy</code> <code>tuple</code> <p>A tuple of bin positions for the tuning curves i.e. xy=(x,y)</p> required <code>time_units</code> <code>str, optional</code> <p>Time unit of the bin size ('s' [default], 'ms', 'us').</p> <code>'s'</code> <code>features</code> <code>TsdFrame</code> <p>The 2 columns features used to compute the tuning curves. Used to correct for occupancy. If feature is not passed, the occupancy is uniform.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tsd</code> <p>The decoded feature in 2d</p> <code>numpy.ndarray</code> <p>The probability distribution of the decoded trajectory for each time bin</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a dict of Ts/Tsd or TsGroup. If different size of neurons for tuning_curves and group. If indexes don't match between tuning_curves and group.</p> Source code in <code>pynapple/process/decoding.py</code> <pre><code>def decode_2d(tuning_curves, group, ep, bin_size, xy, time_units=\"s\", features=None):\n\"\"\"\n    Performs Bayesian decoding over a two dimensional feature.\n    See:\n    Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J.\n    (1998). Interpreting neuronal population activity by\n    reconstruction: unified framework with application to\n    hippocampal place cells. Journal of neurophysiology, 79(2),\n    1017-1044.\n\n    Parameters\n    ----------\n    tuning_curves : dict\n        Dictionnay of 2d tuning curves (one for each neuron).\n    group : TsGroup or dict of Ts/Tsd object.\n        A group of neurons with the same keys as tuning_curves dictionnary.\n    ep : IntervalSet\n        The epoch on which decoding is computed\n    bin_size : float\n        Bin size. Default is second. Use the parameter time_units to change it.\n    xy : tuple\n        A tuple of bin positions for the tuning curves i.e. xy=(x,y)\n    time_units : str, optional\n        Time unit of the bin size ('s' [default], 'ms', 'us').\n    features : TsdFrame\n        The 2 columns features used to compute the tuning curves. Used to correct for occupancy.\n        If feature is not passed, the occupancy is uniform.\n\n    Returns\n    -------\n    Tsd\n        The decoded feature in 2d\n    numpy.ndarray\n        The probability distribution of the decoded trajectory for each time bin\n\n    Raises\n    ------\n    RuntimeError\n        If group is not a dict of Ts/Tsd or TsGroup.\n        If different size of neurons for tuning_curves and group.\n        If indexes don't match between tuning_curves and group.\n\n    \"\"\"\n\n    if type(group) is dict:\n        newgroup = nap.TsGroup(group, time_support=ep)\n        numcells = len(newgroup)\n    elif type(group) is nap.TsGroup:\n        newgroup = group.restrict(ep)\n        numcells = len(newgroup)\n    else:\n        raise RuntimeError(\"Unknown format for group\")\n\n    if len(tuning_curves) != numcells:\n        raise RuntimeError(\"Different shapes for tuning_curves and group\")\n\n    if not np.all(np.array(list(tuning_curves.keys())) == np.array(newgroup.keys())):\n        raise RuntimeError(\"Difference indexes for tuning curves and group keys\")\n\n    # Bin spikes\n    # if type(newgroup) is not nap.TsdFrame:\n    count = newgroup.count(bin_size, ep, time_units)\n    # else:\n    #     #Spikes already \"binned\" with continuous TsdFrame input\n    #     count = newgroup\n\n    indexes = list(tuning_curves.keys())\n\n    # Occupancy\n    if features is None:\n        occupancy = np.ones_like(tuning_curves[indexes[0]]).flatten()\n    else:\n        binsxy = []\n        for i in range(len(xy)):\n            diff = np.diff(xy[i])\n            bins = xy[i][:-1] - diff / 2\n            bins = np.hstack(\n                (bins, [bins[-1] + diff[-1], bins[-1] + 2 * diff[-1]])\n            )  # assuming the size of the last 2 bins is equal\n            binsxy.append(bins)\n\n        occupancy, _, _ = np.histogram2d(\n            features.iloc[:, 0], features.iloc[:, 1], [binsxy[0], binsxy[1]]\n        )\n        occupancy = occupancy.flatten()\n\n    # Transforming to pure numpy array\n    tc = np.array([tuning_curves[i] for i in tuning_curves.keys()])\n    tc = tc.reshape(tc.shape[0], np.prod(tc.shape[1:]))\n    tc = tc.T\n\n    ct = count.values\n\n    bin_size_s = nap.format_timestamps(\n        np.array([bin_size], dtype=np.float64), time_units\n    )[0]\n\n    p1 = np.exp(-bin_size_s * np.nansum(tc, 1))\n    p2 = occupancy / occupancy.sum()\n\n    ct2 = np.tile(ct[:, np.newaxis, :], (1, tc.shape[0], 1))\n\n    p3 = np.nanprod(tc**ct2, -1)\n\n    p = p1 * p2 * p3\n    p = p / p.sum(1)[:, np.newaxis]\n\n    idxmax = np.argmax(p, 1)\n\n    p = p.reshape(p.shape[0], len(xy[0]), len(xy[1]))\n\n    idxmax2d = np.unravel_index(idxmax, (len(xy[0]), len(xy[1])))\n\n    if features is not None:\n        cols = features.columns\n    else:\n        cols = np.arange(2)\n\n    decoded = nap.TsdFrame(\n        t=count.index.values,\n        d=np.vstack((xy[0][idxmax2d[0]], xy[1][idxmax2d[1]])).T,\n        time_support=ep,\n        columns=cols,\n    )\n\n    return decoded, p\n</code></pre>"},{"location":"process.perievent/","title":"Peri-Stimulus","text":""},{"location":"process.perievent/#pynapple.process.perievent.compute_perievent","title":"<code>compute_perievent(data, tref, minmax, time_unit='s')</code>","text":"<p>Center ts/tsd/tsgroup object around the timestamps given by the tref argument. minmax indicates the start and end of the window.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Ts</code> <p>The data to align to tref. If Ts/Tsd, returns a TsGroup. If TsGroup, returns a dictionnary of TsGroup</p> required <code>tref</code> <code>Ts</code> <p>The timestamps of the event to align to</p> required <code>minmax</code> <code>tuple or int or float</code> <p>The window size. Can be unequal on each side i.e. (-500, 1000).</p> required <code>time_unit</code> <code>str, optional</code> <p>Time units of the minmax ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A TsGroup if data is a Ts/Tsd or a dictionnary of TsGroup if data is a TsGroup.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if tref is not a Ts/Tsd object or if data is not a Ts/Tsd or TsGroup</p> Source code in <code>pynapple/process/perievent.py</code> <pre><code>def compute_perievent(data, tref, minmax, time_unit=\"s\"):\n\"\"\"\n    Center ts/tsd/tsgroup object around the timestamps given by the tref argument.\n    minmax indicates the start and end of the window.\n\n    Parameters\n    ----------\n    data : Ts/Tsd/TsGroup\n        The data to align to tref.\n        If Ts/Tsd, returns a TsGroup.\n        If TsGroup, returns a dictionnary of TsGroup\n    tref : Ts/Tsd\n        The timestamps of the event to align to\n    minmax : tuple or int or float\n        The window size. Can be unequal on each side i.e. (-500, 1000).\n    time_unit : str, optional\n        Time units of the minmax ('s' [default], 'ms', 'us').\n\n    Returns\n    -------\n    dict\n        A TsGroup if data is a Ts/Tsd or\n        a dictionnary of TsGroup if data is a TsGroup.\n\n    Raises\n    ------\n    RuntimeError\n        if tref is not a Ts/Tsd object or if data is not a Ts/Tsd or TsGroup\n    \"\"\"\n    if not isinstance(tref, (nap.Ts, nap.Tsd)):\n        raise RuntimeError(\"tref should be a Tsd object.\")\n\n    if isinstance(minmax, float) or isinstance(minmax, int):\n        minmax = np.array([minmax, minmax], dtype=np.float64)\n\n    window = np.abs(nap.format_timestamps(np.array(minmax), time_unit))\n\n    time_support = nap.IntervalSet(start=-window[0], end=window[1])\n\n    if isinstance(data, nap.TsGroup):\n        toreturn = {}\n\n        for n in data.index:\n            toreturn[n] = _align_tsd(data[n], tref, window, time_support)\n\n        return toreturn\n\n    elif isinstance(data, (nap.Ts, nap.Tsd)):\n        return _align_tsd(data, tref, window, time_support)\n\n    else:\n        raise RuntimeError(\"Unknown format for data\")\n</code></pre>"},{"location":"process.perievent/#pynapple.process.perievent.compute_event_trigger_average","title":"<code>compute_event_trigger_average(group, feature, binsize, windowsize, ep, time_units='s')</code>","text":"<p>Bin the spike train in binsize and compute the Spike Trigger Average (STA) within windowsize. If C is the spike count matrix and feature is a Tsd array, the function computes the Hankel matrix H from windowsize=(-t1,+t2) by offseting the Tsd array.</p> <p>The STA is then defined as the dot product between H and C divided by the number of spikes.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects that hold the trigger time.</p> required <code>feature</code> <code>Tsd</code> <p>The 1-dimensional feature to average</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which STA are computed</p> required <code>time_units</code> <code>str, optional</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>TsdFrame</code> <p>A TsdFrame of Spike-Trigger Average. Each column is an element from the group.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if group is not a Ts/Tsd or TsGroup</p> Source code in <code>pynapple/process/perievent.py</code> <pre><code>def compute_event_trigger_average(\n    group, feature, binsize, windowsize, ep, time_units=\"s\"\n):\n\"\"\"\n    Bin the spike train in binsize and compute the Spike Trigger Average (STA) within windowsize.\n    If C is the spike count matrix and feature is a Tsd array, the function computes\n    the Hankel matrix H from windowsize=(-t1,+t2) by offseting the Tsd array.\n\n    The STA is then defined as the dot product between H and C divided by the number of spikes.\n\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects that hold the trigger time.\n    feature : Tsd\n        The 1-dimensional feature to average\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which STA are computed\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n\n    Returns\n    -------\n    TsdFrame\n        A TsdFrame of Spike-Trigger Average. Each column is an element from the group.\n\n    Raises\n    ------\n    RuntimeError\n        if group is not a Ts/Tsd or TsGroup\n    \"\"\"\n    if type(group) is not nap.TsGroup:\n        raise RuntimeError(\"Unknown format for group\")\n\n    binsize = nap.format_timestamps(np.array([binsize], dtype=np.float64), time_units)[\n        0\n    ]\n    start = np.abs(\n        nap.format_timestamps(np.array([windowsize[0]], dtype=np.float64), time_units)[\n            0\n        ]\n    )\n    end = np.abs(\n        nap.format_timestamps(np.array([windowsize[1]], dtype=np.float64), time_units)[\n            0\n        ]\n    )\n    idx1 = -np.arange(0, start + binsize, binsize)[::-1][:-1]\n    idx2 = np.arange(0, end + binsize, binsize)[1:]\n    time_idx = np.hstack((idx1, np.zeros(1), idx2))\n\n    count = group.count(binsize, ep)\n\n    tmp = feature.bin_average(binsize, ep)\n\n    # Build the Hankel matrix\n    n_p = len(idx1)\n    n_f = len(idx2)\n    pad_tmp = np.pad(tmp, (n_p, n_f))\n    offset_tmp = hankel(pad_tmp, pad_tmp[-(n_p + n_f + 1) :])[0 : len(tmp)]\n\n    sta = np.dot(offset_tmp.T, count.values)\n\n    sta = sta / count.sum(0).values\n\n    sta = nap.TsdFrame(t=time_idx, d=sta, columns=group.index)\n\n    return sta\n</code></pre>"},{"location":"process.randomize/","title":"Randomization","text":""},{"location":"process.randomize/#pynapple.process.randomize.shift_timestamps","title":"<code>shift_timestamps(ts, min_shift=0.0, max_shift=None)</code>","text":"<p>Shifts all the time stamps of a random amount between min_shift and max_shift, wrapping the end of the time support to the beginning.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to shift. If TsGroup, shifts all Ts in the group independently.</p> required <code>min_shift</code> <code>float, optional</code> <p>minimum shift (default: 0 )</p> <code>0.0</code> <code>max_shift</code> <code>float, optional</code> <p>maximum shift, (default: length of time support)</p> <code>None</code> <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The randomly shifted timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def shift_timestamps(ts, min_shift=0.0, max_shift=None):\n\"\"\"\n    Shifts all the time stamps of a random amount between min_shift and max_shift, wrapping the\n    end of the time support to the beginning.\n\n\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to shift. If TsGroup, shifts all Ts in the group independently.\n    min_shift : float, optional\n        minimum shift (default: 0 )\n    max_shift : float, optional\n        maximum shift, (default: length of time support)\n\n    Returns\n    -------\n    Ts or TsGroup\n        The randomly shifted timestamps\n    \"\"\"\n    strategies = {\n        nap.time_series.Ts: _shift_ts,\n        nap.ts_group.TsGroup: _shift_tsgroup,\n    }\n    # checks input type\n    if type(ts) not in strategies.keys():\n        raise TypeError(\"Invalid input type, should be Ts or TsGroup\")\n\n    strategy = strategies[type(ts)]\n    return strategy(ts, min_shift, max_shift)\n</code></pre>"},{"location":"process.randomize/#pynapple.process.randomize.shuffle_ts_intervals","title":"<code>shuffle_ts_intervals(ts, min_shift=0.0, max_shift=None)</code>","text":"<p>Randomizes the timestamps by shuffling the intervals between them.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to randomize. If TsGroup, randomizes all Ts in the group independently.</p> required <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The randomized timestamps, with shuffled intervals</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def shuffle_ts_intervals(ts, min_shift=0.0, max_shift=None):\n\"\"\"\n    Randomizes the timestamps by shuffling the intervals between them.\n\n\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to randomize. If TsGroup, randomizes all Ts in the group independently.\n\n    Returns\n    -------\n    Ts or TsGroup\n        The randomized timestamps, with shuffled intervals\n    \"\"\"\n    strategies = {\n        nap.time_series.Ts: _shuffle_intervals_ts,\n        nap.ts_group.TsGroup: _shuffle_intervals_tsgroup,\n    }\n    # checks input type\n    if type(ts) not in strategies.keys():\n        raise TypeError(\"Invalid input type, should be Ts or TsGroup\")\n\n    strategy = strategies[type(ts)]\n    return strategy(ts)\n</code></pre>"},{"location":"process.randomize/#pynapple.process.randomize.jitter_timestamps","title":"<code>jitter_timestamps(ts, max_jitter=None, keep_tsupport=False)</code>","text":"<p>Jitters each time stamp independently of random amounts uniformly drawn between -max_jitter and max_jitter.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to jitter. If TsGroup, jitter is applied to each element of the group.</p> required <code>max_jitter</code> <code>float</code> <p>maximum jitter</p> <code>None</code> <code>keep_tsupport</code> <p>If True, keep time support of the input. The number of timestamps will not be conserved. If False, the time support is inferred from the jittered timestamps. The number of tmestamps is conserved. (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The jittered timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def jitter_timestamps(ts, max_jitter=None, keep_tsupport=False):\n\"\"\"\n    Jitters each time stamp independently of random amounts uniformly drawn between -max_jitter and max_jitter.\n\n\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to jitter. If TsGroup, jitter is applied to each element of the group.\n    max_jitter : float\n        maximum jitter\n    keep_tsupport: bool, optional\n        If True, keep time support of the input. The number of timestamps will not be conserved.\n        If False, the time support is inferred from the jittered timestamps. The number of tmestamps\n        is conserved. (default: False)\n\n    Returns\n    -------\n    Ts or TsGroup\n        The jittered timestamps\n    \"\"\"\n    strategies = {\n        nap.time_series.Ts: _jitter_ts,\n        nap.ts_group.TsGroup: _jitter_tsgroup,\n    }\n    # checks input type\n    if type(ts) not in strategies.keys():\n        raise TypeError(\"Invalid input type, should be Ts or TsGroup\")\n\n    if max_jitter is None:\n        raise TypeError(\"missing required argument: max_jitter \")\n\n    strategy = strategies[type(ts)]\n    return strategy(ts, max_jitter, keep_tsupport)\n</code></pre>"},{"location":"process.randomize/#pynapple.process.randomize.resample_timestamps","title":"<code>resample_timestamps(ts)</code>","text":"<p>Resamples the timestamps in the time support, with uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to resample. If TsGroup, each Ts object in the group is independently resampled, in the time support of the whole group.</p> required <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The resampled timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def resample_timestamps(ts):\n\"\"\"\n    Resamples the timestamps in the time support, with uniform distribution.\n\n\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to resample. If TsGroup, each Ts object in the group is independently\n        resampled, in the time support of the whole group.\n\n\n    Returns\n    -------\n    Ts or TsGroup\n        The resampled timestamps\n    \"\"\"\n    strategies = {\n        nap.time_series.Ts: _resample_ts,\n        nap.ts_group.TsGroup: _resample_tsgroup,\n    }\n    # checks input type\n    if type(ts) not in strategies.keys():\n        raise TypeError(\"Invalid input type, should be Ts or TsGroup\")\n\n    strategy = strategies[type(ts)]\n    return strategy(ts)\n</code></pre>"},{"location":"process.tuning_curves/","title":"Tuning curves","text":"<p>Summary</p>"},{"location":"process.tuning_curves/#pynapple.process.tuning_curves.compute_discrete_tuning_curves","title":"<code>compute_discrete_tuning_curves(group, dict_ep)</code>","text":"<pre><code>Compute discrete tuning curves of a TsGroup using a dictionnary of epochs.\n</code></pre> <p>The function returns a pandas DataFrame with each row being a key of the dictionnary of epochs and each column being a neurons.</p> <p>This function can typically being used for a set of stimulus being presented for multiple epochs. An example of the dictionnary is :</p> <pre><code>&gt;&gt;&gt; dict_ep =  {\n        \"stim0\": nap.IntervalSet(start=0, end=1),\n        \"stim1\":nap.IntervalSet(start=2, end=3)\n    }\n</code></pre> <p>In this case, the function will return a pandas DataFrame :</p> <pre><code>&gt;&gt;&gt; tc\n           neuron0    neuron1    neuron2\nstim0        0 Hz       1 Hz       2 Hz\nstim1        3 Hz       4 Hz       5 Hz\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>nap.TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>dict_ep</code> <code>dict</code> <p>Dictionary of IntervalSets</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>Table of firing rate for each neuron and each IntervalSet</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_discrete_tuning_curves(group, dict_ep):\n\"\"\"\n        Compute discrete tuning curves of a TsGroup using a dictionnary of epochs.\n    The function returns a pandas DataFrame with each row being a key of the dictionnary of epochs\n    and each column being a neurons.\n\n       This function can typically being used for a set of stimulus being presented for multiple epochs.\n    An example of the dictionnary is :\n\n        &gt;&gt;&gt; dict_ep =  {\n                \"stim0\": nap.IntervalSet(start=0, end=1),\n                \"stim1\":nap.IntervalSet(start=2, end=3)\n            }\n    In this case, the function will return a pandas DataFrame :\n\n        &gt;&gt;&gt; tc\n                   neuron0    neuron1    neuron2\n        stim0        0 Hz       1 Hz       2 Hz\n        stim1        3 Hz       4 Hz       5 Hz\n\n\n    Parameters\n    ----------\n    group : nap.TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    dict_ep : dict\n        Dictionary of IntervalSets\n\n    Returns\n    -------\n    pandas.DataFrame\n        Table of firing rate for each neuron and each IntervalSet\n\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object.\n    \"\"\"\n    if not isinstance(group, nap.TsGroup):\n        raise RuntimeError(\"Unknown format for group\")\n\n    idx = np.sort(list(dict_ep.keys()))\n\n    tuning_curves = pd.DataFrame(index=idx, columns=list(group.keys()), data=0)\n\n    for k in dict_ep.keys():\n        if not isinstance(dict_ep[k], nap.IntervalSet):\n            raise RuntimeError(\"Key {} in dict_ep is not an IntervalSet\".format(k))\n\n        for n in group.keys():\n            tuning_curves.loc[k, n] = float(len(group[n].restrict(dict_ep[k])))\n\n        tuning_curves.loc[k] = tuning_curves.loc[k] / dict_ep[k].tot_length(\"s\")\n\n    return tuning_curves\n</code></pre>"},{"location":"process.tuning_curves/#pynapple.process.tuning_curves.compute_1d_tuning_curves","title":"<code>compute_1d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None)</code>","text":"<p>Computes 1-dimensional tuning curves relative to a 1d feature.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>feature</code> <code>Tsd</code> <p>The 1-dimensional target feature (e.g. head-direction)</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curve</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>DataFrame to hold the tuning curves</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None):\n\"\"\"\n    Computes 1-dimensional tuning curves relative to a 1d feature.\n\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    feature : Tsd\n        The 1-dimensional target feature (e.g. head-direction)\n    nb_bins : int\n        Number of bins in the tuning curve\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame to hold the tuning curves\n\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object.\n\n    \"\"\"\n    if not isinstance(group, nap.TsGroup):\n        raise RuntimeError(\"Unknown format for group\")\n\n    if minmax is None:\n        bins = np.linspace(np.min(feature), np.max(feature), nb_bins + 1)\n    else:\n        bins = np.linspace(minmax[0], minmax[1], nb_bins + 1)\n    idx = bins[0:-1] + np.diff(bins) / 2\n\n    tuning_curves = pd.DataFrame(index=idx, columns=list(group.keys()))\n\n    if isinstance(ep, nap.IntervalSet):\n        group_value = group.value_from(feature, ep)\n        occupancy, _ = np.histogram(feature.restrict(ep).values, bins)\n    else:\n        group_value = group.value_from(feature)\n        occupancy, _ = np.histogram(feature.values, bins)\n\n    for k in group_value:\n        count, _ = np.histogram(group_value[k].values, bins)\n        count = count / occupancy\n        count[np.isnan(count)] = 0.0\n        tuning_curves[k] = count\n        tuning_curves[k] = count * feature.rate\n\n    return tuning_curves\n</code></pre>"},{"location":"process.tuning_curves/#pynapple.process.tuning_curves.compute_2d_tuning_curves","title":"<code>compute_2d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None)</code>","text":"<p>Computes 2-dimensional tuning curves relative to a 2d feature</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>feature</code> <code>TsdFrame</code> <p>The 2d feature (i.e. 2 columns features).</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curves</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves given as: (minx, maxx, miny, maxy) If None, the boundaries are inferred from the target variable</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: </p> <p>tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).</p> <p>xy (list): List of bins center in the two dimensions</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object or if feature is not 2 columns only.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None):\n\"\"\"\n    Computes 2-dimensional tuning curves relative to a 2d feature\n\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    feature : TsdFrame\n        The 2d feature (i.e. 2 columns features).\n    nb_bins : int\n        Number of bins in the tuning curves\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves given as:\n        (minx, maxx, miny, maxy)\n        If None, the boundaries are inferred from the target variable\n\n    Returns\n    -------\n    tuple\n        A tuple containing: \\n\n        tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).\\n\n        xy (list): List of bins center in the two dimensions\n\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object or if feature is not 2 columns only.\n\n    \"\"\"\n    if feature.shape[1] != 2:\n        raise RuntimeError(\"feature should have 2 columns only.\")\n\n    if type(group) is not nap.TsGroup:\n        raise RuntimeError(\"Unknown format for group\")\n\n    if isinstance(ep, nap.IntervalSet):\n        feature = feature.restrict(ep)\n    else:\n        ep = feature.time_support\n\n    cols = list(feature.columns)\n    groups_value = {}\n    binsxy = {}\n\n    for i, c in enumerate(cols):\n        groups_value[c] = group.value_from(feature[c], ep)\n        if minmax is None:\n            bins = np.linspace(np.min(feature[c]), np.max(feature[c]), nb_bins + 1)\n        else:\n            bins = np.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins + 1)\n        binsxy[c] = bins\n\n    occupancy, _, _ = np.histogram2d(\n        feature[cols[0]].values,\n        feature[cols[1]].values,\n        [binsxy[cols[0]], binsxy[cols[1]]],\n    )\n\n    tc = {}\n    for n in group.keys():\n        count, _, _ = np.histogram2d(\n            groups_value[cols[0]][n].values,\n            groups_value[cols[1]][n].values,\n            [binsxy[cols[0]], binsxy[cols[1]]],\n        )\n        count = count / occupancy\n        # count[np.isnan(count)] = 0.0\n        tc[n] = count * feature.rate\n\n    xy = [binsxy[c][0:-1] + np.diff(binsxy[c]) / 2 for c in binsxy.keys()]\n\n    return tc, xy\n</code></pre>"},{"location":"process.tuning_curves/#pynapple.process.tuning_curves.compute_1d_mutual_info","title":"<code>compute_1d_mutual_info(tc, feature, ep=None, minmax=None, bitssec=False)</code>","text":"<p>Mutual information as defined in</p> <p>Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993). An information-theoretic approach to deciphering the hippocampal code. In Advances in neural information processing systems (pp. 1030-1037).</p> <p>Parameters:</p> Name Type Description Default <code>tc</code> <code>pandas.DataFrame or numpy.ndarray</code> <p>Tuning curves in columns</p> required <code>feature</code> <code>Tsd</code> <p>The feature that was used to compute the tuning curves</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch over which the tuning curves were computed If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <code>bitssec</code> <code>bool, optional</code> <p>By default, the function return bits per spikes. Set to true for bits per seconds</p> <code>False</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>Spatial Information (default is bits/spikes)</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_mutual_info(tc, feature, ep=None, minmax=None, bitssec=False):\n\"\"\"\n    Mutual information as defined in\n\n    Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993).\n    An information-theoretic approach to deciphering the hippocampal code.\n    In Advances in neural information processing systems (pp. 1030-1037).\n\n    Parameters\n    ----------\n    tc : pandas.DataFrame or numpy.ndarray\n        Tuning curves in columns\n    feature : Tsd\n        The feature that was used to compute the tuning curves\n    ep : IntervalSet, optional\n        The epoch over which the tuning curves were computed\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n    bitssec : bool, optional\n        By default, the function return bits per spikes.\n        Set to true for bits per seconds\n\n    Returns\n    -------\n    pandas.DataFrame\n        Spatial Information (default is bits/spikes)\n    \"\"\"\n    if isinstance(tc, pd.DataFrame):\n        columns = tc.columns.values\n        fx = np.atleast_2d(tc.values)\n    elif isinstance(tc, np.ndarray):\n        fx = np.atleast_2d(tc)\n        columns = np.arange(tc.shape[1])\n\n    nb_bins = tc.shape[0] + 1\n    if minmax is None:\n        bins = np.linspace(np.min(feature), np.max(feature), nb_bins)\n    else:\n        bins = np.linspace(minmax[0], minmax[1], nb_bins)\n\n    if isinstance(ep, nap.IntervalSet):\n        occupancy, _ = np.histogram(feature.restrict(ep).values, bins)\n    else:\n        occupancy, _ = np.histogram(feature.values, bins)\n    occupancy = occupancy / occupancy.sum()\n    occupancy = occupancy[:, np.newaxis]\n\n    fr = np.sum(fx * occupancy, 0)\n    fxfr = fx / fr\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        logfx = np.log2(fxfr)\n    logfx[np.isinf(logfx)] = 0.0\n    SI = np.sum(occupancy * fx * logfx, 0)\n\n    if bitssec:\n        SI = pd.DataFrame(index=columns, columns=[\"SI\"], data=SI)\n        return SI\n    else:\n        SI = SI / fr\n        SI = pd.DataFrame(index=columns, columns=[\"SI\"], data=SI)\n        return SI\n</code></pre>"},{"location":"process.tuning_curves/#pynapple.process.tuning_curves.compute_2d_mutual_info","title":"<code>compute_2d_mutual_info(tc, features, ep=None, minmax=None, bitssec=False)</code>","text":"<p>Mutual information as defined in</p> <p>Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993). An information-theoretic approach to deciphering the hippocampal code. In Advances in neural information processing systems (pp. 1030-1037).</p> <p>Parameters:</p> Name Type Description Default <code>tc</code> <code>dict or numpy.ndarray</code> <p>If array, first dimension should be the neuron</p> required <code>features</code> <code>TsdFrame</code> <p>The 2 columns features that were used to compute the tuning curves</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch over which the tuning curves were computed If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target features</p> <code>None</code> <code>bitssec</code> <code>bool, optional</code> <p>By default, the function return bits per spikes. Set to true for bits per seconds</p> <code>False</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>Spatial Information (default is bits/spikes)</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_mutual_info(tc, features, ep=None, minmax=None, bitssec=False):\n\"\"\"\n    Mutual information as defined in\n\n    Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993).\n    An information-theoretic approach to deciphering the hippocampal code.\n    In Advances in neural information processing systems (pp. 1030-1037).\n\n    Parameters\n    ----------\n    tc : dict or numpy.ndarray\n        If array, first dimension should be the neuron\n    features : TsdFrame\n        The 2 columns features that were used to compute the tuning curves\n    ep : IntervalSet, optional\n        The epoch over which the tuning curves were computed\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target features\n    bitssec : bool, optional\n        By default, the function return bits per spikes.\n        Set to true for bits per seconds\n\n    Returns\n    -------\n    pandas.DataFrame\n        Spatial Information (default is bits/spikes)\n    \"\"\"\n    # A bit tedious here\n    if type(tc) is dict:\n        fx = np.array([tc[i] for i in tc.keys()])\n        idx = list(tc.keys())\n    elif type(tc) is np.ndarray:\n        fx = tc\n        idx = np.arange(len(tc))\n\n    nb_bins = (fx.shape[1] + 1, fx.shape[2] + 1)\n\n    cols = features.columns\n\n    bins = []\n    for i, c in enumerate(cols):\n        if minmax is None:\n            bins.append(\n                np.linspace(np.min(features[c]), np.max(features[c]), nb_bins[i])\n            )\n        else:\n            bins.append(\n                np.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins[i])\n            )\n\n    if isinstance(ep, nap.IntervalSet):\n        features = features.restrict(ep)\n\n    occupancy, _, _ = np.histogram2d(\n        features[cols[0]].values, features[cols[1]].values, [bins[0], bins[1]]\n    )\n    occupancy = occupancy / occupancy.sum()\n\n    fr = np.nansum(fx * occupancy, (1, 2))\n    fr = fr[:, np.newaxis, np.newaxis]\n    fxfr = fx / fr\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        logfx = np.log2(fxfr)\n    logfx[np.isinf(logfx)] = 0.0\n    SI = np.nansum(occupancy * fx * logfx, (1, 2))\n\n    if bitssec:\n        SI = pd.DataFrame(index=idx, columns=[\"SI\"], data=SI)\n        return SI\n    else:\n        SI = SI / fr[:, 0, 0]\n        SI = pd.DataFrame(index=idx, columns=[\"SI\"], data=SI)\n        return SI\n</code></pre>"},{"location":"process.tuning_curves/#pynapple.process.tuning_curves.compute_1d_tuning_curves_continous","title":"<code>compute_1d_tuning_curves_continous(tsdframe, feature, nb_bins, ep=None, minmax=None)</code>","text":"<p>Computes 1-dimensional tuning curves relative to a feature with continous data.</p> <p>Parameters:</p> Name Type Description Default <code>tsdframe</code> <code>Tsd or TsdFrame</code> <p>Input data (e.g. continus calcium data where each column is the calcium activity of one neuron)</p> required <code>feature</code> <code>Tsd</code> <p>The feature (one column)</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curves</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>DataFrame to hold the tuning curves</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tsdframe is not a Tsd or a TsdFrame object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_tuning_curves_continous(\n    tsdframe, feature, nb_bins, ep=None, minmax=None\n):\n\"\"\"\n    Computes 1-dimensional tuning curves relative to a feature with continous data.\n\n    Parameters\n    ----------\n    tsdframe : Tsd or TsdFrame\n        Input data (e.g. continus calcium data\n        where each column is the calcium activity of one neuron)\n    feature : Tsd\n        The feature (one column)\n    nb_bins : int\n        Number of bins in the tuning curves\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame to hold the tuning curves\n\n    Raises\n    ------\n    RuntimeError\n        If tsdframe is not a Tsd or a TsdFrame object.\n\n    \"\"\"\n    if not isinstance(tsdframe, (nap.Tsd, nap.TsdFrame)):\n        raise RuntimeError(\"Unknown format for tsdframe.\")\n\n    if isinstance(ep, nap.IntervalSet):\n        feature = feature.restrict(ep)\n        tsdframe = tsdframe.restrict(ep)\n    else:\n        tsdframe = tsdframe.restrict(feature.time_support)\n\n    if minmax is None:\n        bins = np.linspace(np.min(feature), np.max(feature), nb_bins + 1)\n    else:\n        bins = np.linspace(minmax[0], minmax[1], nb_bins + 1)\n\n    align_times = tsdframe.value_from(feature)\n    idx = np.digitize(align_times.values, bins) - 1\n    tmp = pd.DataFrame(tsdframe).groupby(idx).mean()\n    tmp = tmp.reindex(np.arange(0, len(bins) - 1))\n    tmp.index = pd.Index(bins[0:-1] + np.diff(bins) / 2)\n\n    tmp = tmp.fillna(0)\n\n    return pd.DataFrame(tmp)\n</code></pre>"},{"location":"process.tuning_curves/#pynapple.process.tuning_curves.compute_2d_tuning_curves_continuous","title":"<code>compute_2d_tuning_curves_continuous(tsdframe, features, nb_bins, ep=None, minmax=None)</code>","text":"<p>Computes 2-dimensional tuning curves relative to a 2d feature with continous data.</p> <p>Parameters:</p> Name Type Description Default <code>tsdframe</code> <code>Tsd or TsdFrame</code> <p>Input data (e.g. continuous calcium data where each column is the calcium activity of one neuron)</p> required <code>features</code> <code>TsdFrame</code> <p>The 2d feature (two columns)</p> required <code>nb_bins</code> <code>int or tuple</code> <p>Number of bins in the tuning curves (separate for 2 feature dimensions if tuple provided)</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. Should be a tuple of minx, maxx, miny, maxy If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: </p> <p>tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).</p> <p>xy (list): List of bins center in the two dimensions</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tsdframe is not a Tsd/TsdFrame or if features is not 2 columns</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_tuning_curves_continuous(\n    tsdframe, features, nb_bins, ep=None, minmax=None\n):\n\"\"\"\n    Computes 2-dimensional tuning curves relative to a 2d feature with continous data.\n\n    Parameters\n    ----------\n    tsdframe : Tsd or TsdFrame\n        Input data (e.g. continuous calcium data\n        where each column is the calcium activity of one neuron)\n    features : TsdFrame\n        The 2d feature (two columns)\n    nb_bins : int or tuple\n        Number of bins in the tuning curves (separate for 2 feature dimensions if tuple provided)\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        Should be a tuple of minx, maxx, miny, maxy\n        If None, the boundaries are inferred from the target feature\n\n    Returns\n    -------\n    tuple\n        A tuple containing: \\n\n        tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).\\n\n        xy (list): List of bins center in the two dimensions\n\n    Raises\n    ------\n    RuntimeError\n        If tsdframe is not a Tsd/TsdFrame or if features is not 2 columns\n\n    \"\"\"\n    if not isinstance(tsdframe, (nap.Tsd, nap.TsdFrame)):\n        raise RuntimeError(\"Unknown format for tsdframe.\")\n\n    if not isinstance(features, nap.TsdFrame):\n        raise RuntimeError(\"Unknown format for features.\")\n\n    if isinstance(ep, nap.IntervalSet):\n        features = features.restrict(ep)\n        tsdframe = tsdframe.restrict(ep)\n    else:\n        tsdframe = tsdframe.restrict(features.time_support)\n\n    if features.shape[1] != 2:\n        raise RuntimeError(\"features input is not 2 columns.\")\n\n    if isinstance(nb_bins, int):\n        nb_bins = (nb_bins, nb_bins)\n    elif len(nb_bins) != 2:\n        raise RuntimeError(\"nb_bins should be int or tuple of 2 ints\")\n\n    cols = list(features.columns)\n\n    binsxy = {}\n    idxs = {}\n\n    for i, c in enumerate(cols):\n        if minmax is None:\n            bins = np.linspace(np.min(features[c]), np.max(features[c]), nb_bins[i] + 1)\n        else:\n            bins = np.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins[i] + 1)\n\n        align_times = tsdframe.value_from(features[c], ep)\n        idxs[c] = np.digitize(align_times.values, bins) - 1\n        binsxy[c] = bins\n\n    idxs = pd.DataFrame(idxs)\n\n    tc_np = np.zeros((tsdframe.shape[1], nb_bins[0], nb_bins[1])) * np.nan\n\n    for k, tmp in idxs.groupby(cols):\n        if (0 &lt;= k[0] &lt; nb_bins[0]) and (0 &lt;= k[1] &lt; nb_bins[1]):\n            tc_np[:, k[0], k[1]] = tsdframe.iloc[tmp.index].mean(0).values\n\n    tc_np[np.isnan(tc_np)] = 0.0\n\n    xy = [binsxy[c][0:-1] + np.diff(binsxy[c]) / 2 for c in binsxy.keys()]\n\n    tc = {c: tc_np[i] for i, c in enumerate(tsdframe.columns)}\n\n    return tc, xy\n</code></pre>"},{"location":"process.tuning_curves/#pynapple.process.tuning_curves.compute_1d_poisson_glm","title":"<code>compute_1d_poisson_glm(group, feature, binsize, windowsize, ep, time_units='s', niter=100, tolerance=1e-05)</code>","text":"<p>Poisson GLM</p> <p>Warning : this function is still experimental!</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>Spike trains</p> required <code>feature</code> <code>Tsd</code> <p>The regressors</p> required <code>binsize</code> <code>float</code> <p>Bin size</p> required <code>windowsize</code> <code>Float</code> <p>The window for offsetting the regressors</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>On which epoch to perfom the GLM</p> required <code>time_units</code> <code>str, optional</code> <p>Time units of binsize and windowsize</p> <code>'s'</code> <code>niter</code> <code>int, optional</code> <p>Number of iteration for fitting the GLM</p> <code>100</code> <code>tolerance</code> <code>float, optional</code> <p>Tolerance for stopping the IRLS</p> <code>1e-05</code> <p>Returns:</p> Type Description <code>tuple</code> <p>regressors : TsdFrame</p> <p>offset : pandas.Series</p> <p>prediction : TsdFrame</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if group is not a TsGroup</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_poisson_glm(\n    group, feature, binsize, windowsize, ep, time_units=\"s\", niter=100, tolerance=1e-5\n):\n\"\"\"\n    Poisson GLM\n\n    Warning : this function is still experimental!\n\n    Parameters\n    ----------\n    group : TsGroup\n        Spike trains\n    feature : Tsd\n        The regressors\n    binsize : float\n        Bin size\n    windowsize : Float\n        The window for offsetting the regressors\n    ep : IntervalSet, optional\n        On which epoch to perfom the GLM\n    time_units : str, optional\n        Time units of binsize and windowsize\n    niter : int, optional\n        Number of iteration for fitting the GLM\n    tolerance : float, optional\n        Tolerance for stopping the IRLS\n\n    Returns\n    -------\n    tuple\n        regressors : TsdFrame\\n\n        offset : pandas.Series\\n\n        prediction : TsdFrame\\n\n\n    Raises\n    ------\n    RuntimeError\n        if group is not a TsGroup\n\n    \"\"\"\n    if type(group) is nap.TsGroup:\n        newgroup = group.restrict(ep)\n    else:\n        raise RuntimeError(\"Unknown format for group\")\n\n    binsize = nap.format_timestamps(binsize, time_units)[0]\n    windowsize = nap.format_timestamps(windowsize, time_units)[0]\n\n    # Bin the spike train\n    count = newgroup.count(binsize)\n\n    # Downsample the feature to binsize\n    tidx = []\n    dfeat = []\n    for i in ep.index:\n        bins = np.arange(ep.start[i], ep.end[i] + binsize, binsize)\n        idx = np.digitize(feature.index.values, bins) - 1\n        tmp = feature.groupby(idx).mean()\n        tidx.append(bins[0:-1] + np.diff(bins) / 2)\n        dfeat.append(tmp)\n    dfeat = nap.Tsd(t=np.hstack(tidx), d=np.hstack(dfeat), time_support=ep)\n\n    # Build the Hankel matrix\n    nt = np.abs(windowsize // binsize).astype(\"int\") + 1\n    X = hankel(\n        np.hstack((np.zeros(nt - 1), dfeat.values))[: -nt + 1], dfeat.values[-nt:]\n    )\n    X = np.hstack((np.ones((len(dfeat), 1)), X))\n\n    # Fitting GLM for each neuron\n    regressors = []\n    for i, n in enumerate(group.keys()):\n        print(\"Fitting Poisson GLM for unit %i\" % n)\n        b = nap.jitted_functions.jit_poisson_IRLS(\n            X, count[n].values, niter=niter, tolerance=tolerance\n        )\n        regressors.append(b)\n\n    regressors = np.array(regressors).T\n    offset = regressors[0]\n    regressors = regressors[1:]\n    regressors = nap.TsdFrame(\n        t=np.arange(-nt + 1, 1) * binsize, d=regressors, columns=list(group.keys())\n    )\n    offset = pd.Series(index=group.keys(), data=offset)\n\n    prediction = nap.TsdFrame(\n        t=dfeat.index.values,\n        d=np.exp(np.dot(X[:, 1:], regressors.values) + offset.values) * binsize,\n    )\n\n    return (regressors, offset, prediction)\n</code></pre>"},{"location":"pynacollada/","title":"pynacollada","text":""},{"location":"pynacollada/#python-neural-analysis-collaborative-repository","title":"Python neural analysis collaborative repository","text":""},{"location":"pynacollada/#pynacollada","title":"pynacollada","text":"<p>Collaborative platform for high-level analysis with pynapple. Check it out!</p>"},{"location":"notebooks/pynapple-core-notebook/","title":"Core","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pynapple as nap\nimport pandas as pd\n</pre> import numpy as np import matplotlib.pyplot as plt import pynapple as nap import pandas as pd In\u00a0[2]: Copied! <pre>tsd = nap.Tsd(t = np.arange(100), d = np.random.rand(100), time_units = 's')\n\nprint(tsd)\n</pre> tsd = nap.Tsd(t = np.arange(100), d = np.random.rand(100), time_units = 's')  print(tsd) <pre>Time (s)\n0.0     0.614948\n1.0     0.386567\n2.0     0.349398\n3.0     0.249369\n4.0     0.194101\n          ...   \n95.0    0.871019\n96.0    0.963933\n97.0    0.927906\n98.0    0.934819\n99.0    0.294858\nLength: 100, dtype: float64\n</pre> <p>It is possible to toggle between seconds, milliseconds and microseconds. Note that when using as_units, the returned object is a simple pandas series.</p> In\u00a0[3]: Copied! <pre>print(tsd.as_units('ms'))\nprint(tsd.as_units('us'))\n</pre> print(tsd.as_units('ms')) print(tsd.as_units('us')) <pre>Time (ms)\n0.0        0.614948\n1000.0     0.386567\n2000.0     0.349398\n3000.0     0.249369\n4000.0     0.194101\n             ...   \n95000.0    0.871019\n96000.0    0.963933\n97000.0    0.927906\n98000.0    0.934819\n99000.0    0.294858\nLength: 100, dtype: float64\nTime (us)\n0           0.614948\n1000000     0.386567\n2000000     0.349398\n3000000     0.249369\n4000000     0.194101\n              ...   \n95000000    0.871019\n96000000    0.963933\n97000000    0.927906\n98000000    0.934819\n99000000    0.294858\nLength: 100, dtype: float64\n</pre> <p>Pynapple is able to handle data that only contains timestamps, such as an object containing only spike times. To do so, we construct a Ts object which holds only times. In this case, we generate 10 random spike times between 0 and 100 ms.</p> In\u00a0[4]: Copied! <pre>ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 'ms')\n\nprint(ts)\n</pre> ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 'ms')  print(ts) <pre>Time (s)\n0.006015    \n0.033591    \n0.041823    \n0.045532    \n0.049214    \n0.050702    \n0.052091    \n0.057266    \n0.086364    \n0.088195    \ndtype: object\n</pre> <p>If the time series contains multiple columns, we use a TsdFrame.</p> In\u00a0[5]: Copied! <pre>tsdframe = nap.TsdFrame(t = np.arange(100), \n                        d = np.random.rand(100,3), \n                        time_units = 's', \n                        columns = ['a', 'b', 'c'])\n\nprint(tsdframe)\n</pre> tsdframe = nap.TsdFrame(t = np.arange(100),                          d = np.random.rand(100,3),                          time_units = 's',                          columns = ['a', 'b', 'c'])  print(tsdframe) <pre>                 a         b         c\nTime (s)                              \n0.0       0.915293  0.766933  0.117816\n1.0       0.208782  0.831549  0.341053\n2.0       0.888413  0.835257  0.998087\n3.0       0.291597  0.517460  0.104060\n4.0       0.356592  0.534000  0.728561\n...            ...       ...       ...\n95.0      0.825300  0.922398  0.624141\n96.0      0.127517  0.769646  0.872305\n97.0      0.961033  0.329894  0.959614\n98.0      0.614398  0.035959  0.140430\n99.0      0.554571  0.219659  0.077762\n\n[100 rows x 3 columns]\n</pre> In\u00a0[6]: Copied! <pre>epochs = nap.IntervalSet(start = [0, 10], end = [5, 15], time_units = 's')\n\nnew_tsd = tsd.restrict(epochs)\n\nprint(epochs)\nprint('\\n')\nprint(new_tsd)\n</pre> epochs = nap.IntervalSet(start = [0, 10], end = [5, 15], time_units = 's')  new_tsd = tsd.restrict(epochs)  print(epochs) print('\\n') print(new_tsd) <pre>   start   end\n0    0.0   5.0\n1   10.0  15.0\n\n\nTime (s)\n0.0     0.614948\n1.0     0.386567\n2.0     0.349398\n3.0     0.249369\n4.0     0.194101\n5.0     0.712666\n10.0    0.499971\n11.0    0.222697\n12.0    0.642140\n13.0    0.390834\n14.0    0.498306\n15.0    0.101749\ndtype: float64\n</pre> <p>Multiple operations are available for IntervalSet. For example, IntervalSet can be merged. See the full documentation of the class here for a list of all the functions that can be used to manipulate IntervalSets.</p> In\u00a0[7]: Copied! <pre>epoch1 = nap.IntervalSet(start=[0], end=[10]) # no time units passed. Default is us.\nepoch2 = nap.IntervalSet(start=[5,30],end=[20,45])\n\nepoch = epoch1.union(epoch2)\nprint(epoch1, '\\n')\nprint(epoch2, '\\n')\nprint(epoch)\n</pre> epoch1 = nap.IntervalSet(start=[0], end=[10]) # no time units passed. Default is us. epoch2 = nap.IntervalSet(start=[5,30],end=[20,45])  epoch = epoch1.union(epoch2) print(epoch1, '\\n') print(epoch2, '\\n') print(epoch) <pre>   start   end\n0    0.0  10.0 \n\n   start   end\n0    5.0  20.0\n1   30.0  45.0 \n\n   start   end\n0    0.0  20.0\n1   30.0  45.0\n</pre> In\u00a0[8]: Copied! <pre>my_ts = {0:nap.Ts(t = np.sort(np.random.uniform(0, 100, 1000)), time_units = 's'), # here a simple dictionary\n         1:nap.Ts(t = np.sort(np.random.uniform(0, 100, 2000)), time_units = 's'),\n         2:nap.Ts(t = np.sort(np.random.uniform(0, 100, 3000)), time_units = 's')}\n\ntsgroup = nap.TsGroup(my_ts)\n\nprint(tsgroup, '\\n')\nprint(tsgroup[0], '\\n') # dictionary like indexing returns directly the Ts object\nprint(tsgroup[[0,2]]) # list like indexing\n</pre> my_ts = {0:nap.Ts(t = np.sort(np.random.uniform(0, 100, 1000)), time_units = 's'), # here a simple dictionary          1:nap.Ts(t = np.sort(np.random.uniform(0, 100, 2000)), time_units = 's'),          2:nap.Ts(t = np.sort(np.random.uniform(0, 100, 3000)), time_units = 's')}  tsgroup = nap.TsGroup(my_ts)  print(tsgroup, '\\n') print(tsgroup[0], '\\n') # dictionary like indexing returns directly the Ts object print(tsgroup[[0,2]]) # list like indexing <pre>  Index    rate\n-------  ------\n      0   10\n      1   20.01\n      2   30.01 \n\nTime (s)\n0.044468    NaN\n0.147408    NaN\n0.316770    NaN\n0.371238    NaN\n0.455870    NaN\n             ..\n99.474377   NaN\n99.604871   NaN\n99.712532   NaN\n99.915507   NaN\n99.921997   NaN\nLength: 1000, dtype: float64 \n\n  Index    rate\n-------  ------\n      0   10\n      2   30.01\n</pre> <p>Operations such as restrict can thus be directly applied to the TsGroup as well as other operations.</p> In\u00a0[9]: Copied! <pre>newtsgroup = tsgroup.restrict(epochs)\n\ncount = tsgroup.count(1, epochs, time_units='s') # Here counting the elements within bins of 1 seconds\n\nprint(count)\n</pre> newtsgroup = tsgroup.restrict(epochs)  count = tsgroup.count(1, epochs, time_units='s') # Here counting the elements within bins of 1 seconds  print(count) <pre>           0   1   2\nTime (s)            \n0.5       12  18  28\n1.5       11  15  21\n2.5       13  16  31\n3.5        4  19  32\n4.5       10  25  34\n10.5      13  19  30\n11.5       7  19  28\n12.5      11  17  35\n13.5       8  18  23\n14.5       9  24  37\n</pre> <p>One advantage of grouping time series is that metainformation can be appended directly on an element-wise basis. In this case, we add labels to each Ts object when instantiating the group and after. We can then use this label to split the group. See the TsGroup documentation for a complete methodology for splitting TsGroup objects.</p> In\u00a0[10]: Copied! <pre>label1 = pd.Series(index=list(my_ts.keys()), data = [0,1,0])\n\ntsgroup = nap.TsGroup(my_ts, time_units = 's', label1=label1)\ntsgroup.set_info(label2=np.array(['a', 'a', 'b']))\n\nprint(tsgroup, '\\n')\n\nnewtsgroup= tsgroup.getby_category('label1')\nprint(newtsgroup[0], '\\n')\nprint(newtsgroup[1])\n</pre> label1 = pd.Series(index=list(my_ts.keys()), data = [0,1,0])  tsgroup = nap.TsGroup(my_ts, time_units = 's', label1=label1) tsgroup.set_info(label2=np.array(['a', 'a', 'b']))  print(tsgroup, '\\n')  newtsgroup= tsgroup.getby_category('label1') print(newtsgroup[0], '\\n') print(newtsgroup[1])  <pre>  Index    rate    label1  label2\n-------  ------  --------  --------\n      0   10            0  a\n      1   20.01         1  a\n      2   30.01         0  b \n\n  Index    rate    label1  label2\n-------  ------  --------  --------\n      0   10            0  a\n      2   30.01         0  b \n\n  Index    rate    label1  label2\n-------  ------  --------  --------\n      1   20.01         1  a\n</pre> In\u00a0[11]: Copied! <pre>time_support = nap.IntervalSet(start = 0, end = 200, time_units = 's')\n\nmy_ts = {0:nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's'), # here a simple dictionnary\n         1:nap.Ts(t = np.sort(np.random.uniform(0, 100, 20)), time_units = 's'),\n         2:nap.Ts(t = np.sort(np.random.uniform(0, 100, 30)), time_units = 's')}\n\ntsgroup = nap.TsGroup(my_ts)\n\ntsgroup_with_time_support = nap.TsGroup(my_ts, time_support = time_support)\n\nprint(tsgroup, '\\n')\n\nprint(tsgroup_with_time_support, '\\n')\n\nprint(tsgroup_with_time_support.time_support) # acceding the time support\n</pre> time_support = nap.IntervalSet(start = 0, end = 200, time_units = 's')  my_ts = {0:nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's'), # here a simple dictionnary          1:nap.Ts(t = np.sort(np.random.uniform(0, 100, 20)), time_units = 's'),          2:nap.Ts(t = np.sort(np.random.uniform(0, 100, 30)), time_units = 's')}  tsgroup = nap.TsGroup(my_ts)  tsgroup_with_time_support = nap.TsGroup(my_ts, time_support = time_support)  print(tsgroup, '\\n')  print(tsgroup_with_time_support, '\\n')  print(tsgroup_with_time_support.time_support) # acceding the time support <pre>  Index    rate\n-------  ------\n      0    0.1\n      1    0.21\n      2    0.31 \n\n  Index    rate\n-------  ------\n      0    0.05\n      1    0.1\n      2    0.15 \n\n   start    end\n0    0.0  200.0\n</pre>"},{"location":"notebooks/pynapple-core-notebook/#core-tutorial","title":"Core Tutorial\u00b6","text":"<p>This script will introduce the basics of handling time series data with pynapple.</p>"},{"location":"notebooks/pynapple-core-notebook/#time-series-object","title":"Time series object\u00b6","text":"<p>Let's create a Tsd object with artificial data. In this example, every time point is 1 second apart. A Tsd object is a wrapper of a pandas series.</p>"},{"location":"notebooks/pynapple-core-notebook/#interval-sets-object","title":"Interval Sets object\u00b6","text":"<p>The IntervalSet object stores multiple epochs with a common time unit. It can then be used to restrict time series to this particular set of epochs.</p>"},{"location":"notebooks/pynapple-core-notebook/#tsgroup","title":"TsGroup\u00b6","text":"<p>Multiple time series with different time stamps (.i.e. a group of neurons with different spike times from one session) can be grouped with the TsGroup object. The TsGroup behaves like a dictionary but it is also possible to slice with a list of indexes</p>"},{"location":"notebooks/pynapple-core-notebook/#time-support","title":"Time support\u00b6","text":"<p>A key feature of how pynapple manipulates time series is an inherent time support object defined for Ts, Tsd, TsdFrame and TsGroup objects. The time support object is defined as an IntervalSet that provides the time serie with a context. For example, the restrict operation will automatically update the time support object for the new time series. Ideally, the time support object should be defined for all time series when instantiating them. If no time series is given, the time support is inferred from the start and end of the time series.</p> <p>In this example, a TsGroup is instantiated with and without a time support. Notice how the frequency of each Ts element is changed when the time support is defined explicitly.</p>"},{"location":"notebooks/pynapple-io-notebook/","title":"IO","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport pynapple as nap\nimport matplotlib.pyplot as plt\n</pre> import numpy as np import pandas as pd import pynapple as nap import matplotlib.pyplot as plt <p>In this example dataset, the data contains a sample recording from the anterodorsal nucleus of the thalamus and the hippocampus, with both a sleep and a wake phase. It contains both head-direction cells (i.e. cells that fire for a particular direction of the head in the horizontal plane) and place cells (i.e. cells that fire for a particular position in the environment).</p> <p>Preprocessing of the data was made with Kilosort 2.0 and spike sorting was made with Klusters.</p> <p>A typical directory containing data processed with neurosuite is similar to this:</p> <p>The first step is to call the function load_session. It will then open a GUI for filling manually the information. The following screenshots show what parameters to use.</p> In\u00a0[2]: Copied! <pre>data_directory = '../../your/path/to/A2929-200711'\n\ndata = nap.load_session(data_directory, 'neurosuite')\n</pre> data_directory = '../../your/path/to/A2929-200711'  data = nap.load_session(data_directory, 'neurosuite') <p>The second step is to provides information about the session and the subject. All the fields shown are suggested by the NWB format (see here and here).</p> <p>The epochs tab loads the epochs within the session (typically wake and sleep). In this case, we load the file Epoch_Ts.csv in the data folder. The first column contains the start of the epoch and the second column contains the end of the epoch.</p> <p>If the CSV file contains a third column with an epoch label, the loader will automatically write it in the label column. Otherwise, it is necessary to manually write the epoch labels.</p> <p>The tracking tab allows to load tracking data saved with a CSV file. Reading a CSV file is always a challenge when the header is unknown. The default csv file should contains only one row for the header with the column names. The first column should be the time index in seconds. Other formats are DeepLabCut and Optitrack.</p> <p>Frame alignement can vary as well. Pynapple offers three ways to align the tracking frames :</p> <ul> <li>Global timestamps: the time column of the CSV file contains the timestamps aligned to the global timeframe of the session.</li> <li>Local timestamps: the time column of the CSV file contains the timestamps aligned to one epoch. In this case, the user should select which epoch.</li> <li>TTL detection: a binary file containing TTL pulses for each tracking frame is located within the folder and can be loaded. Alignement is made with TTL detection.</li> </ul> <p>In this example session, Tracking was made with Optitrack and TTL pulses were written to an analogin file recorded by an Intan RHD2000 recording system. The parameters for the tracking tab are shown below.</p> <p>The next step is specific to NeuroSuite. In this case, 2 electrophysiological probes were implanted, one to the ADN and another to the CA1. This step allows to label groups of electrodes as shown below.</p> <p>If successful, a NWB file should be created in session_folder/pynapplenwb/session_name.nwb</p> <p>Calling the function load_session should directly read the NWB file and bypass the GUI loader.</p> In\u00a0[3]: Copied! <pre>data = nap.load_session(data_directory, 'neurosuite')\n</pre> data = nap.load_session(data_directory, 'neurosuite') <p>In this case, the data that can be used for analysis are spikes, position and epochs.</p> In\u00a0[4]: Copied! <pre>spikes = data.spikes\nposition = data.position\nepochs = data.epochs\n\nprint(spikes, '\\n')\nprint(position, '\\n')\nprint(epochs, '\\n')\n</pre> spikes = data.spikes position = data.position epochs = data.epochs  print(spikes, '\\n') print(position, '\\n') print(epochs, '\\n') <pre>  Index    Freq. (Hz)    group\n-------  ------------  -------\n      0          7.3         0\n      1          5.73        0\n      2          8.12        0\n      3          6.68        0\n      4         10.77        0\n      5         11           0\n      6         16.52        0\n      7          2.2         1\n      8          2.02        1\n      9          1.07        1\n     10          3.92        1\n     11          3.31        1\n     12          1.09        1\n     13          1.28        1\n     14          1.32        1 \n\n                  rx        ry        rz         x         y         z\nTime (s)                                                              \n670.64070   0.343163  5.207148  5.933598 -0.042857  0.425023 -0.195725\n670.64900   0.346745  5.181029  5.917368 -0.043863  0.424850 -0.195110\n670.65735   0.344035  5.155508  5.905679 -0.044853  0.424697 -0.194674\n670.66565   0.322240  5.136537  5.892457 -0.045787  0.424574 -0.194342\n670.67400   0.315836  5.120850  5.891577 -0.046756  0.424563 -0.194059\n...              ...       ...       ...       ...       ...       ...\n1199.96160  6.009812  3.665954  0.230562  0.011241  0.037891 -0.001479\n1199.96995  6.014660  3.634619  0.260742  0.010974  0.038677 -0.002370\n1199.97825  6.031694  3.617849  0.276835  0.010786  0.039410 -0.003156\n1199.98660  6.040435  3.609446  0.287006  0.010661  0.040064 -0.003821\n1199.99495  6.050059  3.609375  0.293275  0.010624  0.040568 -0.004435\n\n[63527 rows x 6 columns] \n\n{'sleep':    start    end\n0    0.0  600.0, 'wake':    start     end\n0  600.0  1200.0} \n\n</pre> In\u00a0[5]: Copied! <pre># -*- coding: utf-8 -*-\n# @Author: gviejo\n# @Date:   2022-01-06 20:01:32\n# @Last Modified by:   gviejo\n# @Last Modified time: 2022-01-06 20:01:57\n\nimport os\nfrom pynapple.io.loader import BaseLoader\nfrom pynwb import NWBFile, NWBHDF5IO\n\nclass MyCustomIO(BaseLoader):\n\n    def __init__(self, path):\n\"\"\"        \n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"     \n        self.basename = os.path.basename(path)\n        \n        super().__init__(path)\n\n        # Need to check if nwb file exists and if data are there\n        loading_my_data = True\n        if self.path is not None:\n            nwb_path = os.path.join(self.path, 'pynapplenwb')\n            if os.path.exists(nwb_path):\n                files = os.listdir(nwb_path)\n                if len([f for f in files if f.endswith('.nwb')]):                    \n                    success = self.load_my_nwb(path)\n                    if success: loading_my_data = False\n\n        # Bypass if data have already been transfered to nwb\n        if loading_my_data:\n            self.load_my_data(path)\n\n            self.save_my_data_in_nwb(path)\n\n    def load_my_data(self, path):\n\"\"\"\n        This load the raw data\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\n'''\n        Load Raw data here\n        '''\n        print(path)\n        return None\n\n    def save_my_data_in_nwb(self, path):\n\"\"\"\n        Save the raw data to NWB\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\n        self.nwb_path = os.path.join(path, 'pynapplenwb')\n        if os.path.exists(self.nwb_path):\n            files = os.listdir(self.nwb_path)\n        else:\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if 'nwb' in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n\n        io = NWBHDF5IO(self.nwbfilepath, 'r+')\n'''\n        Save data in NWB here\n        '''\n\n        io.close()\n\n        return\n        \n\n\n    def load_my_nwb(self, path):\n\"\"\"\n        This load the nwb that is already create by the base loader\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\n        self.nwb_path = os.path.join(path, 'pynapplenwb')\n        if os.path.exists(self.nwb_path):\n            files = os.listdir(self.nwb_path)\n        else:\n            raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\n        self.nwbfilename = [f for f in os.listdir(self.nwb_path) if 'nwb' in f][0]\n        self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\n\n        io = NWBHDF5IO(self.nwbfilepath, 'r')\n        nwbfile = io.read()\n\n'''\n        Add code to write to nwb file here\n        '''\n\n        io.close()\n\n            \nmydata = MyCustomIO('.')\n\nprint(type(mydata))\n</pre> # -*- coding: utf-8 -*- # @Author: gviejo # @Date:   2022-01-06 20:01:32 # @Last Modified by:   gviejo # @Last Modified time: 2022-01-06 20:01:57  import os from pynapple.io.loader import BaseLoader from pynwb import NWBFile, NWBHDF5IO  class MyCustomIO(BaseLoader):      def __init__(self, path):         \"\"\"                          Parameters         ----------         path : str             The path to the data.         \"\"\"              self.basename = os.path.basename(path)                  super().__init__(path)          # Need to check if nwb file exists and if data are there         loading_my_data = True         if self.path is not None:             nwb_path = os.path.join(self.path, 'pynapplenwb')             if os.path.exists(nwb_path):                 files = os.listdir(nwb_path)                 if len([f for f in files if f.endswith('.nwb')]):                                         success = self.load_my_nwb(path)                     if success: loading_my_data = False          # Bypass if data have already been transfered to nwb         if loading_my_data:             self.load_my_data(path)              self.save_my_data_in_nwb(path)      def load_my_data(self, path):         \"\"\"         This load the raw data                  Parameters         ----------         path : str             Path to the session         \"\"\"         '''         Load Raw data here         '''         print(path)         return None      def save_my_data_in_nwb(self, path):         \"\"\"         Save the raw data to NWB                  Parameters         ----------         path : TYPE             Description         \"\"\"         self.nwb_path = os.path.join(path, 'pynapplenwb')         if os.path.exists(self.nwb_path):             files = os.listdir(self.nwb_path)         else:             raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))         self.nwbfilename = [f for f in os.listdir(self.nwb_path) if 'nwb' in f][0]         self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)           io = NWBHDF5IO(self.nwbfilepath, 'r+')                  '''         Save data in NWB here         '''          io.close()          return                def load_my_nwb(self, path):         \"\"\"         This load the nwb that is already create by the base loader                  Parameters         ----------         path : str             Path to the session         \"\"\"         self.nwb_path = os.path.join(path, 'pynapplenwb')         if os.path.exists(self.nwb_path):             files = os.listdir(self.nwb_path)         else:             raise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))         self.nwbfilename = [f for f in os.listdir(self.nwb_path) if 'nwb' in f][0]         self.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)          io = NWBHDF5IO(self.nwbfilepath, 'r')         nwbfile = io.read()          '''         Add code to write to nwb file here         '''          io.close()               mydata = MyCustomIO('.')  print(type(mydata)) <pre>.\n&lt;class '__main__.MyCustomIO'&gt;\n</pre>"},{"location":"notebooks/pynapple-io-notebook/#io-tutorial","title":"IO Tutorial\u00b6","text":"<p>This script demonstrates how to load data in pynapple.</p> <p>The general workflow of loading a session is described by the infographic below. As it is challenging to accomodate all possible types of format, we aimed to keep the IO of pynapple minimal while allowing the user to inherit the base loader and import their own custom io functions. The base loader is thus responsible for initializing the NWB file containing the tracking data, the epochs and the session informations. An example of a custom IO class is shown at the end of this tutorial.</p> <p></p>"},{"location":"notebooks/pynapple-io-notebook/#custom-loading-io","title":"Custom Loading IO\u00b6","text":"<p>This example shows how to construct a custom IO loader.</p>"},{"location":"notebooks/pynapple-process-notebook/","title":"Process","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport pynapple as nap\nimport matplotlib.pyplot as plt\n</pre> import numpy as np import pandas as pd import pynapple as nap import matplotlib.pyplot as plt In\u00a0[2]: Copied! <pre>ts1 = nap.Ts(t = np.sort(np.random.uniform(0, 1000, 1000)), time_units = 's')\nts2 = nap.Ts(t = np.sort(np.random.uniform(0, 1000, 1000)), time_units = 's')\n\nts1_time_array = ts1.as_units('s').index.values\nts2_time_array = ts2.as_units('s').index.values\n\nbinsize = 0.1 # second\ncc12, xt = nap.cross_correlogram(t1 = ts1_time_array,\n                                 t2 = ts2_time_array,\n                                 binsize=binsize,\n                                 windowsize=1 # second\n                                )\n\nplt.figure()\nplt.bar(xt, cc12, binsize)\nplt.xlabel(\"Time t1 (us)\")\nplt.ylabel(\"CC\")\n</pre> ts1 = nap.Ts(t = np.sort(np.random.uniform(0, 1000, 1000)), time_units = 's') ts2 = nap.Ts(t = np.sort(np.random.uniform(0, 1000, 1000)), time_units = 's')  ts1_time_array = ts1.as_units('s').index.values ts2_time_array = ts2.as_units('s').index.values  binsize = 0.1 # second cc12, xt = nap.cross_correlogram(t1 = ts1_time_array,                                  t2 = ts2_time_array,                                  binsize=binsize,                                  windowsize=1 # second                                 )  plt.figure() plt.bar(xt, cc12, binsize) plt.xlabel(\"Time t1 (us)\") plt.ylabel(\"CC\")  Out[2]: <pre>Text(0, 0.5, 'CC')</pre> <p>To simplify converting to a numpy.ndarray, pynapple provides wrappers for computing autocorrelogram and crosscorrelogram for TsGroup. The function is then called for each unit or each pairs of units. It returns directly a pandas.DataFrame holding all the correlograms. In this example, autocorrelograms and cross-correlograms are computed for the same TsGroup.</p> In\u00a0[3]: Copied! <pre>epoch = nap.IntervalSet(start = 0, end = 1000, time_units = 's')\nts_group = nap.TsGroup({0:ts1,1:ts2}, time_support = epoch)\n\nautocorrs = nap.compute_autocorrelogram(group=ts_group,                                         \n                                        binsize=100, # ms\n                                        windowsize=1000, # ms                                        \n                                        time_units='ms',\n                                        ep=epoch\n                                       )\ncrosscorrs = nap.compute_crosscorrelogram(group=ts_group,                                        \n                                        binsize=100, # ms\n                                        windowsize=1000, # ms                                        \n                                        time_units='ms'\n                                       )\n\nprint(autocorrs, '\\n')\nprint(crosscorrs, '\\n')\n</pre> epoch = nap.IntervalSet(start = 0, end = 1000, time_units = 's') ts_group = nap.TsGroup({0:ts1,1:ts2}, time_support = epoch)  autocorrs = nap.compute_autocorrelogram(group=ts_group,                                                                                  binsize=100, # ms                                         windowsize=1000, # ms                                                                                 time_units='ms',                                         ep=epoch                                        ) crosscorrs = nap.compute_crosscorrelogram(group=ts_group,                                                                                 binsize=100, # ms                                         windowsize=1000, # ms                                                                                 time_units='ms'                                        )  print(autocorrs, '\\n') print(crosscorrs, '\\n')  <pre>         0     1\n-0.9  0.93  0.96\n-0.8  1.15  1.01\n-0.7  0.95  1.07\n-0.6  1.09  0.78\n-0.5  1.11  0.89\n-0.4  0.88  1.10\n-0.3  1.00  1.05\n-0.2  0.97  0.93\n-0.1  0.99  1.09\n 0.0  0.00  0.00\n 0.1  0.99  1.09\n 0.2  0.97  0.93\n 0.3  1.00  1.05\n 0.4  0.88  1.10\n 0.5  1.11  0.89\n 0.6  1.09  0.78\n 0.7  0.95  1.07\n 0.8  1.15  1.01\n 0.9  0.93  0.96 \n\n         0\n         1\n-0.9  0.93\n-0.8  1.02\n-0.7  0.95\n-0.6  0.85\n-0.5  0.81\n-0.4  1.06\n-0.3  1.09\n-0.2  1.01\n-0.1  1.11\n 0.0  0.98\n 0.1  0.87\n 0.2  0.96\n 0.3  1.09\n 0.4  1.16\n 0.5  0.93\n 0.6  0.97\n 0.7  1.18\n 0.8  1.02\n 0.9  0.86 \n\n</pre> In\u00a0[4]: Copied! <pre>stim = nap.Tsd(t = np.sort(np.random.uniform(0, 1000, 10)), \n               d = np.random.rand(10),\n               time_units = 's')\n\npeth0 = nap.compute_perievent(ts1, stim, minmax = (-1, 1), time_unit = 's')\n\nprint(peth0)\n</pre> stim = nap.Tsd(t = np.sort(np.random.uniform(0, 1000, 10)),                 d = np.random.rand(10),                time_units = 's')  peth0 = nap.compute_perievent(ts1, stim, minmax = (-1, 1), time_unit = 's')  print(peth0) <pre>  Index    Freq. (Hz)    ref_times\n-------  ------------  -----------\n      0         nan        297.474\n      1           1.5      458.283\n      2           1.5      478.343\n      3           1        565.465\n      4           1        580.24\n      5           1        628.165\n      6           1        730.55\n      7         nan        837.449\n      8           1        932.426\n      9         nan        986.733\n</pre> <pre>/home/guillaume/pynapple/pynapple/core/time_series.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  self.rate = len(t)/self.time_support.tot_length('s')\n/home/guillaume/pynapple/pynapple/core/time_series.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  self.rate = len(t)/self.time_support.tot_length('s')\n/home/guillaume/pynapple/pynapple/core/time_series.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  self.rate = len(t)/self.time_support.tot_length('s')\n/home/guillaume/pynapple/pynapple/core/time_series.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  self.rate = len(t)/self.time_support.tot_length('s')\n/home/guillaume/pynapple/pynapple/core/time_series.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  self.rate = len(t)/self.time_support.tot_length('s')\n/home/guillaume/pynapple/pynapple/core/time_series.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  self.rate = len(t)/self.time_support.tot_length('s')\n</pre> <p>It is then easy to create a raster plot around the times of the stimulation event by calling the fillna function of pandas.</p> In\u00a0[5]: Copied! <pre>plt.figure()\nplt.subplot(211)\nplt.plot(peth0.count(0.1, time_units = 's').mean(1))\nplt.subplot(212)\nfor stim_index in peth0.keys():\n    plt.plot(peth0[stim_index].as_units('s').fillna(stim_index), '|')\nplt.xlabel(\"Time from stim (s)\")\n</pre> plt.figure() plt.subplot(211) plt.plot(peth0.count(0.1, time_units = 's').mean(1)) plt.subplot(212) for stim_index in peth0.keys():     plt.plot(peth0[stim_index].as_units('s').fillna(stim_index), '|') plt.xlabel(\"Time from stim (s)\") Out[5]: <pre>Text(0.5, 0, 'Time from stim (s)')</pre> <p>The same function can be applied to a group of neurons.</p> In\u00a0[6]: Copied! <pre>pethall = nap.compute_perievent(ts_group, stim, minmax = (-1, 1), time_unit = 's')\n\nprint(pethall[0])\nprint(pethall[1])\n</pre> pethall = nap.compute_perievent(ts_group, stim, minmax = (-1, 1), time_unit = 's')  print(pethall[0]) print(pethall[1]) <pre>  Index    Freq. (Hz)    ref_times\n-------  ------------  -----------\n      0         nan        297.474\n      1           1.5      458.283\n      2           1.5      478.343\n      3           1        565.465\n      4           1        580.24\n      5           1        628.165\n      6           1        730.55\n      7         nan        837.449\n      8           1        932.426\n      9         nan        986.733\n  Index    Freq. (Hz)    ref_times\n-------  ------------  -----------\n      0           3        297.474\n      1           0.5      458.283\n      2           1        478.343\n      3           0.5      565.465\n      4           1.5      580.24\n      5           0.5      628.165\n      6           2.5      730.55\n      7           0.5      837.449\n      8           2        932.426\n      9           1        986.733\n</pre> <pre>/home/guillaume/pynapple/pynapple/core/time_series.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  self.rate = len(t)/self.time_support.tot_length('s')\n/home/guillaume/pynapple/pynapple/core/time_series.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  self.rate = len(t)/self.time_support.tot_length('s')\n/home/guillaume/pynapple/pynapple/core/time_series.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  self.rate = len(t)/self.time_support.tot_length('s')\n/home/guillaume/pynapple/pynapple/core/time_series.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  self.rate = len(t)/self.time_support.tot_length('s')\n/home/guillaume/pynapple/pynapple/core/time_series.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  self.rate = len(t)/self.time_support.tot_length('s')\n/home/guillaume/pynapple/pynapple/core/time_series.py:130: RuntimeWarning: invalid value encountered in double_scalars\n  self.rate = len(t)/self.time_support.tot_length('s')\n</pre> In\u00a0[7]: Copied! <pre>dt = 0.01\nfeatures = np.vstack((np.cos(np.arange(0, 1000, dt)),np.sin(np.arange(0,1000,dt)))).T\nfeatures = nap.TsdFrame(t = np.arange(0, 1000, dt), d = features, time_units = 's', time_support = epoch, columns=['a', 'b'])\n\nprint(features)\n\ntcurves1d = nap.compute_1d_tuning_curves(group=ts_group,\n                                         feature=features['a'],\n                                         nb_bins=10)\n\ntcurves2d, binsxy = nap.compute_2d_tuning_curves(group=ts_group,\n                                                 feature = features,                                                 \n                                                 nb_bins=10)\n\nplt.figure(figsize=(15,5))\nplt.subplot(131)\nplt.plot(features.as_units('s').loc[0:10])\nplt.title(\"Features\")\nplt.xlabel(\"Time(s)\")\nplt.subplot(132)\nplt.title(\"Features\")\nplt.plot(features['a'].as_units('s').loc[0:10], features['b'].as_units('s').loc[0:10])\nplt.xlabel(\"Feature a\")\nplt.ylabel(\"Feature b\")\nplt.subplot(133)\nplt.title(\"Tuning curve unit 0 2d\")\nplt.imshow(tcurves2d[0], extent=(binsxy[1][0],binsxy[1][-1],binsxy[0][0],binsxy[0][-1]))\nplt.tight_layout()\nplt.show()\n</pre> dt = 0.01 features = np.vstack((np.cos(np.arange(0, 1000, dt)),np.sin(np.arange(0,1000,dt)))).T features = nap.TsdFrame(t = np.arange(0, 1000, dt), d = features, time_units = 's', time_support = epoch, columns=['a', 'b'])  print(features)  tcurves1d = nap.compute_1d_tuning_curves(group=ts_group,                                          feature=features['a'],                                          nb_bins=10)  tcurves2d, binsxy = nap.compute_2d_tuning_curves(group=ts_group,                                                  feature = features,                                                                                                   nb_bins=10)  plt.figure(figsize=(15,5)) plt.subplot(131) plt.plot(features.as_units('s').loc[0:10]) plt.title(\"Features\") plt.xlabel(\"Time(s)\") plt.subplot(132) plt.title(\"Features\") plt.plot(features['a'].as_units('s').loc[0:10], features['b'].as_units('s').loc[0:10]) plt.xlabel(\"Feature a\") plt.ylabel(\"Feature b\") plt.subplot(133) plt.title(\"Tuning curve unit 0 2d\") plt.imshow(tcurves2d[0], extent=(binsxy[1][0],binsxy[1][-1],binsxy[0][0],binsxy[0][-1])) plt.tight_layout() plt.show() <pre>                 a         b\nTime (s)                    \n0.00      1.000000  0.000000\n0.01      0.999950  0.010000\n0.02      0.999800  0.019999\n0.03      0.999550  0.029996\n0.04      0.999200  0.039989\n...            ...       ...\n999.95    0.603003  0.797739\n999.96    0.594996  0.803729\n999.97    0.586929  0.809639\n999.98    0.578803  0.815467\n999.99    0.570620  0.821214\n\n[100000 rows x 2 columns]\n</pre> <pre>/home/guillaume/pynapple/pynapple/process/tuning_curves.py:139: RuntimeWarning: invalid value encountered in true_divide\n  count = count / occupancy\n</pre> In\u00a0[8]: Copied! <pre>#############################################################################################\n# This part is just to generate units with a relationship to the features (i.e. \"place fields\")\ntimes = features.as_units('us').index.values\nft = features.values\nalpha = np.arctan2(ft[:,1], ft[:,0])\nbins = np.repeat(np.linspace(-np.pi, np.pi, 13)[::,np.newaxis], 2, 1)\nbins += np.array([- 2*np.pi/24, 2*np.pi/24])\nts_group = {}\nfor i in range(12):\n    ts = times[(alpha&gt;=bins[i,0]) &amp; (alpha &lt;= bins[i+1,1])]\n    ts_group[i] = nap.Ts(ts, time_units = 'us')\n\nts_group = nap.TsGroup(ts_group, time_support = epoch)\n\n##############################################################################################\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ntcurves2d, binsxy = nap.compute_2d_tuning_curves(group=ts_group,\n                                                 feature=features,                                                 \n                                                 nb_bins=10,\n                                                 ep=epoch,\n                                                 minmax=(-1.,1.,-1.,1.))\n</pre> ############################################################################################# # This part is just to generate units with a relationship to the features (i.e. \"place fields\") times = features.as_units('us').index.values ft = features.values alpha = np.arctan2(ft[:,1], ft[:,0]) bins = np.repeat(np.linspace(-np.pi, np.pi, 13)[::,np.newaxis], 2, 1) bins += np.array([- 2*np.pi/24, 2*np.pi/24]) ts_group = {} for i in range(12):     ts = times[(alpha&gt;=bins[i,0]) &amp; (alpha &lt;= bins[i+1,1])]     ts_group[i] = nap.Ts(ts, time_units = 'us')  ts_group = nap.TsGroup(ts_group, time_support = epoch)  ##############################################################################################  import warnings warnings.filterwarnings('ignore')  tcurves2d, binsxy = nap.compute_2d_tuning_curves(group=ts_group,                                                  feature=features,                                                                                                   nb_bins=10,                                                  ep=epoch,                                                  minmax=(-1.,1.,-1.,1.)) <p>Then we plot the \"place fields\".</p> In\u00a0[9]: Copied! <pre>plt.figure(figsize = (16,4))\nfor i in ts_group.keys():\n    plt.subplot(2,6,i+1)\n    plt.imshow(tcurves2d[i], extent=(binsxy[1][0],binsxy[1][-1],binsxy[0][0],binsxy[0][-1]))\n    plt.xticks()\nplt.show()\n</pre> plt.figure(figsize = (16,4)) for i in ts_group.keys():     plt.subplot(2,6,i+1)     plt.imshow(tcurves2d[i], extent=(binsxy[1][0],binsxy[1][-1],binsxy[0][0],binsxy[0][-1]))     plt.xticks() plt.show() <p>Then we call the actual decoding function in 2d.</p> In\u00a0[10]: Copied! <pre>decoded, proba_feature = nap.decode_2d(tuning_curves=tcurves2d, \n                                     group=ts_group,                                   \n                                     ep=epoch,\n                                     bin_size=0.1, # second\n                                     xy=binsxy,\n                                    features=features,\n                                    )\n\n\nplt.figure(figsize=(15,5))\nplt.subplot(131)\nplt.plot(features['a'].as_units('s').loc[0:20], label = 'True')\nplt.plot(decoded['a'].as_units('s').loc[0:20], label = 'Decoded')\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Feature a\")\nplt.subplot(132)\nplt.plot(features['b'].as_units('s').loc[0:20], label = 'True')\nplt.plot(decoded['b'].as_units('s').loc[0:20], label = 'Decoded')\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Feature b\")\nplt.subplot(133)\nplt.plot(features['a'].as_units('s').loc[0:20], features['b'].as_units('s').loc[0:20], label = 'True')\nplt.plot(decoded['a'].as_units('s').loc[0:20], decoded['b'].as_units('s').loc[0:20], label = 'Decoded')\nplt.xlabel(\"Feature a\")\nplt.ylabel(\"Feature b\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</pre> decoded, proba_feature = nap.decode_2d(tuning_curves=tcurves2d,                                       group=ts_group,                                                                         ep=epoch,                                      bin_size=0.1, # second                                      xy=binsxy,                                     features=features,                                     )   plt.figure(figsize=(15,5)) plt.subplot(131) plt.plot(features['a'].as_units('s').loc[0:20], label = 'True') plt.plot(decoded['a'].as_units('s').loc[0:20], label = 'Decoded') plt.legend() plt.xlabel(\"Time (s)\") plt.ylabel(\"Feature a\") plt.subplot(132) plt.plot(features['b'].as_units('s').loc[0:20], label = 'True') plt.plot(decoded['b'].as_units('s').loc[0:20], label = 'Decoded') plt.legend() plt.xlabel(\"Time (s)\") plt.ylabel(\"Feature b\") plt.subplot(133) plt.plot(features['a'].as_units('s').loc[0:20], features['b'].as_units('s').loc[0:20], label = 'True') plt.plot(decoded['a'].as_units('s').loc[0:20], decoded['b'].as_units('s').loc[0:20], label = 'Decoded') plt.xlabel(\"Feature a\") plt.ylabel(\"Feature b\") plt.legend() plt.tight_layout() plt.show() <p><code>shift_timestamps</code> shifts all the timestamps in a <code>Ts</code> object by the same random amount, wrapping the end of the time support to its beginning. This randomization preserves the temporal structure in the data but destroys the temporal relationships with other quantities (e.g. behavioural data). When applied on a <code>TsGroup</code> object, each series in the group is shifted independently.</p> In\u00a0[14]: Copied! <pre>ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 'ms')\nrand_ts = nap.shift_timestamps(ts,min_shift=1, max_shift=20)\n\nplt.eventplot([ts.times(),rand_ts.times()])\nplt.xlabel('time (s)')\nplt.yticks([0,1],['original','randomized']);\n</pre> ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 'ms') rand_ts = nap.shift_timestamps(ts,min_shift=1, max_shift=20)  plt.eventplot([ts.times(),rand_ts.times()]) plt.xlabel('time (s)') plt.yticks([0,1],['original','randomized']); <p><code>shuffle_ts_intervals</code> computes the intervals between consecutive timestamps, permutes them, and generates a new set of timestamps with the permuted intervals. This procedure preserve the distribution of intervals, but not their sequence.</p> In\u00a0[15]: Copied! <pre>ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's')\nrand_ts = nap.shuffle_ts_intervals(ts)\n\nplt.eventplot([ts.times(),rand_ts.times()])\nplt.xlabel('time (s)')\nplt.yticks([0,1],['original','randomized']);\n</pre> ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's') rand_ts = nap.shuffle_ts_intervals(ts)  plt.eventplot([ts.times(),rand_ts.times()]) plt.xlabel('time (s)') plt.yticks([0,1],['original','randomized']); <p><code>jitter_timestamps</code> shifts each timestamp in the data of an independent random amount. When applied with a small <code>max_jitter</code>, this procedure destroys the fine temporal structure of the data, while preserving structure on longer timescales.</p> In\u00a0[18]: Copied! <pre>ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's')\nrand_ts = nap.jitter_timestamps(ts,max_jitter=1)\n\nplt.eventplot([ts.times(),rand_ts.times()])\nplt.xlabel('time (s)')\nplt.yticks([0,1],['original','randomized']);\n</pre> ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's') rand_ts = nap.jitter_timestamps(ts,max_jitter=1)  plt.eventplot([ts.times(),rand_ts.times()]) plt.xlabel('time (s)') plt.yticks([0,1],['original','randomized']); <p><code>resample_timestamps</code> uniformly re-draws the same number of timestamps in <code>ts</code>, in the same time support. This procedures preserve the total number of timestamps, but destroys any other feature of the original data.</p> In\u00a0[19]: Copied! <pre>ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's')\nrand_ts = nap.resample_timestamps(ts)\n\nplt.eventplot([ts.times(),rand_ts.times()])\nplt.xlabel('time (s)')\nplt.yticks([0,1],['original','randomized']);\n</pre> ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's') rand_ts = nap.resample_timestamps(ts)  plt.eventplot([ts.times(),rand_ts.times()]) plt.xlabel('time (s)') plt.yticks([0,1],['original','randomized']);"},{"location":"notebooks/pynapple-process-notebook/#advanced-processing","title":"Advanced processing\u00b6","text":"<p>The pynapple package provides a small set of high-level functions that are widely used in systems neuroscience.</p> <ul> <li>Discrete correlograms</li> <li>Tuning curves</li> <li>Decoding</li> <li>PETH</li> <li>Randomization</li> </ul> <p>This notebook provides few examples with artificial data.</p>"},{"location":"notebooks/pynapple-process-notebook/#discrete-correlograms","title":"Discrete correlograms\u00b6","text":"<p>The function to compute cross-correlogram is cross_correlogram.</p> <p>The function is compiled with numba to improve performances. This means it only accepts pure numpy arrays as input arguments.</p>"},{"location":"notebooks/pynapple-process-notebook/#peri-event-time-histogram-peth","title":"Peri-Event Time Histogram (PETH)\u00b6","text":"<p>A second way to examine the relationship between spkiking and an event (i.e. stimulus) is to compute a PETH. pynapple uses the function compute_perievent to center spike time around the timestamps of an event within a given window.</p>"},{"location":"notebooks/pynapple-process-notebook/#tuning-curves","title":"Tuning curves\u00b6","text":"<p>At time of writing, pynapple can compute 1 dimension tuning curves (for example firing rate as a function of angular direction) and 2 dimension tuning curves ( for example firing rate as a function of position). In both cases, a TsGroup object can be directly passed to the function.</p>"},{"location":"notebooks/pynapple-process-notebook/#decoding","title":"Decoding\u00b6","text":"<p>Pynapple supports 1 dimensional and 2 dimensional bayesian decoding. The function returns the decoded feature as well as the probabilities for each timestamps.</p> <p>First we generate some artificial \"place fields\" in 2 dimensions based on the features.</p>"},{"location":"notebooks/pynapple-process-notebook/#randomization","title":"Randomization\u00b6","text":"<p>Pynapple provides some ready-to-use randomization methods to compute null distributions for statistical testing. Different methods preserve or destroy different features of the data, here's a brief overview.</p>"},{"location":"notebooks/pynapple-quick-start/","title":"Quick start","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport pynapple as nap\n</pre> import numpy as np import pandas as pd import pynapple as nap <p>The first step is to give the path to the data folder.</p> In\u00a0[2]: Copied! <pre>data_directory = '/home/guillaume/pynapple/data/A2929-200711'\n</pre> data_directory = '/home/guillaume/pynapple/data/A2929-200711' <p>We can load the session with the function load_session. When loading a session for the first time, pynapple will show a GUI for the user to provide information about the session, the subject, the tracking, the epochs and the neuronal data. When this information has been entered, a NWB file is created. In this example dataset, the NWB file already exists.</p> In\u00a0[3]: Copied! <pre>data = nap.load_session(data_directory, 'neurosuite')\n</pre> data = nap.load_session(data_directory, 'neurosuite') <p>The object data contains the information about the session such as the spike times of all the neurons, the tracking data and the start and ends of the epochs. We can individually call each object.</p> In\u00a0[4]: Copied! <pre>spikes = data.spikes\nspikes\n</pre> spikes = data.spikes spikes Out[4]: <pre>  Index    rate    group  location\n-------  ------  -------  ----------\n      0    7.3         0  adn\n      1    5.73        0  adn\n      2    8.12        0  adn\n      3    6.68        0  adn\n      4   10.77        0  adn\n      5   11           0  adn\n      6   16.52        0  adn\n      7    2.2         1  ca1\n      8    2.02        1  ca1\n      9    1.07        1  ca1\n     10    3.92        1  ca1\n     11    3.31        1  ca1\n     12    1.09        1  ca1\n     13    1.28        1  ca1\n     14    1.32        1  ca1</pre> <p>spikes is a TsGroup object. It allows to group together time series with different timestamps and couple metainformation to each neuron. In this case, the location of where the neuron was recorded has been added when loading the session for the first time.</p> <p>In this case, the TsGroup holds 15 neurons and it is possible to access, similar to a dictionnary, the spike times of a single neuron:</p> In\u00a0[5]: Copied! <pre>neuron_0 = spikes[0]\nneuron_0\n</pre> neuron_0 = spikes[0] neuron_0 Out[5]: <pre>Time (s)\n0.00845      NaN\n0.03265      NaN\n0.13230      NaN\n0.30340      NaN\n0.32900      NaN\n              ..\n1186.12755   NaN\n1189.38400   NaN\n1194.13475   NaN\n1196.20750   NaN\n1196.67675   NaN\nLength: 8764, dtype: float64</pre> <p>neuron_0 is a Ts object containing the times of the spikes.</p> <p>The other information about the session is contained in data.epochs. In this case, the start and end of the sleep and wake epochs.</p> In\u00a0[6]: Copied! <pre>epochs = data.epochs\nepochs\n</pre> epochs = data.epochs epochs Out[6]: <pre>{'sleep':    start    end\n 0    0.0  600.0,\n 'wake':    start     end\n 0  600.0  1200.0}</pre> <p>Finally this dataset contains tracking of the animal in the environment. It can be accessed through data.position. rx, ry, rz represent respectively the roll, the yaw and the pitch of the head of the animal. x and z represent the position of the animal in the horizontal plane while y represents the elevation.</p> In\u00a0[7]: Copied! <pre>position = data.position\nprint(position)\n</pre> position = data.position print(position) <pre>                  rx        ry        rz         x         y         z\nTime (s)                                                              \n670.64070   0.343163  5.207148  5.933598 -0.042857  0.425023 -0.195725\n670.64900   0.346745  5.181029  5.917368 -0.043863  0.424850 -0.195110\n670.65735   0.344035  5.155508  5.905679 -0.044853  0.424697 -0.194674\n670.66565   0.322240  5.136537  5.892457 -0.045787  0.424574 -0.194342\n670.67400   0.315836  5.120850  5.891577 -0.046756  0.424563 -0.194059\n...              ...       ...       ...       ...       ...       ...\n1199.96160  6.009812  3.665954  0.230562  0.011241  0.037891 -0.001479\n1199.96995  6.014660  3.634619  0.260742  0.010974  0.038677 -0.002370\n1199.97825  6.031694  3.617849  0.276835  0.010786  0.039410 -0.003156\n1199.98660  6.040435  3.609446  0.287006  0.010661  0.040064 -0.003821\n1199.99495  6.050059  3.609375  0.293275  0.010624  0.040568 -0.004435\n\n[63527 rows x 6 columns]\n</pre> <p>The core functions of pynapple provides many ways to manipulate time series. In this example, spike times are restricted to the wake epoch. Notice how the frequencies change.</p> In\u00a0[8]: Copied! <pre>spikes_wake = spikes.restrict(epochs['wake'])\n\nprint(spikes_wake)\n</pre> spikes_wake = spikes.restrict(epochs['wake'])  print(spikes_wake) <pre>  Index    rate    group  location\n-------  ------  -------  ----------\n      0    4.85        0  adn\n      1    8.06        0  adn\n      2    7.11        0  adn\n      3    7.66        0  adn\n      4    7.97        0  adn\n      5   11.29        0  adn\n      6   22.08        0  adn\n      7    1.82        1  ca1\n      8    2.84        1  ca1\n      9    0.7         1  ca1\n     10    4.78        1  ca1\n     11    4.93        1  ca1\n     12    1.71        1  ca1\n     13    0.97        1  ca1\n     14    0.26        1  ca1\n</pre> <p>The same operation can be applied to position. But in this example, we want all the epochs for which position in x is above a certain threhsold. For this we use the function threshold.</p> In\u00a0[9]: Copied! <pre>import matplotlib.pyplot as plt\n\nposx = position['x']\n\nthreshold = 0.08\n\nposxpositive = posx.threshold(threshold)\n\nplt.plot(posx.as_units('s'))\nplt.plot(posxpositive.as_units('s'), '.')\nplt.axhline(threshold)\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"x\")\n</pre> import matplotlib.pyplot as plt  posx = position['x']  threshold = 0.08  posxpositive = posx.threshold(threshold)  plt.plot(posx.as_units('s')) plt.plot(posxpositive.as_units('s'), '.') plt.axhline(threshold) plt.xlabel(\"Time (s)\") plt.ylabel(\"x\") Out[9]: <pre>Text(0, 0.5, 'x')</pre> <p>The epochs above the threshold can be accessed through the time support of the Tsd object. The time support is an important concept in the pynapple package. It helps the user to define the epochs for which the time serie should be defined. By default, Ts, Tsd and TsGroup objects possess a time support (defined as an IntervalSet). It is recommended to pass the time support when instantiating one of those objects.</p> In\u00a0[10]: Copied! <pre>epochs_above_thr = posxpositive.time_support\nprint(epochs_above_thr)\n</pre> epochs_above_thr = posxpositive.time_support print(epochs_above_thr) <pre>        start         end\n0  682.660850  745.565725\n1  752.240350  752.440325\n2  752.582000  752.673650\n3  757.498375  758.998300\n4  789.863275  790.271575\n5  875.225250  876.066875\n6  878.158425  878.641725\n</pre> In\u00a0[11]: Copied! <pre>tuning_curves = nap.compute_1d_tuning_curves(group=spikes, \n                                             feature=position['ry'],                                              \n                                             nb_bins=121, \n                                             minmax=(0, 2*np.pi))\n\ntuning_curves\n</pre> tuning_curves = nap.compute_1d_tuning_curves(group=spikes,                                               feature=position['ry'],                                                                                            nb_bins=121,                                               minmax=(0, 2*np.pi))  tuning_curves Out[11]: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 0.025964 45.520459 0.0 0.000000 6.207335 6.207335 6.207335 0.0 6.207335 12.414671 2.069112 6.207335 10.345559 14.483782 0.000000 2.069112 0.077891 55.049762 0.0 0.000000 5.504976 3.302986 3.302986 0.0 11.009952 11.009952 2.201990 8.807962 16.514929 1.100995 0.000000 0.000000 0.129818 76.369034 0.0 0.000000 17.144069 4.675655 1.558552 0.0 12.468414 4.675655 1.558552 3.117103 12.468414 9.351310 1.558552 0.000000 0.181745 82.179721 0.0 0.000000 6.522200 1.304440 2.608880 0.0 19.566600 9.131080 0.000000 6.522200 19.566600 9.131080 2.608880 0.000000 0.233672 73.851374 0.0 0.000000 13.187745 5.275098 2.637549 0.0 6.593873 3.956324 1.318775 15.825294 30.331814 7.912647 2.637549 0.000000 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 6.049513 15.001060 0.0 0.000000 12.273595 1.363733 15.001060 0.0 0.000000 1.363733 0.000000 8.182397 2.727466 0.000000 0.000000 1.363733 6.101440 22.327159 0.0 0.000000 13.954475 0.000000 11.163580 0.0 8.372685 2.790895 0.000000 2.790895 11.163580 2.790895 0.000000 0.000000 6.153367 47.062150 0.0 0.000000 21.177967 0.000000 7.059322 0.0 2.353107 4.706215 2.353107 7.059322 11.765537 0.000000 2.353107 0.000000 6.205295 56.003958 0.0 2.000141 8.000565 2.000141 10.000707 0.0 8.000565 4.000283 0.000000 14.000990 24.001696 6.000424 0.000000 0.000000 6.257222 38.712414 0.0 0.000000 7.742483 0.000000 7.742483 0.0 0.000000 11.613724 0.000000 7.742483 7.742483 7.742483 0.000000 0.000000 <p>121 rows \u00d7 15 columns</p> <p>We can plot tuning curves in polar plots.</p> In\u00a0[12]: Copied! <pre>neuron_location = spikes.get_info('location') # to know where the neuron was recorded\nplt.figure(figsize=(12,9))\n\nfor i,n in enumerate(tuning_curves.columns):\n    plt.subplot(3,5,i+1, projection = 'polar')\n    plt.plot(tuning_curves[n])\n    plt.title(neuron_location[n] + '-' + str(n), fontsize = 18)\n    \nplt.tight_layout()\nplt.show()\n</pre> neuron_location = spikes.get_info('location') # to know where the neuron was recorded plt.figure(figsize=(12,9))  for i,n in enumerate(tuning_curves.columns):     plt.subplot(3,5,i+1, projection = 'polar')     plt.plot(tuning_curves[n])     plt.title(neuron_location[n] + '-' + str(n), fontsize = 18)      plt.tight_layout() plt.show()  <p>While ADN neurons show obvious modulation for head-direction, it is not obvious for all CA1 cells. Therefore we want to restrict the remaining of the analyses to only ADN neurons. We can split the spikes group with the function getby_category.</p> In\u00a0[13]: Copied! <pre>spikes_by_location = spikes.getby_category('location')\n\nprint(spikes_by_location['adn'])\nprint(spikes_by_location['ca1'])\n\nspikes_adn = spikes_by_location['adn']\n</pre> spikes_by_location = spikes.getby_category('location')  print(spikes_by_location['adn']) print(spikes_by_location['ca1'])  spikes_adn = spikes_by_location['adn'] <pre>  Index    rate    group  location\n-------  ------  -------  ----------\n      0    7.3         0  adn\n      1    5.73        0  adn\n      2    8.12        0  adn\n      3    6.68        0  adn\n      4   10.77        0  adn\n      5   11           0  adn\n      6   16.52        0  adn\n  Index    rate    group  location\n-------  ------  -------  ----------\n      7    2.2         1  ca1\n      8    2.02        1  ca1\n      9    1.07        1  ca1\n     10    3.92        1  ca1\n     11    3.31        1  ca1\n     12    1.09        1  ca1\n     13    1.28        1  ca1\n     14    1.32        1  ca1\n</pre> In\u00a0[14]: Copied! <pre>cc_wake = nap.compute_crosscorrelogram(group=spikes_adn,                                        \n                                       binsize=20, # ms\n                                       windowsize=4000, # ms\n                                       ep=epochs['wake'], \n                                       norm=True,\n                                       time_units='ms')\n                                      \ncc_sleep = nap.compute_crosscorrelogram(group=spikes_adn,                                       \n                                       binsize=5, # ms\n                                       windowsize=400, # ms\n                                        ep=epochs['sleep'], \n                                       norm=True,\n                                       time_units='ms')\n</pre> cc_wake = nap.compute_crosscorrelogram(group=spikes_adn,                                                                                binsize=20, # ms                                        windowsize=4000, # ms                                        ep=epochs['wake'],                                         norm=True,                                        time_units='ms')                                        cc_sleep = nap.compute_crosscorrelogram(group=spikes_adn,                                                                               binsize=5, # ms                                        windowsize=400, # ms                                         ep=epochs['sleep'],                                         norm=True,                                        time_units='ms') <p>From the previous figure, we can see that neurons 0 and 1 fires for opposite directions during wake. Therefore we expect their cross-correlograms to show a trough around 0 time lag, meaning those two neurons do not fire spikes together. A similar trough during sleep for the same pair thus indicates a persistence of their coordination even if the animal is not moving its head.</p> In\u00a0[15]: Copied! <pre>xtwake = cc_wake.index.values\nxtsleep = cc_sleep.index.values\n\nplt.figure(figsize = (15, 5))\nplt.subplot(131, projection = 'polar')\nplt.plot(tuning_curves[[0,1]]) # The tuning curves of the pair [0,1]\nplt.subplot(132)\n# plt.plot(cc_wake[(0,1)], color = 'red') # The wake cross-corr of pair (0,1)\nplt.bar(xtwake, cc_wake[(0,1)].values, 0.02, color = 'green') # The wake cross-corr of pair (0,1)\nplt.title('wake')\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"CC\")\nplt.subplot(133)\n# plt.plot(cc_sleep[(0,1)], color = 'red')\nplt.bar(xtsleep, cc_sleep[(0,1)].values, 0.005, color = 'green') # The wake cross-corr of pair (0,1)\nplt.title('sleep')\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"CC\")\nplt.tight_layout()\nplt.show()\n</pre> xtwake = cc_wake.index.values xtsleep = cc_sleep.index.values  plt.figure(figsize = (15, 5)) plt.subplot(131, projection = 'polar') plt.plot(tuning_curves[[0,1]]) # The tuning curves of the pair [0,1] plt.subplot(132) # plt.plot(cc_wake[(0,1)], color = 'red') # The wake cross-corr of pair (0,1) plt.bar(xtwake, cc_wake[(0,1)].values, 0.02, color = 'green') # The wake cross-corr of pair (0,1) plt.title('wake') plt.xlabel(\"Time (ms)\") plt.ylabel(\"CC\") plt.subplot(133) # plt.plot(cc_sleep[(0,1)], color = 'red') plt.bar(xtsleep, cc_sleep[(0,1)].values, 0.005, color = 'green') # The wake cross-corr of pair (0,1) plt.title('sleep') plt.xlabel(\"Time (ms)\") plt.ylabel(\"CC\") plt.tight_layout() plt.show() In\u00a0[16]: Copied! <pre>tuning_curves_adn = nap.compute_1d_tuning_curves(spikes_adn,\n                                                 position['ry'],\n                                                 nb_bins=121,\n                                                 minmax=(0, 2*np.pi))\n\ndecoded, proba_angle = nap.decode_1d(tuning_curves=tuning_curves_adn, \n                                     group=spikes_adn, \n                                     ep=epochs['wake'],                                 \n                                     bin_size=0.3, # second\n                                     feature=position['ry'], \n                                    )\nprint(decoded)\n</pre> tuning_curves_adn = nap.compute_1d_tuning_curves(spikes_adn,                                                  position['ry'],                                                  nb_bins=121,                                                  minmax=(0, 2*np.pi))  decoded, proba_angle = nap.decode_1d(tuning_curves=tuning_curves_adn,                                       group=spikes_adn,                                       ep=epochs['wake'],                                                                       bin_size=0.3, # second                                      feature=position['ry'],                                      ) print(decoded) <pre>Time (s)\n600.15     2.154977\n600.45     2.154977\n600.75     2.258831\n601.05     1.947268\n601.35     2.154977\n             ...   \n1198.65    4.647480\n1198.95    4.232063\n1199.25    4.543626\n1199.55    4.647480\n1199.85    3.920500\nLength: 2000, dtype: float64\n</pre> <p>We can plot the decoded head-direction along with the true head-direction.</p> In\u00a0[17]: Copied! <pre>plt.figure(figsize=(15,5))\nplt.plot(position['ry'].as_units('s'), label = 'True')\nplt.plot(decoded.as_units('s'), label = 'Decoded')\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Head-direction (rad)\")\nplt.show()\n</pre> plt.figure(figsize=(15,5)) plt.plot(position['ry'].as_units('s'), label = 'True') plt.plot(decoded.as_units('s'), label = 'Decoded') plt.legend() plt.xlabel(\"Time (s)\") plt.ylabel(\"Head-direction (rad)\") plt.show() <p>Finally we can plot the decoded activity during sleep and overlay spiking activity of ADN neurons as a raster plot (in this case only during the first 10 seconds).</p> In\u00a0[18]: Copied! <pre>decoded_sleep, proba_angle_Sleep = nap.decode_1d(tuning_curves=tuning_curves_adn,\n                                                 group=spikes_adn, \n                                                 ep=epochs['sleep'],\n                                                 bin_size=0.1, # second\n                                                 feature=position['ry'], \n                                                 )\n\n# Finding quickly max direction of tuning curves\npeaks_adn = tuning_curves_adn.idxmax()\n\n# Defining a sub epoch during sleep\nsubep = nap.IntervalSet(start=0, end=10, time_units='s')\n\nplt.figure(figsize=(16,5))\n# create a raster plot\nfor n in spikes_adn.keys():\n    plt.plot(spikes_adn[n].restrict(subep).as_units('s').fillna(peaks_adn[n]), '|')\n\nplt.plot(decoded_sleep.restrict(subep).as_units('s'), label = 'Decoded sleep')\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Head-direction (rad)\")\nplt.show()\n</pre> decoded_sleep, proba_angle_Sleep = nap.decode_1d(tuning_curves=tuning_curves_adn,                                                  group=spikes_adn,                                                   ep=epochs['sleep'],                                                  bin_size=0.1, # second                                                  feature=position['ry'],                                                   )  # Finding quickly max direction of tuning curves peaks_adn = tuning_curves_adn.idxmax()  # Defining a sub epoch during sleep subep = nap.IntervalSet(start=0, end=10, time_units='s')  plt.figure(figsize=(16,5)) # create a raster plot for n in spikes_adn.keys():     plt.plot(spikes_adn[n].restrict(subep).as_units('s').fillna(peaks_adn[n]), '|')  plt.plot(decoded_sleep.restrict(subep).as_units('s'), label = 'Decoded sleep') plt.legend() plt.xlabel(\"Time (s)\") plt.ylabel(\"Head-direction (rad)\") plt.show()"},{"location":"notebooks/pynapple-quick-start/#quick-start","title":"Quick start\u00b6","text":"<p>The examplar data to replicate the figure in the jupyter notebook can be found here.</p> <p>The data contains a sample recordings taken simultaineously from the anterodorsal thalamus and the hippocampus and contains both a sleep and wake session. It contains both head-direction cells (i.e. cells that fire for a particular head direction in the horizontal plane) and place cells (i.e. cells that fire for a particular position in the environment).</p> <p>Preprocessing of the data was made with Kilosort 2.0 and spike sorting was made with Klusters.</p> <p>Instructions for installing pynapple can be found here.</p> <p>This notebook is meant to provide an overview of pynapple by going through:</p> <ol> <li>Input output (IO). In this case, pynapple will load a session containing data processed with NeuroSuite and automatically create a NWB file. See this notebook for more informations about IO and how to make a custom IO.</li> <li>Core functions that handle time series, interval sets and groups of time series. See this notebook for a detailled usage of the core functions.</li> <li>Process functions. A small collection of high-level functions widely used in system neuroscience. This notebook details those functions. Examples of higher analysis can be found in the collaborative repository pynacollada.</li> </ol>"},{"location":"notebooks/pynapple-quick-start/#tuning-curves","title":"Tuning curves\u00b6","text":"<p>Let's do a more advanced analysis. Neurons from ADn (group 0 in the spikes group object) are know to fire for a particular direction. Therefore, we can compute their tuning curves, i.e. their firing rates as a function of the head-direction of the animal in the horizontal plane (ry). To do this, we can use the function compute_1d_tuning_curves. In this case, the tuning curves are computed over 120 bins and between 0 and 2$\\pi$.</p>"},{"location":"notebooks/pynapple-quick-start/#correlograms","title":"Correlograms\u00b6","text":"<p>A classical question with head-direction cells is how pairs stay coordinated across brain states i.e. wake vs sleep (see Peyrache, A., Lacroix, M. M., Petersen, P. C., &amp; Buzs\u00e1ki, G. (2015). Internally organized mechanisms of the head direction sense. Nature neuroscience, 18(4), 569-575.)</p> <p>In this example, this coordination across brain states will be evaluated with cross-correlograms of pairs of neurons. We can call the function compute_crosscorrelogram during both sleep and wake epochs.</p>"},{"location":"notebooks/pynapple-quick-start/#decoding","title":"Decoding\u00b6","text":"<p>This last analysis shows how to use the pynapple's decoding function.</p> <p>The previous result indicates a persistent coordination of head-direction cells during sleep. Therefore it is possible to decode a virtual head-direction signal even if the animal is not moving its head.  This example uses the function decode_1d which implements bayesian decoding (see : Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.)</p> <p>First we can validate the decoding function with the real position of the head of the animal during wake.</p>"},{"location":"tutorials/tutorial_pynapple_core/","title":"-*- coding: utf-8 -*-","text":"<p>@Author: gviejo @Date:   2022-01-26 21:06:38 @Last Modified by:   gviejo @Last Modified time: 2022-01-26 21:13:18</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\nimport pynapple as nap\n</pre> import numpy as np import matplotlib.pyplot as plt import pynapple as nap <p>Time series object</p> <p>Let's create a Tsd object with fake data.  In this case, every time point is 1 second apart.  A Tsd object is a wrapper of pandas series.</p> In\u00a0[\u00a0]: Copied! <pre>tsd = nap.Tsd(t = np.arange(100), d = np.random.rand(100), time_units = 's')\n</pre> tsd = nap.Tsd(t = np.arange(100), d = np.random.rand(100), time_units = 's') In\u00a0[\u00a0]: Copied! <pre>print(tsd)\n</pre> print(tsd) <p>While the tsd object appears in second,  it actually holds the values in microseconds by default.  It is possible to switch between seconds, milliseconds and microseconds.  Note that when using as_units, the returned object is a simple pandas series.</p> In\u00a0[\u00a0]: Copied! <pre>print(tsd.as_units('ms'))\nprint(tsd.as_units('us'))\n</pre> print(tsd.as_units('ms')) print(tsd.as_units('us')) <p>If only timestamps are available, for example spike times,  we can construct a Ts object which holds only times.  In this case, we generate 10 random spike times between 0 and 100 ms.</p> In\u00a0[\u00a0]: Copied! <pre>ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 'ms')\nprint(ts)\n</pre> ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 'ms') print(ts) <p>If the time series contains multiple columns, we can use a TsdFrame.</p> In\u00a0[\u00a0]: Copied! <pre>tsdframe = nap.TsdFrame(t = np.arange(100), d = np.random.rand(100,3), time_units = 's', columns = ['a', 'b', 'c'])\nprint(tsdframe)\n</pre> tsdframe = nap.TsdFrame(t = np.arange(100), d = np.random.rand(100,3), time_units = 's', columns = ['a', 'b', 'c']) print(tsdframe) In\u00a0[\u00a0]: Copied! <pre>epochs = nap.IntervalSet(start = [0, 10], end = [5, 15], time_units = 's')\n</pre> epochs = nap.IntervalSet(start = [0, 10], end = [5, 15], time_units = 's') In\u00a0[\u00a0]: Copied! <pre>new_tsd = tsd.restrict(epochs)\n</pre> new_tsd = tsd.restrict(epochs) In\u00a0[\u00a0]: Copied! <pre>print(epochs)\nprint('\\n')\nprint(new_tsd)\n</pre> print(epochs) print('\\n') print(new_tsd) <p>Multiple operations are available for IntervalSet.  For example, IntervalSet can be merged.  See the full documentation of the class at  https://pynapple-org.github.io/pynapple/core.interval_set/#pynapple.core.interval_set.IntervalSet.intersect  for a list of all the functions that can be used to manipulate IntervalSets.</p> In\u00a0[\u00a0]: Copied! <pre>epoch1 = nap.IntervalSet(start=[0], end=[10]) # no time units passed. Default is us.\nepoch2 = nap.IntervalSet(start=[5,30],end=[20,45])\n</pre> epoch1 = nap.IntervalSet(start=[0], end=[10]) # no time units passed. Default is us. epoch2 = nap.IntervalSet(start=[5,30],end=[20,45]) In\u00a0[\u00a0]: Copied! <pre>epoch = epoch1.union(epoch2)\nprint(epoch1, '\\n')\nprint(epoch2, '\\n')\nprint(epoch)\n</pre> epoch = epoch1.union(epoch2) print(epoch1, '\\n') print(epoch2, '\\n') print(epoch) In\u00a0[\u00a0]: Copied! <pre>my_ts = {0:nap.Ts(t = np.sort(np.random.uniform(0, 100, 1000)), time_units = 's'), # here a simple dictionnary\n         1:nap.Ts(t = np.sort(np.random.uniform(0, 100, 2000)), time_units = 's'),\n         2:nap.Ts(t = np.sort(np.random.uniform(0, 100, 3000)), time_units = 's')}\n</pre> my_ts = {0:nap.Ts(t = np.sort(np.random.uniform(0, 100, 1000)), time_units = 's'), # here a simple dictionnary          1:nap.Ts(t = np.sort(np.random.uniform(0, 100, 2000)), time_units = 's'),          2:nap.Ts(t = np.sort(np.random.uniform(0, 100, 3000)), time_units = 's')} In\u00a0[\u00a0]: Copied! <pre>tsgroup = nap.TsGroup(my_ts)\n</pre> tsgroup = nap.TsGroup(my_ts) In\u00a0[\u00a0]: Copied! <pre>print(tsgroup, '\\n')\nprint(tsgroup[0], '\\n') # dictionnary like indexing returns directly the Ts object\nprint(tsgroup[[0,2]]) # list like indexing\n</pre> print(tsgroup, '\\n') print(tsgroup[0], '\\n') # dictionnary like indexing returns directly the Ts object print(tsgroup[[0,2]]) # list like indexing <p>Operations such as restrict can thus be directly applied to the TsGroup as well as other operations.</p> In\u00a0[\u00a0]: Copied! <pre>newtsgroup = tsgroup.restrict(epochs)\ncount = tsgroup.count(1, epochs, time_units='s') # Here counting the elements within bins of 1 seconds\nprint(count)\n</pre> newtsgroup = tsgroup.restrict(epochs) count = tsgroup.count(1, epochs, time_units='s') # Here counting the elements within bins of 1 seconds print(count) <p>One advantage of grouping time series is that metainformation can be added about each elements.  In this case, we add labels to each Ts object when instantiating the group and after.  We can then use this label to split the group.  See the documentation about TsGroup at  https://pynapple-org.github.io/pynapple/core.ts_group/  for all the ways to split TsGroup.</p> In\u00a0[\u00a0]: Copied! <pre>tsgroup = nap.TsGroup(my_ts, time_units = 's', label1=[0,1,0])\ntsgroup.set_info(label1=np.array(['a', 'a', 'b']))\n</pre> tsgroup = nap.TsGroup(my_ts, time_units = 's', label1=[0,1,0]) tsgroup.set_info(label1=np.array(['a', 'a', 'b'])) In\u00a0[\u00a0]: Copied! <pre>print(tsgroup, '\\n')\n</pre> print(tsgroup, '\\n') In\u00a0[\u00a0]: Copied! <pre>newtsgroup= tsgroup.getby_category('label1')\nprint(newtsgroup['a'], '\\n')\nprint(newtsgroup['b'])\n</pre> newtsgroup= tsgroup.getby_category('label1') print(newtsgroup['a'], '\\n') print(newtsgroup['b']) In\u00a0[\u00a0]: Copied! <pre>time_support = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n</pre> time_support = nap.IntervalSet(start = 0, end = 100, time_units = 's') In\u00a0[\u00a0]: Copied! <pre>my_ts = {0:nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's'), # here a simple dictionnary\n         1:nap.Ts(t = np.sort(np.random.uniform(0, 100, 20)), time_units = 's'),\n         2:nap.Ts(t = np.sort(np.random.uniform(0, 100, 30)), time_units = 's')}\n</pre> my_ts = {0:nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's'), # here a simple dictionnary          1:nap.Ts(t = np.sort(np.random.uniform(0, 100, 20)), time_units = 's'),          2:nap.Ts(t = np.sort(np.random.uniform(0, 100, 30)), time_units = 's')} In\u00a0[\u00a0]: Copied! <pre>tsgroup = nap.TsGroup(my_ts)\n</pre> tsgroup = nap.TsGroup(my_ts) In\u00a0[\u00a0]: Copied! <pre>tsgroup_with_time_support = nap.TsGroup(my_ts, time_support = time_support)\n</pre> tsgroup_with_time_support = nap.TsGroup(my_ts, time_support = time_support) In\u00a0[\u00a0]: Copied! <pre>print(tsgroup, '\\n')\n</pre> print(tsgroup, '\\n') In\u00a0[\u00a0]: Copied! <pre>print(tsgroup_with_time_support, '\\n')\n</pre> print(tsgroup_with_time_support, '\\n') In\u00a0[\u00a0]: Copied! <pre>print(tsgroup_with_time_support.time_support) # acceding the time support\n</pre> print(tsgroup_with_time_support.time_support) # acceding the time support"},{"location":"tutorials/tutorial_pynapple_core/#core-tutorial","title":"Core Tutorial\u00b6","text":"<p>This script will introduce you to the basics of time series handling with pynapple.</p>"},{"location":"tutorials/tutorial_pynapple_core/#interval-sets-object","title":"Interval Sets object\u00b6","text":"<p>The IntervalSet object stores multiple epochs with a common time units.  It can then be used to restrict time series to this particular set of epochs.</p>"},{"location":"tutorials/tutorial_pynapple_core/#tsgroup","title":"TsGroup\u00b6","text":"<p>Multiple time series with different time stamps  (.i.e. a group of neurons with different spike times from one session)  can be grouped with the TsGroup object.  The TsGroup behaves like a dictionnary but it is also possible to slice with a list of indexes</p>"},{"location":"tutorials/tutorial_pynapple_core/#time-support","title":"Time support\u00b6","text":"<p>A key element of the manipulation of time series by pynapple is the inherent time support defined for Ts, Tsd, TsdFrame and TsGroup objects.  The time support is defined as an IntervalSet that provides the time serie with a context.  For example,, the restrict operation will update automatically the time support to the new time series.  Ideally the time support should be defined for all time series when instantiating them.  If no time series is given, the time support is inferred from the start and end of the time series.  In this example, a TsGroup is instantiated with and without a time support. Notice how the frequency of each Ts element is changed when the time support is defined explicitely.</p>"},{"location":"tutorials/tutorial_pynapple_process/","title":"-*- coding: utf-8 -*-","text":"In\u00a0[\u00a0]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline In\u00a0[\u00a0]: Copied! <pre># @Author: gviejo\n# @Date:   2022-01-26 21:14:31\n# @Last Modified by:   gviejo\n# @Last Modified time: 2022-01-26 21:17:05\n!/usr/bin/env python\n\n# # Advanced processing\n# \n# The pynapple package provides a small set of high-level functions that are widely used in system neuroscience.\n# - [Discrete correlograms](https://peyrachelab.github.io/pynapple/process.correlograms/)\n# - [Tuning curves](https://peyrachelab.github.io/pynapple/process.tuning_curves/)\n# - [Decoding](https://peyrachelab.github.io/pynapple/process.decoding/)\n# \n# This notebook provides few examples with artificial data.\n\nimport numpy as np\nimport pandas as pd\nimport pynapple as nap\nimport matplotlib.pyplot as plt\n\n\n# ## Discrete correlograms\n# The main function to compute cross-correlogram is cross_correlogram\n# See https://peyrachelab.github.io/pynapple/process.correlograms/#pynapple.process.correlograms.cross_correlogram\n\n# The function is compiled with [numba](https://numba.pydata.org/) to increase performances. \n# This means it accepts only pure numpy array as argument.\n\n\nts1 = nap.Ts(t = np.sort(np.random.uniform(0, 1000, 1000)), time_units = 's')\nts2 = nap.Ts(t = np.sort(np.random.uniform(0, 1000, 1000)), time_units = 's')\n\nts1_time_array = ts1.as_units('ms').index.values\nts2_time_array = ts2.as_units('ms').index.values\n\nbinsize = 100 # ms\ncc12, xt = nap.cross_correlogram(t1 = ts1_time_array,\n                                 t2 = ts2_time_array,\n                                 binsize=binsize,\n                                 windowsize=1000 # ms\n                                )\n\nplt.figure()\nplt.bar(xt, cc12, binsize)\nplt.xlabel(\"Time t1 (ms)\")\nplt.ylabel(\"CC\")\n\n\n# Since it's a bit tedious to convert to a numpy.ndarray, \n# pynapple provides wrappers for computing autocorrelogram and crosscorrelogram for TsGroup. \n# The function is then called for each unit or each pairs of units. \n# It returns directly a pandas.DataFrame holding all the correlograms. \n# In this example, autocorrelograms and cross-correlograms are computed for the same TsGroup.\n\n\nepoch = nap.IntervalSet(start = 0, end = 1000, time_units = 's')\nts_group = nap.TsGroup({0:ts1,1:ts2}, time_support = epoch)\n\nautocorrs = nap.compute_autocorrelogram(group=ts_group, \n                                        ep=epoch,\n                                        binsize=100, # ms\n                                        windowsize=1000, # ms                                        \n                                       )\ncrosscorrs = nap.compute_crosscorrelogram(group=ts_group,\n                                        ep=epoch,\n                                        binsize=100, # ms\n                                        windowsize=1000, # ms                                        \n                                       )\n\nprint(autocorrs, '\\n')\nprint(crosscorrs, '\\n')\n\n\n# ## Tuning curves\n# \n# For now, pynapple can compute 1 dimension tuning curves (for example firing rate as a function of angular direction) \n# and 2 dimension tuning curves ( for example firing rate as a function of position). \n# In both cases, a TsGroup object can be directly passed to the function.\n\n\nfeatures = np.vstack((np.cos(np.arange(0, 1000, 0.1)),np.sin(np.arange(0,1000,0.1)))).T\nfeatures = nap.TsdFrame(t = np.arange(0, 1000, 0.1), d = features, time_units = 's', time_support = epoch, columns=['a', 'b'])\n\nprint(features)\n\ntcurves1d = nap.compute_1d_tuning_curves(group=ts_group,\n                                         feature = features['a'],\n                                         ep = epoch,\n                                         nb_bins=10)\n\n\ntcurves2d, binsxy = nap.compute_2d_tuning_curves(group=ts_group,\n                                                 feature = features,\n                                                 ep = epoch,\n                                                 nb_bins=10)\n\nplt.figure(figsize=(15,5))\nplt.subplot(131)\nplt.plot(features.as_units('s').loc[0:10])\nplt.title(\"Features\")\nplt.xlabel(\"Time(s)\")\nplt.subplot(132)\nplt.title(\"Features\")\nplt.plot(features['a'].as_units('s').loc[0:10], features['b'].as_units('s').loc[0:10])\nplt.xlabel(\"Feature a\")\nplt.ylabel(\"Feature b\")\nplt.subplot(133)\nplt.title(\"Tuning curve unit 0 2d\")\nplt.imshow(tcurves2d[0], extent=(binsxy[1][0],binsxy[1][-1],binsxy[0][0],binsxy[0][-1]))\nplt.tight_layout()\nplt.show()\n\n\n# ## Decoding\n#\n# Pynapple supports 1 dimension and 2 dimension bayesian decoding. \n# The function returns the decoded feature as well as the probabilities for each timestamps.\n# First let's generate some fake \"place fields\" in 2 dimensions based on the features.\n</pre> # @Author: gviejo # @Date:   2022-01-26 21:14:31 # @Last Modified by:   gviejo # @Last Modified time: 2022-01-26 21:17:05 !/usr/bin/env python  # # Advanced processing #  # The pynapple package provides a small set of high-level functions that are widely used in system neuroscience. # - [Discrete correlograms](https://peyrachelab.github.io/pynapple/process.correlograms/) # - [Tuning curves](https://peyrachelab.github.io/pynapple/process.tuning_curves/) # - [Decoding](https://peyrachelab.github.io/pynapple/process.decoding/) #  # This notebook provides few examples with artificial data.  import numpy as np import pandas as pd import pynapple as nap import matplotlib.pyplot as plt   # ## Discrete correlograms # The main function to compute cross-correlogram is cross_correlogram # See https://peyrachelab.github.io/pynapple/process.correlograms/#pynapple.process.correlograms.cross_correlogram  # The function is compiled with [numba](https://numba.pydata.org/) to increase performances.  # This means it accepts only pure numpy array as argument.   ts1 = nap.Ts(t = np.sort(np.random.uniform(0, 1000, 1000)), time_units = 's') ts2 = nap.Ts(t = np.sort(np.random.uniform(0, 1000, 1000)), time_units = 's')  ts1_time_array = ts1.as_units('ms').index.values ts2_time_array = ts2.as_units('ms').index.values  binsize = 100 # ms cc12, xt = nap.cross_correlogram(t1 = ts1_time_array,                                  t2 = ts2_time_array,                                  binsize=binsize,                                  windowsize=1000 # ms                                 )  plt.figure() plt.bar(xt, cc12, binsize) plt.xlabel(\"Time t1 (ms)\") plt.ylabel(\"CC\")   # Since it's a bit tedious to convert to a numpy.ndarray,  # pynapple provides wrappers for computing autocorrelogram and crosscorrelogram for TsGroup.  # The function is then called for each unit or each pairs of units.  # It returns directly a pandas.DataFrame holding all the correlograms.  # In this example, autocorrelograms and cross-correlograms are computed for the same TsGroup.   epoch = nap.IntervalSet(start = 0, end = 1000, time_units = 's') ts_group = nap.TsGroup({0:ts1,1:ts2}, time_support = epoch)  autocorrs = nap.compute_autocorrelogram(group=ts_group,                                          ep=epoch,                                         binsize=100, # ms                                         windowsize=1000, # ms                                                                                ) crosscorrs = nap.compute_crosscorrelogram(group=ts_group,                                         ep=epoch,                                         binsize=100, # ms                                         windowsize=1000, # ms                                                                                )  print(autocorrs, '\\n') print(crosscorrs, '\\n')   # ## Tuning curves #  # For now, pynapple can compute 1 dimension tuning curves (for example firing rate as a function of angular direction)  # and 2 dimension tuning curves ( for example firing rate as a function of position).  # In both cases, a TsGroup object can be directly passed to the function.   features = np.vstack((np.cos(np.arange(0, 1000, 0.1)),np.sin(np.arange(0,1000,0.1)))).T features = nap.TsdFrame(t = np.arange(0, 1000, 0.1), d = features, time_units = 's', time_support = epoch, columns=['a', 'b'])  print(features)  tcurves1d = nap.compute_1d_tuning_curves(group=ts_group,                                          feature = features['a'],                                          ep = epoch,                                          nb_bins=10)   tcurves2d, binsxy = nap.compute_2d_tuning_curves(group=ts_group,                                                  feature = features,                                                  ep = epoch,                                                  nb_bins=10)  plt.figure(figsize=(15,5)) plt.subplot(131) plt.plot(features.as_units('s').loc[0:10]) plt.title(\"Features\") plt.xlabel(\"Time(s)\") plt.subplot(132) plt.title(\"Features\") plt.plot(features['a'].as_units('s').loc[0:10], features['b'].as_units('s').loc[0:10]) plt.xlabel(\"Feature a\") plt.ylabel(\"Feature b\") plt.subplot(133) plt.title(\"Tuning curve unit 0 2d\") plt.imshow(tcurves2d[0], extent=(binsxy[1][0],binsxy[1][-1],binsxy[0][0],binsxy[0][-1])) plt.tight_layout() plt.show()   # ## Decoding # # Pynapple supports 1 dimension and 2 dimension bayesian decoding.  # The function returns the decoded feature as well as the probabilities for each timestamps. # First let's generate some fake \"place fields\" in 2 dimensions based on the features.  <p>This part is just to generate units with a relationship to the features (i.e. \"place fields\")</p> In\u00a0[\u00a0]: Copied! <pre>times = features.index.values\nft = features.values\n\nbounds=np.arange(-1.0, 1.0, 0.5)\nts_group = {}\ncount = 0\nfor x in bounds:\n    for y in bounds:        \n        tidx = times[(ft[:,0]&gt;x) &amp; (ft[:,0]&lt;x+0.5) &amp; (ft[:,1]&gt;y) &amp; (ft[:,1]&lt;y+0.5)]        \n        if len(tidx):\n            ts_group[count] = nap.Ts(t=np.unique(tidx))\n            count += 1\nts_group = nap.TsGroup(ts_group, time_support = epoch)\n</pre> times = features.index.values ft = features.values  bounds=np.arange(-1.0, 1.0, 0.5) ts_group = {} count = 0 for x in bounds:     for y in bounds:                 tidx = times[(ft[:,0]&gt;x) &amp; (ft[:,0]y) &amp; (ft[:,1] In\u00a0[\u00a0]: Copied! <pre>import warnings\nwarnings.filterwarnings('ignore')\n\ntcurves2d, binsxy = nap.compute_2d_tuning_curves(group=ts_group,feature = features,ep = epoch,nb_bins=8)\n\n\n# Let's plot the \"place fields\".\n\n\nplt.figure(figsize = (16,4))\nfor i in ts_group.keys():\n    plt.subplot(2,6,i+1)\n    plt.imshow(tcurves2d[i], extent=(binsxy[1][0],binsxy[1][-1],binsxy[0][0],binsxy[0][-1]))\n    plt.xticks()\nplt.show()\n\n\n# Then we can call the actual decoding function in 2d.\n\n\ndecoded, proba_feature = nap.decode_2d(tuning_curves=tcurves2d, \n                                     group=ts_group, \n                                     features=features, \n                                     ep=epoch,\n                                     bin_size=0.1, # second\n                                     xy=binsxy\n                                    )\n\n\nplt.figure(figsize=(15,5))\nplt.subplot(131)\nplt.plot(features['a'].as_units('s').loc[0:20], label = 'True')\nplt.plot(decoded['a'].as_units('s').loc[0:20], label = 'Decoded')\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Feature a\")\nplt.subplot(132)\nplt.plot(features['b'].as_units('s').loc[0:20], label = 'True')\nplt.plot(decoded['b'].as_units('s').loc[0:20], label = 'Decoded')\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Feature b\")\nplt.subplot(133)\nplt.plot(features['a'].as_units('s').loc[0:20], features['b'].as_units('s').loc[0:20], label = 'True')\nplt.plot(decoded['a'].as_units('s').loc[0:20], decoded['b'].as_units('s').loc[0:20], label = 'Decoded')\nplt.xlabel(\"Feature a\")\nplt.ylabel(\"Feature b\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</pre> import warnings warnings.filterwarnings('ignore')  tcurves2d, binsxy = nap.compute_2d_tuning_curves(group=ts_group,feature = features,ep = epoch,nb_bins=8)   # Let's plot the \"place fields\".   plt.figure(figsize = (16,4)) for i in ts_group.keys():     plt.subplot(2,6,i+1)     plt.imshow(tcurves2d[i], extent=(binsxy[1][0],binsxy[1][-1],binsxy[0][0],binsxy[0][-1]))     plt.xticks() plt.show()   # Then we can call the actual decoding function in 2d.   decoded, proba_feature = nap.decode_2d(tuning_curves=tcurves2d,                                       group=ts_group,                                       features=features,                                       ep=epoch,                                      bin_size=0.1, # second                                      xy=binsxy                                     )   plt.figure(figsize=(15,5)) plt.subplot(131) plt.plot(features['a'].as_units('s').loc[0:20], label = 'True') plt.plot(decoded['a'].as_units('s').loc[0:20], label = 'Decoded') plt.legend() plt.xlabel(\"Time (s)\") plt.ylabel(\"Feature a\") plt.subplot(132) plt.plot(features['b'].as_units('s').loc[0:20], label = 'True') plt.plot(decoded['b'].as_units('s').loc[0:20], label = 'Decoded') plt.legend() plt.xlabel(\"Time (s)\") plt.ylabel(\"Feature b\") plt.subplot(133) plt.plot(features['a'].as_units('s').loc[0:20], features['b'].as_units('s').loc[0:20], label = 'True') plt.plot(decoded['a'].as_units('s').loc[0:20], decoded['b'].as_units('s').loc[0:20], label = 'Decoded') plt.xlabel(\"Feature a\") plt.ylabel(\"Feature b\") plt.legend() plt.tight_layout() plt.show()"},{"location":"tutorials/tutorial_pynapple_quick_start/","title":"-*- coding: utf-8 -*-","text":"In\u00a0[\u00a0]: Copied! <pre># @Author: gviejo\n# @Date:   2022-01-26 21:18:16\n# @Last Modified by:   gviejo\n# @Last Modified time: 2022-01-26 21:27:38\n#!/usr/bin/env python\n</pre> # @Author: gviejo # @Date:   2022-01-26 21:18:16 # @Last Modified by:   gviejo # @Last Modified time: 2022-01-26 21:27:38 #!/usr/bin/env python In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport pynapple as nap\n</pre> import numpy as np import pandas as pd import pynapple as nap <p>The first step is to give the path to the data folder.</p> In\u00a0[\u00a0]: Copied! <pre>data_directory = '../../your/path/to/A2929-200711'\n</pre> data_directory = '../../your/path/to/A2929-200711' <p>The first step is to load the session with the function load_session.  When loading a session for the first time, pynapple will show a GUI  in order for the user to provide the information about the session, the subject, the tracking, the epochs and the neuronal data.  When informations has been entered, a NWB file is created. In this example dataset, the NWB file already exists.</p> In\u00a0[\u00a0]: Copied! <pre>data = nap.load_session(data_directory, 'neurosuite')\n</pre> data = nap.load_session(data_directory, 'neurosuite') <p>The object data contains the information about the session such as the spike times of all the neurons,  the tracking data and the start and ends of the epochs. We can check each object.</p> In\u00a0[\u00a0]: Copied! <pre>spikes = data.spikes\nspikes\n</pre> spikes = data.spikes spikes <p>spikes is a TsGroup object.  It allows to group together time series with different timestamps and associate metainformation about each neuron.  Under the hood, it wraps a dictionnary.  In this case, the location of where the neuron was recorded has been added when loading the session for the first time.</p> <p>In this case it holds 15 neurons and it is possible to access, similar to a dictionnary, the spike times of a single neuron:</p> In\u00a0[\u00a0]: Copied! <pre>neuron_0 = spikes[0]\nneuron_0\n</pre> neuron_0 = spikes[0] neuron_0 <p>neuron_0 is a Ts object containing the times of the spikes. Under the hood, it's wrapping a pandas series.</p> <p>The other information about the session is contained in data.epochs.  In this case, the start and end of the sleep and wake epochs.</p> In\u00a0[\u00a0]: Copied! <pre>epochs = data.epochs\nepochs\n</pre> epochs = data.epochs epochs <p>Finally this dataset contains tracking of the animal in the environment.  It can be accessed through data.position. rx, ry, rz represent respectively  the roll, the yaw and the pitch of the head of the animal. x and z represent the position of the animal in the horizontal plane while y represent the elevation.</p> In\u00a0[\u00a0]: Copied! <pre>position = data.position\nprint(position)\n</pre> position = data.position print(position) <p>The core functions of pynapple provides many ways to manipulate time series.  In this example, spike times are restricted to the wake epoch. Notice how the frequencies are changing.</p> In\u00a0[\u00a0]: Copied! <pre>spikes_wake = spikes.restrict(epochs['wake'])\n\nprint(spikes_wake)\n</pre> spikes_wake = spikes.restrict(epochs['wake'])  print(spikes_wake) <p>The same operation can be applied to position.  But in this example, we want all the epochs for which position in x is above a certain threhsold.  We can used the function threshold.</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nposx = position['x']\nthreshold = 0.08\nposxpositive = posx.threshold(threshold)\n\nplt.plot(posx.as_units('s'))\nplt.plot(posxpositive.as_units('s'), '.')\nplt.axhline(threshold)\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"x\")\n</pre> import matplotlib.pyplot as plt  posx = position['x'] threshold = 0.08 posxpositive = posx.threshold(threshold)  plt.plot(posx.as_units('s')) plt.plot(posxpositive.as_units('s'), '.') plt.axhline(threshold) plt.xlabel(\"Time (s)\") plt.ylabel(\"x\") <p>The epochs above the threshold can be accessed through the time support of the Tsd object.  The time support is an important concept in the pynapple package.  It helps the user to define the epochs for which the time serie should be defined.  By default, Ts, Tsd and TsGroup objects possess a time support (defined as an IntervalSet).  It is recommended to pass the time support when instantiating one of those objects.</p> In\u00a0[\u00a0]: Copied! <pre>epochs_above_thr = posxpositive.time_support\nprint(epochs_above_thr)\n</pre> epochs_above_thr = posxpositive.time_support print(epochs_above_thr) In\u00a0[\u00a0]: Copied! <pre>tuning_curves = nap.compute_1d_tuning_curves(spikes, \n                                             position['ry'], \n                                             121, \n                                             ep=position['ry'].time_support, \n                                             minmax=(0, 2*np.pi))\n\ntuning_curves\n</pre> tuning_curves = nap.compute_1d_tuning_curves(spikes,                                               position['ry'],                                               121,                                               ep=position['ry'].time_support,                                               minmax=(0, 2*np.pi))  tuning_curves <p>We can plot tuning curves in polar plots.</p> In\u00a0[\u00a0]: Copied! <pre>neuron_location = spikes.get_info('location') # to know where the neuron was recorded\nplt.figure(figsize=(12,9))\nfor i,n in enumerate(tuning_curves.columns):\n    plt.subplot(3,5,i+1, projection = 'polar')\n    plt.plot(tuning_curves[n])\n    plt.title(neuron_location[n] + '-' + str(n), fontsize = 18)\nplt.tight_layout()\nplt.show()\n</pre> neuron_location = spikes.get_info('location') # to know where the neuron was recorded plt.figure(figsize=(12,9)) for i,n in enumerate(tuning_curves.columns):     plt.subplot(3,5,i+1, projection = 'polar')     plt.plot(tuning_curves[n])     plt.title(neuron_location[n] + '-' + str(n), fontsize = 18) plt.tight_layout() plt.show()     <p>While ADN neurons show obvious modulation for head-direction, it is not obvious for all CA1 cells.  Therefore we want to restrict the remaining of the analysis to only ADN neurons.  We can split the spikes group with the function getby_category.</p> In\u00a0[\u00a0]: Copied! <pre>spikes_by_location = spikes.getby_category('location')\n\nprint(spikes_by_location['adn'])\nprint(spikes_by_location['ca1'])\n</pre> spikes_by_location = spikes.getby_category('location')  print(spikes_by_location['adn']) print(spikes_by_location['ca1']) In\u00a0[\u00a0]: Copied! <pre>spikes_adn = spikes_by_location['adn']\n</pre> spikes_adn = spikes_by_location['adn'] In\u00a0[\u00a0]: Copied! <pre>cc_wake = nap.compute_crosscorrelogram(group=spikes_adn, \n                                       binsize=20, # ms\n                                       windowsize=4000, # ms                                       \n                                       ep=epochs['wake'],\n                                       norm=True,\n                                       time_units=\"ms\")\ncc_sleep = nap.compute_crosscorrelogram(group=spikes_adn,\n                                        binsize=5, # ms\n                                        windowsize=400, # ms                                        \n                                        ep=epochs['sleep'], \n                                        norm=True,\n                                        time_units=\"ms\")\n</pre> cc_wake = nap.compute_crosscorrelogram(group=spikes_adn,                                         binsize=20, # ms                                        windowsize=4000, # ms                                                                               ep=epochs['wake'],                                        norm=True,                                        time_units=\"ms\") cc_sleep = nap.compute_crosscorrelogram(group=spikes_adn,                                         binsize=5, # ms                                         windowsize=400, # ms                                                                                 ep=epochs['sleep'],                                          norm=True,                                         time_units=\"ms\") In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>From the previous figure, we can see that neurons 0 and 1 fires for opposite direction during wake.  Therefore we expect their cross-correlograms to show a through around 0 time lag meaning those two neurons do not fire spikes together.  A similar through during sleep for the same pair will thus indicates a persistence of their coordination even if the animal is not moving its head.</p> In\u00a0[\u00a0]: Copied! <pre>xtwake = cc_wake.index.values\nxtsleep = cc_sleep.index.values\n\nplt.figure(figsize = (15, 5))\nplt.subplot(131, projection = 'polar')\nplt.plot(tuning_curves[[0,1]]) # The tuning curves of the pair [0,1]\nplt.subplot(132)\n# plt.plot(cc_wake[(0,1)], color = 'red') # The wake cross-corr of pair (0,1)\nplt.bar(xtwake, cc_wake[(0,1)].values, 0.02, color = 'green') # The wake cross-corr of pair (0,1)\nplt.title('wake')\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"CC\")\nplt.subplot(133)\n# plt.plot(cc_sleep[(0,1)], color = 'red')\nplt.bar(xtsleep, cc_sleep[(0,1)].values, 0.005, color = 'green') # The wake cross-corr of pair (0,1)\nplt.title('sleep')\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"CC\")\nplt.tight_layout()\nplt.show()\n</pre> xtwake = cc_wake.index.values xtsleep = cc_sleep.index.values  plt.figure(figsize = (15, 5)) plt.subplot(131, projection = 'polar') plt.plot(tuning_curves[[0,1]]) # The tuning curves of the pair [0,1] plt.subplot(132) # plt.plot(cc_wake[(0,1)], color = 'red') # The wake cross-corr of pair (0,1) plt.bar(xtwake, cc_wake[(0,1)].values, 0.02, color = 'green') # The wake cross-corr of pair (0,1) plt.title('wake') plt.xlabel(\"Time (ms)\") plt.ylabel(\"CC\") plt.subplot(133) # plt.plot(cc_sleep[(0,1)], color = 'red') plt.bar(xtsleep, cc_sleep[(0,1)].values, 0.005, color = 'green') # The wake cross-corr of pair (0,1) plt.title('sleep') plt.xlabel(\"Time (ms)\") plt.ylabel(\"CC\") plt.tight_layout() plt.show() In\u00a0[\u00a0]: Copied! <pre>tuning_curves_adn = nap.compute_1d_tuning_curves(\n    spikes_adn,\n    position['ry'],\n    nb_bins=61,\n    ep=position['ry'].time_support,\n    minmax=(0, 2*np.pi))\n</pre> tuning_curves_adn = nap.compute_1d_tuning_curves(     spikes_adn,     position['ry'],     nb_bins=61,     ep=position['ry'].time_support,     minmax=(0, 2*np.pi)) In\u00a0[\u00a0]: Copied! <pre>decoded, proba_angle = nap.decode_1d(tuning_curves=tuning_curves_adn, \n                                     group=spikes_adn, \n                                     ep=position['ry'].time_support,\n                                     bin_size=0.3, # second\n                                     feature=position['ry']\n                                    )\nprint(decoded)\n</pre> decoded, proba_angle = nap.decode_1d(tuning_curves=tuning_curves_adn,                                       group=spikes_adn,                                       ep=position['ry'].time_support,                                      bin_size=0.3, # second                                      feature=position['ry']                                     ) print(decoded) <p>We can plot the decoded head-direction along with the true head-direction.</p> In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(15,5))\nplt.plot(position['ry'].as_units('s'), label = 'True')\nplt.plot(decoded.as_units('s'), label = 'Decoded')\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Head-direction (rad)\")\nplt.show()\n</pre> plt.figure(figsize=(15,5)) plt.plot(position['ry'].as_units('s'), label = 'True') plt.plot(decoded.as_units('s'), label = 'Decoded') plt.legend() plt.xlabel(\"Time (s)\") plt.ylabel(\"Head-direction (rad)\") plt.show() <p>Finally we can plot the decoded activity during sleep and overlay spiking activity of ADN neurons  as a raster plot (in this case only during the first 10 seconds).</p> In\u00a0[\u00a0]: Copied! <pre>decoded_sleep, proba_angle_Sleep = nap.decode_1d(tuning_curves=tuning_curves_adn,\n                                                 group=spikes_adn, \n                                                 feature=position['ry'], \n                                                 ep=epochs['sleep'],\n                                                 bin_size=0.03 # second\n                                                )\n</pre> decoded_sleep, proba_angle_Sleep = nap.decode_1d(tuning_curves=tuning_curves_adn,                                                  group=spikes_adn,                                                   feature=position['ry'],                                                   ep=epochs['sleep'],                                                  bin_size=0.03 # second                                                 ) In\u00a0[\u00a0]: Copied! <pre># Finding quickly max direction of tuning curves\npeaks_adn = tuning_curves_adn.idxmax()\n</pre> # Finding quickly max direction of tuning curves peaks_adn = tuning_curves_adn.idxmax() In\u00a0[\u00a0]: Copied! <pre># I am adding peaks_adn to the metadata of spikes_adn\nspikes_adn.set_info(order= np.argsort(peaks_adn).values)\n</pre> # I am adding peaks_adn to the metadata of spikes_adn spikes_adn.set_info(order= np.argsort(peaks_adn).values) In\u00a0[\u00a0]: Copied! <pre># Defining a sub epoch during sleep\nsubep = nap.IntervalSet(start=0, end=10, time_units='s')\n</pre> # Defining a sub epoch during sleep subep = nap.IntervalSet(start=0, end=10, time_units='s') In\u00a0[\u00a0]: Copied! <pre>plt.figure(figsize=(16,5))\nplt.subplot(211)\n# create a raster plot\n# To create a raster plot, we can convert the tsgroup to a tsd and assing to each spikes the order of the peaks\nspikes_adn_tsd = spikes_adn.to_tsd(\"order\")\n# And plot it\nplt.plot(spikes_adn_tsd.restrict(subep), '|', markersize = 10)\nplt.xlim(subep.values[0])\nplt.subplot(212)\ntmp = proba_angle_Sleep.restrict(subep).values.T\nfrom scipy.ndimage import gaussian_filter\ntmp = gaussian_filter(tmp, 1)\nplt.imshow(tmp, aspect='auto', origin='lower')\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Head-direction (rad)\")\nplt.show()\n</pre> plt.figure(figsize=(16,5)) plt.subplot(211) # create a raster plot # To create a raster plot, we can convert the tsgroup to a tsd and assing to each spikes the order of the peaks spikes_adn_tsd = spikes_adn.to_tsd(\"order\") # And plot it plt.plot(spikes_adn_tsd.restrict(subep), '|', markersize = 10) plt.xlim(subep.values[0]) plt.subplot(212) tmp = proba_angle_Sleep.restrict(subep).values.T from scipy.ndimage import gaussian_filter tmp = gaussian_filter(tmp, 1) plt.imshow(tmp, aspect='auto', origin='lower') plt.xlabel(\"Time (s)\") plt.ylabel(\"Head-direction (rad)\") plt.show()"},{"location":"tutorials/tutorial_pynapple_quick_start/#quick-start","title":"Quick start\u00b6","text":"<p>The example data to replicate the figure in the jupyter notebook can be found here : https://www.dropbox.com/s/1kc0ulz7yudd9ru/A2929-200711.tar.gz?dl=1</p> <p>The data contain a short sample of a simultaneous recording during sleep and wake  from the anterodorsal nucleus of the thalamus and the hippocampus.  It contains both head-direction cells (i.e. cells that fire for a particular direction in the horizontal plane) and place cells (i.e. cells that fire for a particular position in the environment).</p> <p>Preprocessing of the data was made with Kilosort 2.0 and spike sorting was made with Klusters.</p> <p>Instructions for installing pynapple can be found here :  https://peyrachelab.github.io/pynapple/#installation</p> <p>This tutorial is meant to provide an overview of pynapple by going through:</p> <ol> <li>Input output (IO). In this case, pynapple will load a session containing data.</li> <li>Core functions that handle time series, interval sets and group of time series.</li> <li>Process functions. A small collection of high-level functions widely used in system neuroscience.</li> </ol>"},{"location":"tutorials/tutorial_pynapple_quick_start/#tuning-curves","title":"Tuning curves\u00b6","text":"<p>Let's do more advanced analysis.  Neurons from ADn (group 0 in the spikes group object) are know for firing for a particular direction.  Therefore, we can compute their tuning curves, i.e. their firing rates as a function of the head-direction  of the animal in the horizontal plane (ry).  We can use the function compute_1d_tuning_curves.  In this case, the tuning curves are computed over 120 bins and between 0 and 2$\\pi$.</p>"},{"location":"tutorials/tutorial_pynapple_quick_start/#correlograms","title":"Correlograms\u00b6","text":"<p>A classical question with head-direction cells is how pairs stay coordinated across brain states i.e. wake vs sleep (see Peyrache, A., Lacroix, M. M., Petersen, P. C., &amp; Buzs\u00e1ki, G. (2015). Internally organized mechanisms of the head direction sense. Nature neuroscience, 18(4), 569-575.) In this example, this coordination across brain states will be evaluated with cross-correlograms of pairs of neurons.  We can call the function compute_crosscorrelogram on both sleep and wake epochs.</p>"},{"location":"tutorials/tutorial_pynapple_quick_start/#decoding","title":"Decoding\u00b6","text":"<p>This last analysis shows how to use the function decoding of pynapple, in this case with head-direction cells.</p> <p>The previous result indicates a persistent coordination of head-direction cells during sleep.  Therefore it is possible to decode a virtual head-direction signal even if the animal is not moving its head.  This example uses the function decode_1d which implements bayesian decoding (see : Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.)</p> <p>First we can validate the decoding function with the real position of the head of the animal during wake.</p>"}]}