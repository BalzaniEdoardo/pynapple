{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>PYthon Neural Analysis Package.</p> <p>pynapple is a light-weight python library for neurophysiological data analysis. The goal is to offer a versatile set of tools to study typical data in the field, i.e. time series (spike times, behavioral events, etc.) and time intervals (trials, brain states, etc.). It also provides users with generic functions for neuroscience such as tuning curves and cross-correlograms.</p> <ul> <li>Free software: MIT License</li> <li>Documentation: https://pynapple-org.github.io/pynapple</li> <li>Notebooks and tutorials : https://pynapple-org.github.io/pynapple/notebooks/pynapple-quick-start/</li> </ul> <p>Note If you are using pynapple, please cite the following biorxiv paper</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#installation","title":"Installation","text":"<p>The best way to install pynapple is with pip within a new conda environment :</p> <pre><code>$ conda create --name pynapple pip python=3.8\n$ conda activate pynapple\n$ pip install pynapple\n</code></pre> <p>or directly from the source code:</p> <pre><code>$ conda create --name pynapple pip python=3.8\n$ conda activate pynapple\n$ # clone the repository\n$ git clone https://github.com/pynapple-org/pynapple.git\n$ cd pynapple\n$ # Install in editable mode with `-e` or, equivalently, `--editable`\n$ pip install -e .\n</code></pre> <p>Note The package is now using a pyproject.toml file for installation and dependencies management. If you want to run the tests, use pip install -e .[dev]</p> <p>This procedure will install all the dependencies including </p> <ul> <li>pandas</li> <li>numpy</li> <li>scipy</li> <li>numba</li> <li>pynwb 2.0</li> <li>tabulate</li> <li>h5py</li> </ul> <p>For spyder users, it is recommended to install spyder after installing pynapple with :</p> <pre><code>$ conda create --name pynapple pip python=3.8\n$ conda activate pynapple\n$ pip install pynapple\n$ pip install spyder\n$ spyder\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>After installation, you can now import the package: </p> <pre><code>$ python\n&gt;&gt;&gt; import pynapple as nap\n</code></pre> <p>You'll find an example of the package below. Click here to download the example dataset. The folder includes a NWB file containing the data.</p> <p><pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nimport pynapple as nap\n# LOADING DATA FROM NWB\ndata = nap.load_file(\"A2929-200711.nwb\")\nspikes = data[\"units\"]\nhead_direction = data[\"ry\"]\nwake_ep = data[\"position_time_support\"]\n# COMPUTING TUNING CURVES\ntuning_curves = nap.compute_1d_tuning_curves(\nspikes, head_direction, 120, minmax=(0, 2 * np.pi)\n)\n# PLOT\nplt.figure()\nfor i in spikes:\nplt.subplot(3, 5, i + 1, projection=\"polar\")\nplt.plot(tuning_curves[i])\nplt.xticks([0, np.pi / 2, np.pi, 3 * np.pi / 2])\nplt.show()\n</code></pre> Shown below, the final figure from the example code displays the firing rate of 15 neurons as a function of the direction of the head of the animal in the horizontal plane.</p> <p> </p>"},{"location":"#credits","title":"Credits","text":"<p>Special thanks to Francesco P. Battaglia (https://github.com/fpbattaglia) for the development of the original TSToolbox (https://github.com/PeyracheLab/TStoolbox) and neuroseries (https://github.com/NeuroNetMem/neuroseries) packages, the latter constituting the core of pynapple.</p> <p>This package was developped by Guillaume Viejo (https://github.com/gviejo) and other members of the Peyrache Lab.</p> <p>Logo: Sofia Skromne Carrasco, 2021.</p>"},{"location":"AUTHORS/","title":"Credits","text":""},{"location":"AUTHORS/#development-lead","title":"Development Lead","text":"<ul> <li>Guillaume Viejo guillaume.viejo@gmail.com</li> </ul>"},{"location":"AUTHORS/#contributors","title":"Contributors","text":"<ul> <li>Adrien Peyrache</li> <li>Dan Levenstein</li> <li>Sofia Skromne Carrasco</li> <li>Sara Mahallati</li> <li>Gilberto Vite</li> <li>Davide Spalla</li> <li>Luigi Petrucco</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"CONTRIBUTING/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"CONTRIBUTING/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/pynapple-org/pynapple/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in     troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"CONTRIBUTING/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \\\"bug\\\" and \\\"help wanted\\\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \\\"enhancement\\\" and \\\"help wanted\\\" is open to whoever wants to implement it.</p>"},{"location":"CONTRIBUTING/#write-documentation","title":"Write Documentation","text":"<p>pynapple could always use more documentation, whether as part of the official pynapple docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"CONTRIBUTING/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/pynapple-org/pynapple/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to     implement.</li> <li>Remember that this is a volunteer-driven project, and that     contributions are welcome :)</li> </ul>"},{"location":"CONTRIBUTING/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up [pynapple]{https://github.com/pynapple-org/pynapple} for local development.</p> <ol> <li>Fork the [pynapple]{https://github.com/pynapple-org/pynapple} repo on GitHub.</li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/pynapple.git\n</code></pre> </li> <li> <p>Install your local copy with pip. </p> <pre><code>$ cd pynapple/\n$ pip install -e .\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> </ol> <ol> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.nd.</li> <li>The pull request should work for Python 3.5, 3.6, 3.7 and 3.8, and     for PyPy.      </li> </ol>"},{"location":"HISTORY/","title":"History","text":"<p>This package somehow started about 20 years ago in Bruce McNaughton's lab. Dave Redish started the TSToolbox package in Matlab.  Another postdoc in the lab, Francesco Battaglia, then made major contributions to the package. Francesco passed it on to Adrien Peyrache and other trainees in Paris and The Netherlands. Around 2016-2017, Luke Sjulson started TSToolbox2, still in Matlab and which includes some important changes.</p> <p>In 2018, Francesco started neuroseries, a Python package built on Pandas. It was quickly adopted in Adrien's lab, especially by Guillaume Viejo, a postdoc in the lab. Gradually, the majority of the lab was using it and new functions were constantly added. In 2021, Guillaume and other trainees in Adrien's lab decided to fork from neuroseries and started pynapple. The core of pynapple is largely built upon neuroseries. Some of the original changes to TSToolbox made by Luke were included in this package, especially the time_support property of all ts/tsd objects.</p>"},{"location":"HISTORY/#035-2023-08-08","title":"0.3.5 (2023-08-08)","text":"<ul> <li>NWB reader class</li> <li>NPZ reader class</li> <li>Folder class for navigating a dataset.</li> <li>Cross-correlograms function can take tuple</li> <li>New doc with mkdocs-gallery</li> </ul>"},{"location":"HISTORY/#034-2023-06-29","title":"0.3.4 (2023-06-29)","text":"<ul> <li>TsGroup.to_tsd and Tsd.to_tsgroup transformations</li> <li>Count can take IntervalSet</li> <li>Saving to npz functions for all objects.</li> <li>tsd.value_from can take TsdFrame</li> <li>Warning message for deprecating current IO. </li> </ul>"},{"location":"HISTORY/#033-2023-04-17","title":"0.3.3 (2023-04-17)","text":"<ul> <li>Fixed minor bug with tkinter</li> </ul>"},{"location":"HISTORY/#032-2023-04-12","title":"0.3.2 (2023-04-12)","text":"<ul> <li>PyQt removed from the list of dependencies</li> </ul>"},{"location":"HISTORY/#031-2022-12-08","title":"0.3.1 (2022-12-08)","text":"<ul> <li>Core functions rewritten with Numba</li> </ul>"},{"location":"HISTORY/#024-2022-05-02","title":"0.2.4 (2022-05-02)","text":""},{"location":"HISTORY/#023-2022-04-05","title":"0.2.3 (2022-04-05)","text":"<ul> <li>Fixed minor bug when saving DLC in NWB.</li> </ul>"},{"location":"HISTORY/#023-2022-04-05_1","title":"0.2.3 (2022-04-05)","text":"<ul> <li>Alpha release</li> </ul>"},{"location":"HISTORY/#022-2022-04-05","title":"0.2.2 (2022-04-05)","text":"<ul> <li>Beta testing version for public</li> </ul>"},{"location":"HISTORY/#021-2022-02-07","title":"0.2.1 (2022-02-07)","text":"<ul> <li>Beta testing version for Peyrache Lab.</li> </ul>"},{"location":"HISTORY/#020-2022-01-10","title":"0.2.0 (2022-01-10)","text":"<ul> <li>First version for pynapple with main features in core, process and IO.</li> </ul>"},{"location":"HISTORY/#020-pre-release-2022-01-06","title":"0.2.0 Pre-release (2022-01-06)","text":"<ul> <li>Pre-release version for pynapple with main features in core and process.</li> </ul>"},{"location":"HISTORY/#011-2021-10-25","title":"0.1.1 (2021-10-25)","text":"<ul> <li>First release on PyPI.</li> <li>Firt minimal version</li> </ul>"},{"location":"pynacollada/","title":"Pynacollada","text":""},{"location":"pynacollada/#python-neural-analysis-collaborative-repository","title":"Python neural analysis collaborative repository","text":""},{"location":"pynacollada/#pynacollada","title":"pynacollada","text":"<p>Collaborative platform for high-level analysis with pynapple. Check it out!</p>"},{"location":"generated/gallery/","title":"Usage","text":""},{"location":"generated/gallery/#examples","title":"Examples","text":"<p>Tutorials and examples for the pynapple package.</p> <p> IO Tutorial </p> <p> Streaming data from Dandi </p> <p> Core Tutorial </p> <p> Quick start </p> <p> Advanced processing </p> <p> Download all examples in Python source code: gallery_python.zip</p> <p> Download all examples in Jupyter notebooks: gallery_jupyter.zip</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/mg_execution_times/","title":"Computation times","text":"<p>00:00.471 total execution time for generated_gallery files:</p> <p>+----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_pynapple_io (docs/examples/tutorial_pynapple_io.py)                            | 00:00.471 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_pynapple_core (docs/examples/tutorial_pynapple_core.py)                      | 00:00.000 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_pynapple_dandi (docs/examples/tutorial_pynapple_dandi.py)                   | 00:00.000 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_pynapple_process (docs/examples/tutorial_pynapple_process.py)             | 00:00.000 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+ | tutorial_pynapple_quick_start (docs/examples/tutorial_pynapple_quick_start.py) | 00:00.000 | 0.0 MB | +----------------------------------------------------------------------------------------------------------------------+-----------+--------+</p>"},{"location":"generated/gallery/tutorial_pynapple_core/","title":"Core Tutorial","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_pynapple_core/#core-tutorial","title":"Core Tutorial","text":"<p>This script will introduce the basics of handling time series data with pynapple.</p> <p>Warning</p> <p>This tutorial uses seaborn and matplotlib for displaying the figure.</p> <p>You can install both with <code>pip install matplotlib seaborn</code></p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nimport pynapple as nap\nimport pandas as pd\nimport seaborn as sns\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", palette = \"colorblind\", font_scale=1.5, rc=custom_params)\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_core/#time-series-object","title":"Time series object","text":"<p>Let's create a Tsd object with artificial data. In this example, every time point is 1 second apart. A Tsd object is a wrapper of a pandas series.</p> <pre><code>tsd = nap.Tsd(t = np.arange(100), d = np.random.rand(100), time_units = 's')\nprint(tsd)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n0.0     0.611191\n1.0     0.843763\n2.0     0.861998\n3.0     0.888085\n4.0     0.068080\n          ...   95.0    0.151213\n96.0    0.628415\n97.0    0.824927\n98.0    0.942029\n99.0    0.269149\nLength: 100, dtype: float64\n</code></pre> <p>It is possible to toggle between seconds, milliseconds and microseconds. Note that when using as_units, the returned object is a simple pandas series.</p> <pre><code>print(tsd.as_units('ms'), '\\n')\nprint(tsd.as_units('us'))\n</code></pre> <p>Out:</p> <pre><code>Time (ms)\n0.0        0.611191\n1000.0     0.843763\n2000.0     0.861998\n3000.0     0.888085\n4000.0     0.068080\n             ...   95000.0    0.151213\n96000.0    0.628415\n97000.0    0.824927\n98000.0    0.942029\n99000.0    0.269149\nLength: 100, dtype: float64 Time (us)\n0           0.611191\n1000000     0.843763\n2000000     0.861998\n3000000     0.888085\n4000000     0.068080\n              ...   95000000    0.151213\n96000000    0.628415\n97000000    0.824927\n98000000    0.942029\n99000000    0.269149\nLength: 100, dtype: float64\n</code></pre> <p>Pynapple is able to handle data that only contains timestamps, such as an object containing only spike times. To do so, we construct a Ts object which holds only times. In this case, we generate 10 random spike times between 0 and 100 ms.</p> <pre><code>ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 'ms')\nprint(ts)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n0.006871    0.023494    0.025957    0.041955    0.042453    0.046514    0.067760    0.072906    0.080214    0.095031    dtype: object\n</code></pre> <p>If the time series contains multiple columns, we use a TsdFrame.</p> <pre><code>tsdframe = nap.TsdFrame(t = np.arange(100), \nd = np.random.rand(100,3), \ntime_units = 's', \ncolumns = ['a', 'b', 'c'])\nprint(tsdframe)\n</code></pre> <p>Out:</p> <pre><code>                 a         b         c\nTime (s)                              0.0       0.959606  0.457205  0.322174\n1.0       0.556377  0.264453  0.856505\n2.0       0.865180  0.554026  0.851278\n3.0       0.405155  0.916787  0.333745\n4.0       0.591612  0.491983  0.631693\n...            ...       ...       ...\n95.0      0.605441  0.782700  0.381767\n96.0      0.824578  0.053340  0.605894\n97.0      0.238219  0.430104  0.575002\n98.0      0.464170  0.255258  0.364408\n99.0      0.377430  0.909931  0.686135\n\n[100 rows x 3 columns]\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_core/#interval-sets-object","title":"Interval Sets object","text":"<p>The IntervalSet object stores multiple epochs with a common time unit. It can then be used to restrict time series to this particular set of epochs.</p> <pre><code>epochs = nap.IntervalSet(start = [0, 10], end = [5, 15], time_units = 's')\nnew_tsd = tsd.restrict(epochs)\nprint(epochs)\nprint('\\n')\nprint(new_tsd)\n</code></pre> <p>Out:</p> <pre><code>   start   end\n0    0.0   5.0\n1   10.0  15.0\n\nTime (s)\n0.0     0.611191\n1.0     0.843763\n2.0     0.861998\n3.0     0.888085\n4.0     0.068080\n5.0     0.485966\n10.0    0.766216\n11.0    0.173171\n12.0    0.984113\n13.0    0.035854\n14.0    0.426283\n15.0    0.685514\ndtype: float64\n</code></pre> <p>Multiple operations are available for IntervalSet. For example, IntervalSet can be merged. See the full documentation of the class here for a list of all the functions that can be used to manipulate IntervalSets.</p> <pre><code>epoch1 = nap.IntervalSet(start=[0], end=[10]) # no time units passed. Default is us.\nepoch2 = nap.IntervalSet(start=[5,30],end=[20,45])\nepoch = epoch1.union(epoch2)\nprint(epoch1, '\\n')\nprint(epoch2, '\\n')\nprint(epoch)\n</code></pre> <p>Out:</p> <pre><code>   start   end\n0    0.0  10.0 start   end\n0    5.0  20.0\n1   30.0  45.0 start   end\n0    0.0  20.0\n1   30.0  45.0\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_core/#tsgroup-object","title":"TsGroup object","text":"<p>Multiple time series with different time stamps (.i.e. a group of neurons with different spike times from one session) can be grouped with the TsGroup object. The TsGroup behaves like a dictionary but it is also possible to slice with a list of indexes</p> <pre><code>my_ts = {0:nap.Ts(t = np.sort(np.random.uniform(0, 100, 1000)), time_units = 's'), # here a simple dictionary\n1:nap.Ts(t = np.sort(np.random.uniform(0, 100, 2000)), time_units = 's'),\n2:nap.Ts(t = np.sort(np.random.uniform(0, 100, 3000)), time_units = 's')}\ntsgroup = nap.TsGroup(my_ts)\nprint(tsgroup, '\\n')\nprint(tsgroup[0], '\\n') # dictionary like indexing returns directly the Ts object\nprint(tsgroup[[0,2]]) # list like indexing\n</code></pre> <p>Out:</p> <pre><code>  Index    rate\n-------  ------\n      0   10\n1   20\n2   30.01 Time (s)\n0.143768    NaN\n0.228171    NaN\n0.457721    NaN\n0.525452    NaN\n0.546878    NaN\n             ..\n99.662813   NaN\n99.765021   NaN\n99.806223   NaN\n99.953830   NaN\n99.981290   NaN\nLength: 1000, dtype: float64 Index    rate\n-------  ------\n      0   10\n2   30.01\n</code></pre> <p>Operations such as restrict can thus be directly applied to the TsGroup as well as other operations.</p> <pre><code>newtsgroup = tsgroup.restrict(epochs)\ncount = tsgroup.count(1, epochs, time_units='s') # Here counting the elements within bins of 1 seconds\nprint(count)\n</code></pre> <p>Out:</p> <pre><code>           0   1   2\nTime (s)            0.5        7  20  39\n1.5       11  15  28\n2.5       14  16  26\n3.5        5  23  23\n4.5        7  23  20\n10.5       5  25  28\n11.5      10  28  28\n12.5      10  15  33\n13.5       9  19  23\n14.5       9  19  28\n</code></pre> <p>One advantage of grouping time series is that metainformation can be appended directly on an element-wise basis. In this case, we add labels to each Ts object when instantiating the group and after. We can then use this label to split the group. See the TsGroup documentation for a complete methodology for splitting TsGroup objects.</p> <pre><code>label1 = pd.Series(index=list(my_ts.keys()), data = [0,1,0])\ntsgroup = nap.TsGroup(my_ts, time_units = 's', label1=label1)\ntsgroup.set_info(label2=np.array(['a', 'a', 'b']))\nprint(tsgroup, '\\n')\nnewtsgroup= tsgroup.getby_category('label1')\nprint(newtsgroup[0], '\\n')\nprint(newtsgroup[1])\n</code></pre> <p>Out:</p> <pre><code>  Index    rate    label1  label2\n-------  ------  --------  --------\n      0   10            0  a\n      1   20            1  a\n      2   30.01         0  b Index    rate    label1  label2\n-------  ------  --------  --------\n      0   10            0  a\n      2   30.01         0  b Index    rate    label1  label2\n-------  ------  --------  --------\n      1      20         1  a\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_core/#time-support","title":"Time support","text":"<p>A key feature of how pynapple manipulates time series is an inherent time support object defined for Ts, Tsd, TsdFrame and TsGroup objects. The time support object is defined as an IntervalSet that provides the time serie with a context. For example, the restrict operation will automatically update the time support object for the new time series. Ideally, the time support object should be defined for all time series when instantiating them. If no time series is given, the time support is inferred from the start and end of the time series. </p> <p>In this example, a TsGroup is instantiated with and without a time support. Notice how the frequency of each Ts element is changed when the time support is defined explicitly.</p> <pre><code>time_support = nap.IntervalSet(start = 0, end = 200, time_units = 's')\nmy_ts = {0:nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's'), # here a simple dictionnary\n1:nap.Ts(t = np.sort(np.random.uniform(0, 100, 20)), time_units = 's'),\n2:nap.Ts(t = np.sort(np.random.uniform(0, 100, 30)), time_units = 's')}\ntsgroup = nap.TsGroup(my_ts)\ntsgroup_with_time_support = nap.TsGroup(my_ts, time_support = time_support)\nprint(tsgroup, '\\n')\nprint(tsgroup_with_time_support, '\\n')\nprint(tsgroup_with_time_support.time_support) # acceding the time support\n</code></pre> <p>Out:</p> <pre><code>  Index    rate\n-------  ------\n      0     0.1\n      1     0.2\n      2     0.3 Index    rate\n-------  ------\n      0    0.05\n      1    0.1\n      2    0.15 start    end\n0    0.0  200.0\n</code></pre> <p>We can use value_from which as it indicates assign to every timestamps the closed value in time from another time series. Let's define the time series we want to assign values from.</p> <pre><code>tsd_sin = nap.Tsd(t=np.arange(0, 100, 1), d=np.sin(np.arange(0, 10, 0.1)))\ntsgroup_sin = tsgroup.value_from(tsd_sin)\nplt.figure(figsize = (12, 6))\nplt.plot(tsgroup[0].fillna(0), '|', markersize=20, mew = 3)\nplt.plot(tsd_sin, linewidth=2)\nplt.plot(tsgroup_sin[0], 'o', markersize=20)\nplt.title(\"ts.value_from(tsd)\")\nplt.xlabel(\"Time (s)\")\nplt.yticks([-1, 0, 1])\nplt.show()\n</code></pre> <p></p> <p>Total running time of the script: ( 0 minutes  2.241 seconds)</p> <p> Download Python source code: tutorial_pynapple_core.py</p> <p> Download Jupyter notebook: tutorial_pynapple_core.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/tutorial_pynapple_dandi/","title":"Streaming data from Dandi","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_pynapple_dandi/#streaming-data-from-dandi","title":"Streaming data from Dandi","text":"<p>This script shows how to stream data from the dandi archive all the way to pynapple.</p> <p>Warning This tutorial is still under construction.</p> <p>Warning</p> <p>This tutorial uses seaborn and matplotlib for displaying the figure as well as the dandi package</p> <p>You can install all with <code>pip install matplotlib seaborn dandi dandischema</code></p>"},{"location":"generated/gallery/tutorial_pynapple_dandi/#prelude","title":"Prelude","text":"<p>The data used in this tutorial are hosted by the DANDI archive and were used in this publication: Senzai, Yuta, and Gy\u00f6rgy Buzs\u00e1ki. \"Physiological properties and behavioral correlates of hippocampal granule cells and mossy cells.\" Neuron 93.3 (2017): 691-704.</p>"},{"location":"generated/gallery/tutorial_pynapple_dandi/#dandi","title":"Dandi","text":"<p>Dandi allows you to stream data without downloading all the files. In this case the data extracted from the NWB file are stored in the nwb-cache folder.</p> <pre><code>from pynwb import NWBHDF5IO\nfrom dandi.dandiapi import DandiAPIClient\nimport fsspec\nfrom fsspec.implementations.cached import CachingFileSystem\nimport h5py\n# ecephys, Buzsaki Lab (15.2 GB)\ndandiset_id, filepath = \"000003\", \"sub-YutaMouse56/sub-YutaMouse56_ses-YutaMouse56-160911_behavior+ecephys.nwb\"\nwith DandiAPIClient() as client:\nasset = client.get_dandiset(dandiset_id, \"draft\").get_asset_by_path(filepath)\ns3_url = asset.get_content_url(follow_redirects=1, strip_query=True)\n# first, create a virtual filesystem based on the http protocol\nfs=fsspec.filesystem(\"http\")\n# create a cache to save downloaded data to disk (optional)\nfs = CachingFileSystem(\nfs=fs,\ncache_storage=\"nwb-cache\",  # Local folder for the cache\n)\n# next, open the file\nfile = h5py.File(fs.open(s3_url, \"rb\"))\nio = NWBHDF5IO(file=file, load_namespaces=True)\nprint(io)\n</code></pre> <p>Out:</p> <pre><code>/mnt/home/gviejo/envs/pynapple/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'hdmf-common' version 1.3.0 because version 1.5.1 is already loaded.\n  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n/mnt/home/gviejo/envs/pynapple/lib/python3.9/site-packages/hdmf/spec/namespace.py:531: UserWarning: Ignoring cached namespace 'core' version 2.2.5 because version 2.6.0-alpha is already loaded.\n  warn(\"Ignoring cached namespace '%s' version %s because version %s is already loaded.\"\n&lt;pynwb.NWBHDF5IO object at 0x7f74972d6910&gt;\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_dandi/#pynapple","title":"Pynapple","text":"<p>If opening the NWB works, you can start streaming data straight into pynapple with the <code>NWBFile</code> class.</p> <pre><code>import pynapple as nap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", palette = \"colorblind\", font_scale=1.5, rc=custom_params)\nnwb = nap.NWBFile(io.read())\nprint(nwb)\n</code></pre> <p>Out:</p> <pre><code>/mnt/sw/nix/store/1bljgds808py1sq0av8nn8izvjnpfsbs-python-3.9.15-view/lib/python3.9/site-packages/jupyter_client/connect.py:27: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\ngiven by the platformdirs library.  To remove this warning and\nsee the appropriate new directories, set the environment variable\n`JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\nThe use of platformdirs will be the default in `jupyter_core` v6\n  from jupyter_core.paths import jupyter_data_dir\n                            YutaMouse56                             \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Keys                                               \u2503 Type        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 units                                              \u2502 TsGroup     \u2502\n\u2502 epochs                                             \u2502 IntervalSet \u2502\n\u2502 LFP                                                \u2502 TsdFrame    \u2502\n\u2502 states                                             \u2502 IntervalSet \u2502\n\u2502 OpenFieldPosition_Oldlast_twhl_norm_spatial_series \u2502 TsdFrame    \u2502\n\u2502 OpenFieldPosition_Old_twhl_norm_spatial_series     \u2502 TsdFrame    \u2502\n\u2502 OpenFieldPosition_New_twhl_norm_spatial_series     \u2502 TsdFrame    \u2502\n\u2502 PulseStim_5V_500ms_LD1                             \u2502 Ts          \u2502\n\u2502 PulseStim_5V_20ms_LD12                             \u2502 Ts          \u2502\n\u2502 DS2.ch26                                           \u2502 Ts          \u2502\n\u2502 DS1.ch26                                           \u2502 Ts          \u2502\n\u2502 position_sensor1                                   \u2502 TsdFrame    \u2502\n\u2502 position_sensor0                                   \u2502 TsdFrame    \u2502\n\u2502 ch_wait                                            \u2502 Tsd         \u2502\n\u2502 ch_solR                                            \u2502 Tsd         \u2502\n\u2502 ch_solL                                            \u2502 Tsd         \u2502\n\u2502 ch_entR                                            \u2502 Tsd         \u2502\n\u2502 ch_entL                                            \u2502 Tsd         \u2502\n\u2502 ch_dig2                                            \u2502 Tsd         \u2502\n\u2502 ch_dig1                                            \u2502 Tsd         \u2502\n\u2502 ch_arm                                             \u2502 Tsd         \u2502\n\u2502 ch_SsolR                                           \u2502 Tsd         \u2502\n\u2502 ch_SsolL                                           \u2502 Tsd         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We can load the spikes as a TsGroup for inspection.</p> <pre><code>units = nwb['units']\nprint(units)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate    global_id  cell_type                                                      shank_id\n-------  ------  -----------  -----------------------------------------------------------  ----------\n      0    0.53         3130  pyramidal cell                                                        0\n1    0.41         3131  pyramidal cell                                                        1\n2    0.05         3132  pyramidal cell                                                        2\n3    0.06         3133  pyramidal cell                                                        3\n4    0.07         3134  unknown                                                               4\n5   21.25         3135  narrow waveform cell                                                  5\n6   22.67         3136  narrow waveform cell                                                  6\n7    0.12         3137  unknown                                                               7\n8    1.35         3138  pyramidal cell                                                        8\n9    1.05         3139  pyramidal cell                                                        9\n10    0.12         3140  narrow waveform cell                                                 10\n11    0.54         3141  granule cell                                                         11\n12    2.78         3142  unknown                                                               0\n13    0.33         3143  mossy cell                                                            1\n14    4.53         3144  wide waveform cell (narrower, exclude opto tagged SST cell)           2\n15    0.02         3145  unknown                                                               3\n16    2.41         3146  wide waveform cell (wider)                                            4\n17    0.12         3147  pyramidal cell                                                        5\n18   11.05         3148  narrow waveform cell                                                  6\n19    0.49         3149  pyramidal cell                                                        7\n20    0.2          3150  pyramidal cell                                                        8\n21    0.17         3151  pyramidal cell                                                        9\n22    0.34         3152  pyramidal cell                                                       10\n23    0.28         3153  pyramidal cell                                                       11\n24    0.24         3154  granule cell                                                          0\n25    0.31         3155  mossy cell                                                            1\n26    0.16         3156  granule cell                                                          2\n27    0.5          3157  granule cell                                                          3\n28    0.15         3158  granule cell                                                          4\n29    3.78         3159  unknown                                                               5\n30    0.06         3160  granule cell                                                          6\n31    0.81         3161  unknown                                                               7\n32    5.34         3162  unknown                                                               8\n33    0.77         3163  wide waveform cell (wider)                                            9\n34    0.25         3164  granule cell                                                         10\n35    0.55         3165  mossy cell                                                           11\n36    2.35         3166  mossy cell                                                           12\n37    0.06         3167  granule cell                                                         13\n38    0.14         3168  granule cell                                                         14\n39    0.12         3169  granule cell                                                         15\n40    0.14         3170  wide waveform cell (wider)                                           16\n41    0.07         3171  granule cell                                                         17\n42    8.59         3172  wide waveform cell (narrower, exclude opto tagged SST cell)          18\n43    0.23         3173  granule cell                                                          0\n44    0.52         3174  mossy cell                                                            1\n45    0.86         3175  wide waveform cell (narrower, exclude opto tagged SST cell)           2\n46    0.08         3176  granule cell                                                          3\n47    0.14         3177  granule cell                                                          4\n48    0.5          3178  unknown                                                               5\n49    0.16         3179  granule cell                                                          6\n50    0.17         3180  mossy cell                                                            7\n51    0.21         3181  wide waveform cell (narrower, exclude opto tagged SST cell)           8\n52    0.09         3182  granule cell                                                          9\n53    0.11         3183  granule cell                                                         10\n54    0.55         3184  granule cell                                                         11\n55    0.54         3185  granule cell                                                         12\n56    0.14         3186  wide waveform cell (wider)                                           13\n57    0.25         3187  granule cell                                                         14\n58    0.06         3188  granule cell                                                         15\n59    0.08         3189  granule cell                                                         16\n60    0.71         3190  narrow waveform cell                                                 17\n61    0.12         3191  granule cell                                                         18\n62    0.26         3192  wide waveform cell (wider)                                           19\n63    0.27         3193  granule cell                                                         20\n64    0.1          3194  granule cell                                                         21\n65    1.02         3195  mossy cell                                                           22\n66    2.03         3196  mossy cell                                                           23\n67    0.14         3197  unknown                                                               0\n68    0.11         3198  unknown                                                               1\n69    0.27         3199  narrow waveform cell                                                  2\n70    0.44         3200  unknown                                                               3\n71   18.92         3201  narrow waveform cell                                                  4\n72    0.09         3202  granule cell                                                          5\n73    0.16         3203  narrow waveform cell                                                  6\n74    0.3          3204  granule cell                                                          7\n75    0.61         3205  unknown                                                               0\n76    0.97         3206  unknown                                                               0\n77    5.25         3207  unknown                                                               1\n78    7.46         3208  unknown                                                               2\n79    3.79         3209  unknown                                                               3\n80    9.49         3210  unknown                                                               4\n81    5.76         3211  unknown                                                               5\n</code></pre> <p>Let's get all the neurons whose spikes sufficiently</p> <pre><code>units = units.getby_threshold(\"rate\", 0.5)\n</code></pre> <p>Let's do some cross-corrs during wake and non-REM sleep We start by retrieveing the epochs</p> <pre><code>epochs = nwb[\"states\"]\nprint(epochs)\n# In this case, it's a dictionnary of IntervalSet\nwake_ep = epochs[\"awake\"]\nnrem_ep = epochs[\"nrem\"]\n</code></pre> <p>Out:</p> <pre><code>{'awake':       start      end\n0   16619.0  16677.0\n1   16850.0  17151.0\n2   17393.0  17504.0\n3   17666.0  18732.0\n4   19352.0  19487.0\n5   19618.0  20123.0\n6   20411.0  20606.0\n7   20748.0  21193.0\n8   23479.0  23666.0\n9   23776.0  24132.0\n10  24486.0  24519.0\n11  24601.0  26667.0\n12  27340.0  29308.0\n13  35959.0  36970.0\n14  37139.0  37244.0\n15  37464.0  38573.0\n16  38724.0  38781.0\n17  43064.0  43557.0\n18  43895.0  50637.0, 'nrem':       start      end\n0   16199.0  16240.0\n1   21193.0  22219.0\n2   22367.0  23198.0\n3   23319.0  23411.0\n4   29308.0  29958.0\n5   30094.0  30718.0\n6   30825.0  31382.0\n7   31521.0  31574.0\n8   31865.0  31914.0\n9   32001.0  33605.0\n10  33640.0  33883.0\n11  33925.0  34566.0\n12  34725.0  35615.0\n13  35709.0  35959.0\n14  38781.0  39611.0\n15  39644.0  40032.0\n16  40112.0  40586.0\n17  40688.0  42218.0\n18  42267.0  42476.0\n19  42617.0  42700.0, 'quiet':       start      end\n0   16240.0  16619.0\n1   16677.0  16850.0\n2   17151.0  17393.0\n3   17504.0  17666.0\n4   18732.0  19352.0\n5   19487.0  19618.0\n6   20123.0  20411.0\n7   20606.0  20748.0\n8   23411.0  23479.0\n9   23666.0  23776.0\n10  24132.0  24486.0\n11  24519.0  24601.0\n12  26667.0  27340.0\n13  30745.0  30825.0\n14  31574.0  31865.0\n15  31914.0  32001.0\n16  36970.0  37139.0\n17  37244.0  37464.0\n18  38573.0  38724.0\n19  42700.0  43064.0\n20  43557.0  43895.0, 'rem':       start      end\n0   22234.0  22367.0\n1   23207.0  23319.0\n2   29971.0  30094.0\n3   30722.0  30745.0\n4   31398.0  31521.0\n5   33616.0  33640.0\n6   33897.0  33925.0\n7   34575.0  34725.0\n8   35624.0  35709.0\n9   39620.0  39644.0\n10  40045.0  40112.0\n11  40595.0  40688.0\n12  42225.0  42267.0\n13  42487.0  42617.0, 'transit':       start      end\n0   22219.0  22234.0\n1   23198.0  23207.0\n2   29958.0  29971.0\n3   30718.0  30722.0\n4   31382.0  31398.0\n5   33605.0  33616.0\n6   33883.0  33897.0\n7   34566.0  34575.0\n8   35615.0  35624.0\n9   39611.0  39620.0\n10  40032.0  40045.0\n11  40586.0  40595.0\n12  42218.0  42225.0\n13  42476.0  42487.0}\n</code></pre> <p>Let's replicate one result of the study. The authors report cross-correlograms between mossy cells and granule cells as a function of brain states (wake_ep vs nrem_ep in this case). First let's retrieve both sub-population</p> <pre><code>mossy_cells = units.getby_category(\"cell_type\")[\"mossy cell\"]\ngranule_cells = units.getby_category(\"cell_type\")[\"granule cell\"]\nprint(mossy_cells)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate    global_id  cell_type      shank_id\n-------  ------  -----------  -----------  ----------\n     35    0.55         3165  mossy cell           11\n36    2.35         3166  mossy cell           12\n44    0.52         3174  mossy cell            1\n65    1.02         3195  mossy cell           22\n66    2.03         3196  mossy cell           23\n</code></pre> <p>Let's compute their cross-correlogram during wake and nREM sleep. The order in the tuple matters. In this case, granule cell is the reference unit.</p> <pre><code>cc_wake = nap.compute_crosscorrelogram((granule_cells, mossy_cells), 0.002, 0.2, ep=wake_ep, norm=True)\ncc_nrem = nap.compute_crosscorrelogram((granule_cells, mossy_cells), 0.002, 0.2, ep=nrem_ep, norm=True)\nplt.figure(figsize=(16,10))\ngs = plt.GridSpec(len(mossy_cells), len(granule_cells))\nfor i, n in enumerate(mossy_cells.keys()):\nfor j, k in enumerate(granule_cells.keys()):\np = (k,n)\nplt.subplot(gs[i,j])\ntidx = cc_wake[p].index.values\nplt.fill_between(tidx, np.zeros_like(cc_wake[p]), cc_wake[p].values, color = 'lightgrey')\nplt.plot()\nplt.grid()\nplt.title(p)\nif i == len(mossy_cells)-1: plt.xlabel(\"Time (s)\")\nif j == 0: plt.ylabel(\"Norm.\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>TODO : Run the stats for the connection, find a good example, display NREM</p> <p>Total running time of the script: ( 0 minutes  47.096 seconds)</p> <p> Download Python source code: tutorial_pynapple_dandi.py</p> <p> Download Jupyter notebook: tutorial_pynapple_dandi.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/tutorial_pynapple_io/","title":"IO Tutorial","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_pynapple_io/#io-tutorial","title":"IO Tutorial","text":"<p>This notebook is designed to demonstrate the pynapple IO. It is build around the specifications of the BIDS standard for sharing datasets. The key ideas are summarized as follow :</p> <ul> <li> <p>Hierarchy of folders</p> <p></p> </li> <li> <p>Filename template</p> <p></p> </li> <li> <p>Metadata files</p> <p></p> </li> </ul>"},{"location":"generated/gallery/tutorial_pynapple_io/#navigating-a-structured-dataset","title":"Navigating a structured dataset","text":"<p>The dataset in this example can be found here.</p> <pre><code>import numpy as np\nimport pynapple as nap\n# mkdocs_gallery_thumbnail_path = '_static/treeview.png'\nproject_path = '../../your/path/to/MyProject'\nproject = nap.load_folder(project_path)\nprint(project)\n</code></pre> <p>Out:</p> <pre><code>\ud83d\udcc2 MyProject\n\u2514\u2500\u2500 \ud83d\udcc2 sub-A2929\n</code></pre> <p>The pynapple IO offers a convenient way of visualizing and navigating a folder based dataset. To visualize the whole hierarchy of Folders, you can call the view property or the expand function.</p> <pre><code>project.view\n</code></pre> <p>Out:</p> <pre><code>\ud83d\udcc2 MyProject\n\u2514\u2500\u2500 \ud83d\udcc2 sub-A2929\n    \u2514\u2500\u2500 \ud83d\udcc2 ses-A2929-200711\n        \u251c\u2500\u2500 \ud83d\udcc2 derivatives\n        \u2502   \u251c\u2500\u2500 spikes.npz      |        TsGroup\n        \u2502   \u251c\u2500\u2500 sleep_ep.npz    |        IntervalSet\n        \u2502   \u251c\u2500\u2500 position.npz    |        TsdFrame\n        \u2502   \u2514\u2500\u2500 wake_ep.npz     |        IntervalSet\n        \u251c\u2500\u2500 \ud83d\udcc2 pynapplenwb\n        \u2502   \u2514\u2500\u2500 A2929-200711    |        NWB file\n        \u251c\u2500\u2500 x_plus_y.npz    |        Tsd\n        \u2514\u2500\u2500 stimulus-fish.npz       |        IntervalSet\n</code></pre> <p>Here it shows all the subjects (in this case only A2929), all the sessions and all of the derivatives folders. It shows as well all the NPZ files that contains a pynapple object and the NWB files.</p> <p>The object project behaves like a nested dictionnary. It is then easy to loop and navigate through a hierarchy of folders when doing analyses. In this case, we are gonna take only the session A2929-200711.</p> <pre><code>session = project[\"sub-A2929\"][\"ses-A2929-200711\"]\nprint(session)\n</code></pre> <p>Out:</p> <pre><code>\ud83d\udcc2 ses-A2929-200711\n\u251c\u2500\u2500 \ud83d\udcc2 derivatives\n\u251c\u2500\u2500 \ud83d\udcc2 pynapplenwb\n\u251c\u2500\u2500 x_plus_y.npz    |        Tsd\n\u2514\u2500\u2500 stimulus-fish.npz       |        IntervalSet\n</code></pre> <p>I can expand to see what the folders contains.</p> <pre><code>print(session.expand())\n</code></pre> <p>Out:</p> <pre><code>\ud83d\udcc2 ses-A2929-200711\n\u251c\u2500\u2500 \ud83d\udcc2 derivatives\n\u2502   \u251c\u2500\u2500 spikes.npz      |        TsGroup\n\u2502   \u251c\u2500\u2500 sleep_ep.npz    |        IntervalSet\n\u2502   \u251c\u2500\u2500 position.npz    |        TsdFrame\n\u2502   \u2514\u2500\u2500 wake_ep.npz     |        IntervalSet\n\u251c\u2500\u2500 \ud83d\udcc2 pynapplenwb\n\u2502   \u2514\u2500\u2500 A2929-200711    |        NWB file\n\u251c\u2500\u2500 x_plus_y.npz    |        Tsd\n\u2514\u2500\u2500 stimulus-fish.npz       |        IntervalSet\nNone\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_io/#loading-files","title":"Loading files","text":"<p>By default, pynapple save objects as NPZ. It is a convenient way to save all the properties of an object such as the time support. The pynapple IO offers an easy way to load any NPZ files that matches the structures defined for a pynapple object.</p> <pre><code>spikes = session[\"derivatives\"][\"spikes\"]\nposition = session[\"derivatives\"][\"position\"]\nwake_ep = session[\"derivatives\"][\"wake_ep\"]\nsleep_ep = session[\"derivatives\"][\"sleep_ep\"]\n</code></pre> <p>Objects are only loaded when they are called.</p> <pre><code>print(session[\"derivatives\"][\"spikes\"])\n</code></pre> <p>Out:</p> <pre><code>  Index    rate    group  location\n-------  ------  -------  ----------\n      0    7.3         0  adn\n      1    5.73        0  adn\n      2    8.12        0  adn\n      3    6.68        0  adn\n      4   10.77        0  adn\n      5   11           0  adn\n      6   16.52        0  adn\n      7    2.2         1  ca1\n      8    2.02        1  ca1\n      9    1.07        1  ca1\n     10    3.92        1  ca1\n     11    3.31        1  ca1\n     12    1.09        1  ca1\n     13    1.28        1  ca1\n     14    1.32        1  ca1\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_io/#metadata","title":"Metadata","text":"<p>A good practice for sharing datasets is to write as many metainformation as possible. Following BIDS specifications, any data files should be accompagned by a JSON sidecar file. </p> <pre><code>import os\nfor f in os.listdir(session['derivatives'].path):\nprint(f)\n</code></pre> <p>Out:</p> <pre><code>wake_ep.json\nposition.json\nsleep_ep.json\nspikes.npz\nsleep_ep.npz\nspikes.json\nposition.npz\nwake_ep.npz\n</code></pre> <p>To read the metainformation associated with a file, you can use the functions <code>doc</code>, <code>info</code> or <code>metadata</code> : </p> <pre><code>session['derivatives'].doc(\"spikes\")\nsession['derivatives'].doc(\"position\")\n</code></pre> <p>Out:</p> <pre><code>\u256d\u2500 ../../your/path/to/MyProject/sub-A2929/ses-A2929-200711/derivatives/spikes.npz \u2500\u256e\n\u2502 time : 2023-07-11 12:40:10.338066                                                \u2502\n\u2502 info : Neurons recorded simultaneously in ADN and CA1                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 ../../your/path/to/MyProject/sub-A2929/ses-A2929-200711/derivatives/position.npz \u2500\u256e\n\u2502 time : 2023-07-11 12:40:10.364061                                                  \u2502\n\u2502 info : Position and head-direction of the mouse recorded with Optitrack            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_io/#saving-a-pynapple-object","title":"Saving a pynapple object","text":"<p>In this case, we define a new Tsd and a new IntervalSet that we would like to save in the session folder.</p> <pre><code>tsd = position['x'] + position['y']\nepoch = nap.IntervalSet(start=np.array([0, 3]), end = np.array([1, 6]))\nsession.save(\"x_plus_y\", tsd, description = \"Random position\")\nsession.save(\"stimulus-fish\", epoch, description = \"Fish pictures to V1\")\n</code></pre> <p>We can visualize the newly saved objects.</p> <pre><code>session.expand()\n</code></pre> <p>Out:</p> <pre><code>\ud83d\udcc2 ses-A2929-200711\n\u251c\u2500\u2500 \ud83d\udcc2 derivatives\n\u2502   \u251c\u2500\u2500 spikes.npz      |        TsGroup\n\u2502   \u251c\u2500\u2500 sleep_ep.npz    |        IntervalSet\n\u2502   \u251c\u2500\u2500 position.npz    |        TsdFrame\n\u2502   \u2514\u2500\u2500 wake_ep.npz     |        IntervalSet\n\u251c\u2500\u2500 \ud83d\udcc2 pynapplenwb\n\u2502   \u2514\u2500\u2500 A2929-200711    |        NWB file\n\u251c\u2500\u2500 x_plus_y.npz    |        Tsd\n\u2514\u2500\u2500 stimulus-fish.npz       |        IntervalSet\n</code></pre> <pre><code>session.doc('stimulus-fish')\n</code></pre> <p>Out:</p> <pre><code>\u256d\u2500 ../../your/path/to/MyProject/sub-A2929/ses-A2929-200711/stimulus-fish.npz \u2500\u256e\n\u2502 time : 2023-08-07 18:45:24.586022                                           \u2502\n\u2502 info : Fish pictures to V1                                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <pre><code>session['x_plus_y']\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n670.64070     0.382166\n670.64900     0.380987\n670.65735     0.379844\n670.66565     0.378787\n670.67400     0.377807\n                ...   1199.96160    0.049132\n1199.96995    0.049651\n1199.97825    0.050196\n1199.98660    0.050725\n1199.99495    0.051192\nLength: 63527, dtype: float64\n</code></pre> <p>Total running time of the script: ( 0 minutes  0.471 seconds)</p> <p> Download Python source code: tutorial_pynapple_io.py</p> <p> Download Jupyter notebook: tutorial_pynapple_io.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/tutorial_pynapple_process/","title":"Advanced processing","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_pynapple_process/#advanced-processing","title":"Advanced processing","text":"<p>The pynapple package provides a small set of high-level functions that are widely used in systems neuroscience.</p> <ul> <li>Discrete correlograms</li> <li>Tuning curves</li> <li>Decoding</li> <li>PETH</li> <li>Randomization</li> </ul> <p>This notebook provides few examples with artificial data.</p> <p>Warning</p> <p>This tutorial uses seaborn and matplotlib for displaying the figure.</p> <p>You can install both with <code>pip install matplotlib seaborn</code></p> <pre><code>import numpy as np\nimport pandas as pd\nimport pynapple as nap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", palette = \"colorblind\", font_scale=1.5, rc=custom_params)\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_process/#discrete-correlograms","title":"Discrete correlograms","text":"<p>The function to compute cross-correlogram is cross_correlogram.</p> <p>The function is compiled with numba to improve performances. This means it only accepts pure numpy arrays as input arguments.</p> <pre><code>ts1 = nap.Ts(t = np.sort(np.random.uniform(0, 1000, 1000)), time_units = 's')\nts2 = nap.Ts(t = np.sort(np.random.uniform(0, 1000, 10)), time_units = 's')\nts1_time_array = ts1.as_units('s').index.values\nts2_time_array = ts2.as_units('s').index.values\nbinsize = 0.1 # second\ncc12, xt = nap.cross_correlogram(t1 = ts1_time_array,\nt2 = ts2_time_array,\nbinsize=binsize,\nwindowsize=1 # second\n)\nplt.figure(figsize = (10, 6))\nplt.bar(xt, cc12, binsize)\nplt.xlabel(\"Time t1 (us)\")\nplt.ylabel(\"CC\")\n</code></pre> <p></p> <p>Out:</p> <pre><code>Text(40.625, 0.5, 'CC')\n</code></pre> <p>To simplify converting to a numpy.ndarray, pynapple provides wrappers for computing autocorrelogram and crosscorrelogram for TsGroup. The function is then called for each unit or each pairs of units. It returns directly a pandas.DataFrame holding all the correlograms. In this example, autocorrelograms and cross-correlograms are computed for the same TsGroup.</p> <pre><code>epoch = nap.IntervalSet(start = 0, end = 1000, time_units = 's')\nts_group = nap.TsGroup({0:ts1,1:ts2}, time_support = epoch)\nautocorrs = nap.compute_autocorrelogram(group=ts_group,                                         \nbinsize=100, # ms\nwindowsize=1000, # ms                                        \ntime_units='ms',\nep=epoch\n)\ncrosscorrs = nap.compute_crosscorrelogram(group=ts_group,                                        \nbinsize=100, # ms\nwindowsize=1000, # ms                                        \ntime_units='ms'\n)\nprint(autocorrs, '\\n')\nprint(crosscorrs, '\\n')\n</code></pre> <p>Out:</p> <pre><code>         0    1\n-0.9  0.83  0.0\n-0.8  1.07  0.0\n-0.7  1.01  0.0\n-0.6  1.18  0.0\n-0.5  0.89  0.0\n-0.4  1.02  0.0\n-0.3  1.15  0.0\n-0.2  0.84  0.0\n-0.1  0.84  0.0\n 0.0  0.00  0.0\n 0.1  0.84  0.0\n 0.2  0.84  0.0\n 0.3  1.15  0.0\n 0.4  1.02  0.0\n 0.5  0.89  0.0\n 0.6  1.18  0.0\n 0.7  1.01  0.0\n 0.8  1.07  0.0\n 0.9  0.83  0.0 0\n1\n-0.9  0.0\n-0.8  3.0\n-0.7  0.0\n-0.6  0.0\n-0.5  1.0\n-0.4  1.0\n-0.3  1.0\n-0.2  1.0\n-0.1  1.0\n 0.0  2.0\n 0.1  3.0\n 0.2  1.0\n 0.3  1.0\n 0.4  1.0\n 0.5  2.0\n 0.6  0.0\n 0.7  0.0\n 0.8  1.0\n 0.9  1.0 </code></pre>"},{"location":"generated/gallery/tutorial_pynapple_process/#peri-event-time-histogram-peth","title":"Peri-Event Time Histogram (PETH)","text":"<p>A second way to examine the relationship between spiking and an event (i.e. stimulus) is to compute a PETH. pynapple uses the function <code>_compute_perievent_</code> to center spike time around the timestamps of an event within a given window.</p> <pre><code>stim = nap.Tsd(t = np.sort(np.random.uniform(0, 1000, 50)), \nd = np.random.rand(50),\ntime_units = 's')\npeth0 = nap.compute_perievent(ts1, stim, minmax = (-0.1, 0.2), time_unit = 's')\nprint(peth0)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate    ref_times\n-------  ------  -----------\n      0    0         11.2503\n      1    3.33      12.0641\n      2    0         16.1707\n      3    0         44.5992\n      4    3.33      54.3664\n      5    0         69.9383\n      6    0         80.3114\n      7    0         95.4514\n      8    0        101.135\n      9    0        127.961\n     10    0        129.893\n     11    0        163.846\n     12    3.33     167.349\n     13    3.33     182.681\n     14    3.33     202.576\n     15    3.33     242.675\n     16    3.33     252.635\n     17    3.33     271.798\n     18    0        303.741\n     19    0        318.019\n     20    0        344.258\n     21    0        347.42\n     22    3.33     363.56\n     23    6.67     376.035\n     24    0        401.097\n     25    0        421.945\n     26    3.33     462.981\n     27    0        479.427\n     28    0        482.936\n     29    3.33     499.198\n     30    0        515.336\n     31    0        546.984\n     32    0        568.632\n     33    0        570.638\n     34    0        699.086\n     35    0        702.429\n     36    0        706.091\n     37    0        785.28\n     38    0        813.498\n     39    0        840.927\n     40    0        842.795\n     41    0        847.521\n     42    0        854.812\n     43    3.33     872.71\n     44    0        907.902\n     45    3.33     908.995\n     46    0        909.74\n     47    0        917.418\n     48    0        942.385\n     49    0        968.598\n</code></pre> <p>It is then easy to create a raster plot around the times of the stimulation event by calling the <code>to_tsd</code> function of pynapple to \"flatten\" the TsGroup peth0.</p> <p>mkdocs_gallery_thumbnail_number = 2</p> <pre><code>plt.figure(figsize = (10, 6))\nplt.subplot(211)\nplt.plot(peth0.count(0.01).sum(1), linewidth = 3, color = 'red')\nplt.xlim(-0.1, 0.2)\nplt.ylabel(\"Count\")\nplt.axvline(0.0)\nplt.subplot(212)\nplt.plot(peth0.to_tsd(), \"|\",  markersize = 20, color = 'red', mew = 4)\nplt.xlabel(\"Time from stim (s)\")\nplt.ylabel(\"Stimulus\")\nplt.xlim(-0.1, 0.2)\nplt.axvline(0.0)\n</code></pre> <p></p> <p>Out:</p> <pre><code>&lt;matplotlib.lines.Line2D object at 0x7f6ed5b71790&gt;\n</code></pre> <p>The same function can be applied to a group of neurons. In this case, it returns a dict of TsGroup</p> <pre><code>pethall = nap.compute_perievent(ts_group, stim, minmax = (-0.1, 0.2), time_unit = 's')\nprint(pethall[1])\n</code></pre> <p>Out:</p> <pre><code>  Index    rate    ref_times\n-------  ------  -----------\n      0       0      11.2503\n      1       0      12.0641\n      2       0      16.1707\n      3       0      44.5992\n      4       0      54.3664\n      5       0      69.9383\n      6       0      80.3114\n      7       0      95.4514\n      8       0     101.135\n      9       0     127.961\n     10       0     129.893\n     11       0     163.846\n     12       0     167.349\n     13       0     182.681\n     14       0     202.576\n     15       0     242.675\n     16       0     252.635\n     17       0     271.798\n     18       0     303.741\n     19       0     318.019\n     20       0     344.258\n     21       0     347.42\n     22       0     363.56\n     23       0     376.035\n     24       0     401.097\n     25       0     421.945\n     26       0     462.981\n     27       0     479.427\n     28       0     482.936\n     29       0     499.198\n     30       0     515.336\n     31       0     546.984\n     32       0     568.632\n     33       0     570.638\n     34       0     699.086\n     35       0     702.429\n     36       0     706.091\n     37       0     785.28\n     38       0     813.498\n     39       0     840.927\n     40       0     842.795\n     41       0     847.521\n     42       0     854.812\n     43       0     872.71\n     44       0     907.902\n     45       0     908.995\n     46       0     909.74\n     47       0     917.418\n     48       0     942.385\n     49       0     968.598\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_process/#tuning-curves","title":"Tuning curves","text":"<p>pynapple can compute 1 dimension tuning curves (for example firing rate as a function of angular direction) and 2 dimension tuning curves ( for example firing rate as a function of position). In both cases, a TsGroup object can be directly passed to the function.</p> <p>First we will create the 2D features:</p> <pre><code>dt = 0.1\nfeatures = np.vstack((np.cos(np.arange(0, 1000, dt)),np.sin(np.arange(0,1000,dt)))).T\n# features += np.random.randn(features.shape[0], features.shape[1])*0.05\nfeatures = nap.TsdFrame(t = np.arange(0, 1000, dt), d = features, time_units = 's', time_support = epoch, columns=['a', 'b'])\nprint(features)\nplt.figure(figsize=(15,7))\nplt.subplot(121)\nplt.plot(features.loc[0:10])\nplt.title(\"Features\")\nplt.xlabel(\"Time(s)\")\nplt.subplot(122)\nplt.title(\"Features\")\nplt.plot(features['a'].loc[0:10], features['b'].loc[0:10])\nplt.xlabel(\"Feature a\")\nplt.ylabel(\"Feature b\")\n</code></pre> <p></p> <p>Out:</p> <pre><code>                 a         b\nTime (s)                    0.0       1.000000  0.000000\n0.1       0.995004  0.099833\n0.2       0.980067  0.198669\n0.3       0.955336  0.295520\n0.4       0.921061  0.389418\n...            ...       ...\n999.5     0.889961  0.456036\n999.6     0.839987  0.542606\n999.7     0.781621  0.623754\n999.8     0.715445  0.698670\n999.9     0.642120  0.766604\n\n[10000 rows x 2 columns]\nText(732.5909090909089, 0.5, 'Feature b')\n</code></pre> <p>Here we call the function <code>compute_2d_tuning_curves</code>. To check the accuracy of the tuning curves, we will display the spikes aligned to the features with the function <code>value_from</code> which assign to each spikes the corresponding feature value for neuron 0.</p> <pre><code>tcurves2d, binsxy = nap.compute_2d_tuning_curves(group=ts_group,\nfeature = features,                                                 \nnb_bins=10)\nts_to_features = ts_group[1].value_from(features)\nplt.figure()\nplt.plot(ts_to_features['a'], ts_to_features['b'], 'o', color = 'red', markersize=4)\nextents = (np.min(features['b']), np.max(features['b']), np.min(features['a']), np.max(features['a']))\nplt.imshow(tcurves2d[1].T, origin=\"lower\", extent=extents, cmap=\"viridis\")\nplt.title(\"Tuning curve unit 0 2d\")\nplt.xlabel(\"feature a\")\nplt.ylabel(\"feature b\")\nplt.grid(False)\nplt.show()\n</code></pre> <p></p> <p>Out:</p> <pre><code>/mnt/home/gviejo/pynapple/pynapple/process/tuning_curves.py:202: RuntimeWarning: invalid value encountered in true_divide\n  count = count / occupancy\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_process/#decoding","title":"Decoding","text":"<p>Pynapple supports 1 dimensional and 2 dimensional bayesian decoding. The function returns the decoded feature as well as the probabilities for each timestamps.</p> <p>First we generate some artificial \"place fields\" in 2 dimensions based on the features.</p> <p>This part is just to generate units with a relationship to the features (i.e. \"place fields\")</p> <pre><code>times = features.as_units('us').index.values\nft = features.values\nalpha = np.arctan2(ft[:,1], ft[:,0])\nbins = np.repeat(np.linspace(-np.pi, np.pi, 13)[::,np.newaxis], 2, 1)\nbins += np.array([- 2*np.pi/24, 2*np.pi/24])\nts_group = {}\nfor i in range(12):\nts = times[(alpha&gt;=bins[i,0]) &amp; (alpha &lt;= bins[i+1,1])]\nts_group[i] = nap.Ts(ts, time_units = 'us')\nts_group = nap.TsGroup(ts_group, time_support = epoch)\nprint(ts_group)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate\n-------  ------\n      0    1.25\n      1    1.67\n      2    1.67\n      3    1.66\n      4    1.67\n      5    1.67\n      6    1.67\n      7    1.67\n      8    1.67\n      9    1.67\n     10    1.67\n     11    1.25\n</code></pre> <p>To decode we need to compute tuning curves in 2D.</p> <pre><code>import warnings\nwarnings.filterwarnings('ignore')\ntcurves2d, binsxy = nap.compute_2d_tuning_curves(group=ts_group,\nfeature=features,                                                 \nnb_bins=10,\nep=epoch,\nminmax=(-1.,1.,-1.,1.))\n</code></pre> <p>Then we plot the \"place fields\".</p> <pre><code>plt.figure(figsize = (20,9))\nfor i in ts_group.keys():\nplt.subplot(2,6,i+1)\nplt.imshow(tcurves2d[i], extent=(binsxy[1][0],binsxy[1][-1],binsxy[0][0],binsxy[0][-1]))\nplt.xticks()\nplt.show()\n</code></pre> <p></p> <p>Then we call the actual decoding function in 2d.</p> <pre><code>decoded, proba_feature = nap.decode_2d(tuning_curves=tcurves2d, \ngroup=ts_group,                                   \nep=epoch,\nbin_size=0.1, # second\nxy=binsxy,\nfeatures=features,\n)\nplt.figure(figsize=(15,5))\nplt.subplot(131)\nplt.plot(features['a'].as_units('s').loc[0:20], label = 'True')\nplt.plot(decoded['a'].as_units('s').loc[0:20], label = 'Decoded')\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Feature a\")\nplt.subplot(132)\nplt.plot(features['b'].as_units('s').loc[0:20], label = 'True')\nplt.plot(decoded['b'].as_units('s').loc[0:20], label = 'Decoded')\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Feature b\")\nplt.subplot(133)\nplt.plot(features['a'].as_units('s').loc[0:20], features['b'].as_units('s').loc[0:20], label = 'True')\nplt.plot(decoded['a'].as_units('s').loc[0:20], decoded['b'].as_units('s').loc[0:20], label = 'Decoded')\nplt.xlabel(\"Feature a\")\nplt.ylabel(\"Feature b\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"generated/gallery/tutorial_pynapple_process/#randomization","title":"Randomization","text":"<p>Pynapple provides some ready-to-use randomization methods to compute null distributions for statistical testing. Different methods preserve or destroy different features of the data, here's a brief overview.</p> <p><code>shift_timestamps</code> shifts all the timestamps in a <code>Ts</code> object by the same random amount, wrapping the end of the time support to its beginning. This randomization preserves the temporal structure in the data but destroys the temporal relationships with other quantities (e.g. behavioural data). When applied on a <code>TsGroup</code> object, each series in the group is shifted independently.</p> <pre><code>ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 'ms')\nrand_ts = nap.shift_timestamps(ts,min_shift=1, max_shift=20)\n</code></pre> <p><code>shuffle_ts_intervals</code> computes the intervals between consecutive timestamps, permutes them, and generates a new set of timestamps with the permuted intervals. This procedure preserve the distribution of intervals, but not their sequence.</p> <pre><code>ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's')\nrand_ts = nap.shuffle_ts_intervals(ts)\n</code></pre> <p><code>jitter_timestamps</code> shifts each timestamp in the data of an independent random amount. When applied with a small <code>max_jitter</code>, this procedure destroys the fine temporal structure of the data, while preserving structure on longer timescales.</p> <pre><code>ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's')\nrand_ts = nap.jitter_timestamps(ts,max_jitter=1)\n</code></pre> <p><code>resample_timestamps</code> uniformly re-draws the same number of timestamps in <code>ts</code>, in the same time support. This procedures preserve the total number of timestamps, but destroys any other feature of the original data.</p> <pre><code>ts = nap.Ts(t = np.sort(np.random.uniform(0, 100, 10)), time_units = 's')\nrand_ts = nap.resample_timestamps(ts)\n</code></pre> <p>Total running time of the script: ( 0 minutes  5.747 seconds)</p> <p> Download Python source code: tutorial_pynapple_process.py</p> <p> Download Jupyter notebook: tutorial_pynapple_process.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/","title":"Quick start","text":"<p>Note</p> <p>Click here to download the full example code</p>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#quick-start","title":"Quick start","text":"<p>The examplar data to replicate the figure in the jupyter notebook can be found here. </p> <p>The data contains a sample recordings taken simultaneously from the anterodorsal thalamus and the hippocampus and contains both a sleep and wake session. It contains both head-direction cells (i.e. cells that fire for a particular head direction in the horizontal plane) and place cells (i.e. cells that fire for a particular position in the environment).</p> <p>Preprocessing of the data was made with Kilosort 2.0 and spike sorting was made with Klusters.</p> <p>Instructions for installing pynapple can be found here.</p> <p>This notebook is meant to provide an overview of pynapple by going through:</p> <ul> <li>Input output (IO). In this case, pynapple will load a NWB file using the NWBFile object within a project Folder that represent a dataset. </li> <li>Core functions that handle time series, interval sets and groups of time series. See this notebook for a detailled usage of the core functions.</li> <li>Process functions. A small collection of high-level functions widely used in system neuroscience. This notebook details those functions.</li> </ul> <p>Warning</p> <p>This tutorial uses seaborn and matplotlib for displaying the figure.</p> <p>You can install both with <code>pip install matplotlib seaborn</code></p> <pre><code>import numpy as np\nimport pandas as pd\nimport pynapple as nap\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", palette = \"colorblind\", font_scale=1.5, rc=custom_params)\n</code></pre> <p>Out:</p> <pre><code>/mnt/sw/nix/store/1bljgds808py1sq0av8nn8izvjnpfsbs-python-3.9.15-view/lib/python3.9/site-packages/jupyter_client/connect.py:27: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\ngiven by the platformdirs library.  To remove this warning and\nsee the appropriate new directories, set the environment variable\n`JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\nThe use of platformdirs will be the default in `jupyter_core` v6\n  from jupyter_core.paths import jupyter_data_dir\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#io","title":"IO","text":"<p>The first step is to give the path to the data folder.</p> <pre><code>DATA_DIRECTORY = \"../../your/path/to/MyProject/\"\n</code></pre> <p>We can load the session with the function load_folder. Pynapple will walks throught the folder and collects every subfolders. We can use the attribute <code>view</code> or the function <code>expand</code> to display a tree view of the dataset. The treeview shows all the compatible data format (i.e npz files or NWBs files) and their equivalent pynapple type.</p> <pre><code>data = nap.load_folder(DATA_DIRECTORY)\ndata.view\n</code></pre> <p>Out:</p> <pre><code>\ud83d\udcc2 MyProject\n\u2514\u2500\u2500 \ud83d\udcc2 sub-A2929\n    \u2514\u2500\u2500 \ud83d\udcc2 ses-A2929-200711\n        \u251c\u2500\u2500 \ud83d\udcc2 derivatives\n        \u2502   \u251c\u2500\u2500 spikes.npz      |        TsGroup\n        \u2502   \u251c\u2500\u2500 sleep_ep.npz    |        IntervalSet\n        \u2502   \u251c\u2500\u2500 position.npz    |        TsdFrame\n        \u2502   \u2514\u2500\u2500 wake_ep.npz     |        IntervalSet\n        \u251c\u2500\u2500 \ud83d\udcc2 pynapplenwb\n        \u2502   \u2514\u2500\u2500 A2929-200711    |        NWB file\n        \u251c\u2500\u2500 x_plus_y.npz    |        Tsd\n        \u2514\u2500\u2500 stimulus-fish.npz       |        IntervalSet\n</code></pre> <p>The object <code>data</code> is a <code>Folder</code> object that allows easy navigation and interaction with a dataset.  In this case, we want to load the NWB file in the folder <code>/pynapplenwb</code>. Data are always lazy loaded. No time series is loaded until it's actually called. When calling the NWB file, the object <code>nwb</code> is an interface to the NWB file. All the data inside the NWB file that are compatible with one of the pynapple objects are shown with their corresponding keys.</p> <pre><code>nwb = data[\"sub-A2929\"][\"ses-A2929-200711\"][\"pynapplenwb\"][\"A2929-200711\"]\nprint(nwb)\n</code></pre> <p>Out:</p> <pre><code>             A2929-200711              \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Keys                  \u2503 Type        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 units                 \u2502 TsGroup     \u2502\n\u2502 position_time_support \u2502 IntervalSet \u2502\n\u2502 epochs                \u2502 IntervalSet \u2502\n\u2502 z                     \u2502 Tsd         \u2502\n\u2502 y                     \u2502 Tsd         \u2502\n\u2502 x                     \u2502 Tsd         \u2502\n\u2502 rz                    \u2502 Tsd         \u2502\n\u2502 ry                    \u2502 Tsd         \u2502\n\u2502 rx                    \u2502 Tsd         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>We can individually call each object and they are actually loaded. </p> <p><code>units</code> is a TsGroup object. It allows to group together time series with different timestamps and couple metainformation to each neuron. In this case, the location of where the neuron was recorded has been added when loading the session for the first time. We load <code>units</code> as <code>spikes</code></p> <pre><code>spikes = nwb['units']\nprint(spikes)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate  location      group\n-------  ------  ----------  -------\n      0    7.3   adn               0\n1    5.73  adn               0\n2    8.12  adn               0\n3    6.68  adn               0\n4   10.77  adn               0\n5   11     adn               0\n6   16.52  adn               0\n7    2.2   ca1               1\n8    2.02  ca1               1\n9    1.07  ca1               1\n10    3.92  ca1               1\n11    3.31  ca1               1\n12    1.09  ca1               1\n13    1.28  ca1               1\n14    1.32  ca1               1\n</code></pre> <p>In this case, the TsGroup holds 15 neurons and it is possible to access, similar to a dictionnary, the spike times of a single neuron: </p> <pre><code>neuron_0 = spikes[0]\nprint(neuron_0)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n0.00845      NaN\n0.03265      NaN\n0.13230      NaN\n0.30340      NaN\n0.32900      NaN\n              ..\n1186.12755   NaN\n1189.38400   NaN\n1194.13475   NaN\n1196.20750   NaN\n1196.67675   NaN\nLength: 8764, dtype: float64\n</code></pre> <p>neuron_0 is a Ts object containing the times of the spikes.</p> <p>The other information about the session is contained in <code>nwb[\"epochs\"]</code>. In this case, the start and end of the sleep and wake epochs. If the NWB time intervals contains tags of the epochs, pynapple will try to group them together and return a dictionnary of IntervalSet instead of IntervalSet.</p> <pre><code>epochs = nwb[\"epochs\"]\nprint(epochs)\n</code></pre> <p>Out:</p> <pre><code>{'sleep':    start    end\n0    0.0  600.0, 'wake':    start     end\n0  600.0  1200.0}\n</code></pre> <p>Finally this dataset contains tracking of the animal in the environment. <code>rx</code>, <code>ry</code>, <code>rz</code> represent respectively the roll, the yaw and the pitch of the head of the animal. <code>x</code> and <code>z</code> represent the position of the animal in the horizontal plane while <code>y</code> represents the elevation. Here we load only the head-direction as a Tsd object.</p> <pre><code>head_direction = nwb[\"ry\"]\nprint(head_direction)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n670.64070     5.207148\n670.64900     5.181029\n670.65735     5.155508\n670.66565     5.136537\n670.67400     5.120850\n                ...   1199.96160    3.665954\n1199.96995    3.634619\n1199.97825    3.617849\n1199.98660    3.609446\n1199.99495    3.609375\nLength: 63527, dtype: float64\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#core","title":"Core","text":"<p>The core functions of pynapple provides many ways to manipulate time series. In this example, spike times are restricted to the wake epoch. Notice how the frequencies change from the original object.</p> <pre><code>wake_ep = epochs[\"wake\"]\nspikes_wake = spikes.restrict(wake_ep)\nprint(spikes_wake)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate  location      group\n-------  ------  ----------  -------\n      0    4.85  adn               0\n1    8.06  adn               0\n2    7.11  adn               0\n3    7.66  adn               0\n4    7.97  adn               0\n5   11.29  adn               0\n6   22.08  adn               0\n7    1.82  ca1               1\n8    2.84  ca1               1\n9    0.7   ca1               1\n10    4.78  ca1               1\n11    4.93  ca1               1\n12    1.71  ca1               1\n13    0.97  ca1               1\n14    0.26  ca1               1\n</code></pre> <p>The same operation can be applied to all time series. </p> <pre><code># In this example, we want all the epochs for which position in `x` is above a certain threhsold. For this we use the function `threshold`.\nposx = nwb['x']\nthreshold = 0.08\nposxpositive = posx.threshold(threshold)\nplt.figure()\nplt.plot(posx)\nplt.plot(posxpositive, '.')\nplt.axhline(threshold)\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"x\")\nplt.title(\"x &gt; {}\".format(threshold))\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>The epochs above the threshold can be accessed through the time support of the Tsd object. The time support is an important concept in the pynapple package. It helps the user to define the epochs for which the time serie should be defined. By default, Ts, Tsd and TsGroup objects possess a time support (defined as an IntervalSet). It is recommended to pass the time support when instantiating one of those objects.</p> <pre><code>epochs_above_thr = posxpositive.time_support\nprint(epochs_above_thr)\n</code></pre> <p>Out:</p> <pre><code>        start         end\n0  682.660850  745.565725\n1  752.240350  752.440325\n2  752.582000  752.673650\n3  757.498375  758.998300\n4  789.863275  790.271575\n5  875.225250  876.066875\n6  878.158425  878.641725\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#tuning-curves","title":"Tuning curves","text":"<p>Let's do a more advanced analysis. Neurons from ADn (group 0 in the <code>spikes</code> group object) are know to fire for a particular direction. Therefore, we can compute their tuning curves, i.e. their firing rates as a function of the head-direction of the animal in the horizontal plane (ry). To do this, we can use the function <code>compute_1d_tuning_curves</code>. In this case, the tuning curves are computed over 120 bins and between 0 and 2$\\pi$.</p> <pre><code>tuning_curves = nap.compute_1d_tuning_curves(group=spikes, \nfeature=head_direction,                                             \nnb_bins=121, \nminmax=(0, 2*np.pi))\nprint(tuning_curves)\n</code></pre> <p>Out:</p> <pre><code>                 0    1         2          3         4   ...         10         11         12        13        14\n0.025964  45.520459  0.0  0.000000   6.207335  6.207335  ...   6.207335  10.345559  14.483782  0.000000  2.069112\n0.077891  55.049762  0.0  0.000000   5.504976  3.302986  ...   8.807962  16.514929   1.100995  0.000000  0.000000\n0.129818  76.369034  0.0  0.000000  17.144069  4.675655  ...   3.117103  12.468414   9.351310  1.558552  0.000000\n0.181745  82.179721  0.0  0.000000   6.522200  1.304440  ...   6.522200  19.566600   9.131080  2.608880  0.000000\n0.233672  73.851374  0.0  0.000000  13.187745  5.275098  ...  15.825294  30.331814   7.912647  2.637549  0.000000\n...             ...  ...       ...        ...       ...  ...        ...        ...        ...       ...       ...\n6.049513  15.001060  0.0  0.000000  12.273595  1.363733  ...   8.182397   2.727466   0.000000  0.000000  1.363733\n6.101440  22.327159  0.0  0.000000  13.954475  0.000000  ...   2.790895  11.163580   2.790895  0.000000  0.000000\n6.153367  47.062150  0.0  0.000000  21.177967  0.000000  ...   7.059322  11.765537   0.000000  2.353107  0.000000\n6.205295  56.003958  0.0  2.000141   8.000565  2.000141  ...  14.000990  24.001696   6.000424  0.000000  0.000000\n6.257222  38.712414  0.0  0.000000   7.742483  0.000000  ...   7.742483   7.742483   7.742483  0.000000  0.000000\n\n[121 rows x 15 columns]\n</code></pre> <p>We can plot tuning curves in polar plots.</p> <pre><code>neuron_location = spikes.get_info('location') # to know where the neuron was recorded\nplt.figure(figsize=(12,9))\nfor i,n in enumerate(tuning_curves.columns):\nplt.subplot(3,5,i+1, projection = 'polar')\nplt.plot(tuning_curves[n])\nplt.title(neuron_location[n] + '-' + str(n), fontsize = 18)\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p> <p>While ADN neurons show obvious modulation for head-direction, it is not obvious for all CA1 cells. Therefore we want to restrict the remaining of the analyses to only ADN neurons. We can split the <code>spikes</code> group with the function <code>getby_category</code>.</p> <pre><code>spikes_by_location = spikes.getby_category('location')\nprint(spikes_by_location['adn'])\nprint(spikes_by_location['ca1'])\nspikes_adn = spikes_by_location['adn']\n</code></pre> <p>Out:</p> <pre><code>  Index    rate  location      group\n-------  ------  ----------  -------\n      0    7.3   adn               0\n1    5.73  adn               0\n2    8.12  adn               0\n3    6.68  adn               0\n4   10.77  adn               0\n5   11     adn               0\n6   16.52  adn               0\nIndex    rate  location      group\n-------  ------  ----------  -------\n      7    2.2   ca1               1\n8    2.02  ca1               1\n9    1.07  ca1               1\n10    3.92  ca1               1\n11    3.31  ca1               1\n12    1.09  ca1               1\n13    1.28  ca1               1\n14    1.32  ca1               1\n</code></pre>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#correlograms","title":"Correlograms","text":"<p>A classical question with head-direction cells is how pairs stay coordinated across brain states i.e. wake vs sleep (see Peyrache, A., Lacroix, M. M., Petersen, P. C., &amp; Buzs\u00e1ki, G. (2015). Internally organized mechanisms of the head direction sense. Nature neuroscience, 18(4), 569-575.)</p> <p>In this example, this coordination across brain states will be evaluated with cross-correlograms of pairs of neurons. We can call the function <code>compute_crosscorrelogram</code> during both sleep and wake epochs.</p> <pre><code>cc_wake = nap.compute_crosscorrelogram(group=spikes_adn, \nbinsize=20, # ms\nwindowsize=4000, # ms\nep=epochs['wake'], \nnorm=True,\ntime_units='ms')\ncc_sleep = nap.compute_crosscorrelogram(group=spikes_adn,\nbinsize=5, # ms\nwindowsize=400, # ms\nep=epochs['sleep'], \nnorm=True,\ntime_units='ms')\n</code></pre> <p>From the previous figure, we can see that neurons 0 and 1 fires for opposite directions during wake. Therefore we expect their cross-correlograms to show a trough around 0 time lag, meaning those two neurons do not fire spikes together. A similar trough during sleep for the same pair thus indicates a persistence of their coordination even if the animal is not moving its head. mkdocs_gallery_thumbnail_number = 3</p> <pre><code>xtwake = cc_wake.index.values\nxtsleep = cc_sleep.index.values\nplt.figure(figsize = (15, 5))\nplt.subplot(131, projection = 'polar')\nplt.plot(tuning_curves[[0,1]]) # The tuning curves of the pair [0,1]\nplt.subplot(132)\nplt.fill_between(xtwake, np.zeros_like(xtwake), cc_wake[(0,1)].values, color = 'darkgray')\nplt.title('wake')\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"CC\")\nplt.subplot(133)\nplt.fill_between(xtsleep, np.zeros_like(xtsleep), cc_sleep[(0,1)].values, color = 'lightgrey')\nplt.title('sleep')\nplt.xlabel(\"Time (ms)\")\nplt.ylabel(\"CC\")\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#decoding","title":"Decoding","text":"<p>This last analysis shows how to use the pynapple's decoding function.</p> <p>The previous result indicates a persistent coordination of head-direction cells during sleep. Therefore it is possible to decode a virtual head-direction signal even if the animal is not moving its head.  This example uses the function <code>decode_1d</code> which implements bayesian decoding (see : Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.)</p> <p>First we can validate the decoding function with the real position of the head of the animal during wake.</p> <pre><code>tuning_curves_adn = nap.compute_1d_tuning_curves(spikes_adn,\nhead_direction,\nnb_bins=61,\nminmax=(0, 2*np.pi))\ndecoded, proba_angle = nap.decode_1d(tuning_curves=tuning_curves_adn, \ngroup=spikes_adn, \nep=epochs['wake'],                                 \nbin_size=0.3, # second\nfeature=head_direction, \n)\nprint(decoded)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n600.15     2.111562\n600.45     2.111562\n600.75     2.317568\n601.05     2.008559\n601.35     2.111562\n             ...   1198.65    4.583635\n1198.95    4.274626\n1199.25    4.583635\n1199.55    4.583635\n1199.85    3.862614\nLength: 2000, dtype: float64\n</code></pre> <p>We can plot the decoded head-direction along with the true head-direction.</p> <pre><code>plt.figure(figsize=(20,5))\nplt.plot(head_direction.as_units('s'), label = 'True')\nplt.plot(decoded.as_units('s'), label = 'Decoded')\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Head-direction (rad)\")\nplt.show()\n</code></pre> <p></p>"},{"location":"generated/gallery/tutorial_pynapple_quick_start/#raster","title":"Raster","text":"<p>Finally we can decode activity during sleep and overlay spiking activity of ADN neurons as a raster plot (in this case only during the first 4 seconds). Pynapple return as well the probability of being in a particular state. We can display it next to the spike train.</p> <p>First let's decode during sleep with a bin size of 40 ms.</p> <pre><code>decoded_sleep, proba_angle_Sleep = nap.decode_1d(tuning_curves=tuning_curves_adn,\ngroup=spikes_adn, \nep=epochs['sleep'],\nbin_size=0.04, # second\nfeature=head_direction, \n)\n</code></pre> <p>Here we are gonna chain the TsGroup function <code>set_info</code> and the function <code>to_tsd</code> to flatten the TsGroup and quickly assign to each spikes a corresponding value found in the metadata table. Any columns of the metadata table can be assigned to timestamps in a TsGroup.</p> <p>Here the value assign to the spikes comes from the preferred firing direction of the neurons. The following line is a quick way to sort the neurons based on their preferred firing direction</p> <pre><code>order = np.argsort(np.argmax(tuning_curves_adn.values,0))\nprint(order)\n</code></pre> <p>Out:</p> <pre><code>[0 4 2 6 1 3 5]\n</code></pre> <p>Assigning order as a metadata of TsGroup</p> <pre><code>spikes_adn.set_info(order=order)\nprint(spikes_adn)\n</code></pre> <p>Out:</p> <pre><code>  Index    rate  location      group    order\n-------  ------  ----------  -------  -------\n      0    7.3   adn               0        0\n1    5.73  adn               0        4\n2    8.12  adn               0        2\n3    6.68  adn               0        6\n4   10.77  adn               0        1\n5   11     adn               0        3\n6   16.52  adn               0        5\n</code></pre> <p>\"Flattening\" the TsGroup to a Tsd based on <code>order</code>. It's then very easy to call plot on <code>tsd_adn</code> to display the raster</p> <pre><code>tsd_adn = spikes_adn.to_tsd(\"order\")\nprint(tsd_adn)\n</code></pre> <p>Out:</p> <pre><code>Time (s)\n0.00845       0.0\n0.03265       0.0\n0.07745       6.0\n0.13230       0.0\n0.14045       5.0\n             ... 1199.90650    5.0\n1199.91745    5.0\n1199.94065    5.0\n1199.95035    5.0\n1199.96795    5.0\nLength: 79349, dtype: float64\n</code></pre> <p>Plotting everything</p> <pre><code>subep = nap.IntervalSet(start=0, end=10, time_units='s')\nplt.figure(figsize=(19,10))\nplt.subplot(211)\nplt.plot(tsd_adn.restrict(subep), '|', markersize=20)\nplt.xlim(subep.start[0], subep.end[0])\nplt.ylabel(\"Order\")\nplt.title(\"Decoding during sleep\")\nplt.subplot(212)\np = proba_angle_Sleep.restrict(subep)\nplt.imshow(p.values.T, aspect='auto', origin='lower', cmap='viridis')\nplt.title(\"Probability\")\nplt.xticks([0, p.shape[0]-1], subep.values[0])\nplt.yticks([0, p.shape[1]], ['0', '360'])\nplt.legend()\nplt.xlabel(\"Time (s)\")\nplt.ylabel(\"Head-direction (deg)\")\nplt.legend()\nplt.show()\n</code></pre> <p></p> <p>Out:</p> <pre><code>No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\nNo artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n</code></pre> <p>Total running time of the script: ( 0 minutes  16.805 seconds)</p> <p> Download Python source code: tutorial_pynapple_quick_start.py</p> <p> Download Jupyter notebook: tutorial_pynapple_quick_start.ipynb</p> <p>Gallery generated by mkdocs-gallery</p>"},{"location":"old_pages/core.interval_set/","title":"Core.interval set","text":""},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet","title":"<code>IntervalSet</code>","text":"<p>         Bases: <code>pd.DataFrame</code></p> <p>A subclass of pandas.DataFrame representing a (irregular) set of time intervals in elapsed time, with relative operations</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>class IntervalSet(pd.DataFrame):\n# class IntervalSet():\n\"\"\"\n    A subclass of pandas.DataFrame representing a (irregular) set of time intervals in elapsed time, with relative operations\n    \"\"\"\ndef __init__(self, start, end=None, time_units=\"s\", **kwargs):\n\"\"\"\n        IntervalSet initializer\n        If start and end and not aligned, meaning that \\n\n        1. len(start) != len(end)\n        2. end[i] &gt; start[i]\n        3. start[i+1] &gt; end[i]\n        4. start and end are not sorted,\n        IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point\n        Parameters\n        ----------\n        start : numpy.ndarray or number or pandas.DataFrame\n            Beginning of intervals\n        end : numpy.ndarray or number, optional\n            Ends of intervals\n        time_units : str, optional\n            Time unit of the intervals ('us', 'ms', 's' [default])\n        **kwargs\n            Additional parameters passed ot pandas.DataFrame\n        Returns\n        -------\n        IntervalSet\n            _\n        Raises\n        ------\n        RuntimeError\n            Description\n        ValueError\n            If a pandas.DataFrame is passed, it should contains\n            a column 'start' and a column 'end'.\n        \"\"\"\nif end is None:\ndf = pd.DataFrame(start)\nif \"start\" not in df.columns or \"end\" not in df.columns:\nraise ValueError(\"wrong columns name\")\nstart = df[\"start\"].values.astype(np.float64)\nend = df[\"end\"].values.astype(np.float64)\nstart = sort_timestamps(format_timestamps(start.ravel(), time_units))\nend = sort_timestamps(format_timestamps(end.ravel(), time_units))\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\nself._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\nreturn\nstart = np.array(start).astype(np.float64)\nend = np.array(end).astype(np.float64)\nstart = format_timestamps(np.array(start).ravel(), time_units)\nend = format_timestamps(np.array(end).ravel(), time_units)\nif len(start) != len(end):\nraise RuntimeError(\"Starts end ends are not of the same length\")\nif not (np.diff(start) &gt; 0).all():\nwarnings.warn(\"start is not sorted.\", stacklevel=2)\nstart = np.sort(start)\nif not (np.diff(end) &gt; 0).all():\nwarnings.warn(\"end is not sorted.\", stacklevel=2)\nend = np.sort(end)\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\n# self._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\ndef __repr__(self):\nreturn self.as_units(\"s\").__repr__()\ndef __str__(self):\nreturn self.__repr__()\ndef time_span(self):\n\"\"\"\n        Time span of the interval set.\n        Returns\n        -------\n        out: IntervalSet\n            an IntervalSet with a single interval encompassing the whole IntervalSet\n        \"\"\"\ns = self[\"start\"][0]\ne = self[\"end\"].iloc[-1]\nreturn IntervalSet(s, e)\ndef tot_length(self, time_units=\"s\"):\n\"\"\"\n        Total elapsed time in the set.\n        Parameters\n        ----------\n        time_units : None, optional\n            The time units to return the result in ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: float\n            _\n        \"\"\"\ntot_l = (self[\"end\"] - self[\"start\"]).sum()\nreturn return_timestamps(np.array([tot_l]), time_units)[0]\ndef intersect(self, a):\n\"\"\"\n        set intersection of IntervalSet\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to intersect self with\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitintersect(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\ndef union(self, a):\n\"\"\"\n        set union of IntervalSet\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to union self with\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitunion(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\ndef set_diff(self, a):\n\"\"\"\n        set difference of IntervalSet\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to set-substract from self\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitdiff(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\ndef in_interval(self, tsd):\n\"\"\"\n        finds out in which element of the interval set each point in a time series fits.\n        NaNs for those that don't fit an interval\n        Parameters\n        ----------\n        tsd : Tsd\n            The tsd to be binned\n        Returns\n        -------\n        out: numpy.ndarray\n            an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet\n        \"\"\"\ntimes = tsd.index.values\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nreturn jitin_interval(times, starts, ends)\ndef drop_short_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Drops the short intervals in the interval set.\n        Parameters\n        ----------\n        threshold : numeric\n            Time threshold for \"short\" intervals\n        time_units : None, optional\n            The time units for the treshold ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: IntervalSet\n            A copied IntervalSet with the dropped intervals\n        \"\"\"\nthreshold = format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &gt; threshold].reset_index(\ndrop=True\n)\ndef drop_long_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Drops the long intervals in the interval set.\n        Parameters\n        ----------\n        threshold : numeric\n            Time threshold for \"long\" intervals\n        time_units : None, optional\n            The time units for the treshold ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: IntervalSet\n            A copied IntervalSet with the dropped intervals\n        \"\"\"\nthreshold = format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &lt; threshold].reset_index(\ndrop=True\n)\ndef as_units(self, units=\"s\"):\n\"\"\"\n        returns a DataFrame with time expressed in the desired unit\n        Parameters\n        ----------\n        units : None, optional\n            'us', 'ms', or 's' [default]\n        Returns\n        -------\n        out: pandas.DataFrame\n            DataFrame with adjusted times\n        \"\"\"\ndata = self.values.copy()\ndata = return_timestamps(data, units)\nif units == \"us\":\ndata = data.astype(np.int64)\ndf = pd.DataFrame(index=self.index.values, data=data, columns=self.columns)\nreturn df\ndef merge_close_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Merges intervals that are very close.\n        Parameters\n        ----------\n        threshold : numeric\n            time threshold for the closeness of the intervals\n        time_units : None, optional\n            time units for the threshold ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: IntervalSet\n            a copied IntervalSet with merged intervals\n        \"\"\"\nif len(self) == 0:\nreturn IntervalSet(start=[], end=[])\nthreshold = format_timestamps(\nnp.array((threshold,), dtype=np.float64).ravel(), time_units\n)[0]\nstart = self[\"start\"].values\nend = self[\"end\"].values\ntojoin = (start[1:] - end[0:-1]) &gt; threshold\nstart = np.hstack((start[0], start[1:][tojoin]))\nend = np.hstack((end[0:-1][tojoin], end[-1]))\nreturn IntervalSet(start=start, end=end)\ndef get_intervals_center(self, alpha=0.5):\n\"\"\"\n        Returns by default the centers of each intervals.\n        It is possible to bias the midpoint by changing the alpha parameter between [0, 1]\n        For each epoch:\n        t = start + (end-start)*alpha\n        Parameters\n        ----------\n        alpha : float, optional\n            The midpoint within each interval.\n        Returns\n        -------\n        Ts\n            Timestamps object\n        \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nif not isinstance(alpha, float):\nraise RuntimeError(\"Parameter alpha should be float type\")\nalpha = np.clip(alpha, 0, 1)\nt = starts + (ends - starts) * alpha\nreturn time_series.Ts(t=t, time_support=self)\ndef save(self, filename):\n\"\"\"\n        Save IntervalSet object in npz format. The file will contain the starts and ends.\n        The main purpose of this function is to save small/medium sized IntervalSet\n        objects. For example, you determined some epochs for one session that you want to save\n        to avoid recomputing them.\n        You can load the object with numpy.load. Keys are 'start', 'end' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n        &gt;&gt;&gt; ep.save(\"my_ep.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['start', 'end', 'type']\n        &gt;&gt;&gt; print(file['start'])\n        [0. 10. 20.]\n        It is then easy to recreate the IntervalSet object.\n        &gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n           start   end\n        0    0.0   5.0\n        1   10.0  12.0\n        2   20.0  33.0\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nstart=self.start.values,\nend=self.end.values,\ntype=np.array([\"IntervalSet\"], dtype=np.str_),\n)\nreturn\n@property\ndef _constructor(self):\nreturn IntervalSet\n@property\ndef starts(self):\n\"\"\"Return the starts of the IntervalSet as a Ts object\n        Returns\n        -------\n        Ts\n            The starts of the IntervalSet\n        \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nreturn time_series.Ts(t=self.values[:, 0], time_support=self)\n@property\ndef ends(self):\n\"\"\"Return the ends of the IntervalSet as a Ts object\n        Returns\n        -------\n        Ts\n            The ends of the IntervalSet\n        \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nreturn time_series.Ts(t=self.values[:, 1], time_support=self)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.starts","title":"<code>starts</code>  <code>property</code>","text":"<p>Return the starts of the IntervalSet as a Ts object</p> <p>Returns:</p> Type Description <code>Ts</code> <p>The starts of the IntervalSet</p>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.ends","title":"<code>ends</code>  <code>property</code>","text":"<p>Return the ends of the IntervalSet as a Ts object</p> <p>Returns:</p> Type Description <code>Ts</code> <p>The ends of the IntervalSet</p>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.__init__","title":"<code>__init__(start, end=None, time_units='s', **kwargs)</code>","text":"<p>IntervalSet initializer</p> <p>If start and end and not aligned, meaning that </p> <ol> <li>len(start) != len(end)</li> <li>end[i] &gt; start[i]</li> <li>start[i+1] &gt; end[i]</li> <li>start and end are not sorted,</li> </ol> <p>IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>numpy.ndarray or number or pandas.DataFrame</code> <p>Beginning of intervals</p> required <code>end</code> <code>numpy.ndarray or number, optional</code> <p>Ends of intervals</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>Time unit of the intervals ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>**kwargs</code> <p>Additional parameters passed ot pandas.DataFrame</p> <code>{}</code> <p>Returns:</p> Type Description <code>IntervalSet</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Description</p> <code>ValueError</code> <p>If a pandas.DataFrame is passed, it should contains a column 'start' and a column 'end'.</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def __init__(self, start, end=None, time_units=\"s\", **kwargs):\n\"\"\"\n    IntervalSet initializer\n    If start and end and not aligned, meaning that \\n\n    1. len(start) != len(end)\n    2. end[i] &gt; start[i]\n    3. start[i+1] &gt; end[i]\n    4. start and end are not sorted,\n    IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point\n    Parameters\n    ----------\n    start : numpy.ndarray or number or pandas.DataFrame\n        Beginning of intervals\n    end : numpy.ndarray or number, optional\n        Ends of intervals\n    time_units : str, optional\n        Time unit of the intervals ('us', 'ms', 's' [default])\n    **kwargs\n        Additional parameters passed ot pandas.DataFrame\n    Returns\n    -------\n    IntervalSet\n        _\n    Raises\n    ------\n    RuntimeError\n        Description\n    ValueError\n        If a pandas.DataFrame is passed, it should contains\n        a column 'start' and a column 'end'.\n    \"\"\"\nif end is None:\ndf = pd.DataFrame(start)\nif \"start\" not in df.columns or \"end\" not in df.columns:\nraise ValueError(\"wrong columns name\")\nstart = df[\"start\"].values.astype(np.float64)\nend = df[\"end\"].values.astype(np.float64)\nstart = sort_timestamps(format_timestamps(start.ravel(), time_units))\nend = sort_timestamps(format_timestamps(end.ravel(), time_units))\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\nself._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\nreturn\nstart = np.array(start).astype(np.float64)\nend = np.array(end).astype(np.float64)\nstart = format_timestamps(np.array(start).ravel(), time_units)\nend = format_timestamps(np.array(end).ravel(), time_units)\nif len(start) != len(end):\nraise RuntimeError(\"Starts end ends are not of the same length\")\nif not (np.diff(start) &gt; 0).all():\nwarnings.warn(\"start is not sorted.\", stacklevel=2)\nstart = np.sort(start)\nif not (np.diff(end) &gt; 0).all():\nwarnings.warn(\"end is not sorted.\", stacklevel=2)\nend = np.sort(end)\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\n# self._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.time_span","title":"<code>time_span()</code>","text":"<p>Time span of the interval set.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>an IntervalSet with a single interval encompassing the whole IntervalSet</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def time_span(self):\n\"\"\"\n    Time span of the interval set.\n    Returns\n    -------\n    out: IntervalSet\n        an IntervalSet with a single interval encompassing the whole IntervalSet\n    \"\"\"\ns = self[\"start\"][0]\ne = self[\"end\"].iloc[-1]\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.tot_length","title":"<code>tot_length(time_units='s')</code>","text":"<p>Total elapsed time in the set.</p> <p>Parameters:</p> Name Type Description Default <code>time_units</code> <code>None, optional</code> <p>The time units to return the result in ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def tot_length(self, time_units=\"s\"):\n\"\"\"\n    Total elapsed time in the set.\n    Parameters\n    ----------\n    time_units : None, optional\n        The time units to return the result in ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: float\n        _\n    \"\"\"\ntot_l = (self[\"end\"] - self[\"start\"]).sum()\nreturn return_timestamps(np.array([tot_l]), time_units)[0]\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.intersect","title":"<code>intersect(a)</code>","text":"<p>set intersection of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to intersect self with</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def intersect(self, a):\n\"\"\"\n    set intersection of IntervalSet\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to intersect self with\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitintersect(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.union","title":"<code>union(a)</code>","text":"<p>set union of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to union self with</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def union(self, a):\n\"\"\"\n    set union of IntervalSet\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to union self with\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitunion(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.set_diff","title":"<code>set_diff(a)</code>","text":"<p>set difference of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to set-substract from self</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def set_diff(self, a):\n\"\"\"\n    set difference of IntervalSet\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to set-substract from self\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitdiff(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.in_interval","title":"<code>in_interval(tsd)</code>","text":"<p>finds out in which element of the interval set each point in a time series fits.</p> <p>NaNs for those that don't fit an interval</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>Tsd</code> <p>The tsd to be binned</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def in_interval(self, tsd):\n\"\"\"\n    finds out in which element of the interval set each point in a time series fits.\n    NaNs for those that don't fit an interval\n    Parameters\n    ----------\n    tsd : Tsd\n        The tsd to be binned\n    Returns\n    -------\n    out: numpy.ndarray\n        an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet\n    \"\"\"\ntimes = tsd.index.values\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nreturn jitin_interval(times, starts, ends)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.drop_short_intervals","title":"<code>drop_short_intervals(threshold, time_units='s')</code>","text":"<p>Drops the short intervals in the interval set.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>Time threshold for \"short\" intervals</p> required <code>time_units</code> <code>None, optional</code> <p>The time units for the treshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>A copied IntervalSet with the dropped intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def drop_short_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Drops the short intervals in the interval set.\n    Parameters\n    ----------\n    threshold : numeric\n        Time threshold for \"short\" intervals\n    time_units : None, optional\n        The time units for the treshold ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: IntervalSet\n        A copied IntervalSet with the dropped intervals\n    \"\"\"\nthreshold = format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &gt; threshold].reset_index(\ndrop=True\n)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.drop_long_intervals","title":"<code>drop_long_intervals(threshold, time_units='s')</code>","text":"<p>Drops the long intervals in the interval set.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>Time threshold for \"long\" intervals</p> required <code>time_units</code> <code>None, optional</code> <p>The time units for the treshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>A copied IntervalSet with the dropped intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def drop_long_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Drops the long intervals in the interval set.\n    Parameters\n    ----------\n    threshold : numeric\n        Time threshold for \"long\" intervals\n    time_units : None, optional\n        The time units for the treshold ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: IntervalSet\n        A copied IntervalSet with the dropped intervals\n    \"\"\"\nthreshold = format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &lt; threshold].reset_index(\ndrop=True\n)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.as_units","title":"<code>as_units(units='s')</code>","text":"<p>returns a DataFrame with time expressed in the desired unit</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>None, optional</code> <p>'us', 'ms', or 's' [default]</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>pandas.DataFrame</code> <p>DataFrame with adjusted times</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    returns a DataFrame with time expressed in the desired unit\n    Parameters\n    ----------\n    units : None, optional\n        'us', 'ms', or 's' [default]\n    Returns\n    -------\n    out: pandas.DataFrame\n        DataFrame with adjusted times\n    \"\"\"\ndata = self.values.copy()\ndata = return_timestamps(data, units)\nif units == \"us\":\ndata = data.astype(np.int64)\ndf = pd.DataFrame(index=self.index.values, data=data, columns=self.columns)\nreturn df\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.merge_close_intervals","title":"<code>merge_close_intervals(threshold, time_units='s')</code>","text":"<p>Merges intervals that are very close.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>time threshold for the closeness of the intervals</p> required <code>time_units</code> <code>None, optional</code> <p>time units for the threshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>a copied IntervalSet with merged intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def merge_close_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Merges intervals that are very close.\n    Parameters\n    ----------\n    threshold : numeric\n        time threshold for the closeness of the intervals\n    time_units : None, optional\n        time units for the threshold ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: IntervalSet\n        a copied IntervalSet with merged intervals\n    \"\"\"\nif len(self) == 0:\nreturn IntervalSet(start=[], end=[])\nthreshold = format_timestamps(\nnp.array((threshold,), dtype=np.float64).ravel(), time_units\n)[0]\nstart = self[\"start\"].values\nend = self[\"end\"].values\ntojoin = (start[1:] - end[0:-1]) &gt; threshold\nstart = np.hstack((start[0], start[1:][tojoin]))\nend = np.hstack((end[0:-1][tojoin], end[-1]))\nreturn IntervalSet(start=start, end=end)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.get_intervals_center","title":"<code>get_intervals_center(alpha=0.5)</code>","text":"<p>Returns by default the centers of each intervals.</p> <p>It is possible to bias the midpoint by changing the alpha parameter between [0, 1] For each epoch: t = start + (end-start)*alpha</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float, optional</code> <p>The midpoint within each interval.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Ts</code> <p>Timestamps object</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def get_intervals_center(self, alpha=0.5):\n\"\"\"\n    Returns by default the centers of each intervals.\n    It is possible to bias the midpoint by changing the alpha parameter between [0, 1]\n    For each epoch:\n    t = start + (end-start)*alpha\n    Parameters\n    ----------\n    alpha : float, optional\n        The midpoint within each interval.\n    Returns\n    -------\n    Ts\n        Timestamps object\n    \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nif not isinstance(alpha, float):\nraise RuntimeError(\"Parameter alpha should be float type\")\nalpha = np.clip(alpha, 0, 1)\nt = starts + (ends - starts) * alpha\nreturn time_series.Ts(t=t, time_support=self)\n</code></pre>"},{"location":"old_pages/core.interval_set/#pynapple.core.interval_set.IntervalSet.save","title":"<code>save(filename)</code>","text":"<p>Save IntervalSet object in npz format. The file will contain the starts and ends.</p> <p>The main purpose of this function is to save small/medium sized IntervalSet objects. For example, you determined some epochs for one session that you want to save to avoid recomputing them.</p> <p>You can load the object with numpy.load. Keys are 'start', 'end' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n&gt;&gt;&gt; ep.save(\"my_ep.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['start', 'end', 'type']\n&gt;&gt;&gt; print(file['start'])\n[0. 10. 20.]\n</code></pre> <p>It is then easy to recreate the IntervalSet object.</p> <pre><code>&gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n   start   end\n0    0.0   5.0\n1   10.0  12.0\n2   20.0  33.0\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save IntervalSet object in npz format. The file will contain the starts and ends.\n    The main purpose of this function is to save small/medium sized IntervalSet\n    objects. For example, you determined some epochs for one session that you want to save\n    to avoid recomputing them.\n    You can load the object with numpy.load. Keys are 'start', 'end' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n    &gt;&gt;&gt; ep.save(\"my_ep.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['start', 'end', 'type']\n    &gt;&gt;&gt; print(file['start'])\n    [0. 10. 20.]\n    It is then easy to recreate the IntervalSet object.\n    &gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n       start   end\n    0    0.0   5.0\n    1   10.0  12.0\n    2   20.0  33.0\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nstart=self.start.values,\nend=self.end.values,\ntype=np.array([\"IntervalSet\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"old_pages/core.time_series/","title":"Core.time series","text":""},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd","title":"<code>Tsd</code>","text":"<p>         Bases: <code>pd.Series</code></p> <p>A subclass of pandas.Series specialized for neurophysiology time series.</p> <p>Tsd provides standardized time representation, plus various functions for manipulating times series.</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class Tsd(pd.Series):\n# class Tsd():\n\"\"\"\n    A subclass of pandas.Series specialized for neurophysiology time series.\n    Tsd provides standardized time representation, plus various functions for manipulating times series.\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, d=None, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n        Tsd Initializer.\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.Series\n            An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n        d : numpy.ndarray, optional\n            The data of the time series\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default])\n        time_support : IntervalSet, optional\n            The time support of the tsd object\n        **kwargs\n            Arguments that will be passed to the pandas.Series initializer.\n        \"\"\"\nif isinstance(t, SingleBlockManager):\nd = t.array\nt = t.index.values\nif \"index\" in kwargs:\nkwargs.pop(\"index\")\nelif isinstance(t, pd.Series):\nd = t.values\nt = t.index.values\nt = t.astype(np.float64).flatten()\nt = format_timestamps(t, time_units)\nt = sort_timestamps(t)\nif len(t):\nif time_support is not None:\nstarts = time_support.start.values\nends = time_support.end.values\nif d is not None:\nt, d = jitrestrict(t, d, starts, ends)\nsuper().__init__(index=t, data=d)\nelse:\nt = jittsrestrict(t, starts, ends)\nsuper().__init__(index=t, data=None, dtype=np.int8)\nelse:\ntime_support = IntervalSet(start=t[0], end=t[-1])\nif d is not None:\nsuper().__init__(index=t, data=d)\nelse:\nsuper().__init__(index=t, data=d, dtype=np.float64)\nself.time_support = time_support\nself.rate = t.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\ntime_support = IntervalSet(pd.DataFrame(columns=[\"start\", \"end\"]))\nsuper().__init__(index=t, data=d, dtype=np.float64)\nself.time_support = time_support\nself.rate = 0.0\nself.index.name = \"Time (s)\"\n# self._metadata.append(\"nap_class\")\nself.nap_class = self.__class__.__name__\ndef __add__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__add__(value), time_support=ts)\ndef __sub__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__sub__(value), time_support=ts)\ndef __truediv__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__truediv__(value), time_support=ts)\ndef __floordiv__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__floordiv__(value), time_support=ts)\ndef __mul__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__mul__(value), time_support=ts)\ndef __mod__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__mod__(value), time_support=ts)\ndef __pow__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__pow__(value), time_support=ts)\ndef __lt__(self, value):\nreturn self.as_series().__lt__(value)\ndef __gt__(self, value):\nreturn self.as_series().__gt__(value)\ndef __le__(self, value):\nreturn self.as_series().__le__(value)\ndef __ge__(self, value):\nreturn self.as_series().__ge__(value)\ndef __ne__(self, value):\nreturn self.as_series().__ne__(value)\ndef __eq__(self, value):\nreturn self.as_series().__eq__(value)\ndef __repr__(self):\nreturn self.as_series().__repr__()\ndef __str__(self):\nreturn self.__repr__()\ndef times(self, units=\"s\"):\n\"\"\"\n        The time index of the Tsd, returned as np.double in the desired time units.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: numpy.ndarray\n            the time indexes\n        \"\"\"\nreturn return_timestamps(self.index.values, units)\ndef as_series(self):\n\"\"\"\n        Convert the Ts/Tsd object to a pandas.Series object.\n        Returns\n        -------\n        out: pandas.Series\n            _\n        \"\"\"\nreturn pd.Series(self, copy=True)\ndef as_units(self, units=\"s\"):\n\"\"\"\n        Returns a Series with time expressed in the desired unit.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        pandas.Series\n            the series object with adjusted times\n        \"\"\"\nss = self.as_series()\nt = self.index.values\nt = return_timestamps(t, units)\nif units == \"us\":\nt = t.astype(np.int64)\nss.index = t\nss.index.name = \"Time (\" + str(units) + \")\"\nreturn ss\ndef data(self):\n\"\"\"\n        The data in the Tsd object\n        Returns\n        -------\n        out: numpy.ndarray\n            _\n        \"\"\"\nreturn self.values\ndef value_from(self, data, ep=None):\n\"\"\"\n        Replace the value with the closest value from Tsd/TsdFrame argument\n        If data is TsdFrame, the output is also TsdFrame.\n        Parameters\n        ----------\n        data : Tsd/TsdFrame\n            The Tsd/TsdFrame object holding the values to replace.\n        ep : IntervalSet (optional)\n            The IntervalSet object to restrict the operation.\n            If None, the time support of the tsd input object is used.\n        Returns\n        -------\n        out : Tsd/TsdFrame\n            Object with the new values\n        Examples\n        --------\n        In this example, the ts object will receive the closest values in time from tsd.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n        &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n        The variable ts is a time series object containing only nan.\n        The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n        &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n        newts is the same size as ts restrict to ep.\n        &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n            52 52\n        \"\"\"\nif isinstance(data, Tsd):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\nelif isinstance(data, TsdFrame):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefromtsdframe(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nraise RuntimeError(\"The time series to align to should be Tsd/TsdFrame.\")\ndef restrict(self, ep):\n\"\"\"\n        Restricts a Tsd object to a set of time intervals delimited by an IntervalSet object\n        Parameters\n        ----------\n        ep : IntervalSet\n            the IntervalSet object\n        Returns\n        -------\n        out: Tsd\n            Tsd object restricted to ep\n        Examples\n        --------\n        The Ts object is restrict to the intervals defined by ep.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n        &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n        &gt;&gt;&gt; newts = ts.restrict(ep)\n        The time support of newts automatically inherit the epochs defined by ep.\n        &gt;&gt;&gt; newts.time_support\n        &gt;&gt;&gt;    start    end\n        &gt;&gt;&gt; 0    0.0  500.0\n        \"\"\"\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nreturn Tsd(t=t, d=d, time_support=ep)\ndef count(self, *args, **kwargs):\n\"\"\"\n        Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n        You can call this function in multiple ways :\n        1. *tsd.count(bin_size=1, time_units = 'ms')*\n        -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n        2. *tsd.count(1, ep=my_epochs)*\n        -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n        3. *tsd.count(ep=my_bins)*\n        -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n        4. *tsd.count()*\n        -&gt; Count occurent of events within each epoch of the time support.\n        bin_size should be seconds unless specified.\n        If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n        Parameters\n        ----------\n        bin_size : None or float, optional\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: Tsd\n            A Tsd object indexed by the center of the bins.\n        Examples\n        --------\n        This example shows how to count events within bins of 0.1 second.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n        &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n        &gt;&gt;&gt; bincount = ts.count(0.1)\n        An epoch can be specified:\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n        &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n        And bincount automatically inherit ep as time support:\n        &gt;&gt;&gt; bincount.time_support\n        &gt;&gt;&gt;    start    end\n        &gt;&gt;&gt; 0  100.0  800.0\n        \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\ntime_array = self.index.values\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = float(bin_size)\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\nt, d = jitcount(time_array, starts, ends, bin_size)\ntime_support = IntervalSet(start=starts, end=ends)\nreturn Tsd(t=t, d=d, time_support=time_support)\nelse:\n_, countin = jittsrestrict_with_count(time_array, starts, ends)\nt = starts + (ends - starts) / 2\nreturn Tsd(t=t, d=countin, time_support=ep)\ndef bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n        Bin the data by averaging points within bin_size\n        bin_size should be seconds unless specified.\n        If no epochs is passed, the data will be binned based on the time support.\n        Parameters\n        ----------\n        bin_size : float\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: Tsd\n            A Tsd object indexed by the center of the bins and holding the averaged data points.\n        Examples\n        --------\n        This example shows how to bin data within bins of 0.1 second.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n        &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n        An epoch can be specified:\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n        &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n        And bintsd automatically inherit ep as time support:\n        &gt;&gt;&gt; bintsd.time_support\n        &gt;&gt;&gt;    start    end\n        &gt;&gt;&gt; 0  10.0     80.0\n        \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nt, d = jitbin(time_array, data_array, starts, ends, bin_size)\ntime_support = IntervalSet(start=starts, end=ends)\nreturn Tsd(t=t, d=d, time_support=time_support)\ndef threshold(self, thr, method=\"above\"):\n\"\"\"\n        Apply a threshold function to the tsd to return a new tsd\n        with the time support being the epochs above/below/&gt;=/&lt;= the threshold\n        Parameters\n        ----------\n        thr : float\n            The threshold value\n        method : str, optional\n            The threshold method (above/below/aboveequal/belowequal)\n        Returns\n        -------\n        out: Tsd\n            All the time points below/ above/greater than equal to/less than equal to the threshold\n        Raises\n        ------\n        ValueError\n            Raise an error if method is not 'below' or 'above'\n        RuntimeError\n            Raise an error if thr is too high/low and no epochs is found.\n        Examples\n        --------\n        This example finds all epoch above 0.5 within the tsd object.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n        &gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n        The epochs with the times above/below the threshold can be accessed through the time support:\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n        &gt;&gt;&gt; tsd.threshold(50).time_support\n        &gt;&gt;&gt;    start   end\n        &gt;&gt;&gt; 0   50.5  99.0\n        \"\"\"\ntime_array = self.index.values\ndata_array = self.values\nstarts = self.time_support.start.values\nends = self.time_support.end.values\nif method not in [\"above\", \"below\", \"aboveequal\", \"belowequal\"]:\nraise ValueError(\n\"Method {} for thresholding is not accepted.\".format(method)\n)\nt, d, ns, ne = jitthreshold(time_array, data_array, starts, ends, thr, method)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\ndef to_tsgroup(self):\n\"\"\"\n        Convert Tsd to a TsGroup by grouping timestamps with the same values.\n        By default, the values are converted to integers.\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\n        Time (s)\n        0.0    0\n        1.0    2\n        2.0    0\n        3.0    1\n        dtype: int64\n        &gt;&gt;&gt; tsd.to_tsgroup()\n        Index    rate\n        -------  ------\n            0    0.67\n            1    0.33\n            2    0.33\n        The reverse operation can be done with the TsGroup.to_tsd function :\n        &gt;&gt;&gt; tsgroup.to_tsd()\n        Time (s)\n        0.0    0.0\n        1.0    2.0\n        2.0    0.0\n        3.0    1.0\n        dtype: float64\n        Returns\n        -------\n        TsGroup\n            Grouped timestamps\n        \"\"\"\nts_group = importlib.import_module(\".ts_group\", \"pynapple.core\")\nt = self.index.values\nd = self.values.astype(\"int\")\nidx = np.unique(d)\ngroup = {}\nfor k in idx:\ngroup[k] = Ts(t=t[d == k], time_support=self.time_support)\nreturn ts_group.TsGroup(group, time_support=self.time_support)\ndef save(self, filename):\n\"\"\"\n        Save Tsd object in npz format. The file will contain the timestamps, the\n        data and the time support.\n        The main purpose of this function is to save small/medium sized time series\n        objects. For example, you extracted one channel from your recording and\n        filtered it. You can save the filtered channel as a npz to avoid\n        reprocessing it.\n        You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n        &gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'd', 'start', 'end', 'type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1.]\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\n        Time (s)\n        0.0    2\n        1.0    3\n        dtype: int64\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([\"Tsd\"], dtype=np.str_),\n)\nreturn\n# def find_gaps(self, min_gap, method=\"absolute\"):\n#     \"\"\"\n#     finds gaps in a tsd larger than min_gap\n#     Parameters\n#     ----------\n#     min_gap : TYPE\n#         Description\n#     method : str, optional\n#         Description\n#     \"\"\"\n#     print(\"TODO\")\n#     return\n# def find_support(self, min_gap, method=\"absolute\"):\n#     \"\"\"\n#     find the smallest (to a min_gap resolution) IntervalSet containing all the times in the Tsd\n#     Parameters\n#     ----------\n#     min_gap : TYPE\n#         Description\n#     method : str, optional\n#         Description\n#     Returns\n#     -------\n#     TYPE\n#         Description\n#     \"\"\"\n#     print(\"TODO\")\n#     return\ndef start_time(self, units=\"s\"):\n\"\"\"\n        The first time index in the Ts/Tsd object\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: numpy.float64\n            _\n        \"\"\"\nreturn self.times(units=units)[0]\ndef end_time(self, units=\"s\"):\n\"\"\"\n        The last time index in the Ts/Tsd object\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: numpy.float64\n            _\n        \"\"\"\nreturn self.times(units=units)[-1]\n@property\ndef _constructor(self):\nreturn Tsd\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.__init__","title":"<code>__init__(t, d=None, time_units='s', time_support=None, **kwargs)</code>","text":"<p>Tsd Initializer.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>numpy.ndarray or pandas.Series</code> <p>An object transformable in a time series, or a pandas.Series equivalent (if d is None)</p> required <code>d</code> <code>numpy.ndarray, optional</code> <p>The data of the time series</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>The time units in which times are specified ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet, optional</code> <p>The time support of the tsd object</p> <code>None</code> <code>**kwargs</code> <p>Arguments that will be passed to the pandas.Series initializer.</p> <code>{}</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, d=None, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n    Tsd Initializer.\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.Series\n        An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n    d : numpy.ndarray, optional\n        The data of the time series\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default])\n    time_support : IntervalSet, optional\n        The time support of the tsd object\n    **kwargs\n        Arguments that will be passed to the pandas.Series initializer.\n    \"\"\"\nif isinstance(t, SingleBlockManager):\nd = t.array\nt = t.index.values\nif \"index\" in kwargs:\nkwargs.pop(\"index\")\nelif isinstance(t, pd.Series):\nd = t.values\nt = t.index.values\nt = t.astype(np.float64).flatten()\nt = format_timestamps(t, time_units)\nt = sort_timestamps(t)\nif len(t):\nif time_support is not None:\nstarts = time_support.start.values\nends = time_support.end.values\nif d is not None:\nt, d = jitrestrict(t, d, starts, ends)\nsuper().__init__(index=t, data=d)\nelse:\nt = jittsrestrict(t, starts, ends)\nsuper().__init__(index=t, data=None, dtype=np.int8)\nelse:\ntime_support = IntervalSet(start=t[0], end=t[-1])\nif d is not None:\nsuper().__init__(index=t, data=d)\nelse:\nsuper().__init__(index=t, data=d, dtype=np.float64)\nself.time_support = time_support\nself.rate = t.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\ntime_support = IntervalSet(pd.DataFrame(columns=[\"start\", \"end\"]))\nsuper().__init__(index=t, data=d, dtype=np.float64)\nself.time_support = time_support\nself.rate = 0.0\nself.index.name = \"Time (s)\"\n# self._metadata.append(\"nap_class\")\nself.nap_class = self.__class__.__name__\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.times","title":"<code>times(units='s')</code>","text":"<p>The time index of the Tsd, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>the time indexes</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the Tsd, returned as np.double in the desired time units.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.ndarray\n        the time indexes\n    \"\"\"\nreturn return_timestamps(self.index.values, units)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.as_series","title":"<code>as_series()</code>","text":"<p>Convert the Ts/Tsd object to a pandas.Series object.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>pandas.Series</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_series(self):\n\"\"\"\n    Convert the Ts/Tsd object to a pandas.Series object.\n    Returns\n    -------\n    out: pandas.Series\n        _\n    \"\"\"\nreturn pd.Series(self, copy=True)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.as_units","title":"<code>as_units(units='s')</code>","text":"<p>Returns a Series with time expressed in the desired unit.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Type Description <code>pandas.Series</code> <p>the series object with adjusted times</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    Returns a Series with time expressed in the desired unit.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    pandas.Series\n        the series object with adjusted times\n    \"\"\"\nss = self.as_series()\nt = self.index.values\nt = return_timestamps(t, units)\nif units == \"us\":\nt = t.astype(np.int64)\nss.index = t\nss.index.name = \"Time (\" + str(units) + \")\"\nreturn ss\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.data","title":"<code>data()</code>","text":"<p>The data in the Tsd object</p> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    The data in the Tsd object\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.value_from","title":"<code>value_from(data, ep=None)</code>","text":"<p>Replace the value with the closest value from Tsd/TsdFrame argument If data is TsdFrame, the output is also TsdFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tsd</code> <p>The Tsd/TsdFrame object holding the values to replace.</p> required <code>ep</code> <code>IntervalSet(optional)</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>Object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, data, ep=None):\n\"\"\"\n    Replace the value with the closest value from Tsd/TsdFrame argument\n    If data is TsdFrame, the output is also TsdFrame.\n    Parameters\n    ----------\n    data : Tsd/TsdFrame\n        The Tsd/TsdFrame object holding the values to replace.\n    ep : IntervalSet (optional)\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    out : Tsd/TsdFrame\n        Object with the new values\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n    newts is the same size as ts restrict to ep.\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\nif isinstance(data, Tsd):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\nelif isinstance(data, TsdFrame):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefromtsdframe(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nraise RuntimeError(\"The time series to align to should be Tsd/TsdFrame.\")\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.restrict","title":"<code>restrict(ep)</code>","text":"<p>Restricts a Tsd object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>ep</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>Tsd object restricted to ep</p> <p>Examples:</p> <p>The Ts object is restrict to the intervals defined by ep.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n&gt;&gt;&gt; newts = ts.restrict(ep)\n</code></pre> <p>The time support of newts automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newts.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0    0.0  500.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, ep):\n\"\"\"\n    Restricts a Tsd object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    ep : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    out: Tsd\n        Tsd object restricted to ep\n    Examples\n    --------\n    The Ts object is restrict to the intervals defined by ep.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n    &gt;&gt;&gt; newts = ts.restrict(ep)\n    The time support of newts automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newts.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0    0.0  500.0\n    \"\"\"\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nreturn Tsd(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.count","title":"<code>count(*args, **kwargs)</code>","text":"<p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsd.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsd.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsd.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsd.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float, optional</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet, optional</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str, optional</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; bincount = ts.count(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n&gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n</code></pre> <p>And bincount automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bincount.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  100.0  800.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsd.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsd.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsd.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsd.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; bincount = ts.count(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n    &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n    And bincount automatically inherit ep as time support:\n    &gt;&gt;&gt; bincount.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  100.0  800.0\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\ntime_array = self.index.values\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = float(bin_size)\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\nt, d = jitcount(time_array, starts, ends, bin_size)\ntime_support = IntervalSet(start=starts, end=ends)\nreturn Tsd(t=t, d=d, time_support=time_support)\nelse:\n_, countin = jittsrestrict_with_count(time_array, starts, ends)\nt = starts + (ends - starts) / 2\nreturn Tsd(t=t, d=countin, time_support=ep)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.bin_average","title":"<code>bin_average(bin_size, ep=None, time_units='s')</code>","text":"<p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet, optional</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsd automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsd.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins and holding the averaged data points.\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n    And bintsd automatically inherit ep as time support:\n    &gt;&gt;&gt; bintsd.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nt, d = jitbin(time_array, data_array, starts, ends, bin_size)\ntime_support = IntervalSet(start=starts, end=ends)\nreturn Tsd(t=t, d=d, time_support=time_support)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.threshold","title":"<code>threshold(thr, method='above')</code>","text":"<p>Apply a threshold function to the tsd to return a new tsd with the time support being the epochs above/below/&gt;=/&lt;= the threshold</p> <p>Parameters:</p> Name Type Description Default <code>thr</code> <code>float</code> <p>The threshold value</p> required <code>method</code> <code>str, optional</code> <p>The threshold method (above/below/aboveequal/belowequal)</p> <code>'above'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>All the time points below/ above/greater than equal to/less than equal to the threshold</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raise an error if method is not 'below' or 'above'</p> <code>RuntimeError</code> <p>Raise an error if thr is too high/low and no epochs is found.</p> <p>Examples:</p> <p>This example finds all epoch above 0.5 within the tsd object.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n</code></pre> <p>The epochs with the times above/below the threshold can be accessed through the time support:</p> <pre><code>&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n&gt;&gt;&gt; tsd.threshold(50).time_support\n&gt;&gt;&gt;    start   end\n&gt;&gt;&gt; 0   50.5  99.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def threshold(self, thr, method=\"above\"):\n\"\"\"\n    Apply a threshold function to the tsd to return a new tsd\n    with the time support being the epochs above/below/&gt;=/&lt;= the threshold\n    Parameters\n    ----------\n    thr : float\n        The threshold value\n    method : str, optional\n        The threshold method (above/below/aboveequal/belowequal)\n    Returns\n    -------\n    out: Tsd\n        All the time points below/ above/greater than equal to/less than equal to the threshold\n    Raises\n    ------\n    ValueError\n        Raise an error if method is not 'below' or 'above'\n    RuntimeError\n        Raise an error if thr is too high/low and no epochs is found.\n    Examples\n    --------\n    This example finds all epoch above 0.5 within the tsd object.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n    The epochs with the times above/below the threshold can be accessed through the time support:\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n    &gt;&gt;&gt; tsd.threshold(50).time_support\n    &gt;&gt;&gt;    start   end\n    &gt;&gt;&gt; 0   50.5  99.0\n    \"\"\"\ntime_array = self.index.values\ndata_array = self.values\nstarts = self.time_support.start.values\nends = self.time_support.end.values\nif method not in [\"above\", \"below\", \"aboveequal\", \"belowequal\"]:\nraise ValueError(\n\"Method {} for thresholding is not accepted.\".format(method)\n)\nt, d, ns, ne = jitthreshold(time_array, data_array, starts, ends, thr, method)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.to_tsgroup","title":"<code>to_tsgroup()</code>","text":"<p>Convert Tsd to a TsGroup by grouping timestamps with the same values. By default, the values are converted to integers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\nTime (s)\n0.0    0\n1.0    2\n2.0    0\n3.0    1\ndtype: int64\n</code></pre> <pre><code>&gt;&gt;&gt; tsd.to_tsgroup()\nIndex    rate\n-------  ------\n    0    0.67\n    1    0.33\n    2    0.33\n</code></pre> <p>The reverse operation can be done with the TsGroup.to_tsd function :</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd()\nTime (s)\n0.0    0.0\n1.0    2.0\n2.0    0.0\n3.0    1.0\ndtype: float64\n</code></pre> <p>Returns:</p> Type Description <code>TsGroup</code> <p>Grouped timestamps</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def to_tsgroup(self):\n\"\"\"\n    Convert Tsd to a TsGroup by grouping timestamps with the same values.\n    By default, the values are converted to integers.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\n    Time (s)\n    0.0    0\n    1.0    2\n    2.0    0\n    3.0    1\n    dtype: int64\n    &gt;&gt;&gt; tsd.to_tsgroup()\n    Index    rate\n    -------  ------\n        0    0.67\n        1    0.33\n        2    0.33\n    The reverse operation can be done with the TsGroup.to_tsd function :\n    &gt;&gt;&gt; tsgroup.to_tsd()\n    Time (s)\n    0.0    0.0\n    1.0    2.0\n    2.0    0.0\n    3.0    1.0\n    dtype: float64\n    Returns\n    -------\n    TsGroup\n        Grouped timestamps\n    \"\"\"\nts_group = importlib.import_module(\".ts_group\", \"pynapple.core\")\nt = self.index.values\nd = self.values.astype(\"int\")\nidx = np.unique(d)\ngroup = {}\nfor k in idx:\ngroup[k] = Ts(t=t[d == k], time_support=self.time_support)\nreturn ts_group.TsGroup(group, time_support=self.time_support)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.save","title":"<code>save(filename)</code>","text":"<p>Save Tsd object in npz format. The file will contain the timestamps, the data and the time support.</p> <p>The main purpose of this function is to save small/medium sized time series objects. For example, you extracted one channel from your recording and filtered it. You can save the filtered channel as a npz to avoid reprocessing it.</p> <p>You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n&gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'd', 'start', 'end', 'type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1.]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\nTime (s)\n0.0    2\n1.0    3\ndtype: int64\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save Tsd object in npz format. The file will contain the timestamps, the\n    data and the time support.\n    The main purpose of this function is to save small/medium sized time series\n    objects. For example, you extracted one channel from your recording and\n    filtered it. You can save the filtered channel as a npz to avoid\n    reprocessing it.\n    You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n    &gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'd', 'start', 'end', 'type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1.]\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\n    Time (s)\n    0.0    2\n    1.0    3\n    dtype: int64\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([\"Tsd\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.start_time","title":"<code>start_time(units='s')</code>","text":"<p>The first time index in the Ts/Tsd object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def start_time(self, units=\"s\"):\n\"\"\"\n    The first time index in the Ts/Tsd object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nreturn self.times(units=units)[0]\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Tsd.end_time","title":"<code>end_time(units='s')</code>","text":"<p>The last time index in the Ts/Tsd object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def end_time(self, units=\"s\"):\n\"\"\"\n    The last time index in the Ts/Tsd object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nreturn self.times(units=units)[-1]\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts","title":"<code>Ts</code>","text":"<p>         Bases: <code>Tsd</code></p> <p>A subclass of the Tsd object for a time series with only time index, By default, the values are set to nan. All the functions of a Tsd object are available in a Ts object.</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class Ts(Tsd):\n\"\"\"\n    A subclass of the Tsd object for a time series with only time index,\n    By default, the values are set to nan.\n    All the functions of a Tsd object are available in a Ts object.\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n        Ts Initializer\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.Series\n            An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default])\n        time_support : IntervalSet, optional\n            The time support of the Ts object\n        **kwargs\n            Arguments that will be passed to the pandas.Series initializer.\n        \"\"\"\nsuper().__init__(\nt,\nNone,\ntime_units=time_units,\ntime_support=time_support,\ndtype=np.float64,\n**kwargs,\n)\nself.nts_class = self.__class__.__name__\ndef __repr__(self):\nreturn self.as_series().fillna(\"\").__repr__()\ndef __str__(self):\nreturn self.__repr__()\ndef save(self, filename):\n\"\"\"\n        Save Ts object in npz format. The file will contain the timestamps and\n        the time support.\n        The main purpose of this function is to save small/medium sized timestamps\n        object.\n        You can load the object with numpy.load. Keys are 't', 'start' and 'end' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n        &gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'start', 'end', 'type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1. 1.5]\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\n        Time (s)\n        0.0\n        1.0\n        1.5\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([\"Ts\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.__init__","title":"<code>__init__(t, time_units='s', time_support=None, **kwargs)</code>","text":"<p>Ts Initializer</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>numpy.ndarray or pandas.Series</code> <p>An object transformable in a time series, or a pandas.Series equivalent (if d is None)</p> required <code>time_units</code> <code>str, optional</code> <p>The time units in which times are specified ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet, optional</code> <p>The time support of the Ts object</p> <code>None</code> <code>**kwargs</code> <p>Arguments that will be passed to the pandas.Series initializer.</p> <code>{}</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n    Ts Initializer\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.Series\n        An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default])\n    time_support : IntervalSet, optional\n        The time support of the Ts object\n    **kwargs\n        Arguments that will be passed to the pandas.Series initializer.\n    \"\"\"\nsuper().__init__(\nt,\nNone,\ntime_units=time_units,\ntime_support=time_support,\ndtype=np.float64,\n**kwargs,\n)\nself.nts_class = self.__class__.__name__\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.Ts.save","title":"<code>save(filename)</code>","text":"<p>Save Ts object in npz format. The file will contain the timestamps and the time support.</p> <p>The main purpose of this function is to save small/medium sized timestamps object.</p> <p>You can load the object with numpy.load. Keys are 't', 'start' and 'end' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n&gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'start', 'end', 'type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1. 1.5]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\nTime (s)\n0.0\n1.0\n1.5\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save Ts object in npz format. The file will contain the timestamps and\n    the time support.\n    The main purpose of this function is to save small/medium sized timestamps\n    object.\n    You can load the object with numpy.load. Keys are 't', 'start' and 'end' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n    &gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'start', 'end', 'type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1. 1.5]\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\n    Time (s)\n    0.0\n    1.0\n    1.5\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([\"Ts\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame","title":"<code>TsdFrame</code>","text":"<p>         Bases: <code>pd.DataFrame</code></p> <p>A subclass of pandas.DataFrame specialized for neurophysiological time series.</p> <p>TsdFrame provides standardized time representation, plus various functions for manipulating times series with identical sampling frequency.</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class TsdFrame(pd.DataFrame):\n# class TsdFrame():\n\"\"\"\n    A subclass of pandas.DataFrame specialized for neurophysiological time series.\n    TsdFrame provides standardized time representation, plus various functions for manipulating times series with identical sampling frequency.\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, d=None, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n        TsdFrame initializer\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.DataFrame\n            the time index t,  or a pandas.DataFrame (if d is None)\n        d : numpy.ndarray\n            The data\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default]).\n        time_support : IntervalSet, optional\n            The time support of the TsdFrame object\n        **kwargs\n            Arguments that will be passed to the pandas.DataFrame initializer.\n        \"\"\"\nif isinstance(t, BlockManager):\nd = t.as_array()\nc = t.axes[0].values\nt = t.axes[1].values\nelif isinstance(t, pd.DataFrame):\nd = t.values\nc = t.columns.values\nt = t.index.values\nelse:\nif \"columns\" in kwargs:\nc = kwargs[\"columns\"]\nelse:\nif isinstance(d, np.ndarray):\nif len(d.shape) == 2:\nc = np.arange(d.shape[1])\nelif len(d.shape) == 1:\nc = np.zeros(1)\nelse:\nc = np.array([])\nelse:\nc = None\nt = t.astype(np.float64).flatten()\nt = format_timestamps(t, time_units)\nt = sort_timestamps(t)\nif len(t):\nif time_support is not None:\nstarts = time_support.start.values\nends = time_support.end.values\nif d is not None:\nt, d = jitrestrict(t, d, starts, ends)\nsuper().__init__(index=t, data=d, columns=c)\nelse:\nt = jittsrestrict(t, starts, ends)\nsuper().__init__(index=t, data=None, columns=c)\nelse:\ntime_support = IntervalSet(start=t[0], end=t[-1])\nsuper().__init__(index=t, data=d, columns=c)\nself.rate = t.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\ntime_support = IntervalSet(pd.DataFrame(columns=[\"start\", \"end\"]))\nsuper().__init__(index=np.array([]), dtype=np.float64)\nself.rate = 0.0\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nself.time_support = time_support\nself.index.name = \"Time (s)\"\n# self._metadata.append(\"nap_class\")\nself.nap_class = self.__class__.__name__\ndef __repr__(self):\nreturn self.as_units(\"s\").__repr__()\ndef __str__(self):\nreturn self.__repr__()\ndef __getitem__(self, key):\nresult = super().__getitem__(key)\ntime_support = self.time_support\nif isinstance(result, pd.Series):\nreturn Tsd(result, time_support=time_support)\nelif isinstance(result, pd.DataFrame):\nreturn TsdFrame(result, time_support=time_support)\ndef __add__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__add__(value), time_support=ts)\ndef __sub__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__sub__(value), time_support=ts)\ndef __truediv__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__truediv__(value), time_support=ts)\ndef __floordiv__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__floordiv__(value), time_support=ts)\ndef __mul__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__mul__(value), time_support=ts)\ndef __mod__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__mod__(value), time_support=ts)\ndef __pow__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__pow__(value), time_support=ts)\ndef __lt__(self, value):\nreturn self.as_dataframe().__lt__(value)\ndef __gt__(self, value):\nreturn self.as_dataframe().__gt__(value)\ndef __le__(self, value):\nreturn self.as_dataframe().__le__(value)\ndef __ge__(self, value):\nreturn self.as_dataframe().__ge__(value)\ndef __ne__(self, value):\nreturn self.as_dataframe().__ne__(value)\ndef __eq__(self, value):\nreturn self.as_dataframe().__eq__(value)\n@property\ndef _constructor(self):\nreturn TsdFrame\ndef times(self, units=\"s\"):\n\"\"\"\n        The time index of the TsdFrame, returned as np.double in the desired time units.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: numpy.ndarray\n            _\n        \"\"\"\nreturn return_timestamps(self.index.values, units)\ndef as_dataframe(self, copy=True):\n\"\"\"\n        Convert the TsdFrame object to a pandas.DataFrame object.\n        Returns\n        -------\n        out: pandas.DataFrame\n            _\n        \"\"\"\nreturn pd.DataFrame(self, copy=copy)\ndef as_units(self, units=\"s\"):\n\"\"\"\n        Returns a DataFrame with time expressed in the desired unit.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        pandas.DataFrame\n            the series object with adjusted times\n        \"\"\"\nt = self.index.values.copy()\nt = return_timestamps(t, units)\nif units == \"us\":\nt = t.astype(np.int64)\ndf = pd.DataFrame(index=t, data=self.values)\ndf.index.name = \"Time (\" + str(units) + \")\"\ndf.columns = self.columns.copy()\nreturn df\ndef data(self):\n\"\"\"\n        The data in the TsdFrame object\n        Returns\n        -------\n        out: numpy.ndarray\n            _\n        \"\"\"\nreturn self.values\ndef value_from(self, data, ep=None):\n\"\"\"\n        Replace the value with the closest value from Tsd/TsdFrame argument\n        If data is TsdFrame, the output is also TsdFrame.\n        Parameters\n        ----------\n        data : Tsd/TsdFrame\n            The Tsd/TsdFrame object holding the values to replace.\n        ep : IntervalSet (optional)\n            The IntervalSet object to restrict the operation.\n            If None, the time support of the tsd input object is used.\n        Returns\n        -------\n        out : Tsd/TsdFrame\n            Object with the new values\n        Examples\n        --------\n        In this example, the ts object will receive the closest values in time from tsd.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n        &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n        The variable ts is a time series object containing only nan.\n        The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n        &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n        newts is the same size as ts restrict to ep.\n        &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n            52 52\n        \"\"\"\nif isinstance(data, Tsd):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\nelif isinstance(data, TsdFrame):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefromtsdframe(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nraise RuntimeError(\"The time series to align to should be Tsd/TsdFrame.\")\ndef restrict(self, iset):\n\"\"\"\n        Restricts a TsdFrame object to a set of time intervals delimited by an IntervalSet object`\n        Parameters\n        ----------\n        iset : IntervalSet\n            the IntervalSet object\n        Returns\n        -------\n        TsdFrame\n            TsdFrame object restricted to ep\n        \"\"\"\nc = self.columns.values\ntime_array = self.index.values\ndata_array = self.values\nstarts = iset.start.values\nends = iset.end.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nreturn TsdFrame(t=t, d=d, columns=c, time_support=iset)\ndef bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n        Bin the data by averaging points within bin_size\n        bin_size should be seconds unless specified.\n        If no epochs is passed, the data will be binned based on the time support.\n        Parameters\n        ----------\n        bin_size : float\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: TsdFrame\n            A TsdFrame object indexed by the center of the bins and holding the averaged data points.\n        Examples\n        --------\n        This example shows how to bin data within bins of 0.1 second.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.arange(100), d=np.random.rand(100, 3))\n        &gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1)\n        An epoch can be specified:\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n        &gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1, ep=ep)\n        And bintsdframe automatically inherit ep as time support:\n        &gt;&gt;&gt; bintsdframe.time_support\n        &gt;&gt;&gt;    start    end\n        &gt;&gt;&gt; 0  10.0     80.0\n        \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nt, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\ntime_support = IntervalSet(start=starts, end=ends)\nreturn TsdFrame(t=t, d=d, time_support=time_support)\ndef save(self, filename):\n\"\"\"\n        Save TsdFrame object in npz format. The file will contain the timestamps, the\n        data and the time support.\n        The main purpose of this function is to save small/medium sized time series\n        objects. For example, you extracted several channels from your recording and\n        filtered them. You can save the filtered channels as a npz to avoid\n        reprocessing it.\n        You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type'\n        and 'columns' for columns names.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n        &gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'd', 'start', 'end', 'columns', 'type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1.]\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n                  a  b\n        Time (s)\n        0.0       2  3\n        1.0       4  5\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ncols_name = self.columns.values\nif cols_name.dtype == np.dtype(\"O\"):\ncols_name = cols_name.astype(str)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ncolumns=cols_name,\ntype=np.array([\"TsdFrame\"], dtype=np.str_),\n)\nreturn\n# def find_gaps(self, min_gap, time_units='s'):\n#     \"\"\"\n#     finds gaps in a tsd larger than min_gap. Return an IntervalSet.\n#     Epochs are defined by adding and removing 1 microsecond to the time index.\n#     Parameters\n#     ----------\n#     min_gap : float\n#         The minimum interval size considered to be a gap (default is second).\n#     time_units : str, optional\n#         Time units of min_gap ('us', 'ms', 's' [default])\n#     \"\"\"\n#     min_gap = format_timestamps(np.array([min_gap]), time_units)[0]\n#     time_array = self.index.values\n#     starts = self.time_support.start.values\n#     ends = self.time_support.end.values\n#     s, e = jitfind_gaps(time_array, starts, ends, min_gap)\n#     return nap.IntervalSet(s, e)\n# def find_support(self, min_gap, method=\"absolute\"):\n#     \"\"\"\n#     find the smallest (to a min_gap resolution) IntervalSet containing all the times in the Tsd\n#     Parameters\n#     ----------\n#     min_gap : float\n#         Description\n#     method : str, optional\n#         Description\n#     Returns\n#     -------\n#     TYPE\n#         Description\n#     \"\"\"\n#     print(\"TODO\")\n#     return\ndef start_time(self, units=\"s\"):\nreturn self.times(units=units)[0]\ndef end_time(self, units=\"s\"):\nreturn self.times(units=units)[-1]\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.__init__","title":"<code>__init__(t, d=None, time_units='s', time_support=None, **kwargs)</code>","text":"<p>TsdFrame initializer</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>numpy.ndarray or pandas.DataFrame</code> <p>the time index t,  or a pandas.DataFrame (if d is None)</p> required <code>d</code> <code>numpy.ndarray</code> <p>The data</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>The time units in which times are specified ('us', 'ms', 's' [default]).</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet, optional</code> <p>The time support of the TsdFrame object</p> <code>None</code> <code>**kwargs</code> <p>Arguments that will be passed to the pandas.DataFrame initializer.</p> <code>{}</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, d=None, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n    TsdFrame initializer\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.DataFrame\n        the time index t,  or a pandas.DataFrame (if d is None)\n    d : numpy.ndarray\n        The data\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default]).\n    time_support : IntervalSet, optional\n        The time support of the TsdFrame object\n    **kwargs\n        Arguments that will be passed to the pandas.DataFrame initializer.\n    \"\"\"\nif isinstance(t, BlockManager):\nd = t.as_array()\nc = t.axes[0].values\nt = t.axes[1].values\nelif isinstance(t, pd.DataFrame):\nd = t.values\nc = t.columns.values\nt = t.index.values\nelse:\nif \"columns\" in kwargs:\nc = kwargs[\"columns\"]\nelse:\nif isinstance(d, np.ndarray):\nif len(d.shape) == 2:\nc = np.arange(d.shape[1])\nelif len(d.shape) == 1:\nc = np.zeros(1)\nelse:\nc = np.array([])\nelse:\nc = None\nt = t.astype(np.float64).flatten()\nt = format_timestamps(t, time_units)\nt = sort_timestamps(t)\nif len(t):\nif time_support is not None:\nstarts = time_support.start.values\nends = time_support.end.values\nif d is not None:\nt, d = jitrestrict(t, d, starts, ends)\nsuper().__init__(index=t, data=d, columns=c)\nelse:\nt = jittsrestrict(t, starts, ends)\nsuper().__init__(index=t, data=None, columns=c)\nelse:\ntime_support = IntervalSet(start=t[0], end=t[-1])\nsuper().__init__(index=t, data=d, columns=c)\nself.rate = t.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\ntime_support = IntervalSet(pd.DataFrame(columns=[\"start\", \"end\"]))\nsuper().__init__(index=np.array([]), dtype=np.float64)\nself.rate = 0.0\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nself.time_support = time_support\nself.index.name = \"Time (s)\"\n# self._metadata.append(\"nap_class\")\nself.nap_class = self.__class__.__name__\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.times","title":"<code>times(units='s')</code>","text":"<p>The time index of the TsdFrame, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the TsdFrame, returned as np.double in the desired time units.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn return_timestamps(self.index.values, units)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.as_dataframe","title":"<code>as_dataframe(copy=True)</code>","text":"<p>Convert the TsdFrame object to a pandas.DataFrame object.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>pandas.DataFrame</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_dataframe(self, copy=True):\n\"\"\"\n    Convert the TsdFrame object to a pandas.DataFrame object.\n    Returns\n    -------\n    out: pandas.DataFrame\n        _\n    \"\"\"\nreturn pd.DataFrame(self, copy=copy)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.as_units","title":"<code>as_units(units='s')</code>","text":"<p>Returns a DataFrame with time expressed in the desired unit.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>the series object with adjusted times</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    Returns a DataFrame with time expressed in the desired unit.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    pandas.DataFrame\n        the series object with adjusted times\n    \"\"\"\nt = self.index.values.copy()\nt = return_timestamps(t, units)\nif units == \"us\":\nt = t.astype(np.int64)\ndf = pd.DataFrame(index=t, data=self.values)\ndf.index.name = \"Time (\" + str(units) + \")\"\ndf.columns = self.columns.copy()\nreturn df\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.data","title":"<code>data()</code>","text":"<p>The data in the TsdFrame object</p> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    The data in the TsdFrame object\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.value_from","title":"<code>value_from(data, ep=None)</code>","text":"<p>Replace the value with the closest value from Tsd/TsdFrame argument If data is TsdFrame, the output is also TsdFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tsd</code> <p>The Tsd/TsdFrame object holding the values to replace.</p> required <code>ep</code> <code>IntervalSet(optional)</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>Object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, data, ep=None):\n\"\"\"\n    Replace the value with the closest value from Tsd/TsdFrame argument\n    If data is TsdFrame, the output is also TsdFrame.\n    Parameters\n    ----------\n    data : Tsd/TsdFrame\n        The Tsd/TsdFrame object holding the values to replace.\n    ep : IntervalSet (optional)\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    out : Tsd/TsdFrame\n        Object with the new values\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n    newts is the same size as ts restrict to ep.\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\nif isinstance(data, Tsd):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\nelif isinstance(data, TsdFrame):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefromtsdframe(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nraise RuntimeError(\"The time series to align to should be Tsd/TsdFrame.\")\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.restrict","title":"<code>restrict(iset)</code>","text":"<p>Restricts a TsdFrame object to a set of time intervals delimited by an IntervalSet object`</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Type Description <code>TsdFrame</code> <p>TsdFrame object restricted to ep</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, iset):\n\"\"\"\n    Restricts a TsdFrame object to a set of time intervals delimited by an IntervalSet object`\n    Parameters\n    ----------\n    iset : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    TsdFrame\n        TsdFrame object restricted to ep\n    \"\"\"\nc = self.columns.values\ntime_array = self.index.values\ndata_array = self.values\nstarts = iset.start.values\nends = iset.end.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nreturn TsdFrame(t=t, d=d, columns=c, time_support=iset)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.bin_average","title":"<code>bin_average(bin_size, ep=None, time_units='s')</code>","text":"<p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet, optional</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>TsdFrame</code> <p>A TsdFrame object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.arange(100), d=np.random.rand(100, 3))\n&gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsdframe automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsdframe.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: TsdFrame\n        A TsdFrame object indexed by the center of the bins and holding the averaged data points.\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.arange(100), d=np.random.rand(100, 3))\n    &gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1, ep=ep)\n    And bintsdframe automatically inherit ep as time support:\n    &gt;&gt;&gt; bintsdframe.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nt, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\ntime_support = IntervalSet(start=starts, end=ends)\nreturn TsdFrame(t=t, d=d, time_support=time_support)\n</code></pre>"},{"location":"old_pages/core.time_series/#pynapple.core.time_series.TsdFrame.save","title":"<code>save(filename)</code>","text":"<p>Save TsdFrame object in npz format. The file will contain the timestamps, the data and the time support.</p> <p>The main purpose of this function is to save small/medium sized time series objects. For example, you extracted several channels from your recording and filtered them. You can save the filtered channels as a npz to avoid reprocessing it.</p> <p>You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type' and 'columns' for columns names.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n&gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'd', 'start', 'end', 'columns', 'type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1.]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n          a  b\nTime (s)\n0.0       2  3\n1.0       4  5\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save TsdFrame object in npz format. The file will contain the timestamps, the\n    data and the time support.\n    The main purpose of this function is to save small/medium sized time series\n    objects. For example, you extracted several channels from your recording and\n    filtered them. You can save the filtered channels as a npz to avoid\n    reprocessing it.\n    You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type'\n    and 'columns' for columns names.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n    &gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'd', 'start', 'end', 'columns', 'type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1.]\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n              a  b\n    Time (s)\n    0.0       2  3\n    1.0       4  5\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ncols_name = self.columns.values\nif cols_name.dtype == np.dtype(\"O\"):\ncols_name = cols_name.astype(str)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ncolumns=cols_name,\ntype=np.array([\"TsdFrame\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"old_pages/core.ts_group/","title":"Core.ts group","text":""},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup","title":"<code>TsGroup</code>","text":"<p>         Bases: <code>UserDict</code></p> <p>The TsGroup is a dictionnary-like object to hold multiple <code>Ts</code> or <code>Tsd</code> objects with different time index.</p> <p>Attributes:</p> Name Type Description <code>time_support</code> <code>IntervalSet</code> <p>The time support of the TsGroup</p> <code>rates</code> <code>pandas.Series</code> <p>The rate of each element of the TsGroup</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>class TsGroup(UserDict):\n\"\"\"\n    The TsGroup is a dictionnary-like object to hold multiple [`Ts`][pynapple.core.time_series.Ts] or [`Tsd`][pynapple.core.time_series.Tsd] objects with different time index.\n    Attributes\n    ----------\n    time_support: IntervalSet\n        The time support of the TsGroup\n    rates : pandas.Series\n        The rate of each element of the TsGroup\n    \"\"\"\ndef __init__(\nself, data, time_support=None, time_units=\"s\", bypass_check=False, **kwargs\n):\n\"\"\"\n        TsGroup Initializer\n        Parameters\n        ----------\n        data : dict\n            Dictionnary containing Ts/Tsd objects\n        time_support : IntervalSet, optional\n            The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed.\n            If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.\n        time_units : str, optional\n            Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).\n        bypass_check: bool, optional\n            To avoid checking that each element is within time_support.\n            Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand\n        **kwargs\n            Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray.\n            Note that the index should match the index of the input dictionnary.\n        Raises\n        ------\n        RuntimeError\n            Raise error if the union of time support of Ts/Tsd object is empty.\n        \"\"\"\nself._initialized = False\nself.index = np.sort(list(data.keys()))\nself._metadata = pd.DataFrame(index=self.index, columns=[\"rate\"], dtype=\"float\")\n# Transform elements to Ts/Tsd objects\nfor k in self.index:\nif isinstance(data[k], (np.ndarray, list)):\nwarnings.warn(\n\"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\nstacklevel=2,\n)\ndata[k] = Ts(\nt=data[k], time_support=time_support, time_units=time_units\n)\n# If time_support is passed, all elements of data are restricted prior to init\nif isinstance(time_support, IntervalSet):\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nelse:\n# Otherwise do the union of all time supports\ntime_support = union_intervals([data[k].time_support for k in self.index])\nif len(time_support) == 0:\nraise RuntimeError(\n\"Union of time supports is empty. Consider passing a time support as argument.\"\n)\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nUserDict.__init__(self, data)\n# Making the TsGroup non mutable\nself._initialized = True\n# Trying to add argument as metainfo\nself.set_info(**kwargs)\n\"\"\"\n    Base functions\n    \"\"\"\ndef __setitem__(self, key, value):\nif self._initialized:\nraise RuntimeError(\"TsGroup object is not mutable.\")\nself._metadata.loc[int(key), \"rate\"] = float(value.rate)\nsuper().__setitem__(int(key), value)\n# if self.__contains__(key):\n#     raise KeyError(\"Key {} already in group index.\".format(key))\n# else:\n# if isinstance(value, (Ts, Tsd)):\n#     self._metadata.loc[int(key), \"rate\"] = value.rate\n#     super().__setitem__(int(key), value)\n# elif isinstance(value, (np.ndarray, list)):\n#     warnings.warn(\n#         \"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\n#         stacklevel=2,\n#     )\n#     tmp = Ts(t=value, time_units=\"s\")\n#     self._metadata.loc[int(key), \"rate\"] = tmp.rate\n#     super().__setitem__(int(key), tmp)\n# else:\n#     raise ValueError(\"Value with key {} is not an iterable.\".format(key))\ndef __getitem__(self, key):\nif key.__hash__:\nif self.__contains__(key):\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\nelse:\nmetadata = self._metadata.loc[key, self._metadata.columns.drop(\"rate\")]\nreturn TsGroup(\n{k: self[k] for k in key}, time_support=self.time_support, **metadata\n)\ndef __repr__(self):\ncols = self._metadata.columns.drop(\"rate\")\nheaders = [\"Index\", \"rate\"] + [c for c in cols]\nlines = []\nfor i in self.data.keys():\nlines.append(\n[str(i), \"%.2f\" % self._metadata.loc[i, \"rate\"]]\n+ [self._metadata.loc[i, c] for c in cols]\n)\nreturn tabulate(lines, headers=headers)\ndef __str__(self):\nreturn self.__repr__()\ndef keys(self):\n\"\"\"\n        Return index/keys of TsGroup\n        Returns\n        -------\n        list\n            List of keys\n        \"\"\"\nreturn list(self.data.keys())\ndef items(self):\n\"\"\"\n        Return a list of key/object.\n        Returns\n        -------\n        list\n            List of tuples\n        \"\"\"\nreturn list(self.data.items())\ndef values(self):\n\"\"\"\n        Return a list of all the Ts/Tsd objects in the TsGroup\n        Returns\n        -------\n        list\n            List of Ts/Tsd objects\n        \"\"\"\nreturn list(self.data.values())\n@property\ndef rates(self):\n\"\"\"\n        Return the rates of each element of the group in Hz\n        \"\"\"\nreturn self._metadata[\"rate\"]\n#######################\n# Metadata\n#######################\n@property\ndef metadata_columns(self):\n\"\"\"\n        Returns list of metadata columns\n        -------\n        \"\"\"\nreturn list(self._metadata.columns)\ndef set_info(self, *args, **kwargs):\n\"\"\"\n        Add metadata informations about the TsGroup.\n        Metadata are saved as a DataFrame.\n        Parameters\n        ----------\n        *args\n            pandas.Dataframe or list of pandas.DataFrame\n        **kwargs\n            Can be either pandas.Series or numpy.ndarray\n        Raises\n        ------\n        RuntimeError\n            Raise an error if\n                no column labels are found when passing simple arguments,\n                indexes are not equals for a pandas series,\n                not the same length when passing numpy array.\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        To add metadata with a pandas.DataFrame:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n        &gt;&gt;&gt; tsgroup.set_info(structs)\n        &gt;&gt;&gt; tsgroup\n          Index    Freq. (Hz)  struct\n        -------  ------------  --------\n              0             1  pfc\n              1             2  pfc\n              2             4  ca1\n        To add metadata with a pd.Series or numpy.ndarray:\n        &gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n        &gt;&gt;&gt; tsgroup.set_info(hd=hd)\n        &gt;&gt;&gt; tsgroup\n          Index    Freq. (Hz)  struct      hd\n        -------  ------------  --------  ----\n              0             1  pfc          0\n              1             2  pfc          1\n              2             4  ca1          1\n        \"\"\"\nif len(args):\nfor arg in args:\nif isinstance(arg, pd.DataFrame):\nif pd.Index.equals(self._metadata.index, arg.index):\nself._metadata = self._metadata.join(arg)\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(arg, (pd.Series, np.ndarray)):\nraise RuntimeError(\"Columns needs to be labelled for metadata\")\nif len(kwargs):\nfor k, v in kwargs.items():\nif isinstance(v, pd.Series):\nif pd.Index.equals(self._metadata.index, v.index):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(v, np.ndarray):\nif len(self._metadata) == len(v):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Array is not the same length.\")\nreturn\ndef get_info(self, key):\n\"\"\"\n        Returns the metainfo located in one column.\n        The key for the column frequency is \"rate\".\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        Returns\n        -------\n        pandas.Series\n            The metainfo\n        \"\"\"\nif key in [\"freq\", \"frequency\"]:\nkey = \"rate\"\nreturn self._metadata[key]\n#################################\n# Generic functions of Tsd objects\n#################################\ndef restrict(self, ep):\n\"\"\"\n        Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object\n        Parameters\n        ----------\n        ep : IntervalSet\n            the IntervalSet object\n        Returns\n        -------\n        TsGroup\n            TsGroup object restricted to ep\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        &gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n        All objects within the TsGroup automatically inherit the epochs defined by ep.\n        &gt;&gt;&gt; newtsgroup.time_support\n           start    end\n        0    0.0  100.0\n        &gt;&gt;&gt; newtsgroup[0].time_support\n           start    end\n        0    0.0  100.0\n        \"\"\"\nnewgr = {}\nfor k in self.index:\nnewgr[k] = self.data[k].restrict(ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(\nnewgr, time_support=ep, bypass_check=True, **self._metadata[cols]\n)\ndef value_from(self, tsd, ep=None):\n\"\"\"\n        Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument\n        Parameters\n        ----------\n        tsd : Tsd\n            The Tsd object holding the values to replace\n        ep : IntervalSet\n            The IntervalSet object to restrict the operation.\n            If None, the time support of the tsd input object is used.\n        Returns\n        -------\n        TsGroup\n            TsGroup object with the new values\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        The variable tsd is a time series object containing the values to assign, for example the tracking data:\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n        &gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n        \"\"\"\nif ep is None:\nep = tsd.time_support\nnewgr = {}\nfor k in self.data:\nnewgr[k] = self.data[k].value_from(tsd, ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(newgr, time_support=ep, **self._metadata[cols])\ndef count(self, *args, **kwargs):\n\"\"\"\n        Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n        You can call this function in multiple ways :\n        1. *tsgroup.count(bin_size=1, time_units = 'ms')*\n        -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n        2. *tsgroup.count(1, ep=my_epochs)*\n        -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n        3. *tsgroup.count(ep=my_bins)*\n        -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n        4. *tsgroup.count()*\n        -&gt; Count occurent of events within each epoch of the time support.\n        bin_size should be seconds unless specified.\n        If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n        Parameters\n        ----------\n        bin_size : None or float, optional\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: TsdFrame\n            A TsdFrame with the columns being the index of each item in the TsGroup.\n        Examples\n        --------\n        This example shows how to count events within bins of 0.1 second for the first 100 seconds.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        &gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n        &gt;&gt;&gt; bincount\n                  0  1  2\n        Time (s)\n        0.05      0  0  0\n        0.15      0  0  0\n        0.25      0  0  1\n        0.35      0  0  0\n        0.45      0  0  0\n        ...      .. .. ..\n        99.55     0  1  1\n        99.65     0  0  0\n        99.75     0  0  1\n        99.85     0  0  0\n        99.95     1  1  1\n        [1000 rows x 3 columns]\n        \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = float(bin_size)\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\ntime_index, _ = jitcount(np.array([]), starts, ends, bin_size)\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jitcount(\nself.data[self.index[i]].index.values, starts, ends, bin_size\n)[1]\nelse:\ntime_index = starts + (ends - starts) / 2\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jittsrestrict_with_count(\nself.data[self.index[i]].index.values, starts, ends\n)[1]\ntoreturn = TsdFrame(t=time_index, d=count, time_support=ep, columns=self.index)\nreturn toreturn\ndef to_tsd(self, *args):\n\"\"\"\n        Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.\n        Parameters\n        ----------\n        *args\n            string, list, numpy.ndarray or pandas.Series\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\n        Index    rate\n        -------  ------\n        0       1\n        5       1\n        By default, the values of the Tsd is the index of the timestamp in the TsGroup:\n        &gt;&gt;&gt; tsgroup.to_tsd()\n        Time (s)\n        0.0    0.0\n        1.0    0.0\n        2.0    5.0\n        3.0    5.0\n        dtype: float64\n        Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.\n        &gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n        &gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\n        Time (s)\n        0.0    3.141593\n        1.0    3.141593\n        2.0    6.283185\n        3.0    6.283185\n        dtype: float64\n        Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :\n        &gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\n        Time (s)\n        0.0   -1.0\n        1.0   -1.0\n        2.0    1.0\n        3.0    1.0\n        dtype: float64\n        The reverse operation can be done with the Tsd.to_tsgroup function :\n        &gt;&gt;&gt; my_tsd\n        Time (s)\n        0.0    0.0\n        1.0    0.0\n        2.0    5.0\n        3.0    5.0\n        dtype: float64\n        &gt;&gt;&gt; my_tsd.to_tsgroup()\n          Index    rate\n        -------  ------\n              0       1\n              5       1\n        Returns\n        -------\n        Tsd\n        Raises\n        ------\n        RuntimeError\n            \"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes\n            \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object\n            \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata,\n            \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series\n        \"\"\"\nif len(args):\nif isinstance(args[0], pd.Series):\nif pd.Index.equals(self._metadata.index, args[0].index):\n_values = args[0].values.flatten()\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(args[0], (np.ndarray, list)):\nif len(self._metadata) == len(args[0]):\n_values = np.array(args[0])\nelse:\nraise RuntimeError(\"Values is not the same length.\")\nelif isinstance(args[0], str):\nif args[0] in self._metadata.columns:\n_values = self._metadata[args[0]].values\nelse:\nraise RuntimeError(\n\"Key {} not in metadata of TsGroup\".format(args[0])\n)\nelse:\npossible_keys = []\nfor k, d in self._metadata.dtypes.items():\nif \"int\" in str(d) or \"float\" in str(d):\npossible_keys.append(k)\nraise RuntimeError(\n\"Unknown argument format. Must be pandas.Series, numpy.ndarray or a string from one of the following values : [{}]\".format(\n\", \".join(possible_keys)\n)\n)\nelse:\n_values = self.index\nnt = 0\nfor n in self.index:\nnt += self[n].shape[0]\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nk = 0\nfor n, v in zip(self.index, _values):\nkl = self[n].shape[0]\ntimes[k : k + kl] = self[n].index.values\ndata[k : k + kl] = v\nk += kl\nidx = np.argsort(times)\ntoreturn = Tsd(t=times[idx], d=data[idx], time_support=self.time_support)\nreturn toreturn\n\"\"\"\n    Special slicing of metadata\n    \"\"\"\ndef getby_threshold(self, key, thr, op=\"&gt;\"):\n\"\"\"\n        Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        thr : float\n            THe value for thresholding\n        op : str, optional\n            The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.\n        Returns\n        -------\n        TsGroup\n            The new TsGroup\n        Raises\n        ------\n        RuntimeError\n            Raise eror is operation is not recognized.\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n          Index    Freq. (Hz)\n        -------  ------------\n              0             1\n              1             2\n              2             4\n        This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.\n        &gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n          Index    Freq. (Hz)\n        -------  ------------\n              1             2\n              2             4\n        \"\"\"\nif op == \"&gt;\":\nix = list(self._metadata.index[self._metadata[key] &gt; thr])\nreturn self[ix]\nelif op == \"&lt;\":\nix = list(self._metadata.index[self._metadata[key] &lt; thr])\nreturn self[ix]\nelif op == \"&gt;=\":\nix = list(self._metadata.index[self._metadata[key] &gt;= thr])\nreturn self[ix]\nelif op == \"&lt;=\":\nix = list(self._metadata.index[self._metadata[key] &lt;= thr])\nreturn self[ix]\nelse:\nraise RuntimeError(\"Operation {} not recognized.\".format(op))\ndef getby_intervals(self, key, bins):\n\"\"\"\n        Return a list of TsGroup binned.\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        bins : numpy.ndarray or list\n            The bin intervals\n        Returns\n        -------\n        list\n            A list of TsGroup\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n          Index    Freq. (Hz)    alpha\n        -------  ------------  -------\n              0             1        0\n              1             2        1\n              2             4        2\n        This exemple shows how to bin the TsGroup according to one metainfo key.\n        &gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n        &gt;&gt;&gt; newtsgroup\n        [  Index    Freq. (Hz)    alpha\n         -------  ------------  -------\n               0             1        0,\n           Index    Freq. (Hz)    alpha\n         -------  ------------  -------\n               1             2        1]\n        By default, the function returns the center of the bins.\n        &gt;&gt;&gt; bincenter\n        array([0.5, 1.5])\n        \"\"\"\nidx = np.digitize(self._metadata[key], bins) - 1\ngroups = self._metadata.index.groupby(idx)\nix = np.unique(list(groups.keys()))\nix = ix[ix &gt;= 0]\nix = ix[ix &lt; len(bins) - 1]\nxb = bins[0:-1] + np.diff(bins) / 2\nsliced = [self[list(groups[i])] for i in ix]\nreturn sliced, xb[ix]\ndef getby_category(self, key):\n\"\"\"\n        Return a list of TsGroup grouped by category.\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        Returns\n        -------\n        dict\n            A dictionnary of TsGroup\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n          Index    Freq. (Hz)    group\n        -------  ------------  -------\n              0             1        0\n              1             2        1\n              2             4        1\n        This exemple shows how to group the TsGroup according to one metainfo key.\n        &gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n        &gt;&gt;&gt; newtsgroup\n        {0:   Index    Freq. (Hz)    group\n         -------  ------------  -------\n               0             1        0,\n         1:   Index    Freq. (Hz)    group\n         -------  ------------  -------\n               1             2        1\n               2             4        1}\n        \"\"\"\ngroups = self._metadata.groupby(key).groups\nsliced = {k: self[list(groups[k])] for k in groups.keys()}\nreturn sliced\ndef save(self, filename):\n\"\"\"\n        Save TsGroup object in npz format. The file will contain the timestamps,\n        the data (if group of Tsd), group index, the time support and the metadata\n        The main purpose of this function is to save small/medium sized TsGroup\n        objects.\n        The function will \"flatten\" the TsGroup by sorting all the timestamps\n        and assigning to each the corresponding index. Typically, a TsGroup like\n        this :\n            TsGroup({\n                0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n                1 : Tsd(t=[1, 5], d=[5, 6])\n            })\n        will be saved as npz with the following keys:\n            {\n                't' : [0, 1, 2, 4, 5],\n                'd' : [1, 5, 2, 3, 5],\n                'index' : [0, 1, 0, 0, 1],\n                'start' : [0],\n                'end' : [5],\n                'type' : 'TsGroup'\n            }\n        Metadata are saved by columns with the column name as the npz key. To avoid\n        potential conflicts, make sure the columns name of the metadata are different\n        from ['t', 'd', 'start', 'end', 'index']\n        You can load the object with numpy.load. Default keys are 't', 'd'(optional),\n        'start', 'end', 'index' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsgroup = nap.TsGroup({\n            0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n            6 : nap.Ts(t=np.array([1.0, 5.0]))\n            },\n            group = np.array([0, 1]),\n            location = np.array(['right foot', 'left foot'])\n            )\n        &gt;&gt;&gt; tsgroup\n          Index    rate    group  location\n        -------  ------  -------  ----------\n              0     0.6        0  right foot\n              6     0.4        1  left foot\n        &gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['rate', 'group', 'location', 't', 'index', 'start', 'end', 'type']\n        &gt;&gt;&gt; print(file['index'])\n        [0 6 0 0 6]\n        In the case where TsGroup is a set of Ts objects, it is very direct to\n        recreate the TsGroup by using the function to_tsgroup :\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n        &gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n        &gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n        &gt;&gt;&gt; tsgroup\n          Index    rate    group  location\n        -------  ------  -------  ----------\n              0     0.6        0  right foot\n              6     0.4        1  left foot\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ndicttosave = {\"type\": np.array([\"TsGroup\"], dtype=np.str_)}\nfor k in self._metadata.columns:\nif k not in [\"t\", \"d\", \"start\", \"end\", \"index\"]:\ntmp = self._metadata[k].values\nif tmp.dtype == np.dtype(\"O\"):\ntmp = tmp.astype(np.str_)\ndicttosave[k] = tmp\n# We can't use to_tsd here in case tsgroup contains Tsd and not only Ts.\nnt = 0\nfor n in self.index:\nnt += self[n].shape[0]\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nindex = np.zeros(nt, dtype=np.int64)\nk = 0\nfor n in self.index:\nkl = self[n].shape[0]\ntimes[k : k + kl] = self[n].index.values\ndata[k : k + kl] = self[n].values\nindex[k : k + kl] = int(n)\nk += kl\nidx = np.argsort(times)\ntimes = times[idx]\nindex = index[idx]\ndicttosave[\"t\"] = times\ndicttosave[\"index\"] = index\nif not np.all(np.isnan(data)):\ndicttosave[\"d\"] = data[idx]\ndicttosave[\"start\"] = self.time_support.start.values\ndicttosave[\"end\"] = self.time_support.end.values\nnp.savez(filename, **dicttosave)\nreturn\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.rates","title":"<code>rates</code>  <code>property</code>","text":"<p>Return the rates of each element of the group in Hz</p>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.metadata_columns","title":"<code>metadata_columns</code>  <code>property</code>","text":""},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.metadata_columns--returns-list-of-metadata-columns","title":"Returns list of metadata columns","text":""},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.__init__","title":"<code>__init__(data, time_support=None, time_units='s', bypass_check=False, **kwargs)</code>","text":"<p>TsGroup Initializer</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionnary containing Ts/Tsd objects</p> required <code>time_support</code> <code>IntervalSet, optional</code> <p>The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed. If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).</p> <code>'s'</code> <code>bypass_check</code> <p>To avoid checking that each element is within time_support. Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand</p> <code>False</code> <code>**kwargs</code> <p>Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray. Note that the index should match the index of the input dictionnary.</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise error if the union of time support of Ts/Tsd object is empty.</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def __init__(\nself, data, time_support=None, time_units=\"s\", bypass_check=False, **kwargs\n):\n\"\"\"\n    TsGroup Initializer\n    Parameters\n    ----------\n    data : dict\n        Dictionnary containing Ts/Tsd objects\n    time_support : IntervalSet, optional\n        The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed.\n        If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.\n    time_units : str, optional\n        Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).\n    bypass_check: bool, optional\n        To avoid checking that each element is within time_support.\n        Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand\n    **kwargs\n        Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray.\n        Note that the index should match the index of the input dictionnary.\n    Raises\n    ------\n    RuntimeError\n        Raise error if the union of time support of Ts/Tsd object is empty.\n    \"\"\"\nself._initialized = False\nself.index = np.sort(list(data.keys()))\nself._metadata = pd.DataFrame(index=self.index, columns=[\"rate\"], dtype=\"float\")\n# Transform elements to Ts/Tsd objects\nfor k in self.index:\nif isinstance(data[k], (np.ndarray, list)):\nwarnings.warn(\n\"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\nstacklevel=2,\n)\ndata[k] = Ts(\nt=data[k], time_support=time_support, time_units=time_units\n)\n# If time_support is passed, all elements of data are restricted prior to init\nif isinstance(time_support, IntervalSet):\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nelse:\n# Otherwise do the union of all time supports\ntime_support = union_intervals([data[k].time_support for k in self.index])\nif len(time_support) == 0:\nraise RuntimeError(\n\"Union of time supports is empty. Consider passing a time support as argument.\"\n)\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nUserDict.__init__(self, data)\n# Making the TsGroup non mutable\nself._initialized = True\n# Trying to add argument as metainfo\nself.set_info(**kwargs)\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.keys","title":"<code>keys()</code>","text":"<p>Return index/keys of TsGroup</p> <p>Returns:</p> Type Description <code>list</code> <p>List of keys</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def keys(self):\n\"\"\"\n    Return index/keys of TsGroup\n    Returns\n    -------\n    list\n        List of keys\n    \"\"\"\nreturn list(self.data.keys())\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.items","title":"<code>items()</code>","text":"<p>Return a list of key/object.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of tuples</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def items(self):\n\"\"\"\n    Return a list of key/object.\n    Returns\n    -------\n    list\n        List of tuples\n    \"\"\"\nreturn list(self.data.items())\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.values","title":"<code>values()</code>","text":"<p>Return a list of all the Ts/Tsd objects in the TsGroup</p> <p>Returns:</p> Type Description <code>list</code> <p>List of Ts/Tsd objects</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def values(self):\n\"\"\"\n    Return a list of all the Ts/Tsd objects in the TsGroup\n    Returns\n    -------\n    list\n        List of Ts/Tsd objects\n    \"\"\"\nreturn list(self.data.values())\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.set_info","title":"<code>set_info(*args, **kwargs)</code>","text":"<p>Add metadata informations about the TsGroup. Metadata are saved as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>pandas.Dataframe or list of pandas.DataFrame</p> <code>()</code> <code>**kwargs</code> <p>Can be either pandas.Series or numpy.ndarray</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise an error if no column labels are found when passing simple arguments, indexes are not equals for a pandas series, not the same length when passing numpy array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n</code></pre> <p>To add metadata with a pandas.DataFrame:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n&gt;&gt;&gt; tsgroup.set_info(structs)\n&gt;&gt;&gt; tsgroup\n  Index    Freq. (Hz)  struct\n-------  ------------  --------\n      0             1  pfc\n      1             2  pfc\n      2             4  ca1\n</code></pre> <p>To add metadata with a pd.Series or numpy.ndarray:</p> <pre><code>&gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n&gt;&gt;&gt; tsgroup.set_info(hd=hd)\n&gt;&gt;&gt; tsgroup\n  Index    Freq. (Hz)  struct      hd\n-------  ------------  --------  ----\n      0             1  pfc          0\n      1             2  pfc          1\n      2             4  ca1          1\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def set_info(self, *args, **kwargs):\n\"\"\"\n    Add metadata informations about the TsGroup.\n    Metadata are saved as a DataFrame.\n    Parameters\n    ----------\n    *args\n        pandas.Dataframe or list of pandas.DataFrame\n    **kwargs\n        Can be either pandas.Series or numpy.ndarray\n    Raises\n    ------\n    RuntimeError\n        Raise an error if\n            no column labels are found when passing simple arguments,\n            indexes are not equals for a pandas series,\n            not the same length when passing numpy array.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    To add metadata with a pandas.DataFrame:\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n    &gt;&gt;&gt; tsgroup.set_info(structs)\n    &gt;&gt;&gt; tsgroup\n      Index    Freq. (Hz)  struct\n    -------  ------------  --------\n          0             1  pfc\n          1             2  pfc\n          2             4  ca1\n    To add metadata with a pd.Series or numpy.ndarray:\n    &gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n    &gt;&gt;&gt; tsgroup.set_info(hd=hd)\n    &gt;&gt;&gt; tsgroup\n      Index    Freq. (Hz)  struct      hd\n    -------  ------------  --------  ----\n          0             1  pfc          0\n          1             2  pfc          1\n          2             4  ca1          1\n    \"\"\"\nif len(args):\nfor arg in args:\nif isinstance(arg, pd.DataFrame):\nif pd.Index.equals(self._metadata.index, arg.index):\nself._metadata = self._metadata.join(arg)\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(arg, (pd.Series, np.ndarray)):\nraise RuntimeError(\"Columns needs to be labelled for metadata\")\nif len(kwargs):\nfor k, v in kwargs.items():\nif isinstance(v, pd.Series):\nif pd.Index.equals(self._metadata.index, v.index):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(v, np.ndarray):\nif len(self._metadata) == len(v):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Array is not the same length.\")\nreturn\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.get_info","title":"<code>get_info(key)</code>","text":"<p>Returns the metainfo located in one column. The key for the column frequency is \"rate\".</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <p>Returns:</p> Type Description <code>pandas.Series</code> <p>The metainfo</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def get_info(self, key):\n\"\"\"\n    Returns the metainfo located in one column.\n    The key for the column frequency is \"rate\".\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    Returns\n    -------\n    pandas.Series\n        The metainfo\n    \"\"\"\nif key in [\"freq\", \"frequency\"]:\nkey = \"rate\"\nreturn self._metadata[key]\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.restrict","title":"<code>restrict(ep)</code>","text":"<p>Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>ep</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Type Description <code>TsGroup</code> <p>TsGroup object restricted to ep</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n&gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n</code></pre> <p>All objects within the TsGroup automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newtsgroup.time_support\n   start    end\n0    0.0  100.0\n&gt;&gt;&gt; newtsgroup[0].time_support\n   start    end\n0    0.0  100.0\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def restrict(self, ep):\n\"\"\"\n    Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    ep : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    TsGroup\n        TsGroup object restricted to ep\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    &gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n    All objects within the TsGroup automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newtsgroup.time_support\n       start    end\n    0    0.0  100.0\n    &gt;&gt;&gt; newtsgroup[0].time_support\n       start    end\n    0    0.0  100.0\n    \"\"\"\nnewgr = {}\nfor k in self.index:\nnewgr[k] = self.data[k].restrict(ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(\nnewgr, time_support=ep, bypass_check=True, **self._metadata[cols]\n)\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.value_from","title":"<code>value_from(tsd, ep=None)</code>","text":"<p>Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>Tsd</code> <p>The Tsd object holding the values to replace</p> required <code>ep</code> <code>IntervalSet</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>TsGroup</code> <p>TsGroup object with the new values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n</code></pre> <p>The variable tsd is a time series object containing the values to assign, for example the tracking data:</p> <pre><code>&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n&gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def value_from(self, tsd, ep=None):\n\"\"\"\n    Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument\n    Parameters\n    ----------\n    tsd : Tsd\n        The Tsd object holding the values to replace\n    ep : IntervalSet\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    TsGroup\n        TsGroup object with the new values\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    The variable tsd is a time series object containing the values to assign, for example the tracking data:\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n    &gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n    \"\"\"\nif ep is None:\nep = tsd.time_support\nnewgr = {}\nfor k in self.data:\nnewgr[k] = self.data[k].value_from(tsd, ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(newgr, time_support=ep, **self._metadata[cols])\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.count","title":"<code>count(*args, **kwargs)</code>","text":"<p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsgroup.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsgroup.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsgroup.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsgroup.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float, optional</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet, optional</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str, optional</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>TsdFrame</code> <p>A TsdFrame with the columns being the index of each item in the TsGroup.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second for the first 100 seconds.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n&gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n&gt;&gt;&gt; bincount\n          0  1  2\nTime (s)\n0.05      0  0  0\n0.15      0  0  0\n0.25      0  0  1\n0.35      0  0  0\n0.45      0  0  0\n...      .. .. ..\n99.55     0  1  1\n99.65     0  0  0\n99.75     0  0  1\n99.85     0  0  0\n99.95     1  1  1\n[1000 rows x 3 columns]\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsgroup.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsgroup.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsgroup.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsgroup.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: TsdFrame\n        A TsdFrame with the columns being the index of each item in the TsGroup.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second for the first 100 seconds.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    &gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n    &gt;&gt;&gt; bincount\n              0  1  2\n    Time (s)\n    0.05      0  0  0\n    0.15      0  0  0\n    0.25      0  0  1\n    0.35      0  0  0\n    0.45      0  0  0\n    ...      .. .. ..\n    99.55     0  1  1\n    99.65     0  0  0\n    99.75     0  0  1\n    99.85     0  0  0\n    99.95     1  1  1\n    [1000 rows x 3 columns]\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = float(bin_size)\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\ntime_index, _ = jitcount(np.array([]), starts, ends, bin_size)\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jitcount(\nself.data[self.index[i]].index.values, starts, ends, bin_size\n)[1]\nelse:\ntime_index = starts + (ends - starts) / 2\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jittsrestrict_with_count(\nself.data[self.index[i]].index.values, starts, ends\n)[1]\ntoreturn = TsdFrame(t=time_index, d=count, time_support=ep, columns=self.index)\nreturn toreturn\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.to_tsd","title":"<code>to_tsd(*args)</code>","text":"<p>Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>string, list, numpy.ndarray or pandas.Series</p> <code>()</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\nIndex    rate\n-------  ------\n0       1\n5       1\n</code></pre> <p>By default, the values of the Tsd is the index of the timestamp in the TsGroup:</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd()\nTime (s)\n0.0    0.0\n1.0    0.0\n2.0    5.0\n3.0    5.0\ndtype: float64\n</code></pre> <p>Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.</p> <pre><code>&gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n&gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\nTime (s)\n0.0    3.141593\n1.0    3.141593\n2.0    6.283185\n3.0    6.283185\ndtype: float64\n</code></pre> <p>Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\nTime (s)\n0.0   -1.0\n1.0   -1.0\n2.0    1.0\n3.0    1.0\ndtype: float64\n</code></pre> <p>The reverse operation can be done with the Tsd.to_tsgroup function :</p> <pre><code>&gt;&gt;&gt; my_tsd\nTime (s)\n0.0    0.0\n1.0    0.0\n2.0    5.0\n3.0    5.0\ndtype: float64\n&gt;&gt;&gt; my_tsd.to_tsgroup()\n  Index    rate\n-------  ------\n      0       1\n      5       1\n</code></pre> <p>Returns:</p> Type Description <code>Tsd</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>\"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata, \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def to_tsd(self, *args):\n\"\"\"\n    Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.\n    Parameters\n    ----------\n    *args\n        string, list, numpy.ndarray or pandas.Series\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\n    Index    rate\n    -------  ------\n    0       1\n    5       1\n    By default, the values of the Tsd is the index of the timestamp in the TsGroup:\n    &gt;&gt;&gt; tsgroup.to_tsd()\n    Time (s)\n    0.0    0.0\n    1.0    0.0\n    2.0    5.0\n    3.0    5.0\n    dtype: float64\n    Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.\n    &gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n    &gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\n    Time (s)\n    0.0    3.141593\n    1.0    3.141593\n    2.0    6.283185\n    3.0    6.283185\n    dtype: float64\n    Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :\n    &gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\n    Time (s)\n    0.0   -1.0\n    1.0   -1.0\n    2.0    1.0\n    3.0    1.0\n    dtype: float64\n    The reverse operation can be done with the Tsd.to_tsgroup function :\n    &gt;&gt;&gt; my_tsd\n    Time (s)\n    0.0    0.0\n    1.0    0.0\n    2.0    5.0\n    3.0    5.0\n    dtype: float64\n    &gt;&gt;&gt; my_tsd.to_tsgroup()\n      Index    rate\n    -------  ------\n          0       1\n          5       1\n    Returns\n    -------\n    Tsd\n    Raises\n    ------\n    RuntimeError\n        \"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes\n        \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object\n        \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata,\n        \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series\n    \"\"\"\nif len(args):\nif isinstance(args[0], pd.Series):\nif pd.Index.equals(self._metadata.index, args[0].index):\n_values = args[0].values.flatten()\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(args[0], (np.ndarray, list)):\nif len(self._metadata) == len(args[0]):\n_values = np.array(args[0])\nelse:\nraise RuntimeError(\"Values is not the same length.\")\nelif isinstance(args[0], str):\nif args[0] in self._metadata.columns:\n_values = self._metadata[args[0]].values\nelse:\nraise RuntimeError(\n\"Key {} not in metadata of TsGroup\".format(args[0])\n)\nelse:\npossible_keys = []\nfor k, d in self._metadata.dtypes.items():\nif \"int\" in str(d) or \"float\" in str(d):\npossible_keys.append(k)\nraise RuntimeError(\n\"Unknown argument format. Must be pandas.Series, numpy.ndarray or a string from one of the following values : [{}]\".format(\n\", \".join(possible_keys)\n)\n)\nelse:\n_values = self.index\nnt = 0\nfor n in self.index:\nnt += self[n].shape[0]\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nk = 0\nfor n, v in zip(self.index, _values):\nkl = self[n].shape[0]\ntimes[k : k + kl] = self[n].index.values\ndata[k : k + kl] = v\nk += kl\nidx = np.argsort(times)\ntoreturn = Tsd(t=times[idx], d=data[idx], time_support=self.time_support)\nreturn toreturn\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.getby_threshold","title":"<code>getby_threshold(key, thr, op='&gt;')</code>","text":"<p>Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <code>thr</code> <code>float</code> <p>THe value for thresholding</p> required <code>op</code> <code>str, optional</code> <p>The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.</p> <code>'&gt;'</code> <p>Returns:</p> Type Description <code>TsGroup</code> <p>The new TsGroup</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise eror is operation is not recognized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n  Index    Freq. (Hz)\n-------  ------------\n      0             1\n      1             2\n      2             4\n</code></pre> <p>This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.</p> <pre><code>&gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n  Index    Freq. (Hz)\n-------  ------------\n      1             2\n      2             4\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_threshold(self, key, thr, op=\"&gt;\"):\n\"\"\"\n    Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    thr : float\n        THe value for thresholding\n    op : str, optional\n        The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.\n    Returns\n    -------\n    TsGroup\n        The new TsGroup\n    Raises\n    ------\n    RuntimeError\n        Raise eror is operation is not recognized.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n      Index    Freq. (Hz)\n    -------  ------------\n          0             1\n          1             2\n          2             4\n    This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.\n    &gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n      Index    Freq. (Hz)\n    -------  ------------\n          1             2\n          2             4\n    \"\"\"\nif op == \"&gt;\":\nix = list(self._metadata.index[self._metadata[key] &gt; thr])\nreturn self[ix]\nelif op == \"&lt;\":\nix = list(self._metadata.index[self._metadata[key] &lt; thr])\nreturn self[ix]\nelif op == \"&gt;=\":\nix = list(self._metadata.index[self._metadata[key] &gt;= thr])\nreturn self[ix]\nelif op == \"&lt;=\":\nix = list(self._metadata.index[self._metadata[key] &lt;= thr])\nreturn self[ix]\nelse:\nraise RuntimeError(\"Operation {} not recognized.\".format(op))\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.getby_intervals","title":"<code>getby_intervals(key, bins)</code>","text":"<p>Return a list of TsGroup binned.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <code>bins</code> <code>numpy.ndarray or list</code> <p>The bin intervals</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of TsGroup</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n  Index    Freq. (Hz)    alpha\n-------  ------------  -------\n      0             1        0\n      1             2        1\n      2             4        2\n</code></pre> <p>This exemple shows how to bin the TsGroup according to one metainfo key.</p> <pre><code>&gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n&gt;&gt;&gt; newtsgroup\n[  Index    Freq. (Hz)    alpha\n -------  ------------  -------\n       0             1        0,\n   Index    Freq. (Hz)    alpha\n -------  ------------  -------\n       1             2        1]\n</code></pre> <p>By default, the function returns the center of the bins.</p> <pre><code>&gt;&gt;&gt; bincenter\narray([0.5, 1.5])\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_intervals(self, key, bins):\n\"\"\"\n    Return a list of TsGroup binned.\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    bins : numpy.ndarray or list\n        The bin intervals\n    Returns\n    -------\n    list\n        A list of TsGroup\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n      Index    Freq. (Hz)    alpha\n    -------  ------------  -------\n          0             1        0\n          1             2        1\n          2             4        2\n    This exemple shows how to bin the TsGroup according to one metainfo key.\n    &gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n    &gt;&gt;&gt; newtsgroup\n    [  Index    Freq. (Hz)    alpha\n     -------  ------------  -------\n           0             1        0,\n       Index    Freq. (Hz)    alpha\n     -------  ------------  -------\n           1             2        1]\n    By default, the function returns the center of the bins.\n    &gt;&gt;&gt; bincenter\n    array([0.5, 1.5])\n    \"\"\"\nidx = np.digitize(self._metadata[key], bins) - 1\ngroups = self._metadata.index.groupby(idx)\nix = np.unique(list(groups.keys()))\nix = ix[ix &gt;= 0]\nix = ix[ix &lt; len(bins) - 1]\nxb = bins[0:-1] + np.diff(bins) / 2\nsliced = [self[list(groups[i])] for i in ix]\nreturn sliced, xb[ix]\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.getby_category","title":"<code>getby_category(key)</code>","text":"<p>Return a list of TsGroup grouped by category.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionnary of TsGroup</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n  Index    Freq. (Hz)    group\n-------  ------------  -------\n      0             1        0\n      1             2        1\n      2             4        1\n</code></pre> <p>This exemple shows how to group the TsGroup according to one metainfo key.</p> <pre><code>&gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n&gt;&gt;&gt; newtsgroup\n{0:   Index    Freq. (Hz)    group\n -------  ------------  -------\n       0             1        0,\n 1:   Index    Freq. (Hz)    group\n -------  ------------  -------\n       1             2        1\n       2             4        1}\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_category(self, key):\n\"\"\"\n    Return a list of TsGroup grouped by category.\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    Returns\n    -------\n    dict\n        A dictionnary of TsGroup\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n      Index    Freq. (Hz)    group\n    -------  ------------  -------\n          0             1        0\n          1             2        1\n          2             4        1\n    This exemple shows how to group the TsGroup according to one metainfo key.\n    &gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n    &gt;&gt;&gt; newtsgroup\n    {0:   Index    Freq. (Hz)    group\n     -------  ------------  -------\n           0             1        0,\n     1:   Index    Freq. (Hz)    group\n     -------  ------------  -------\n           1             2        1\n           2             4        1}\n    \"\"\"\ngroups = self._metadata.groupby(key).groups\nsliced = {k: self[list(groups[k])] for k in groups.keys()}\nreturn sliced\n</code></pre>"},{"location":"old_pages/core.ts_group/#pynapple.core.ts_group.TsGroup.save","title":"<code>save(filename)</code>","text":"<p>Save TsGroup object in npz format. The file will contain the timestamps, the data (if group of Tsd), group index, the time support and the metadata</p> <p>The main purpose of this function is to save small/medium sized TsGroup objects.</p> <p>The function will \"flatten\" the TsGroup by sorting all the timestamps and assigning to each the corresponding index. Typically, a TsGroup like this :</p> <pre><code>TsGroup({\n    0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n    1 : Tsd(t=[1, 5], d=[5, 6])\n})\n</code></pre> <p>will be saved as npz with the following keys:</p> <pre><code>{\n    't' : [0, 1, 2, 4, 5],\n    'd' : [1, 5, 2, 3, 5],\n    'index' : [0, 1, 0, 0, 1],\n    'start' : [0],\n    'end' : [5],\n    'type' : 'TsGroup'\n}\n</code></pre> <p>Metadata are saved by columns with the column name as the npz key. To avoid potential conflicts, make sure the columns name of the metadata are different from ['t', 'd', 'start', 'end', 'index']</p> <p>You can load the object with numpy.load. Default keys are 't', 'd'(optional), 'start', 'end', 'index' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsgroup = nap.TsGroup({\n    0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n    6 : nap.Ts(t=np.array([1.0, 5.0]))\n    },\n    group = np.array([0, 1]),\n    location = np.array(['right foot', 'left foot'])\n    )\n&gt;&gt;&gt; tsgroup\n  Index    rate    group  location\n-------  ------  -------  ----------\n      0     0.6        0  right foot\n      6     0.4        1  left foot\n&gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['rate', 'group', 'location', 't', 'index', 'start', 'end', 'type']\n&gt;&gt;&gt; print(file['index'])\n[0 6 0 0 6]\n</code></pre> <p>In the case where TsGroup is a set of Ts objects, it is very direct to recreate the TsGroup by using the function to_tsgroup :</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n&gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n&gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n&gt;&gt;&gt; tsgroup\n  Index    rate    group  location\n-------  ------  -------  ----------\n      0     0.6        0  right foot\n      6     0.4        1  left foot\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save TsGroup object in npz format. The file will contain the timestamps,\n    the data (if group of Tsd), group index, the time support and the metadata\n    The main purpose of this function is to save small/medium sized TsGroup\n    objects.\n    The function will \"flatten\" the TsGroup by sorting all the timestamps\n    and assigning to each the corresponding index. Typically, a TsGroup like\n    this :\n        TsGroup({\n            0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n            1 : Tsd(t=[1, 5], d=[5, 6])\n        })\n    will be saved as npz with the following keys:\n        {\n            't' : [0, 1, 2, 4, 5],\n            'd' : [1, 5, 2, 3, 5],\n            'index' : [0, 1, 0, 0, 1],\n            'start' : [0],\n            'end' : [5],\n            'type' : 'TsGroup'\n        }\n    Metadata are saved by columns with the column name as the npz key. To avoid\n    potential conflicts, make sure the columns name of the metadata are different\n    from ['t', 'd', 'start', 'end', 'index']\n    You can load the object with numpy.load. Default keys are 't', 'd'(optional),\n    'start', 'end', 'index' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsgroup = nap.TsGroup({\n        0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n        6 : nap.Ts(t=np.array([1.0, 5.0]))\n        },\n        group = np.array([0, 1]),\n        location = np.array(['right foot', 'left foot'])\n        )\n    &gt;&gt;&gt; tsgroup\n      Index    rate    group  location\n    -------  ------  -------  ----------\n          0     0.6        0  right foot\n          6     0.4        1  left foot\n    &gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['rate', 'group', 'location', 't', 'index', 'start', 'end', 'type']\n    &gt;&gt;&gt; print(file['index'])\n    [0 6 0 0 6]\n    In the case where TsGroup is a set of Ts objects, it is very direct to\n    recreate the TsGroup by using the function to_tsgroup :\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n    &gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n    &gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n    &gt;&gt;&gt; tsgroup\n      Index    rate    group  location\n    -------  ------  -------  ----------\n          0     0.6        0  right foot\n          6     0.4        1  left foot\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ndicttosave = {\"type\": np.array([\"TsGroup\"], dtype=np.str_)}\nfor k in self._metadata.columns:\nif k not in [\"t\", \"d\", \"start\", \"end\", \"index\"]:\ntmp = self._metadata[k].values\nif tmp.dtype == np.dtype(\"O\"):\ntmp = tmp.astype(np.str_)\ndicttosave[k] = tmp\n# We can't use to_tsd here in case tsgroup contains Tsd and not only Ts.\nnt = 0\nfor n in self.index:\nnt += self[n].shape[0]\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nindex = np.zeros(nt, dtype=np.int64)\nk = 0\nfor n in self.index:\nkl = self[n].shape[0]\ntimes[k : k + kl] = self[n].index.values\ndata[k : k + kl] = self[n].values\nindex[k : k + kl] = int(n)\nk += kl\nidx = np.argsort(times)\ntimes = times[idx]\nindex = index[idx]\ndicttosave[\"t\"] = times\ndicttosave[\"index\"] = index\nif not np.all(np.isnan(data)):\ndicttosave[\"d\"] = data[idx]\ndicttosave[\"start\"] = self.time_support.start.values\ndicttosave[\"end\"] = self.time_support.end.values\nnp.savez(filename, **dicttosave)\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/","title":"Io.cnmfe","text":"<p>Loaders for calcium imaging data with miniscope. Support CNMF-E in matlab, inscopix-cnmfe and minian.</p>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E","title":"<code>CNMF_E</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E). The path folder should contain a file ending in .mat when calling Source2d.save_neurons</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>numpy.ndarray</code> <p>Spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class CNMF_E(BaseLoader):\n\"\"\"Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E).\n    The path folder should contain a file ending in .mat\n    when calling Source2d.save_neurons\n    Attributes\n    ----------\n    A : numpy.ndarray\n        Spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_cnmf_e(path)\nself.save_cnmfe_nwb(path)\ndef load_cnmf_e(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nfiles = os.listdir(path)\nmatfiles = [f for f in files if f.endswith(\".mat\")]\nif len(matfiles):\ndata = loadmat(os.path.join(path, matfiles[0]), struct_as_record=False)\nelse:\nraise RuntimeError(\"No mat file found in {}\".format(path))\nself.struct = data[\"neuron_results\"][0][0]\nC = self.struct.C.T\nself.A = self.struct.A.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = np.atleast_2d(self.A[i])\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_cnmf_e(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmf_e","title":"<code>load_cnmf_e(path)</code>","text":"<p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmf_e(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nfiles = os.listdir(path)\nmatfiles = [f for f in files if f.endswith(\".mat\")]\nif len(matfiles):\ndata = loadmat(os.path.join(path, matfiles[0]), struct_as_record=False)\nelse:\nraise RuntimeError(\"No mat file found in {}\".format(path))\nself.struct = data[\"neuron_results\"][0][0]\nC = self.struct.C.T\nself.A = self.struct.A.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nreturn None\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.save_cnmfe_nwb","title":"<code>save_cnmfe_nwb(path)</code>","text":"<p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = np.atleast_2d(self.A[i])\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmfe_nwb","title":"<code>load_cnmfe_nwb(path)</code>","text":"<p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian","title":"<code>Minian</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for data processed with Minian (https://github.com/denisecailab/minian). The path folder should contain a subfolder name minian.</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>numpy.ndarray</code> <p>Spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class Minian(BaseLoader):\n\"\"\"Loader for data processed with Minian (https://github.com/denisecailab/minian).\n    The path folder should contain a subfolder name minian.\n    Attributes\n    ----------\n    A : numpy.ndarray\n        Spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_minian(path)\nself.save_cnmfe_nwb(path)\ndef load_minian(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nminian_folder = os.path.join(path, \"minian\")\nif not os.path.exists(minian_folder):\nraise RuntimeError(\"Path {} does not contain a minian folder\".format(path))\ntry:\nimport zarr\nexcept ImportError as ie:\nprint(\"Please install module zarr for loading minian data\", ie)\nsys.exit()\ndata = zarr.open(minian_folder, \"r\")\nC = data[\"C.zarr\"][\"C\"][:]\nC = C.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nself.A = data[\"A.zarr\"][\"A\"][:]\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_minian(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.load_minian","title":"<code>load_minian(path)</code>","text":"<p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_minian(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nminian_folder = os.path.join(path, \"minian\")\nif not os.path.exists(minian_folder):\nraise RuntimeError(\"Path {} does not contain a minian folder\".format(path))\ntry:\nimport zarr\nexcept ImportError as ie:\nprint(\"Please install module zarr for loading minian data\", ie)\nsys.exit()\ndata = zarr.open(minian_folder, \"r\")\nC = data[\"C.zarr\"][\"C\"][:]\nC = C.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nself.A = data[\"A.zarr\"][\"A\"][:]\nreturn None\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.save_cnmfe_nwb","title":"<code>save_cnmfe_nwb(path)</code>","text":"<p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.Minian.load_cnmfe_nwb","title":"<code>load_cnmfe_nwb(path)</code>","text":"<p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE","title":"<code>InscopixCNMFE</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe). The folder should contain a file ending with '_traces.csv' and a tiff file for spatial footprints.</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>np.ndarray</code> <p>The spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class InscopixCNMFE(BaseLoader):\n\"\"\"Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe).\n    The folder should contain a file ending with '_traces.csv'\n    and a tiff file for spatial footprints.\n    Attributes\n    ----------\n    A : np.ndarray\n        The spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_inscopix_cnmfe(path)\nself.save_cnmfe_nwb(path)\ndef load_inscopix_cnmfe(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nfiles = os.listdir(path)\ntracefile = [f for f in files if f.endswith(\"_traces.csv\")]\nif len(tracefile):\nC = pd.read_csv(os.path.join(path, tracefile[0]), index_col=0)\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*_traces.csv\")\n)\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C.values)\ntry:\nimport tifffile as tiff\nexcept ImportError as ie:\nprint(\"Please install module tifffile for loading inscopix-cnmfe data\", ie)\nsys.exit()\ntifffile = [f for f in files if f.endswith(\".tiff\")]\nif len(tifffile):\nself.A = tiff.imread(os.path.join(path, tifffile[0]))\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*.tiff\")\n)\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_inscopix_cnmfe(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_inscopix_cnmfe","title":"<code>load_inscopix_cnmfe(path)</code>","text":"<p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_inscopix_cnmfe(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nfiles = os.listdir(path)\ntracefile = [f for f in files if f.endswith(\"_traces.csv\")]\nif len(tracefile):\nC = pd.read_csv(os.path.join(path, tracefile[0]), index_col=0)\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*_traces.csv\")\n)\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C.values)\ntry:\nimport tifffile as tiff\nexcept ImportError as ie:\nprint(\"Please install module tifffile for loading inscopix-cnmfe data\", ie)\nsys.exit()\ntifffile = [f for f in files if f.endswith(\".tiff\")]\nif len(tifffile):\nself.A = tiff.imread(os.path.join(path, tifffile[0]))\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*.tiff\")\n)\nreturn None\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.save_cnmfe_nwb","title":"<code>save_cnmfe_nwb(path)</code>","text":"<p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_cnmfe_nwb","title":"<code>load_cnmfe_nwb(path)</code>","text":"<p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.folder/","title":"Io.folder","text":"<p>The Folder class helps to navigate a hierarchical data tree.</p>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder","title":"<code>Folder</code>","text":"<p>         Bases: <code>UserDict</code></p> <p>Base class for all type of folders (i.e. Project, Subject, Sessions, ...). Handles files and sub-folders discovery</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>dict</code> <p>Dictionnary holidng all the pynapple objects found in the folder.</p> <code>name</code> <code>str</code> <p>Name of the folder</p> <code>npz_files</code> <code>list</code> <p>List of npz files found in the folder</p> <code>nwb_files</code> <code>list</code> <p>List of nwb files found in the folder</p> <code>path</code> <code>str</code> <p>Absolute path of the folder</p> <code>subfolds</code> <code>dict</code> <p>Dictionnary of all the subfolders</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>class Folder(UserDict):\n\"\"\"\n    Base class for all type of folders (i.e. Project, Subject, Sessions, ...).\n    Handles files and sub-folders discovery\n    Attributes\n    ----------\n    data : dict\n        Dictionnary holidng all the pynapple objects found in the folder.\n    name : str\n        Name of the folder\n    npz_files : list\n        List of npz files found in the folder\n    nwb_files : list\n        List of nwb files found in the folder\n    path : str\n        Absolute path of the folder\n    subfolds : dict\n        Dictionnary of all the subfolders\n    \"\"\"\ndef __init__(self, path):  # , exclude=(), max_depth=4):\n\"\"\"Initialize the Folder object\n        Parameters\n        ----------\n        path : str\n            Path to the folder\n        \"\"\"\npath = path.rstrip(\"/\")\nself.path = path\nself.name = os.path.basename(path)\nself._basic_view = Tree(\n\":open_file_folder: {}\".format(self.name), guide_style=\"blue\"\n)\nself._full_view = None\n# Search sub-folders\nsubfolds = [\nf.path\nfor f in os.scandir(path)\nif f.is_dir() and not f.name.startswith(\".\")\n]\nsubfolds.sort()\nself.subfolds = {}\nfor s in subfolds:\nsub = os.path.basename(s)\nself.subfolds[sub] = Folder(s)\nself._basic_view.add(\":open_file_folder: [blue]\" + sub)\n# Search files\nself.npz_files = _find_files(path, \"npz\")\nself.nwb_files = _find_files(path, \"nwb\")\nfor filename, file in self.npz_files.items():\nself._basic_view.add(\"[green]\" + file.name + \" \\t|\\t \" + file.type)\nfor file in self.nwb_files.values():\nself._basic_view.add(\"[magenta]\" + file.name + \" \\t|\\t NWB file\")\n# Putting everything together\nself.data = {**self.npz_files, **self.nwb_files, **self.subfolds}\nUserDict.__init__(self, self.data)\ndef __str__(self):\n\"\"\"View of the object\"\"\"\nwith Console() as console:\nconsole.print(self._basic_view)\nreturn \"\"\n# def __repr__(self):\n#     \"\"\"View of the object\"\"\"\n#     print(self._basic_view)\ndef __getitem__(self, key):\n\"\"\"Get subfolder or load file.\n        Parameters\n        ----------\n        key : str\n        Returns\n        -------\n        (Ts, Tsd, TsdFrame, TsGroup, IntervalSet, Folder or NWBFile)\n        Raises\n        ------\n        KeyError\n            If key is not in the dictionnary\n        \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], NPZFile):\ndata = self.data[key].load()\nself.data[key] = data\n# setattr(self, key, data)\nreturn data\nelif isinstance(self.data[key], NWBFile):\nreturn self.data[key]\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n# # # Gets called when an attribute is accessed\n# def __getattribute__(self, item):\n#     value = super(Folder, self).__getattribute__(item)\n#     if isinstance(value, NPZFile):\n#         data = value.load()\n#         setattr(self, item, data)\n#         self.data[item] = data\n#         return data\n#     else:\n#         return value\ndef _generate_tree_view(self):\ntree = Tree(\":open_file_folder: {}\".format(self.name), guide_style=\"blue\")\n# Folder\nfor fold in self.subfolds.keys():\ntree.add(\":open_file_folder: \" + fold)\n_walk_folder(tree.children[-1], self.subfolds[fold])\n# NPZ files\nfor file in self.npz_files.values():\ntree.add(\"[green]\" + file.name + \" \\t|\\t \" + file.type)\n# NWB files\nfor file in self.nwb_files.values():\ntree.add(\"[magenta]\" + file.name + \" \\t|\\t NWB file\")\nself._full_view = tree\ndef expand(self):\n\"\"\"Display the full tree view. Equivalent to Folder.view\"\"\"\nif not isinstance(self._full_view, Tree):\nself._generate_tree_view()\nwith Console() as console:\nconsole.print(self._full_view)\nreturn None\n@property\ndef view(self):\n\"\"\"Summary\"\"\"\nreturn self.expand()\ndef save(self, name, obj, description=\"\"):\n\"\"\"Save a pynapple object in the folder in a single file in uncompressed ``.npz`` format.\n        By default, the save function overwrite previously save file with the same name.\n        Parameters\n        ----------\n        name : str\n            Filename\n        obj : Ts, Tsd, TsdFrame, TsGroup or IntervalSet\n            Pynapple object.\n        description : str, optional\n            Metainformation added as a json sidecar.\n        \"\"\"\nfilepath = os.path.join(self.path, name)\nobj.save(filepath)\nself.npz_files[name] = NPZFile(filepath + \".npz\")\nself.data[name] = obj\nmetadata = {\"time\": str(datetime.now()), \"info\": str(description)}\nwith open(os.path.join(self.path, name + \".json\"), \"w\") as ff:\njson.dump(metadata, ff, indent=2)\n# regenerate the tree view\nself._generate_tree_view()\ndef load(self):\n\"\"\"Load all compatible NPZ files.\"\"\"\nfor k in self.npz_files.keys():\nself[k] = self.npz_files[k].load()\n# def add_metadata(self):\n#     \"\"\"Summary\"\"\"\n#     pass\ndef info(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n        Parameters\n        ----------\n        name : str\n            Name of the npz file\n        \"\"\"\nself.metadata(name)\ndef doc(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n        Parameters\n        ----------\n        name : str\n            Name of the npz file\n        \"\"\"\nself.metadata(name)\ndef metadata(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n        Parameters\n        ----------\n        name : str\n            Name of the npz file\n        \"\"\"\n# Search for json first\njson_filename = os.path.join(self.path, name + \".json\")\nif os.path.isfile(json_filename):\nwith open(json_filename, \"r\") as ff:\nmetadata = json.load(ff)\ntext = \"\\n\".join([\" : \".join(it) for it in metadata.items()])\npanel = Panel.fit(\ntext, border_style=\"green\", title=os.path.join(self.path, name + \".npz\")\n)\nelse:\npanel = Panel.fit(\n\"No metadata\",\nborder_style=\"red\",\ntitle=os.path.join(self.path, name + \".npz\"),\n)\nwith Console() as console:\nconsole.print(panel)\nreturn None\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.view","title":"<code>view</code>  <code>property</code>","text":"<p>Summary</p>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.__init__","title":"<code>__init__(path)</code>","text":"<p>Initialize the Folder object</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the folder</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def __init__(self, path):  # , exclude=(), max_depth=4):\n\"\"\"Initialize the Folder object\n    Parameters\n    ----------\n    path : str\n        Path to the folder\n    \"\"\"\npath = path.rstrip(\"/\")\nself.path = path\nself.name = os.path.basename(path)\nself._basic_view = Tree(\n\":open_file_folder: {}\".format(self.name), guide_style=\"blue\"\n)\nself._full_view = None\n# Search sub-folders\nsubfolds = [\nf.path\nfor f in os.scandir(path)\nif f.is_dir() and not f.name.startswith(\".\")\n]\nsubfolds.sort()\nself.subfolds = {}\nfor s in subfolds:\nsub = os.path.basename(s)\nself.subfolds[sub] = Folder(s)\nself._basic_view.add(\":open_file_folder: [blue]\" + sub)\n# Search files\nself.npz_files = _find_files(path, \"npz\")\nself.nwb_files = _find_files(path, \"nwb\")\nfor filename, file in self.npz_files.items():\nself._basic_view.add(\"[green]\" + file.name + \" \\t|\\t \" + file.type)\nfor file in self.nwb_files.values():\nself._basic_view.add(\"[magenta]\" + file.name + \" \\t|\\t NWB file\")\n# Putting everything together\nself.data = {**self.npz_files, **self.nwb_files, **self.subfolds}\nUserDict.__init__(self, self.data)\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.__str__","title":"<code>__str__()</code>","text":"<p>View of the object</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def __str__(self):\n\"\"\"View of the object\"\"\"\nwith Console() as console:\nconsole.print(self._basic_view)\nreturn \"\"\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get subfolder or load file.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required <p>Returns:</p> Type Description <code>(Ts, Tsd, TsdFrame, TsGroup, IntervalSet, Folder or NWBFile)</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If key is not in the dictionnary</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def __getitem__(self, key):\n\"\"\"Get subfolder or load file.\n    Parameters\n    ----------\n    key : str\n    Returns\n    -------\n    (Ts, Tsd, TsdFrame, TsGroup, IntervalSet, Folder or NWBFile)\n    Raises\n    ------\n    KeyError\n        If key is not in the dictionnary\n    \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], NPZFile):\ndata = self.data[key].load()\nself.data[key] = data\n# setattr(self, key, data)\nreturn data\nelif isinstance(self.data[key], NWBFile):\nreturn self.data[key]\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.expand","title":"<code>expand()</code>","text":"<p>Display the full tree view. Equivalent to Folder.view</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def expand(self):\n\"\"\"Display the full tree view. Equivalent to Folder.view\"\"\"\nif not isinstance(self._full_view, Tree):\nself._generate_tree_view()\nwith Console() as console:\nconsole.print(self._full_view)\nreturn None\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.save","title":"<code>save(name, obj, description='')</code>","text":"<p>Save a pynapple object in the folder in a single file in uncompressed <code>.npz</code> format. By default, the save function overwrite previously save file with the same name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Filename</p> required <code>obj</code> <code>Ts, Tsd, TsdFrame, TsGroup or IntervalSet</code> <p>Pynapple object.</p> required <code>description</code> <code>str, optional</code> <p>Metainformation added as a json sidecar.</p> <code>''</code> Source code in <code>pynapple/io/folder.py</code> <pre><code>def save(self, name, obj, description=\"\"):\n\"\"\"Save a pynapple object in the folder in a single file in uncompressed ``.npz`` format.\n    By default, the save function overwrite previously save file with the same name.\n    Parameters\n    ----------\n    name : str\n        Filename\n    obj : Ts, Tsd, TsdFrame, TsGroup or IntervalSet\n        Pynapple object.\n    description : str, optional\n        Metainformation added as a json sidecar.\n    \"\"\"\nfilepath = os.path.join(self.path, name)\nobj.save(filepath)\nself.npz_files[name] = NPZFile(filepath + \".npz\")\nself.data[name] = obj\nmetadata = {\"time\": str(datetime.now()), \"info\": str(description)}\nwith open(os.path.join(self.path, name + \".json\"), \"w\") as ff:\njson.dump(metadata, ff, indent=2)\n# regenerate the tree view\nself._generate_tree_view()\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.load","title":"<code>load()</code>","text":"<p>Load all compatible NPZ files.</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def load(self):\n\"\"\"Load all compatible NPZ files.\"\"\"\nfor k in self.npz_files.keys():\nself[k] = self.npz_files[k].load()\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.info","title":"<code>info(name)</code>","text":"<p>Display the metadata within the json sidecar of a NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the npz file</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def info(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n    Parameters\n    ----------\n    name : str\n        Name of the npz file\n    \"\"\"\nself.metadata(name)\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.doc","title":"<code>doc(name)</code>","text":"<p>Display the metadata within the json sidecar of a NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the npz file</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def doc(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n    Parameters\n    ----------\n    name : str\n        Name of the npz file\n    \"\"\"\nself.metadata(name)\n</code></pre>"},{"location":"old_pages/io.folder/#pynapple.io.folder.Folder.metadata","title":"<code>metadata(name)</code>","text":"<p>Display the metadata within the json sidecar of a NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the npz file</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def metadata(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n    Parameters\n    ----------\n    name : str\n        Name of the npz file\n    \"\"\"\n# Search for json first\njson_filename = os.path.join(self.path, name + \".json\")\nif os.path.isfile(json_filename):\nwith open(json_filename, \"r\") as ff:\nmetadata = json.load(ff)\ntext = \"\\n\".join([\" : \".join(it) for it in metadata.items()])\npanel = Panel.fit(\ntext, border_style=\"green\", title=os.path.join(self.path, name + \".npz\")\n)\nelse:\npanel = Panel.fit(\n\"No metadata\",\nborder_style=\"red\",\ntitle=os.path.join(self.path, name + \".npz\"),\n)\nwith Console() as console:\nconsole.print(panel)\nreturn None\n</code></pre>"},{"location":"old_pages/io.loader/","title":"Io.loader","text":"<p>BaseLoader is the general class for loading session with pynapple.</p> <p>@author: Guillaume Viejo</p>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader","title":"<code>BaseLoader</code>","text":"<p>         Bases: <code>object</code></p> <p>General loader for epochs and tracking data</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>class BaseLoader(object):\n\"\"\"\n    General loader for epochs and tracking data\n    \"\"\"\ndef __init__(self, path=None):\nself.path = path\nstart_gui = True\n# Check if a pynapplenwb folder exist to bypass GUI\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nstart_gui = False\nself.load_data(path)\n# Starting the GUI\nif start_gui:\nwarnings.warn(\nget_deprecation_text(), category=DeprecationWarning, stacklevel=2\n)\napp = App()\nwindow = BaseLoaderGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# Extracting all the information from gui loader\nif window.status:\nself.session_information = window.session_information\nself.subject_information = window.subject_information\nself.name = self.session_information[\"name\"]\nself.tracking_frequency = window.tracking_frequency\nself.position = self._make_position(\nwindow.tracking_parameters,\nwindow.tracking_method,\nwindow.tracking_frequency,\nwindow.epochs,\nwindow.time_units_epochs,\nwindow.tracking_alignment,\n)\nself.epochs = self._make_epochs(window.epochs, window.time_units_epochs)\nself.time_support = self._join_epochs(\nwindow.epochs, window.time_units_epochs\n)\n# Save the data\nself.create_nwb_file(path)\ndef load_default_csv(self, csv_file):\n\"\"\"\n        Load tracking data. The default csv should have the time index in the first column in seconds.\n        If no header is provided, the column names will be the column index.\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\ndef load_optitrack_csv(self, csv_file):\n\"\"\"\n        Load tracking data exported with Optitrack.\n        By default, the function reads rows 4 and 5 to build the column names.\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Raises\n        ------\n        RuntimeError\n            If header names are unknown. Should be 'Position' and 'Rotation'\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\ndef load_dlc_csv(self, csv_file):\n\"\"\"\n        Load tracking data exported with DeepLabCut\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\ndef load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n        Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n        Parameters\n        ----------\n        ttl_file : str\n            File name\n        n_channels : int, optional\n            The number of channels in the binary file.\n        channel : int, optional\n            Which channel contains the TTL\n        bytes_size : int, optional\n            Bytes size of the binary file.\n        fs : float, optional\n            Sampling frequency of the binary file\n        Returns\n        -------\n        pd.Series\n            A series containing the time index of the TTL.\n        \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\ndef _make_position(\nself, parameters, method, frequency, epochs, time_units, alignment\n):\n\"\"\"\n        Make the position TSDFrame with the parameters extracted from the GUI.\n        \"\"\"\nif len(parameters.index) == 0:\nreturn None\nelse:\nif len(epochs) == 0:\nepochs.loc[0, \"start\"] = 0.0\nframes = []\ntime_supports_starts = []\ntime_support_ends = []\nfor i in range(len(parameters)):\nif method.lower() == \"optitrack\":\nposition = self.load_optitrack_csv(parameters.loc[i, \"csv\"])\nelif method.lower() == \"deep lab cut\":\nposition = self.load_dlc_csv(parameters.loc[i, \"csv\"])\nelif method.lower() == \"default\":\nposition = self.load_default_csv(parameters.loc[i, \"csv\"])\nif alignment.lower() == \"local\":\nstart_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"start\"], time_units\n)\nend_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"end\"], time_units\n)\ntimestamps = (\nposition.index.values\n+ nap.return_timestamps(start_epoch, \"s\")[0]\n)\n# Make sure timestamps are within the epochs\nidx = np.where(timestamps &lt; end_epoch)[0]\nposition = position.iloc[idx]\nposition.index = pd.Index(timestamps[idx])\nif alignment.lower() == \"ttl\":\nttl = self.load_ttl_pulse(\nttl_file=parameters.loc[i, \"ttl\"],\ntracking_frequency=frequency,\nn_channels=int(parameters.loc[i, \"n_channels\"]),\nchannel=int(parameters.loc[i, \"tracking_channel\"]),\nbytes_size=int(parameters.loc[i, \"bytes_size\"]),\nfs=float(parameters.loc[i, \"fs\"]),\nthreshold=float(parameters.loc[i, \"threshold\"]),\n)\nif len(ttl):\nlength = np.minimum(len(ttl), len(position))\nttl = ttl.iloc[0:length]\nposition = position.iloc[0:length]\nelse:\nraise RuntimeError(\n\"No ttl detected for {}\".format(parameters.loc[i, \"ttl\"])\n)\n# Make sure start epochs in seconds\n# start_epoch = format_timestamp(\n#     epochs.loc[parameters.loc[f, \"epoch\"], \"start\"], time_units\n# )\nstart_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"start\"], time_units\n)\ntimestamps = start_epoch + ttl.index.values\nposition.index = pd.Index(timestamps)\nframes.append(position)\ntime_supports_starts.append(position.index[0])\ntime_support_ends.append(position.index[-1])\nposition = pd.concat(frames)\ntime_supports = nap.IntervalSet(\nstart=time_supports_starts, end=time_support_ends, time_units=\"s\"\n)\n# Specific to optitrACK\nif set([\"rx\", \"ry\", \"rz\"]).issubset(position.columns):\nposition[[\"ry\", \"rx\", \"rz\"]] *= np.pi / 180\nposition[[\"ry\", \"rx\", \"rz\"]] += 2 * np.pi\nposition[[\"ry\", \"rx\", \"rz\"]] %= 2 * np.pi\nposition = nap.TsdFrame(\nt=position.index.values,\nd=position.values,\ncolumns=position.columns.values,\ntime_support=time_supports,\ntime_units=\"s\",\n)\nreturn position\ndef _make_epochs(self, epochs, time_units=\"s\"):\n\"\"\"\n        Split GUI epochs into dict of epochs\n        \"\"\"\nlabels = epochs.groupby(\"label\").groups\nisets = {}\nfor lbs in labels.keys():\ntmp = epochs.loc[labels[lbs]]\nisets[lbs] = nap.IntervalSet(\nstart=tmp[\"start\"], end=tmp[\"end\"], time_units=time_units\n)\nreturn isets\ndef _join_epochs(self, epochs, time_units=\"s\"):\n\"\"\"\n        To create the global time support of the data\n        \"\"\"\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nisets = nap.IntervalSet(\nstart=epochs[\"start\"].sort_values(),\nend=epochs[\"end\"].sort_values(),\ntime_units=time_units,\n)\niset = isets.merge_close_intervals(1, time_units=\"us\")\nif len(iset):\nreturn iset\nelse:\nreturn None\ndef create_nwb_file(self, path):\n\"\"\"\n        Initialize the NWB file in the folder pynapplenwb within the data folder.\n        Parameters\n        ----------\n        path : str\n            The path to save the data\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\ndef load_data(self, path):\n\"\"\"\n        Load NWB data save with pynapple in the pynapplenwb folder\n        Parameters\n        ----------\n        path : str\n            Path to the session folder\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\ndef save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n        Add epochs to the NWB file (e.g. ripples epochs)\n        See pynwb.epoch.TimeIntervals\n        Parameters\n        ----------\n        iset : IntervalSet\n            The intervalSet to save\n        name : str\n            The name in the nwb file\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\ndef save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n        Save timestamps in the NWB file (e.g. ripples time) with the time support.\n        See pynwb.base.TimeSeries\n        Parameters\n        ----------\n        tsd : TsdFrame\n            _\n        name : str\n            _\n        description : str, optional\n            _\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_intervals(self, name):\n\"\"\"\n        Load epochs from the NWB file (e.g. 'ripples')\n        Parameters\n        ----------\n        name : str\n            The name in the nwb file\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\ndef load_nwb_timeseries(self, name):\n\"\"\"\n        Load timestamps in the NWB file (e.g. ripples time)\n        Parameters\n        ----------\n        name : str\n            _\n        Returns\n        -------\n        Tsd\n            _\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_default_csv","title":"<code>load_default_csv(csv_file)</code>","text":"<p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_optitrack_csv","title":"<code>load_optitrack_csv(csv_file)</code>","text":"<p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_dlc_csv","title":"<code>load_dlc_csv(csv_file)</code>","text":"<p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_ttl_pulse","title":"<code>load_ttl_pulse(ttl_file, tracking_frequency, n_channels=1, channel=0, bytes_size=2, fs=20000.0, threshold=0.3)</code>","text":"<p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int, optional</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int, optional</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float, optional</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>pd.Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.create_nwb_file","title":"<code>create_nwb_file(path)</code>","text":"<p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_data","title":"<code>load_data(path)</code>","text":"<p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.save_nwb_intervals","title":"<code>save_nwb_intervals(iset, name, description='')</code>","text":"<p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.save_nwb_timeseries","title":"<code>save_nwb_timeseries(tsd, name, description='')</code>","text":"<p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str, optional</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_nwb_intervals","title":"<code>load_nwb_intervals(name)</code>","text":"<p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.loader/#pynapple.io.loader.BaseLoader.load_nwb_timeseries","title":"<code>load_nwb_timeseries(name)</code>","text":"<p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"old_pages/io/","title":"Io","text":"<p>Various io functions</p>"},{"location":"old_pages/io/#pynapple.io.misc.load_file","title":"<code>load_file(path)</code>","text":"<p>Load file. Current format supported is (npz,nwb,)</p> <p>.npz -&gt; If the file is compatible with a pynapple format, the function will return a pynapple object. Otherwise, the function will return the output of numpy.load</p> <p>.nwb -&gt; Return the pynapple.io.NWBFile class wrapping the NWBFile</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file</p> required <p>Returns:</p> Type Description <code>Tsd, TsdFrame, Ts, IntervalSet, TsGroup, pynapple.io.NWBFile</code> <p>One of the 5 pynapple objects or pynapple.io.NWBFile</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file is missing</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_file(path):\n\"\"\"Load file. Current format supported is (npz,nwb,)\n    .npz -&gt; If the file is compatible with a pynapple format, the function will return a pynapple object.\n    Otherwise, the function will return the output of numpy.load\n    .nwb -&gt; Return the pynapple.io.NWBFile class wrapping the NWBFile\n    Parameters\n    ----------\n    path : str\n        Path to the file\n    Returns\n    -------\n    (Tsd, TsdFrame, Ts, IntervalSet, TsGroup, pynapple.io.NWBFile)\n        One of the 5 pynapple objects or pynapple.io.NWBFile\n    Raises\n    ------\n    FileNotFoundError\n        If file is missing\n    \"\"\"\nif os.path.isfile(path):\nif path.endswith(\".npz\"):\nreturn NPZFile(path).load()\nelif path.endswith(\".nwb\"):\nreturn NWBFile(path)\nelse:\nraise RuntimeError(\"File format not supported\")\nelse:\nraise FileNotFoundError(\"File {} does not exist\".format(path))\n</code></pre>"},{"location":"old_pages/io/#pynapple.io.misc.load_folder","title":"<code>load_folder(path)</code>","text":"<p>Load folder containing files or other folder. Pynapple will walk throught the subfolders to detect compatible npz files or nwb files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the folder</p> required <p>Returns:</p> Type Description <code>Folder</code> <p>A dictionnary-like class containing all the sub-folders and compatible files (i.e. npz, nwb)</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If folder is missing</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_folder(path):\n\"\"\"Load folder containing files or other folder.\n    Pynapple will walk throught the subfolders to detect compatible npz files\n    or nwb files.\n    Parameters\n    ----------\n    path : str\n        Path to the folder\n    Returns\n    -------\n    Folder\n        A dictionnary-like class containing all the sub-folders and compatible files (i.e. npz, nwb)\n    Raises\n    ------\n    RuntimeError\n        If folder is missing\n    \"\"\"\nif os.path.isdir(path):\nreturn Folder(path)\nelse:\nraise RuntimeError(\"Folder {} does not exist\".format(path))\n</code></pre>"},{"location":"old_pages/io/#pynapple.io.misc.load_session","title":"<code>load_session(path=None, session_type=None)</code>","text":"<p>%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % WARNING : THIS FUNCTION IS DEPRECATED % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% General Loader for</p> <ul> <li> <p>Neurosuite</p> </li> <li> <p>Phy</p> </li> <li> <p>Minian</p> </li> <li> <p>Inscopix-cnmfe</p> </li> <li> <p>Matlab-cnmfe</p> </li> <li> <p>Suite2p</p> </li> <li>None for default session.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str, optional</code> <p>The path to load the data</p> <code>None</code> <code>session_type</code> <code>str, optional</code> <p>Can be 'neurosuite', 'phy', 'minian', 'inscopix-cnmfe', 'cnmfe-matlab', 'suite2p' or None for default loader.</p> <code>None</code> <p>Returns:</p> Type Description <code>Session</code> <p>A class holding all the data from the session.</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_session(path=None, session_type=None):\n\"\"\"\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    % WARNING : THIS FUNCTION IS DEPRECATED %\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    General Loader for\n    - Neurosuite\\n\n    - Phy\\n\n    - Minian\\n\n    - Inscopix-cnmfe\\n\n    - Matlab-cnmfe\\n\n    - Suite2p\n    - None for default session.\n    Parameters\n    ----------\n    path : str, optional\n        The path to load the data\n    session_type : str, optional\n        Can be 'neurosuite', 'phy',\n        'minian', 'inscopix-cnmfe', 'cnmfe-matlab',\n        'suite2p' or None for default loader.\n    Returns\n    -------\n    Session\n        A class holding all the data from the session.\n    \"\"\"\nif path:\nif not os.path.isdir(path):\nraise RuntimeError(\"Path {} is not found.\".format(path))\nif isinstance(session_type, str):\nsession_type = session_type.lower()\nif session_type == \"neurosuite\":\nreturn NeuroSuite(path)\nelif session_type == \"phy\":\nreturn Phy(path)\nelif session_type == \"inscopix-cnmfe\":\nreturn InscopixCNMFE(path)\nelif session_type == \"minian\":\nreturn Minian(path)\nelif session_type == \"cnmfe-matlab\":\nreturn CNMF_E(path)\nelif session_type == \"suite2p\":\nreturn Suite2P(path)\nelse:\nreturn BaseLoader(path)\n</code></pre>"},{"location":"old_pages/io/#pynapple.io.misc.load_eeg","title":"<code>load_eeg(filepath, channel=None, n_channels=None, frequency=None, precision='int16', bytes_size=2)</code>","text":"<p>Standalone function to load eeg/lfp/dat file in binary format.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the eeg file</p> required <code>channel</code> <code>int or list of int, optional</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>n_channels</code> <code>int, optional</code> <p>Number of channels</p> <code>None</code> <code>frequency</code> <code>float, optional</code> <p>Sampling rate of the file</p> <code>None</code> <code>precision</code> <code>str, optional</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the binary file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p>"},{"location":"old_pages/io/#pynapple.io.misc.load_eeg--deleted-parameters","title":"Deleted Parameters","text":"<p>extension : str, optional     The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_eeg(\nfilepath,\nchannel=None,\nn_channels=None,\nfrequency=None,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Standalone function to load eeg/lfp/dat file in binary format.\n    Parameters\n    ----------\n    filepath : str\n        The path to the eeg file\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    n_channels : int, optional\n        Number of channels\n    frequency : float, optional\n        Sampling rate of the file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the binary file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    Deleted Parameters\n    ------------------\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    \"\"\"\n# Need to check if a xml file exists\npath = os.path.dirname(filepath)\nbasename = os.path.basename(filepath).split(\".\")[0]\nlistdir = os.listdir(path)\nif frequency is None or n_channels is None:\nif basename + \".xml\" in listdir:\nxmlpath = os.path.join(path, basename + \".xml\")\nxmldoc = minidom.parse(xmlpath)\nelse:\nraise RuntimeError(\n\"Can't find xml file; please specify sampling frequency or number of channels\"\n)\nif frequency is None:\nif filepath.endswith(\".dat\"):\nfs_dat = int(\nxmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nfrequency = fs_dat\nelif filepath.endswith((\".lfp\", \".eeg\")):\nfs_eeg = int(\nxmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nfrequency = fs_eeg\nif n_channels is None:\nn_channels = int(\nxmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn fp\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"old_pages/io/#pynapple.io.misc.append_NWB_LFP","title":"<code>append_NWB_LFP(path, lfp, channel=None)</code>","text":"<p>Standalone function for adding lfp/eeg to already existing nwb files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data. The function will looks for a nwb file in path or in path/pynapplenwb.</p> required <code>lfp</code> <code>Tsd or TsdFrame</code> <p>Description</p> required <code>channel</code> <code>None, optional</code> <p>channel number in int ff lfp is a Tsd</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the nwb file </p> <p>If no channel is specify when passing a Tsd</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def append_NWB_LFP(path, lfp, channel=None):\n\"\"\"Standalone function for adding lfp/eeg to already existing nwb files.\n    Parameters\n    ----------\n    path : str\n        The path to the data. The function will looks for a nwb file in path\n        or in path/pynapplenwb.\n    lfp : Tsd or TsdFrame\n        Description\n    channel : None, optional\n        channel number in int ff lfp is a Tsd\n    Raises\n    ------\n    RuntimeError\n        If can't find the nwb file \\n\n        If no channel is specify when passing a Tsd\n    \"\"\"\nnew_path = os.path.join(path, \"pynapplenwb\")\nnwb_path = \"\"\nif os.path.exists(new_path):\nnwbfilename = [f for f in os.listdir(new_path) if f.endswith(\".nwb\")]\nif len(nwbfilename):\nnwb_path = os.path.join(path, \"pynapplenwb\", nwbfilename[0])\nelse:\nnwbfilename = [f for f in os.listdir(path) if f.endswith(\".nwb\")]\nif len(nwbfilename):\nnwb_path = os.path.join(path, \"pynapplenwb\", nwbfilename[0])\nif len(nwb_path) == 0:\nraise RuntimeError(\"Can't find nwb file in {}\".format(path))\nif isinstance(lfp, nap.TsdFrame):\nchannels = lfp.columns.values\nelif isinstance(lfp, nap.Tsd):\nif isinstance(channel, int):\nchannels = [channel]\nelse:\nraise RuntimeError(\"Please specify which channel it is.\")\nio = NWBHDF5IO(nwb_path, \"r+\")\nnwbfile = io.read()\nall_table_region = nwbfile.create_electrode_table_region(\nregion=channels, description=\"\", name=\"electrodes\"\n)\nlfp_electrical_series = ElectricalSeries(\nname=\"ElectricalSeries\",\ndata=lfp.values,\ntimestamps=lfp.index.values,\nelectrodes=all_table_region,\n)\nlfp = LFP(electrical_series=lfp_electrical_series)\necephys_module = nwbfile.create_processing_module(\nname=\"ecephys\", description=\"processed extracellular electrophysiology data\"\n)\necephys_module.add(lfp)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/","title":"Io.neurosuite","text":"<p>Class and functions for loading data processed with the Neurosuite (Klusters, Neuroscope, NDmanager)</p> <p>@author: Guillaume Viejo</p>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite","title":"<code>NeuroSuite</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for kluster data</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>class NeuroSuite(BaseLoader):\n\"\"\"\n    Loader for kluster data\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Instantiate the data class from a neurosuite folder.\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nself.time_support = None\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_neurosuite = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_nwb_spikes(path)\nif success:\nloading_neurosuite = False\n# Bypass if data have already been transfered to nwb\nif loading_neurosuite:\nself.load_neurosuite_xml(path)\n# print(\"XML loaded\")\n# To label the electrodes groups\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.group_to_channel)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# print(\"GUI DONE\")\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_neurosuite_spikes(path, self.basename, self.time_support)\nself.save_data(path)\ndef load_neurosuite_spikes(self, path, basename, time_support=None, fs=20000.0):\n\"\"\"\n        Read the clus and res files and convert to NWB.\n        Instantiate automatically a TsGroup object.\n        Parameters\n        ----------\n        path : str\n            The path to the data\n        basename : str\n            Basename of the clu and res files.\n        time_support : IntevalSet, optional\n            The time support of the data\n        fs : float, optional\n            Sampling rate of the recording.\n        Raises\n        ------\n        RuntimeError\n            If number of clu and res are not equal.\n        \"\"\"\nfiles = os.listdir(path)\nclu_files = np.sort([f for f in files if \".clu.\" in f and f[0] != \".\"])\nres_files = np.sort([f for f in files if \".res.\" in f and f[0] != \".\"])\nclu1 = np.sort([int(f.split(\".\")[-1]) for f in clu_files])\nclu2 = np.sort([int(f.split(\".\")[-1]) for f in res_files])\nif len(clu_files) != len(res_files) or not (clu1 == clu2).any():\nraise RuntimeError(\n\"Not the same number of clu and res files in \" + path + \"; Exiting ...\"\n)\ncount = 0\nspikes = {}\ngroup = pd.Series(dtype=np.int32)\nfor i, s in zip(range(len(clu_files)), clu1):\nclu = np.genfromtxt(\nos.path.join(path, basename + \".clu.\" + str(s)), dtype=np.int32\n)[1:]\nif np.max(clu) &gt; 1:  # getting rid of mua and noise\nres = np.genfromtxt(os.path.join(path, basename + \".res.\" + str(s)))\ntmp = np.unique(clu).astype(int)\nidx_clu = tmp[tmp &gt; 1]\nidx_out = np.arange(count, count + len(idx_clu))\nfor j, k in zip(idx_clu, idx_out):\nt = res[clu == j] / fs\nspikes[k] = nap.Ts(t=t, time_units=\"s\")\ngroup.loc[k] = s\ncount += len(idx_clu)\ngroup = group - 1  # better to start it a 0\nself.spikes = nap.TsGroup(\nspikes, time_support=time_support, time_units=\"s\", group=group\n)\n# adding some information to help parse the neurons\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\ndef load_neurosuite_xml(self, path):\n\"\"\"\n        path should be the folder session containing the XML file\n        Function reads :\n        1. the number of channels\n        2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder\n            eeg file first if both are present or both are absent\n        3. the mappings shanks to channels as a dict\n        Parameters\n        ----------\n        path: str\n            The path to the data\n        Raises\n        ------\n        RuntimeError\n            If path does not contain the xml file.\n        \"\"\"\nlistdir = os.listdir(path)\nxmlfiles = [f for f in listdir if f.endswith(\".xml\")]\nif not len(xmlfiles):\nraise RuntimeError(\"Path {} contains no xml files;\".format(path))\nsys.exit()\nnew_path = os.path.join(path, xmlfiles[0])\nself.xmldoc = minidom.parse(new_path)\nself.nChannels = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nself.fs_dat = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nself.fs_eeg = int(\nself.xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nself.group_to_channel = {}\ngroups = (\nself.xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n.getElementsByTagName(\"channelGroups\")[0]\n.getElementsByTagName(\"group\")\n)\nfor i in range(len(groups)):\nself.group_to_channel[i] = np.array(\n[\nint(child.firstChild.data)\nfor child in groups[i].getElementsByTagName(\"channel\")\n]\n)\nreturn\ndef save_data(self, path):\n\"\"\"\n        Save the data to NWB format.\n        Parameters\n        ----------\n        path : str\n            The path to save the data\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.group_to_channel:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.group_to_channel[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_spikes(self, path):\n\"\"\"\n        Read the NWB spikes to extract the spike times.\n        Parameters\n        ----------\n        path : str\n            The path to the data\n        Returns\n        -------\n        TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn False\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\ndef load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n        Load the LFP.\n        Parameters\n        ----------\n        filename : str, optional\n            The filename of the lfp file.\n            It can be useful it multiple dat files are present in the data directory\n        channel : int or list of int, optional\n            The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n        extension : str, optional\n            The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n        frequency : float, optional\n            Default 1250 Hz for the eeg file\n        precision : str, optional\n            The precision of the binary file\n        bytes_size : int, optional\n            Bytes size of the lfp file\n        Raises\n        ------\n        RuntimeError\n            If can't find the lfp/eeg/dat file\n        Returns\n        -------\n        Tsd or TsdFrame\n            The lfp in a time series format\n        \"\"\"\nif filename is not None:\nfilepath = os.path.join(self.path, filename)\nelse:\nlistdir = os.listdir(self.path)\neegfile = [f for f in listdir if f.endswith(extension)]\nif not len(eegfile):\nraise RuntimeError(\n\"Path {} contains no {} files;\".format(self.path, extension)\n)\nfilepath = os.path.join(self.path, eegfile[0])\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\ndef read_neuroscope_intervals(self, name=None, path2file=None):\n\"\"\"\n        This function reads .evt files in which odd raws indicate the beginning\n        of the time series and the even raws are the ends.\n        If the file is present in the nwb, provide the just the name. If the file\n        is not present in the nwb, it loads the events from the nwb directory.\n        If just the path is provided but not the name, it takes the name from the file.\n        Parameters\n        ----------\n        name: str\n            name of the epoch in the nwb file, e.g. \"rem\" or desired name save\n            the data in the nwb.\n        path2file: str\n            Path of the file you want to load.\n        Returns\n        -------\n        IntervalSet\n            Contains two columns corresponding to the start and end of the intervals.\n        \"\"\"\nif name:\nisets = self.load_nwb_intervals(name)\nif isinstance(isets, nap.IntervalSet):\nreturn isets\nif name is not None and path2file is None:\npath2file = os.path.join(self.path, self.basename + \".\" + name + \".evt\")\nif path2file is not None:\ntry:\n# df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None)\ntmp = np.genfromtxt(path2file)[:, 0]\ndf = tmp.reshape(len(tmp) // 2, 2)\nexcept ValueError:\nprint(\"specify a valid name\")\nisets = nap.IntervalSet(df[:, 0], df[:, 1], time_units=\"ms\")\nif name is None:\nname = path2file.split(\".\")[-2]\nprint(\"*** saving file in the nwb as\", name)\nself.save_nwb_intervals(isets, name)\nelse:\nraise ValueError(\"specify a valid path\")\nreturn isets\ndef write_neuroscope_intervals(self, extension, isets, name):\n\"\"\"Write events to load with neuroscope (e.g. ripples start and ends)\n        Parameters\n        ----------\n        extension : str\n            The extension of the file (e.g. basename.evt.py.rip)\n        isets : IntervalSet\n            The IntervalSet to write\n        name : str\n            The name of the events (e.g. Ripples)\n        \"\"\"\nstart = isets.as_units(\"ms\")[\"start\"].values\nends = isets.as_units(\"ms\")[\"end\"].values\ndatatowrite = np.vstack((start, ends)).T.flatten()\nn = len(isets)\ntexttowrite = np.vstack(\n(\n(np.repeat(np.array([name + \" start\"]), n)),\n(np.repeat(np.array([name + \" end\"]), n)),\n)\n).T.flatten()\nevt_file = os.path.join(self.path, self.basename + extension)\nf = open(evt_file, \"w\")\nfor t, n in zip(datatowrite, texttowrite):\nf.writelines(\"{:1.6f}\".format(t) + \"\\t\" + n + \"\\n\")\nf.close()\nreturn\ndef load_mean_waveforms(self, epoch=None, waveform_window=None, spike_count=1000):\n\"\"\"\n        Load the mean waveforms from a dat file.\n        Parameters\n        ----------\n        epoch : IntervalSet\n            default = None\n            Restrict spikes to an epoch.\n        waveform_window : IntervalSet\n            default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms')\n            Limit waveform extraction before and after spike time\n        spike_count : int\n            default = 1000\n            Number of spikes used per neuron for the calculation of waveforms\n        Returns\n        -------\n        dictionary\n            the waveforms for all neurons\n        pandas.Series\n            the channel with the maximum waveform for each neuron\n        \"\"\"\nif not isinstance(waveform_window, nap.IntervalSet):\nwaveform_window = nap.IntervalSet(start=-0.5, end=1, time_units=\"ms\")\nspikes = self.spikes\nif not os.path.exists(self.path):  # check if path exists\nprint(\"The path \" + self.path + \" doesn't exist; Exiting ...\")\nsys.exit()\n# Load XML INFO\nself.load_neurosuite_xml(self.path)\nn_channels = self.nChannels\nfs = self.fs_dat\ngroup_to_channel = self.group_to_channel\ngroup = spikes.get_info(\"group\")\n# Check if there is an epoch, restrict spike times to epoch\nif epoch is not None:\nif type(epoch) is not nap.IntervalSet:\nprint(\"Epoch must be an IntervalSet\")\nsys.exit()\nelse:\nprint(\"Restricting spikes to epoch\")\nspikes = spikes.restrict(epoch)\nepstart = int(epoch.as_units(\"s\")[\"start\"].values[0] * fs)\nepend = int(epoch.as_units(\"s\")[\"end\"].values[0] * fs)\n# Find dat file\nfiles = os.listdir(self.path)\ndat_files = np.sort([f for f in files if \"dat\" in f and f[0] != \".\"])\n# Need n_samples collected in the entire recording from dat file to load\nfile = os.path.join(self.path, dat_files[0])\nf = open(\nfile, \"rb\"\n)  # open file to get number of samples collected in the entire recording\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\n# map to memory all samples for all channels, channels are numbered according to neuroscope number\nfp = np.memmap(file, np.int16, \"r\", shape=(n_samples, n_channels))\n# convert spike times to spikes in sample number\nsample_spikes = {\nneuron: (spikes[neuron].as_units(\"s\").index.values * fs).astype(\"int\")\nfor neuron in spikes\n}\n# prep for waveforms\noverlap = int(\nwaveform_window.tot_length(time_units=\"s\")\n)  # one spike's worth of overlap between windows\nwaveform_window = abs(np.array(waveform_window.as_units(\"s\"))[0] * fs).astype(\nint\n)  # convert time to sample number\nneuron_waveforms = {\nn: np.zeros([np.sum(waveform_window), len(group_to_channel[group[n]])])\nfor n in sample_spikes\n}\n# divide dat file into batches that slightly overlap for faster loading\nbatch_size = 3000000\nwindows = np.arange(0, int(endoffile / n_channels / bytes_size), batch_size)\nif epoch is not None:\nprint(\"Restricting dat file to epoch\")\nwindows = windows[(windows &gt;= epstart) &amp; (windows &lt;= epend)]\nbatches = []\nfor (\ni\n) in windows:  # make overlapping batches from the beginning to end of recording\nif i == windows[-1]:  # the last batch cannot overlap with the next one\nbatches.append([i, n_samples])\nelse:\nbatches.append([i, i + batch_size + overlap])\nbatches = [np.int32(batch) for batch in batches]\nsample_counted_spikes = {}\nfor index, neuron in enumerate(sample_spikes):\nif len(sample_spikes[neuron]) &gt;= spike_count:\nsample_counted_spikes[neuron] = np.array(\nnp.random.choice(list(sample_spikes[neuron]), spike_count)\n)\nelif len(sample_spikes[neuron]) &lt; spike_count:\nprint(\n\"Not enough spikes in neuron \" + str(index) + \"... using all spikes\"\n)\nsample_counted_spikes[neuron] = sample_spikes[neuron]\n# Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file\nspike_check = np.array(\n[\nint(spikes_neuron)\nfor spikes_neuron in sample_counted_spikes[neuron]\nfor neuron in sample_counted_spikes\n]\n)\nfor index, timestep in enumerate(batches):\nprint(\nf\"Extracting waveforms from dat file: window {index+1} / {len(windows)}\",\nend=\"\\r\",\n)\nif (\nlen(\nspike_check[\n(timestep[0] &lt; spike_check) &amp; (timestep[1] &gt; spike_check)\n]\n)\n== 0\n):\ncontinue  # if there are no spikes for any neurons in this batch, skip and go to the next one\n# Load dat file for timestep\ntmp = pd.DataFrame(\ndata=fp[timestep[0] : timestep[1], :],\ncolumns=np.arange(n_channels),\nindex=range(timestep[0], timestep[1]),\n)  # load dat file\n# Check if any spikes are present\nfor neuron in sample_counted_spikes:\nneurontmp = sample_counted_spikes[neuron]\ntmp2 = neurontmp[(timestep[0] &lt; neurontmp) &amp; (timestep[1] &gt; neurontmp)]\nif len(neurontmp) == 0:\ncontinue  # skip neuron if it has no spikes in this batch\ntmpn = tmp[\ngroup_to_channel[group[neuron]]\n]  # restrict dat file to the channel group of the neuron\nfor time in tmp2:  # add each spike waveform to neuron_waveform\nspikewindow = tmpn.loc[\ntime - waveform_window[0] : time + waveform_window[1] - 1\n]  # waveform for this spike time\ntry:\nneuron_waveforms[neuron] += spikewindow.values\nexcept (\nException\n):  # ignore if full waveform is not present in this batch\npass\nmeanwf = {\nn: pd.DataFrame(\ndata=np.array(neuron_waveforms[n]) / spike_count,\ncolumns=np.arange(len(group_to_channel[group[n]])),\nindex=np.array(np.arange(-waveform_window[0], waveform_window[1])) / fs,\n)\nfor n in sample_counted_spikes\n}\n# find the max channel for each neuron\nmaxch = pd.Series(\ndata=[meanwf[n][meanwf[n].loc[0].idxmin()].name for n in meanwf],\nindex=spikes.keys(),\n)\nreturn meanwf, maxch\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.__init__","title":"<code>__init__(path)</code>","text":"<p>Instantiate the data class from a neurosuite folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Instantiate the data class from a neurosuite folder.\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nself.time_support = None\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_neurosuite = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_nwb_spikes(path)\nif success:\nloading_neurosuite = False\n# Bypass if data have already been transfered to nwb\nif loading_neurosuite:\nself.load_neurosuite_xml(path)\n# print(\"XML loaded\")\n# To label the electrodes groups\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.group_to_channel)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# print(\"GUI DONE\")\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_neurosuite_spikes(path, self.basename, self.time_support)\nself.save_data(path)\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_spikes","title":"<code>load_neurosuite_spikes(path, basename, time_support=None, fs=20000.0)</code>","text":"<p>Read the clus and res files and convert to NWB. Instantiate automatically a TsGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <code>basename</code> <code>str</code> <p>Basename of the clu and res files.</p> required <code>time_support</code> <code>IntevalSet, optional</code> <p>The time support of the data</p> <code>None</code> <code>fs</code> <code>float, optional</code> <p>Sampling rate of the recording.</p> <code>20000.0</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If number of clu and res are not equal.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_neurosuite_spikes(self, path, basename, time_support=None, fs=20000.0):\n\"\"\"\n    Read the clus and res files and convert to NWB.\n    Instantiate automatically a TsGroup object.\n    Parameters\n    ----------\n    path : str\n        The path to the data\n    basename : str\n        Basename of the clu and res files.\n    time_support : IntevalSet, optional\n        The time support of the data\n    fs : float, optional\n        Sampling rate of the recording.\n    Raises\n    ------\n    RuntimeError\n        If number of clu and res are not equal.\n    \"\"\"\nfiles = os.listdir(path)\nclu_files = np.sort([f for f in files if \".clu.\" in f and f[0] != \".\"])\nres_files = np.sort([f for f in files if \".res.\" in f and f[0] != \".\"])\nclu1 = np.sort([int(f.split(\".\")[-1]) for f in clu_files])\nclu2 = np.sort([int(f.split(\".\")[-1]) for f in res_files])\nif len(clu_files) != len(res_files) or not (clu1 == clu2).any():\nraise RuntimeError(\n\"Not the same number of clu and res files in \" + path + \"; Exiting ...\"\n)\ncount = 0\nspikes = {}\ngroup = pd.Series(dtype=np.int32)\nfor i, s in zip(range(len(clu_files)), clu1):\nclu = np.genfromtxt(\nos.path.join(path, basename + \".clu.\" + str(s)), dtype=np.int32\n)[1:]\nif np.max(clu) &gt; 1:  # getting rid of mua and noise\nres = np.genfromtxt(os.path.join(path, basename + \".res.\" + str(s)))\ntmp = np.unique(clu).astype(int)\nidx_clu = tmp[tmp &gt; 1]\nidx_out = np.arange(count, count + len(idx_clu))\nfor j, k in zip(idx_clu, idx_out):\nt = res[clu == j] / fs\nspikes[k] = nap.Ts(t=t, time_units=\"s\")\ngroup.loc[k] = s\ncount += len(idx_clu)\ngroup = group - 1  # better to start it a 0\nself.spikes = nap.TsGroup(\nspikes, time_support=time_support, time_units=\"s\", group=group\n)\n# adding some information to help parse the neurons\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_xml","title":"<code>load_neurosuite_xml(path)</code>","text":"<p>path should be the folder session containing the XML file</p> <p>Function reads : 1. the number of channels 2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder     eeg file first if both are present or both are absent 3. the mappings shanks to channels as a dict</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>The path to the data</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If path does not contain the xml file.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_neurosuite_xml(self, path):\n\"\"\"\n    path should be the folder session containing the XML file\n    Function reads :\n    1. the number of channels\n    2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder\n        eeg file first if both are present or both are absent\n    3. the mappings shanks to channels as a dict\n    Parameters\n    ----------\n    path: str\n        The path to the data\n    Raises\n    ------\n    RuntimeError\n        If path does not contain the xml file.\n    \"\"\"\nlistdir = os.listdir(path)\nxmlfiles = [f for f in listdir if f.endswith(\".xml\")]\nif not len(xmlfiles):\nraise RuntimeError(\"Path {} contains no xml files;\".format(path))\nsys.exit()\nnew_path = os.path.join(path, xmlfiles[0])\nself.xmldoc = minidom.parse(new_path)\nself.nChannels = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nself.fs_dat = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nself.fs_eeg = int(\nself.xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nself.group_to_channel = {}\ngroups = (\nself.xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n.getElementsByTagName(\"channelGroups\")[0]\n.getElementsByTagName(\"group\")\n)\nfor i in range(len(groups)):\nself.group_to_channel[i] = np.array(\n[\nint(child.firstChild.data)\nfor child in groups[i].getElementsByTagName(\"channel\")\n]\n)\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.save_data","title":"<code>save_data(path)</code>","text":"<p>Save the data to NWB format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def save_data(self, path):\n\"\"\"\n    Save the data to NWB format.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.group_to_channel:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.group_to_channel[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_nwb_spikes","title":"<code>load_nwb_spikes(path)</code>","text":"<p>Read the NWB spikes to extract the spike times.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_nwb_spikes(self, path):\n\"\"\"\n    Read the NWB spikes to extract the spike times.\n    Parameters\n    ----------\n    path : str\n        The path to the data\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn False\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_lfp","title":"<code>load_lfp(filename=None, channel=None, extension='.eeg', frequency=1250.0, precision='int16', bytes_size=2)</code>","text":"<p>Load the LFP.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str, optional</code> <p>The filename of the lfp file. It can be useful it multiple dat files are present in the data directory</p> <code>None</code> <code>channel</code> <code>int or list of int, optional</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>extension</code> <code>str, optional</code> <p>The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> <code>'.eeg'</code> <code>frequency</code> <code>float, optional</code> <p>Default 1250 Hz for the eeg file</p> <code>1250.0</code> <code>precision</code> <code>str, optional</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the lfp file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Load the LFP.\n    Parameters\n    ----------\n    filename : str, optional\n        The filename of the lfp file.\n        It can be useful it multiple dat files are present in the data directory\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    frequency : float, optional\n        Default 1250 Hz for the eeg file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the lfp file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    \"\"\"\nif filename is not None:\nfilepath = os.path.join(self.path, filename)\nelse:\nlistdir = os.listdir(self.path)\neegfile = [f for f in listdir if f.endswith(extension)]\nif not len(eegfile):\nraise RuntimeError(\n\"Path {} contains no {} files;\".format(self.path, extension)\n)\nfilepath = os.path.join(self.path, eegfile[0])\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.read_neuroscope_intervals","title":"<code>read_neuroscope_intervals(name=None, path2file=None)</code>","text":"<p>This function reads .evt files in which odd raws indicate the beginning of the time series and the even raws are the ends. If the file is present in the nwb, provide the just the name. If the file is not present in the nwb, it loads the events from the nwb directory. If just the path is provided but not the name, it takes the name from the file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>name of the epoch in the nwb file, e.g. \"rem\" or desired name save the data in the nwb.</p> <code>None</code> <p>path2file: str     Path of the file you want to load.</p> <p>Returns:</p> Type Description <code>IntervalSet</code> <p>Contains two columns corresponding to the start and end of the intervals.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def read_neuroscope_intervals(self, name=None, path2file=None):\n\"\"\"\n    This function reads .evt files in which odd raws indicate the beginning\n    of the time series and the even raws are the ends.\n    If the file is present in the nwb, provide the just the name. If the file\n    is not present in the nwb, it loads the events from the nwb directory.\n    If just the path is provided but not the name, it takes the name from the file.\n    Parameters\n    ----------\n    name: str\n        name of the epoch in the nwb file, e.g. \"rem\" or desired name save\n        the data in the nwb.\n    path2file: str\n        Path of the file you want to load.\n    Returns\n    -------\n    IntervalSet\n        Contains two columns corresponding to the start and end of the intervals.\n    \"\"\"\nif name:\nisets = self.load_nwb_intervals(name)\nif isinstance(isets, nap.IntervalSet):\nreturn isets\nif name is not None and path2file is None:\npath2file = os.path.join(self.path, self.basename + \".\" + name + \".evt\")\nif path2file is not None:\ntry:\n# df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None)\ntmp = np.genfromtxt(path2file)[:, 0]\ndf = tmp.reshape(len(tmp) // 2, 2)\nexcept ValueError:\nprint(\"specify a valid name\")\nisets = nap.IntervalSet(df[:, 0], df[:, 1], time_units=\"ms\")\nif name is None:\nname = path2file.split(\".\")[-2]\nprint(\"*** saving file in the nwb as\", name)\nself.save_nwb_intervals(isets, name)\nelse:\nraise ValueError(\"specify a valid path\")\nreturn isets\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.write_neuroscope_intervals","title":"<code>write_neuroscope_intervals(extension, isets, name)</code>","text":"<p>Write events to load with neuroscope (e.g. ripples start and ends)</p> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>The extension of the file (e.g. basename.evt.py.rip)</p> required <code>isets</code> <code>IntervalSet</code> <p>The IntervalSet to write</p> required <code>name</code> <code>str</code> <p>The name of the events (e.g. Ripples)</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def write_neuroscope_intervals(self, extension, isets, name):\n\"\"\"Write events to load with neuroscope (e.g. ripples start and ends)\n    Parameters\n    ----------\n    extension : str\n        The extension of the file (e.g. basename.evt.py.rip)\n    isets : IntervalSet\n        The IntervalSet to write\n    name : str\n        The name of the events (e.g. Ripples)\n    \"\"\"\nstart = isets.as_units(\"ms\")[\"start\"].values\nends = isets.as_units(\"ms\")[\"end\"].values\ndatatowrite = np.vstack((start, ends)).T.flatten()\nn = len(isets)\ntexttowrite = np.vstack(\n(\n(np.repeat(np.array([name + \" start\"]), n)),\n(np.repeat(np.array([name + \" end\"]), n)),\n)\n).T.flatten()\nevt_file = os.path.join(self.path, self.basename + extension)\nf = open(evt_file, \"w\")\nfor t, n in zip(datatowrite, texttowrite):\nf.writelines(\"{:1.6f}\".format(t) + \"\\t\" + n + \"\\n\")\nf.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_mean_waveforms","title":"<code>load_mean_waveforms(epoch=None, waveform_window=None, spike_count=1000)</code>","text":"<p>Load the mean waveforms from a dat file.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>IntervalSet</code> <p>default = None Restrict spikes to an epoch.</p> <code>None</code> <code>waveform_window</code> <code>IntervalSet</code> <p>default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms') Limit waveform extraction before and after spike time</p> <code>None</code> <code>spike_count</code> <code>int</code> <p>default = 1000 Number of spikes used per neuron for the calculation of waveforms</p> <code>1000</code> <p>Returns:</p> Type Description <code>dictionary</code> <p>the waveforms for all neurons</p> <code>pandas.Series</code> <p>the channel with the maximum waveform for each neuron</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_mean_waveforms(self, epoch=None, waveform_window=None, spike_count=1000):\n\"\"\"\n    Load the mean waveforms from a dat file.\n    Parameters\n    ----------\n    epoch : IntervalSet\n        default = None\n        Restrict spikes to an epoch.\n    waveform_window : IntervalSet\n        default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms')\n        Limit waveform extraction before and after spike time\n    spike_count : int\n        default = 1000\n        Number of spikes used per neuron for the calculation of waveforms\n    Returns\n    -------\n    dictionary\n        the waveforms for all neurons\n    pandas.Series\n        the channel with the maximum waveform for each neuron\n    \"\"\"\nif not isinstance(waveform_window, nap.IntervalSet):\nwaveform_window = nap.IntervalSet(start=-0.5, end=1, time_units=\"ms\")\nspikes = self.spikes\nif not os.path.exists(self.path):  # check if path exists\nprint(\"The path \" + self.path + \" doesn't exist; Exiting ...\")\nsys.exit()\n# Load XML INFO\nself.load_neurosuite_xml(self.path)\nn_channels = self.nChannels\nfs = self.fs_dat\ngroup_to_channel = self.group_to_channel\ngroup = spikes.get_info(\"group\")\n# Check if there is an epoch, restrict spike times to epoch\nif epoch is not None:\nif type(epoch) is not nap.IntervalSet:\nprint(\"Epoch must be an IntervalSet\")\nsys.exit()\nelse:\nprint(\"Restricting spikes to epoch\")\nspikes = spikes.restrict(epoch)\nepstart = int(epoch.as_units(\"s\")[\"start\"].values[0] * fs)\nepend = int(epoch.as_units(\"s\")[\"end\"].values[0] * fs)\n# Find dat file\nfiles = os.listdir(self.path)\ndat_files = np.sort([f for f in files if \"dat\" in f and f[0] != \".\"])\n# Need n_samples collected in the entire recording from dat file to load\nfile = os.path.join(self.path, dat_files[0])\nf = open(\nfile, \"rb\"\n)  # open file to get number of samples collected in the entire recording\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\n# map to memory all samples for all channels, channels are numbered according to neuroscope number\nfp = np.memmap(file, np.int16, \"r\", shape=(n_samples, n_channels))\n# convert spike times to spikes in sample number\nsample_spikes = {\nneuron: (spikes[neuron].as_units(\"s\").index.values * fs).astype(\"int\")\nfor neuron in spikes\n}\n# prep for waveforms\noverlap = int(\nwaveform_window.tot_length(time_units=\"s\")\n)  # one spike's worth of overlap between windows\nwaveform_window = abs(np.array(waveform_window.as_units(\"s\"))[0] * fs).astype(\nint\n)  # convert time to sample number\nneuron_waveforms = {\nn: np.zeros([np.sum(waveform_window), len(group_to_channel[group[n]])])\nfor n in sample_spikes\n}\n# divide dat file into batches that slightly overlap for faster loading\nbatch_size = 3000000\nwindows = np.arange(0, int(endoffile / n_channels / bytes_size), batch_size)\nif epoch is not None:\nprint(\"Restricting dat file to epoch\")\nwindows = windows[(windows &gt;= epstart) &amp; (windows &lt;= epend)]\nbatches = []\nfor (\ni\n) in windows:  # make overlapping batches from the beginning to end of recording\nif i == windows[-1]:  # the last batch cannot overlap with the next one\nbatches.append([i, n_samples])\nelse:\nbatches.append([i, i + batch_size + overlap])\nbatches = [np.int32(batch) for batch in batches]\nsample_counted_spikes = {}\nfor index, neuron in enumerate(sample_spikes):\nif len(sample_spikes[neuron]) &gt;= spike_count:\nsample_counted_spikes[neuron] = np.array(\nnp.random.choice(list(sample_spikes[neuron]), spike_count)\n)\nelif len(sample_spikes[neuron]) &lt; spike_count:\nprint(\n\"Not enough spikes in neuron \" + str(index) + \"... using all spikes\"\n)\nsample_counted_spikes[neuron] = sample_spikes[neuron]\n# Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file\nspike_check = np.array(\n[\nint(spikes_neuron)\nfor spikes_neuron in sample_counted_spikes[neuron]\nfor neuron in sample_counted_spikes\n]\n)\nfor index, timestep in enumerate(batches):\nprint(\nf\"Extracting waveforms from dat file: window {index+1} / {len(windows)}\",\nend=\"\\r\",\n)\nif (\nlen(\nspike_check[\n(timestep[0] &lt; spike_check) &amp; (timestep[1] &gt; spike_check)\n]\n)\n== 0\n):\ncontinue  # if there are no spikes for any neurons in this batch, skip and go to the next one\n# Load dat file for timestep\ntmp = pd.DataFrame(\ndata=fp[timestep[0] : timestep[1], :],\ncolumns=np.arange(n_channels),\nindex=range(timestep[0], timestep[1]),\n)  # load dat file\n# Check if any spikes are present\nfor neuron in sample_counted_spikes:\nneurontmp = sample_counted_spikes[neuron]\ntmp2 = neurontmp[(timestep[0] &lt; neurontmp) &amp; (timestep[1] &gt; neurontmp)]\nif len(neurontmp) == 0:\ncontinue  # skip neuron if it has no spikes in this batch\ntmpn = tmp[\ngroup_to_channel[group[neuron]]\n]  # restrict dat file to the channel group of the neuron\nfor time in tmp2:  # add each spike waveform to neuron_waveform\nspikewindow = tmpn.loc[\ntime - waveform_window[0] : time + waveform_window[1] - 1\n]  # waveform for this spike time\ntry:\nneuron_waveforms[neuron] += spikewindow.values\nexcept (\nException\n):  # ignore if full waveform is not present in this batch\npass\nmeanwf = {\nn: pd.DataFrame(\ndata=np.array(neuron_waveforms[n]) / spike_count,\ncolumns=np.arange(len(group_to_channel[group[n]])),\nindex=np.array(np.arange(-waveform_window[0], waveform_window[1])) / fs,\n)\nfor n in sample_counted_spikes\n}\n# find the max channel for each neuron\nmaxch = pd.Series(\ndata=[meanwf[n][meanwf[n].loc[0].idxmin()].name for n in meanwf],\nindex=spikes.keys(),\n)\nreturn meanwf, maxch\n</code></pre>"},{"location":"old_pages/io.npz/","title":"Io.npz","text":"<p>File classes help to validate and load pynapple objects or NWB files. Data are always lazy-loaded. Both classes behaves like dictionnary.</p>"},{"location":"old_pages/io.npz/#pynapple.io.interface_npz.NPZFile","title":"<code>NPZFile</code>","text":"<p>         Bases: <code>object</code></p> <p>Class that points to a NPZ file that can be loaded as a pynapple object. Objects have a save function in npz format as well as the Folder class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; tsd = nap.load_file(\"path/to/my_tsd.npz\")\n&gt;&gt;&gt; tsd\nTime (s)\n0.0    0\n0.1    1\n0.2    2\ndtype: int64\n</code></pre> Source code in <code>pynapple/io/interface_npz.py</code> <pre><code>class NPZFile(object):\n\"\"\"Class that points to a NPZ file that can be loaded as a pynapple object.\n    Objects have a save function in npz format as well as the Folder class.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; tsd = nap.load_file(\"path/to/my_tsd.npz\")\n    &gt;&gt;&gt; tsd\n    Time (s)\n    0.0    0\n    0.1    1\n    0.2    2\n    dtype: int64\n    \"\"\"\ndef __init__(self, path):\n\"\"\"Initialization of the NPZ file\n        Parameters\n        ----------\n        path : str\n            Valid path to a NPZ file\n        \"\"\"\nself.path = path\nself.name = os.path.basename(path)\nself.file = np.load(self.path, allow_pickle=True)\nself.type = \"\"\n# First check if type is explicitely defined\npossible = [\"Ts\", \"Tsd\", \"TsdFrame\", \"TsGroup\", \"IntervalSet\"]\nif \"type\" in self.file.keys():\nif len(self.file[\"type\"]) == 1:\nif isinstance(self.file[\"type\"][0], np.str_):\nif self.file[\"type\"] in possible:\nself.type = self.file[\"type\"][0]\n# Second check manually\nif self.type == \"\":\nk = set(self.file.keys())\nif {\"t\", \"start\", \"end\", \"index\"}.issubset(k):\nself.type = \"TsGroup\"\nelif {\"t\", \"d\", \"start\", \"end\", \"columns\"}.issubset(k):\nself.type = \"TsdFrame\"\nelif {\"t\", \"d\", \"start\", \"end\"}.issubset(k):\nself.type = \"Tsd\"\nelif {\"t\", \"start\", \"end\"}.issubset(k):\nself.type = \"Ts\"\nelif {\"start\", \"end\"}.issubset(k):\nself.type = \"IntervalSet\"\nelse:\nself.type = \"npz\"\ndef load(self):\n\"\"\"Load the NPZ file\n        Returns\n        -------\n        (Tsd, Ts, TsdFrame, TsGroup, IntervalSet)\n            A pynapple object\n        \"\"\"\nif self.type == \"npz\":\nreturn self.file\nelse:\ntime_support = nap.IntervalSet(self.file[\"start\"], self.file[\"end\"])\nif self.type == \"TsGroup\":\ntsd = nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"index\"], time_support=time_support\n)\ntsgroup = tsd.to_tsgroup()\nif \"d\" in self.file.keys():\nprint(\"TODO\")\nmetainfo = {}\nfor k in set(self.file.keys()) - {\n\"start\",\n\"end\",\n\"t\",\n\"index\",\n\"d\",\n\"rate\",\n}:\ntmp = self.file[k]\nif len(tmp) == len(tsgroup):\nmetainfo[k] = tmp\ntsgroup.set_info(**metainfo)\nreturn tsgroup\nelif self.type == \"TsdFrame\":\nreturn nap.TsdFrame(\nt=self.file[\"t\"],\nd=self.file[\"d\"],\ntime_support=time_support,\ncolumns=self.file[\"columns\"],\n)\nelif self.type == \"Tsd\":\nreturn nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"d\"], time_support=time_support\n)\nelif self.type == \"Ts\":\nreturn nap.Ts(t=self.file[\"t\"], time_support=time_support)\nelif self.type == \"IntervalSet\":\nreturn time_support\nelse:\nreturn self.file\n</code></pre>"},{"location":"old_pages/io.npz/#pynapple.io.interface_npz.NPZFile.__init__","title":"<code>__init__(path)</code>","text":"<p>Initialization of the NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Valid path to a NPZ file</p> required Source code in <code>pynapple/io/interface_npz.py</code> <pre><code>def __init__(self, path):\n\"\"\"Initialization of the NPZ file\n    Parameters\n    ----------\n    path : str\n        Valid path to a NPZ file\n    \"\"\"\nself.path = path\nself.name = os.path.basename(path)\nself.file = np.load(self.path, allow_pickle=True)\nself.type = \"\"\n# First check if type is explicitely defined\npossible = [\"Ts\", \"Tsd\", \"TsdFrame\", \"TsGroup\", \"IntervalSet\"]\nif \"type\" in self.file.keys():\nif len(self.file[\"type\"]) == 1:\nif isinstance(self.file[\"type\"][0], np.str_):\nif self.file[\"type\"] in possible:\nself.type = self.file[\"type\"][0]\n# Second check manually\nif self.type == \"\":\nk = set(self.file.keys())\nif {\"t\", \"start\", \"end\", \"index\"}.issubset(k):\nself.type = \"TsGroup\"\nelif {\"t\", \"d\", \"start\", \"end\", \"columns\"}.issubset(k):\nself.type = \"TsdFrame\"\nelif {\"t\", \"d\", \"start\", \"end\"}.issubset(k):\nself.type = \"Tsd\"\nelif {\"t\", \"start\", \"end\"}.issubset(k):\nself.type = \"Ts\"\nelif {\"start\", \"end\"}.issubset(k):\nself.type = \"IntervalSet\"\nelse:\nself.type = \"npz\"\n</code></pre>"},{"location":"old_pages/io.npz/#pynapple.io.interface_npz.NPZFile.load","title":"<code>load()</code>","text":"<p>Load the NPZ file</p> <p>Returns:</p> Type Description <code>Tsd, Ts, TsdFrame, TsGroup, IntervalSet</code> <p>A pynapple object</p> Source code in <code>pynapple/io/interface_npz.py</code> <pre><code>def load(self):\n\"\"\"Load the NPZ file\n    Returns\n    -------\n    (Tsd, Ts, TsdFrame, TsGroup, IntervalSet)\n        A pynapple object\n    \"\"\"\nif self.type == \"npz\":\nreturn self.file\nelse:\ntime_support = nap.IntervalSet(self.file[\"start\"], self.file[\"end\"])\nif self.type == \"TsGroup\":\ntsd = nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"index\"], time_support=time_support\n)\ntsgroup = tsd.to_tsgroup()\nif \"d\" in self.file.keys():\nprint(\"TODO\")\nmetainfo = {}\nfor k in set(self.file.keys()) - {\n\"start\",\n\"end\",\n\"t\",\n\"index\",\n\"d\",\n\"rate\",\n}:\ntmp = self.file[k]\nif len(tmp) == len(tsgroup):\nmetainfo[k] = tmp\ntsgroup.set_info(**metainfo)\nreturn tsgroup\nelif self.type == \"TsdFrame\":\nreturn nap.TsdFrame(\nt=self.file[\"t\"],\nd=self.file[\"d\"],\ntime_support=time_support,\ncolumns=self.file[\"columns\"],\n)\nelif self.type == \"Tsd\":\nreturn nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"d\"], time_support=time_support\n)\nelif self.type == \"Ts\":\nreturn nap.Ts(t=self.file[\"t\"], time_support=time_support)\nelif self.type == \"IntervalSet\":\nreturn time_support\nelse:\nreturn self.file\n</code></pre>"},{"location":"old_pages/io.nwb/","title":"Io.nwb","text":"<p>Pynapple class to interface with NWB files. Data are always lazy-loaded. Object behaves like dictionnary.</p>"},{"location":"old_pages/io.nwb/#pynapple.io.interface_nwb.NWBFile","title":"<code>NWBFile</code>","text":"<p>         Bases: <code>UserDict</code></p> <p>Class for reading NWB Files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; data = nap.load_file(\"my_file.nwb\")\n&gt;&gt;&gt; data[\"units\"]\n  Index    rate  location      group\n-------  ------  ----------  -------\n      0    1.0  brain        0\n      1    1.0  brain        0\n      2    1.0  brain        0\n</code></pre> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>class NWBFile(UserDict):\n\"\"\"Class for reading NWB Files.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; data = nap.load_file(\"my_file.nwb\")\n    &gt;&gt;&gt; data[\"units\"]\n      Index    rate  location      group\n    -------  ------  ----------  -------\n          0    1.0  brain        0\n          1    1.0  brain        0\n          2    1.0  brain        0\n    \"\"\"\n_f_eval = {\n\"IntervalSet\": _make_interval_set,\n\"Tsd\": _make_tsd,\n\"Ts\": _make_ts,\n\"TsdFrame\": _make_tsd_frame,\n\"TsGroup\": _make_tsgroup,\n}\ndef __init__(self, file):\n\"\"\"\n        Parameters\n        ----------\n        file : str or pynwb.file.NWBFile\n            Valid file to a NWB file\n        Raises\n        ------\n        FileNotFoundError\n            If path is invalid\n        RuntimeError\n            If file is not an instance of NWBFile\n        \"\"\"\nif isinstance(file, str):\nif os.path.exists(file):\nself.path = file\nself.name = os.path.basename(file).split(\".\")[0]\nself.io = NWBHDF5IO(file, \"r\")\nself.nwb = self.io.read()\nelse:\nraise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), file)\nelif isinstance(file, pynwb.file.NWBFile):\nself.nwb = file\nself.name = self.nwb.subject.subject_id\nelse:\nraise RuntimeError(\n\"unrecognized argument. Please provide path to a valid NWB file or open NWB file.\"\n)\nself.data = _extract_compatible_data_from_nwbfile(self.nwb)\nself.key_to_id = {k: self.data[k][\"id\"] for k in self.data.keys()}\nself._view = Table(title=self.name)\nself._view.add_column(\"Keys\", justify=\"left\", style=\"cyan\", no_wrap=True)\nself._view.add_column(\"Type\", style=\"green\")\n# self._view.add_column(\"NWB module\", justify=\"right\", style=\"magenta\")\nfor k in self.data.keys():\nself._view.add_row(\nk,\nself.data[k][\"type\"],\n# self.data[k]['top_module']\n)\nUserDict.__init__(self, self.data)\ndef __str__(self):\n\"\"\"View of the object\"\"\"\nwith Console() as console:\nconsole.print(self._view)\nreturn \"\"\n# def __repr__(self):\n#     \"\"\"View of the object\"\"\"\n#     return \"\"\ndef __getitem__(self, key):\n\"\"\"Get object from NWB\n        Parameters\n        ----------\n        key : str\n        Returns\n        -------\n        (Ts, Tsd, TsdFrame, TsGroup, IntervalSet or dict of IntervalSet)\n        Raises\n        ------\n        KeyError\n            If key is not in the dictionnary\n        \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], dict) and \"id\" in self.data[key]:\nobj = self.nwb.objects[self.data[key][\"id\"]]\ntry:\ndata = self._f_eval[self.data[key][\"type\"]](obj)\nexcept Exception:\nwarnings.warn(\n\"Failed to build {}.\\n Returning the NWB object for manual inspection\".format(\nself.data[key][\"type\"]\n),\nstacklevel=2,\n)\ndata = obj\nself.data[key] = data\nreturn data\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n</code></pre>"},{"location":"old_pages/io.nwb/#pynapple.io.interface_nwb.NWBFile.__init__","title":"<code>__init__(file)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>file</code> <code>str or pynwb.file.NWBFile</code> <p>Valid file to a NWB file</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If path is invalid</p> <code>RuntimeError</code> <p>If file is not an instance of NWBFile</p> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>def __init__(self, file):\n\"\"\"\n    Parameters\n    ----------\n    file : str or pynwb.file.NWBFile\n        Valid file to a NWB file\n    Raises\n    ------\n    FileNotFoundError\n        If path is invalid\n    RuntimeError\n        If file is not an instance of NWBFile\n    \"\"\"\nif isinstance(file, str):\nif os.path.exists(file):\nself.path = file\nself.name = os.path.basename(file).split(\".\")[0]\nself.io = NWBHDF5IO(file, \"r\")\nself.nwb = self.io.read()\nelse:\nraise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), file)\nelif isinstance(file, pynwb.file.NWBFile):\nself.nwb = file\nself.name = self.nwb.subject.subject_id\nelse:\nraise RuntimeError(\n\"unrecognized argument. Please provide path to a valid NWB file or open NWB file.\"\n)\nself.data = _extract_compatible_data_from_nwbfile(self.nwb)\nself.key_to_id = {k: self.data[k][\"id\"] for k in self.data.keys()}\nself._view = Table(title=self.name)\nself._view.add_column(\"Keys\", justify=\"left\", style=\"cyan\", no_wrap=True)\nself._view.add_column(\"Type\", style=\"green\")\n# self._view.add_column(\"NWB module\", justify=\"right\", style=\"magenta\")\nfor k in self.data.keys():\nself._view.add_row(\nk,\nself.data[k][\"type\"],\n# self.data[k]['top_module']\n)\nUserDict.__init__(self, self.data)\n</code></pre>"},{"location":"old_pages/io.nwb/#pynapple.io.interface_nwb.NWBFile.__str__","title":"<code>__str__()</code>","text":"<p>View of the object</p> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>def __str__(self):\n\"\"\"View of the object\"\"\"\nwith Console() as console:\nconsole.print(self._view)\nreturn \"\"\n</code></pre>"},{"location":"old_pages/io.nwb/#pynapple.io.interface_nwb.NWBFile.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get object from NWB</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required <p>Returns:</p> Type Description <code>(Ts, Tsd, TsdFrame, TsGroup, IntervalSet or dict of IntervalSet)</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If key is not in the dictionnary</p> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>def __getitem__(self, key):\n\"\"\"Get object from NWB\n    Parameters\n    ----------\n    key : str\n    Returns\n    -------\n    (Ts, Tsd, TsdFrame, TsGroup, IntervalSet or dict of IntervalSet)\n    Raises\n    ------\n    KeyError\n        If key is not in the dictionnary\n    \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], dict) and \"id\" in self.data[key]:\nobj = self.nwb.objects[self.data[key][\"id\"]]\ntry:\ndata = self._f_eval[self.data[key][\"type\"]](obj)\nexcept Exception:\nwarnings.warn(\n\"Failed to build {}.\\n Returning the NWB object for manual inspection\".format(\nself.data[key][\"type\"]\n),\nstacklevel=2,\n)\ndata = obj\nself.data[key] = data\nreturn data\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n</code></pre>"},{"location":"old_pages/io.phy/","title":"Io.phy","text":"<p>Class and functions for loading data processed with Phy2</p> <p>@author: Sara Mahallati, Guillaume Viejo</p>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy","title":"<code>Phy</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for Phy data</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>class Phy(BaseLoader):\n\"\"\"\n    Loader for Phy data\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Instantiate the data class from a Phy folder.\n        Parameters\n        ----------\n        path : str or Path object\n            The path to the data.\n        \"\"\"\nself.time_support = None\nself.sample_rate = None\nself.n_channels_dat = None\nself.channel_map = None\nself.ch_to_sh = None\nself.spikes = None\nself.channel_positions = None\nsuper().__init__(path)\n# This path stuff should happen only once in the parent class\nself.path = Path(path)\nself.basename = self.path.name\nself.nwb_path = self.path / \"pynapplenwb\"\n# from what I can see in the loading function, only one nwb file per folder:\ntry:\nself.nwb_file = list(self.nwb_path.glob(\"*.nwb\"))[0]\nexcept IndexError:\nself.nwb_file = None\n# Need to check if nwb file exists and if data are there\n# if self.path is not None:  -&gt; are there any cases where this is None?\nif self.nwb_file is not None:\nloaded_spikes = self.load_nwb_spikes()\nif loaded_spikes is not None:\nreturn\n# Bypass if data have already been transferred to nwb\nself.load_phy_params()\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.channel_map)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_phy_spikes(self.time_support)\nself.save_data()\napp.quit()\ndef load_phy_params(self):\n\"\"\"\n        path should be the folder session containing the params.py file\n        Function reads :\n        1. the number of channels\n        2. the sampling frequency of the dat file\n        Raises\n        ------\n        AssertionError\n            If path does not contain the params file or channel_map.npy\n        \"\"\"\nassert (\nself.path / \"params.py\"\n).exists(), f\"Can't find params.py in {self.path}\"\n# It is strongly recommended not to conflate parameters and code! Also, there's a library called params.\n# I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py!\n# In this way we just read the file, and we don't have to add to sys to import...\n# TODO maybe remove this\nsys.path.append(str(self.path))\nimport params as params\nself.sample_rate = params.sample_rate\nself.n_channels_dat = params.n_channels_dat\nassert (\nself.path / \"channel_map.npy\"\n).exists(), f\"Can't find channel_map.npy in {self.path}\"\nchannel_map = np.load(self.path / \"channel_map.npy\")\nif (self.path / \"channel_shanks.npy\").exists():\nchannel_shank = np.load(self.path / \"channel_shanks.npy\")\nn_shanks = len(np.unique(channel_shank))\nself.channel_map = {\ni: channel_map[channel_shank == i] for i in range(n_shanks)\n}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=channel_shank.flatten(),\n)\nelse:\nself.channel_map = {i: channel_map[i] for i in range(len(channel_map))}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=np.hstack(\n[\nnp.ones(len(channel_map[i]), dtype=int) * i\nfor i in range(len(channel_map))\n]\n),\n)\nreturn\ndef load_phy_spikes(self, time_support=None):\n\"\"\"\n        Load Phy spike times and convert to NWB.\n        Instantiate automatically a TsGroup object.\n        The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv\n        Parameters\n        ----------\n        path : Path object\n            The path to the data\n        time_support : IntevalSet, optional\n            The time support of the data\n        Raises\n        ------\n        RuntimeError\n            If files are missing.\n            The function needs :\n            - cluster_info.tsv or cluster_group.tsv\n            - spike_times.npy\n            - spike_clusters.npy\n            - channel_positions.npy\n            - templates.npy\n        \"\"\"\n# Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used:\nhas_cluster_info = False\nif (self.path / \"cluster_info.tsv\").exists():\ncluster_info_file = self.path / \"cluster_info.tsv\"\nhas_cluster_info = True\nelif (self.path / \"cluster_group.tsv\").exists():\ncluster_info_file = self.path / \"cluster_group.tsv\"\nelse:\nraise RuntimeError(\n\"Can't find cluster_info.tsv or cluster_group.tsv in {};\".format(\nself.path\n)\n)\ncluster_info = pd.read_csv(cluster_info_file, sep=\"\\t\", index_col=\"cluster_id\")\n# In my processed data with KiloSort 3.0, the column is named KSLabel\nif \"group\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.group == \"good\"].index.values\nelif \"KSLabel\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.KSLabel == \"good\"].index.values\nelse:\nraise RuntimeError(\n\"Can't find column group or KSLabel in {};\".format(cluster_info_file)\n)\nspike_times = np.load(self.path / \"spike_times.npy\")\nspike_clusters = np.load(self.path / \"spike_clusters.npy\")\nspikes = {}\nfor n in cluster_id_good:\nspikes[n] = nap.Ts(\nt=spike_times[spike_clusters == n] / self.sample_rate,\ntime_support=time_support,\n)\nself.spikes = nap.TsGroup(spikes, time_support=time_support)\n# Adding the position of the electrodes in case\nself.channel_positions = np.load(self.path / \"channel_positions.npy\")\n# Adding shank group info from cluster_info if present\nif has_cluster_info:\ngroup = cluster_info.loc[cluster_id_good, \"sh\"]\nself.spikes.set_info(group=group)\nelse:\ntemplate = np.load(self.path / \"templates.npy\")\ntemplate = template[cluster_id_good]\nch = np.power(template, 2).max(1).argmax(1)\ngroup = pd.Series(index=cluster_id_good, data=self.ch_to_sh[ch].values)\nself.spikes.set_info(group=group)\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\ndef save_data(self):\n\"\"\"Save the data to NWB format.\"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.channel_map:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.channel_map[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_spikes(self):\n\"\"\"Read the NWB spikes to extract the spike times.\n        Returns\n        -------\n        TYPE\n            Description\n        \"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn None\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\ndef load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n        Load the LFP.\n        Parameters\n        ----------\n        filename : str, optional\n            The filename of the lfp file.\n            It can be useful it multiple dat files are present in the data directory\n        channel : int or list of int, optional\n            The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n        extension : str, optional\n            The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n        frequency : float, optional\n            Default 1250 Hz for the eeg file\n        precision : str, optional\n            The precision of the binary file\n        bytes_size : int, optional\n            Bytes size of the lfp file\n        Raises\n        ------\n        RuntimeError\n            If can't find the lfp/eeg/dat file\n        Returns\n        -------\n        Tsd or TsdFrame\n            The lfp in a time series format\n        \"\"\"\nif filename is not None:\nfilepath = self.path / filename\nelse:\ntry:\nfilepath = list(self.path.glob(f\"*{extension}\"))[0]\nexcept IndexError:\nraise RuntimeError(f\"Path {self.path} contains no {extension} files;\")\n# is it possible that this is a leftover from neurosuite data?\n# This is not implemented for this class.\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.__init__","title":"<code>__init__(path)</code>","text":"<p>Instantiate the data class from a Phy folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path object</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/phy.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Instantiate the data class from a Phy folder.\n    Parameters\n    ----------\n    path : str or Path object\n        The path to the data.\n    \"\"\"\nself.time_support = None\nself.sample_rate = None\nself.n_channels_dat = None\nself.channel_map = None\nself.ch_to_sh = None\nself.spikes = None\nself.channel_positions = None\nsuper().__init__(path)\n# This path stuff should happen only once in the parent class\nself.path = Path(path)\nself.basename = self.path.name\nself.nwb_path = self.path / \"pynapplenwb\"\n# from what I can see in the loading function, only one nwb file per folder:\ntry:\nself.nwb_file = list(self.nwb_path.glob(\"*.nwb\"))[0]\nexcept IndexError:\nself.nwb_file = None\n# Need to check if nwb file exists and if data are there\n# if self.path is not None:  -&gt; are there any cases where this is None?\nif self.nwb_file is not None:\nloaded_spikes = self.load_nwb_spikes()\nif loaded_spikes is not None:\nreturn\n# Bypass if data have already been transferred to nwb\nself.load_phy_params()\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.channel_map)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_phy_spikes(self.time_support)\nself.save_data()\napp.quit()\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_phy_params","title":"<code>load_phy_params()</code>","text":"<p>path should be the folder session containing the params.py file</p> <p>Function reads : 1. the number of channels 2. the sampling frequency of the dat file</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If path does not contain the params file or channel_map.npy</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_phy_params(self):\n\"\"\"\n    path should be the folder session containing the params.py file\n    Function reads :\n    1. the number of channels\n    2. the sampling frequency of the dat file\n    Raises\n    ------\n    AssertionError\n        If path does not contain the params file or channel_map.npy\n    \"\"\"\nassert (\nself.path / \"params.py\"\n).exists(), f\"Can't find params.py in {self.path}\"\n# It is strongly recommended not to conflate parameters and code! Also, there's a library called params.\n# I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py!\n# In this way we just read the file, and we don't have to add to sys to import...\n# TODO maybe remove this\nsys.path.append(str(self.path))\nimport params as params\nself.sample_rate = params.sample_rate\nself.n_channels_dat = params.n_channels_dat\nassert (\nself.path / \"channel_map.npy\"\n).exists(), f\"Can't find channel_map.npy in {self.path}\"\nchannel_map = np.load(self.path / \"channel_map.npy\")\nif (self.path / \"channel_shanks.npy\").exists():\nchannel_shank = np.load(self.path / \"channel_shanks.npy\")\nn_shanks = len(np.unique(channel_shank))\nself.channel_map = {\ni: channel_map[channel_shank == i] for i in range(n_shanks)\n}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=channel_shank.flatten(),\n)\nelse:\nself.channel_map = {i: channel_map[i] for i in range(len(channel_map))}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=np.hstack(\n[\nnp.ones(len(channel_map[i]), dtype=int) * i\nfor i in range(len(channel_map))\n]\n),\n)\nreturn\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_phy_spikes","title":"<code>load_phy_spikes(time_support=None)</code>","text":"<p>Load Phy spike times and convert to NWB. Instantiate automatically a TsGroup object. The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path object</code> <p>The path to the data</p> required <code>time_support</code> <code>IntevalSet, optional</code> <p>The time support of the data</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If files are missing. The function needs : - cluster_info.tsv or cluster_group.tsv - spike_times.npy - spike_clusters.npy - channel_positions.npy - templates.npy</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_phy_spikes(self, time_support=None):\n\"\"\"\n    Load Phy spike times and convert to NWB.\n    Instantiate automatically a TsGroup object.\n    The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv\n    Parameters\n    ----------\n    path : Path object\n        The path to the data\n    time_support : IntevalSet, optional\n        The time support of the data\n    Raises\n    ------\n    RuntimeError\n        If files are missing.\n        The function needs :\n        - cluster_info.tsv or cluster_group.tsv\n        - spike_times.npy\n        - spike_clusters.npy\n        - channel_positions.npy\n        - templates.npy\n    \"\"\"\n# Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used:\nhas_cluster_info = False\nif (self.path / \"cluster_info.tsv\").exists():\ncluster_info_file = self.path / \"cluster_info.tsv\"\nhas_cluster_info = True\nelif (self.path / \"cluster_group.tsv\").exists():\ncluster_info_file = self.path / \"cluster_group.tsv\"\nelse:\nraise RuntimeError(\n\"Can't find cluster_info.tsv or cluster_group.tsv in {};\".format(\nself.path\n)\n)\ncluster_info = pd.read_csv(cluster_info_file, sep=\"\\t\", index_col=\"cluster_id\")\n# In my processed data with KiloSort 3.0, the column is named KSLabel\nif \"group\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.group == \"good\"].index.values\nelif \"KSLabel\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.KSLabel == \"good\"].index.values\nelse:\nraise RuntimeError(\n\"Can't find column group or KSLabel in {};\".format(cluster_info_file)\n)\nspike_times = np.load(self.path / \"spike_times.npy\")\nspike_clusters = np.load(self.path / \"spike_clusters.npy\")\nspikes = {}\nfor n in cluster_id_good:\nspikes[n] = nap.Ts(\nt=spike_times[spike_clusters == n] / self.sample_rate,\ntime_support=time_support,\n)\nself.spikes = nap.TsGroup(spikes, time_support=time_support)\n# Adding the position of the electrodes in case\nself.channel_positions = np.load(self.path / \"channel_positions.npy\")\n# Adding shank group info from cluster_info if present\nif has_cluster_info:\ngroup = cluster_info.loc[cluster_id_good, \"sh\"]\nself.spikes.set_info(group=group)\nelse:\ntemplate = np.load(self.path / \"templates.npy\")\ntemplate = template[cluster_id_good]\nch = np.power(template, 2).max(1).argmax(1)\ngroup = pd.Series(index=cluster_id_good, data=self.ch_to_sh[ch].values)\nself.spikes.set_info(group=group)\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.save_data","title":"<code>save_data()</code>","text":"<p>Save the data to NWB format.</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def save_data(self):\n\"\"\"Save the data to NWB format.\"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.channel_map:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.channel_map[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_nwb_spikes","title":"<code>load_nwb_spikes()</code>","text":"<p>Read the NWB spikes to extract the spike times.</p> <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_nwb_spikes(self):\n\"\"\"Read the NWB spikes to extract the spike times.\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn None\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\n</code></pre>"},{"location":"old_pages/io.phy/#pynapple.io.phy.Phy.load_lfp","title":"<code>load_lfp(filename=None, channel=None, extension='.eeg', frequency=1250.0, precision='int16', bytes_size=2)</code>","text":"<p>Load the LFP.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str, optional</code> <p>The filename of the lfp file. It can be useful it multiple dat files are present in the data directory</p> <code>None</code> <code>channel</code> <code>int or list of int, optional</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>extension</code> <code>str, optional</code> <p>The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> <code>'.eeg'</code> <code>frequency</code> <code>float, optional</code> <p>Default 1250 Hz for the eeg file</p> <code>1250.0</code> <code>precision</code> <code>str, optional</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the lfp file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Load the LFP.\n    Parameters\n    ----------\n    filename : str, optional\n        The filename of the lfp file.\n        It can be useful it multiple dat files are present in the data directory\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    frequency : float, optional\n        Default 1250 Hz for the eeg file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the lfp file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    \"\"\"\nif filename is not None:\nfilepath = self.path / filename\nelse:\ntry:\nfilepath = list(self.path.glob(f\"*{extension}\"))[0]\nexcept IndexError:\nraise RuntimeError(f\"Path {self.path} contains no {extension} files;\")\n# is it possible that this is a leftover from neurosuite data?\n# This is not implemented for this class.\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"old_pages/io.suite2p/","title":"Io.suite2p","text":"<p>Loader for Suite2P https://github.com/MouseLand/suite2p</p>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P","title":"<code>Suite2P</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for data processed with Suite2P.</p> <p>Pynapple will try to look for data in this order :</p> <ol> <li> <p>pynapplenwb/session_name.nwb</p> </li> <li> <p>suite2p/plane/.npy</p> </li> </ol> <p>Attributes:</p> Name Type Description <code>F</code> <code>TsdFrame</code> <p>Fluorescence traces (timepoints x ROIs) for all planes</p> <code>Fneu</code> <code>TsdFrame</code> <p>Neuropil fluorescence traces (timepoints x ROIs) for all planes</p> <code>spks</code> <code>TsdFrame</code> <p>Deconvolved traces (timepoints x ROIS) for all planes</p> <code>plane_info</code> <code>pandas.DataFrame</code> <p>Contains plane identity of each cell</p> <code>stats</code> <code>dict</code> <p>dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells (Can be smaller when loading from the NWB file)</p> <code>ops</code> <code>dict</code> <p>Parameters from Suite2p. (Can be smaller when loading from the NWB file)</p> <code>iscell</code> <code>numpy.ndarray</code> <p>Cell classification</p> Source code in <code>pynapple/io/suite2p.py</code> <pre><code>class Suite2P(BaseLoader):\n\"\"\"Loader for data processed with Suite2P.\n    Pynapple will try to look for data in this order :\n    1. pynapplenwb/session_name.nwb\n    2. suite2p/plane*/*.npy\n    Attributes\n    ----------\n    F : TsdFrame\n        Fluorescence traces (timepoints x ROIs) for all planes\n    Fneu : TsdFrame\n        Neuropil fluorescence traces (timepoints x ROIs) for all planes\n    spks : TsdFrame\n        Deconvolved traces (timepoints x ROIS) for all planes\n    plane_info : pandas.DataFrame\n        Contains plane identity of each cell\n    stats : dict\n        dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells\n        (Can be smaller when loading from the NWB file)\n    ops : dict\n        Parameters from Suite2p. (Can be smaller when loading from the NWB file)\n    iscell : numpy.ndarray\n        Cell classification\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_suite2p_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_suite2p(path)\nself.save_suite2p_nwb(path)\ndef load_suite2p(self, path):\n\"\"\"\n        Looking for suite2/plane*\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.path_suite2p = os.path.join(path, \"suite2p\")\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ndata = {\n\"F\": [],\n\"Fneu\": [],\n\"spks\": [],\n}\nplane_info = []\nself.stats = {}\nself.pops = {}\nself.iscells = {}\nself.planes = []\nif os.path.exists(self.path_suite2p):\nplanes = glob.glob(os.path.join(self.path_suite2p, \"plane*\"))\nif len(planes):\n# count = 0\nfor plane_dir in planes:\nn = int(os.path.basename(plane_dir)[-1])\nself.planes.append(n)\n# Loading iscell.npy\ntry:\niscell = np.load(\nos.path.join(plane_dir, \"iscell.npy\"), allow_pickle=True\n)\nidx = np.where(iscell.astype(\"int\")[:, 0])[0]\nplane_info.append(np.ones(len(idx), dtype=\"int\") * n)\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading F.npy, Fneu.py and spks.npy\nfor obj in [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]:\ntry:\nname = obj.split(\".\")[0]\ntmp = np.load(\nos.path.join(plane_dir, obj), allow_pickle=True\n)\ndata[name].append(tmp[idx])\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading stat.npy and ops.npy\ntry:\nstat = np.load(\nos.path.join(plane_dir, \"stat.npy\"), allow_pickle=True\n)\nops = np.load(\nos.path.join(plane_dir, \"ops.npy\"), allow_pickle=True\n).item()\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Saving stat, ops and iscell\nself.stats[n] = stat\nself.pops[n] = ops\nself.iscells[n] = iscell\n# count += len(idx)\nelse:\nwarnings.warn(\n\"Couldn't find planes in %s\" % self.path_suite2p, stacklevel=2\n)\nsys.exit()\nelse:\nwarnings.warn(\"No suite2p folder in %s\" % path, stacklevel=2)\nsys.exit()\n# Calcium transients\ndata[\"F\"] = np.transpose(np.vstack(data[\"F\"]))\ndata[\"Fneu\"] = np.transpose(np.vstack(data[\"Fneu\"]))\ndata[\"spks\"] = np.transpose(np.vstack(data[\"spks\"]))\ntime_index = np.arange(0, len(data[\"F\"])) / self.sampling_rate\nself.F = nap.TsdFrame(t=time_index, d=data[\"F\"])\nself.Fneu = nap.TsdFrame(t=time_index, d=data[\"Fneu\"])\nself.spks = nap.TsdFrame(t=time_index, d=data[\"spks\"])\nself.ops = self.pops[0]\nself.iscell = np.vstack([self.iscells[k] for k in self.iscells.keys()])\n# Metadata\nself.plane_info = pd.DataFrame.from_dict({\"plane\": np.hstack(plane_info)})\nreturn\ndef save_suite2p_nwb(self, path):\n\"\"\"\n        Save the data to NWB. To ensure continuity, this function is based on :\n        https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nmultiplane = True if len(self.planes) &gt; 1 else False\nops = self.pops[list(self.pops.keys())[0]]\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice = nwbfile.create_device(\nname=self.ophys_information[\"device\"][\"name\"],\ndescription=self.ophys_information[\"device\"][\"description\"],\nmanufacturer=self.ophys_information[\"device\"][\"manufacturer\"],\n)\nimaging_plane = nwbfile.create_imaging_plane(\nname=self.ophys_information[\"ImagingPlane\"][\"name\"],\noptical_channel=OpticalChannel(\nname=self.ophys_information[\"OpticalChannel\"][\"name\"],\ndescription=self.ophys_information[\"OpticalChannel\"][\"description\"],\nemission_lambda=float(\nself.ophys_information[\"OpticalChannel\"][\"emission_lambda\"]\n),\n),\nimaging_rate=self.sampling_rate,\ndescription=self.ophys_information[\"ImagingPlane\"][\"description\"],\ndevice=device,\nexcitation_lambda=float(\nself.ophys_information[\"ImagingPlane\"][\"excitation_lambda\"]\n),\nindicator=self.ophys_information[\"ImagingPlane\"][\"indicator\"],\nlocation=self.ophys_information[\"ImagingPlane\"][\"location\"],\ngrid_spacing=([2.0, 2.0, 30.0] if multiplane else [2.0, 2.0]),\ngrid_spacing_unit=\"microns\",\n)\n# link to external data\nimage_series = TwoPhotonSeries(\nname=\"TwoPhotonSeries\",\ndimension=[ops[\"Ly\"], ops[\"Lx\"]],\nexternal_file=(ops[\"filelist\"] if \"filelist\" in ops else [\"\"]),\nimaging_plane=imaging_plane,\nstarting_frame=[0],\nformat=\"external\",\nstarting_time=0.0,\nrate=ops[\"fs\"] * ops[\"nplanes\"],\n)\nnwbfile.add_acquisition(image_series)\n# processing\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=self.ophys_information[\"PlaneSegmentation\"][\"name\"],\ndescription=self.ophys_information[\"PlaneSegmentation\"][\"description\"],\nimaging_plane=imaging_plane,\n# reference_images=image_series,\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nophys_module.add(img_seg)\nfile_strs = [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]\ntraces = []\nncells = np.zeros(len(self.pops), dtype=np.int_)\nNfr = np.array([self.pops[k][\"nframes\"] for k in self.pops.keys()]).max()\nfor iplane, ops in self.pops.items():\nif iplane == 0:\niscell = self.iscells[iplane]\nfor fstr in file_strs:\ntraces.append(np.load(os.path.join(ops[\"save_path\"], fstr)))\nPlaneCellsIdx = iplane * np.ones(len(iscell))\nelse:\niscell = np.append(\niscell,\nself.iscells[iplane],\naxis=0,\n)\nfor i, fstr in enumerate(file_strs):\ntrace = np.load(os.path.join(ops[\"save_path\"], fstr))\nif trace.shape[1] &lt; Nfr:\nfcat = np.zeros(\n(trace.shape[0], Nfr - trace.shape[1]), \"float32\"\n)\ntrace = np.concatenate((trace, fcat), axis=1)\ntraces[i] = np.append(traces[i], trace, axis=0)\nPlaneCellsIdx = np.append(\nPlaneCellsIdx, iplane * np.ones(len(iscell) - len(PlaneCellsIdx))\n)\nstat = self.stats[iplane]\nncells[iplane] = len(stat)\nfor n in range(ncells[iplane]):\nif multiplane:\npixel_mask = np.array(\n[\nstat[n][\"ypix\"],\nstat[n][\"xpix\"],\niplane * np.ones(stat[n][\"npix\"]),\nstat[n][\"lam\"],\n]\n)\nps.add_roi(voxel_mask=pixel_mask.T)\nelse:\npixel_mask = np.array(\n[stat[n][\"ypix\"], stat[n][\"xpix\"], stat[n][\"lam\"]]\n)\nps.add_roi(pixel_mask=pixel_mask.T)\nps.add_column(\"iscell\", \"two columns - iscell &amp; probcell\", iscell)\nrt_region = []\nfor iplane, ops in self.pops.items():\nif iplane == 0:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(0, ncells[iplane]),\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\nelse:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(\nnp.sum(ncells[:iplane]),\nncells[iplane] + np.sum(ncells[:iplane]),\n)\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\n# FLUORESCENCE (all are required)\nname_strs = [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\nfor i, (fstr, nstr) in enumerate(zip(file_strs, name_strs)):\nfor iplane, ops in self.pops.items():\nroi_resp_series = RoiResponseSeries(\nname=f\"plane{int(iplane)}\",\ndata=traces[i][PlaneCellsIdx == iplane],\nrois=rt_region[iplane],\nunit=\"lumens\",\nrate=ops[\"fs\"],\n)\nif iplane == 0:\nfl = Fluorescence(roi_response_series=roi_resp_series, name=nstr)\nelse:\nfl.add_roi_response_series(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_suite2p_nwb(self, path):\n\"\"\"\n        Load suite2p data from NWB\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\nophys = nwbfile.processing[\"ophys\"]\n#################################################################\n# STATS, OPS and ISCELL\n#################################################################\ndims = nwbfile.acquisition[\"TwoPhotonSeries\"].dimension[:]\nself.ops = {\"Ly\": dims[0], \"Lx\": dims[1]}\nself.rate = nwbfile.acquisition[\n\"TwoPhotonSeries\"\n].imaging_plane.imaging_rate\nself.stats = {0: {}}\nself.iscell = ophys[\"ImageSegmentation\"][\"PlaneSegmentation\"][\n\"iscell\"\n].data[:]\ninfo = pd.DataFrame(\ndata=self.iscell[:, 0].astype(\"int\"), columns=[\"iscell\"]\n)\n#################################################################\n# ROIS\n#################################################################\ntry:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"pixel_mask\"]\nmultiplane = False\nexcept Exception:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"voxel_mask\"]\nmultiplane = True\nidx = np.where(self.iscell[:, 0])[0]\ninfo[\"plane\"] = 0\nfor n in range(len(rois)):\nroi = pd.DataFrame(rois[n])\nif \"z\" in roi.columns:\npl = roi[\"z\"][0]\nelse:\npl = 0\ninfo.loc[n, \"plane\"] = pl\nif pl not in self.stats.keys():\nself.stats[pl] = {}\nif n in idx:\nself.stats[pl][n] = {\n\"xpix\": roi[\"y\"].values,\n\"ypix\": roi[\"x\"].values,\n\"lam\": roi[\"weight\"].values,\n}\n#################################################################\n# Time Series\n#################################################################\nfields = np.intersect1d(\n[\"Fluorescence\", \"Neuropil\", \"Deconvolved\"],\nlist(ophys.fields[\"data_interfaces\"].keys()),\n)\nif len(fields) == 0:\nprint(\n\"No \" + \" or \".join([\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]),\n\"found in nwb {}\".format(self.nwbfilepath),\n)\nreturn False\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\ndata = {}\nif multiplane:\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\nelse:\nplanes = [0]\nfor k, name in zip(\n[\"F\", \"Fneu\", \"spks\"], [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n):\ntmp = []\ntimestamps = []\nfor i, n in enumerate(planes):\nif multiplane:\npl = \"plane{}\".format(n)\nelse:\npl = name  # This doesn't make sense\ntokeep = info[\"iscell\"][info[\"plane\"] == n].values == 1\nd = np.transpose(ophys[name][pl].data[:][tokeep])\nif ophys[name][pl].timestamps is not None:\nt = ophys[name][pl].timestamps[:]\nelse:\nt = (np.arange(0, len(d)) / self.rate) + ophys[name][\npl\n].starting_time\ntmp.append(d)\ntimestamps.append(t)\ndata[k] = nap.TsdFrame(t=timestamps[0], d=np.hstack(tmp))\nif \"F\" in data.keys():\nself.F = data[\"F\"]\nif \"Fneu\" in data.keys():\nself.Fneu = data[\"Fneu\"]\nif \"spks\" in data.keys():\nself.spks = data[\"spks\"]\nself.plane_info = pd.DataFrame(\ndata=info[\"plane\"][info[\"iscell\"] == 1].values, columns=[\"plane\"]\n)\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_suite2p_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_suite2p(path)\nself.save_suite2p_nwb(path)\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p","title":"<code>load_suite2p(path)</code>","text":"<p>Looking for suite2/plane*</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def load_suite2p(self, path):\n\"\"\"\n    Looking for suite2/plane*\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.path_suite2p = os.path.join(path, \"suite2p\")\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ndata = {\n\"F\": [],\n\"Fneu\": [],\n\"spks\": [],\n}\nplane_info = []\nself.stats = {}\nself.pops = {}\nself.iscells = {}\nself.planes = []\nif os.path.exists(self.path_suite2p):\nplanes = glob.glob(os.path.join(self.path_suite2p, \"plane*\"))\nif len(planes):\n# count = 0\nfor plane_dir in planes:\nn = int(os.path.basename(plane_dir)[-1])\nself.planes.append(n)\n# Loading iscell.npy\ntry:\niscell = np.load(\nos.path.join(plane_dir, \"iscell.npy\"), allow_pickle=True\n)\nidx = np.where(iscell.astype(\"int\")[:, 0])[0]\nplane_info.append(np.ones(len(idx), dtype=\"int\") * n)\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading F.npy, Fneu.py and spks.npy\nfor obj in [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]:\ntry:\nname = obj.split(\".\")[0]\ntmp = np.load(\nos.path.join(plane_dir, obj), allow_pickle=True\n)\ndata[name].append(tmp[idx])\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading stat.npy and ops.npy\ntry:\nstat = np.load(\nos.path.join(plane_dir, \"stat.npy\"), allow_pickle=True\n)\nops = np.load(\nos.path.join(plane_dir, \"ops.npy\"), allow_pickle=True\n).item()\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Saving stat, ops and iscell\nself.stats[n] = stat\nself.pops[n] = ops\nself.iscells[n] = iscell\n# count += len(idx)\nelse:\nwarnings.warn(\n\"Couldn't find planes in %s\" % self.path_suite2p, stacklevel=2\n)\nsys.exit()\nelse:\nwarnings.warn(\"No suite2p folder in %s\" % path, stacklevel=2)\nsys.exit()\n# Calcium transients\ndata[\"F\"] = np.transpose(np.vstack(data[\"F\"]))\ndata[\"Fneu\"] = np.transpose(np.vstack(data[\"Fneu\"]))\ndata[\"spks\"] = np.transpose(np.vstack(data[\"spks\"]))\ntime_index = np.arange(0, len(data[\"F\"])) / self.sampling_rate\nself.F = nap.TsdFrame(t=time_index, d=data[\"F\"])\nself.Fneu = nap.TsdFrame(t=time_index, d=data[\"Fneu\"])\nself.spks = nap.TsdFrame(t=time_index, d=data[\"spks\"])\nself.ops = self.pops[0]\nself.iscell = np.vstack([self.iscells[k] for k in self.iscells.keys()])\n# Metadata\nself.plane_info = pd.DataFrame.from_dict({\"plane\": np.hstack(plane_info)})\nreturn\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.save_suite2p_nwb","title":"<code>save_suite2p_nwb(path)</code>","text":"<p>Save the data to NWB. To ensure continuity, this function is based on : https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def save_suite2p_nwb(self, path):\n\"\"\"\n    Save the data to NWB. To ensure continuity, this function is based on :\n    https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nmultiplane = True if len(self.planes) &gt; 1 else False\nops = self.pops[list(self.pops.keys())[0]]\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice = nwbfile.create_device(\nname=self.ophys_information[\"device\"][\"name\"],\ndescription=self.ophys_information[\"device\"][\"description\"],\nmanufacturer=self.ophys_information[\"device\"][\"manufacturer\"],\n)\nimaging_plane = nwbfile.create_imaging_plane(\nname=self.ophys_information[\"ImagingPlane\"][\"name\"],\noptical_channel=OpticalChannel(\nname=self.ophys_information[\"OpticalChannel\"][\"name\"],\ndescription=self.ophys_information[\"OpticalChannel\"][\"description\"],\nemission_lambda=float(\nself.ophys_information[\"OpticalChannel\"][\"emission_lambda\"]\n),\n),\nimaging_rate=self.sampling_rate,\ndescription=self.ophys_information[\"ImagingPlane\"][\"description\"],\ndevice=device,\nexcitation_lambda=float(\nself.ophys_information[\"ImagingPlane\"][\"excitation_lambda\"]\n),\nindicator=self.ophys_information[\"ImagingPlane\"][\"indicator\"],\nlocation=self.ophys_information[\"ImagingPlane\"][\"location\"],\ngrid_spacing=([2.0, 2.0, 30.0] if multiplane else [2.0, 2.0]),\ngrid_spacing_unit=\"microns\",\n)\n# link to external data\nimage_series = TwoPhotonSeries(\nname=\"TwoPhotonSeries\",\ndimension=[ops[\"Ly\"], ops[\"Lx\"]],\nexternal_file=(ops[\"filelist\"] if \"filelist\" in ops else [\"\"]),\nimaging_plane=imaging_plane,\nstarting_frame=[0],\nformat=\"external\",\nstarting_time=0.0,\nrate=ops[\"fs\"] * ops[\"nplanes\"],\n)\nnwbfile.add_acquisition(image_series)\n# processing\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=self.ophys_information[\"PlaneSegmentation\"][\"name\"],\ndescription=self.ophys_information[\"PlaneSegmentation\"][\"description\"],\nimaging_plane=imaging_plane,\n# reference_images=image_series,\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nophys_module.add(img_seg)\nfile_strs = [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]\ntraces = []\nncells = np.zeros(len(self.pops), dtype=np.int_)\nNfr = np.array([self.pops[k][\"nframes\"] for k in self.pops.keys()]).max()\nfor iplane, ops in self.pops.items():\nif iplane == 0:\niscell = self.iscells[iplane]\nfor fstr in file_strs:\ntraces.append(np.load(os.path.join(ops[\"save_path\"], fstr)))\nPlaneCellsIdx = iplane * np.ones(len(iscell))\nelse:\niscell = np.append(\niscell,\nself.iscells[iplane],\naxis=0,\n)\nfor i, fstr in enumerate(file_strs):\ntrace = np.load(os.path.join(ops[\"save_path\"], fstr))\nif trace.shape[1] &lt; Nfr:\nfcat = np.zeros(\n(trace.shape[0], Nfr - trace.shape[1]), \"float32\"\n)\ntrace = np.concatenate((trace, fcat), axis=1)\ntraces[i] = np.append(traces[i], trace, axis=0)\nPlaneCellsIdx = np.append(\nPlaneCellsIdx, iplane * np.ones(len(iscell) - len(PlaneCellsIdx))\n)\nstat = self.stats[iplane]\nncells[iplane] = len(stat)\nfor n in range(ncells[iplane]):\nif multiplane:\npixel_mask = np.array(\n[\nstat[n][\"ypix\"],\nstat[n][\"xpix\"],\niplane * np.ones(stat[n][\"npix\"]),\nstat[n][\"lam\"],\n]\n)\nps.add_roi(voxel_mask=pixel_mask.T)\nelse:\npixel_mask = np.array(\n[stat[n][\"ypix\"], stat[n][\"xpix\"], stat[n][\"lam\"]]\n)\nps.add_roi(pixel_mask=pixel_mask.T)\nps.add_column(\"iscell\", \"two columns - iscell &amp; probcell\", iscell)\nrt_region = []\nfor iplane, ops in self.pops.items():\nif iplane == 0:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(0, ncells[iplane]),\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\nelse:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(\nnp.sum(ncells[:iplane]),\nncells[iplane] + np.sum(ncells[:iplane]),\n)\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\n# FLUORESCENCE (all are required)\nname_strs = [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\nfor i, (fstr, nstr) in enumerate(zip(file_strs, name_strs)):\nfor iplane, ops in self.pops.items():\nroi_resp_series = RoiResponseSeries(\nname=f\"plane{int(iplane)}\",\ndata=traces[i][PlaneCellsIdx == iplane],\nrois=rt_region[iplane],\nunit=\"lumens\",\nrate=ops[\"fs\"],\n)\nif iplane == 0:\nfl = Fluorescence(roi_response_series=roi_resp_series, name=nstr)\nelse:\nfl.add_roi_response_series(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"old_pages/io.suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p_nwb","title":"<code>load_suite2p_nwb(path)</code>","text":"<p>Load suite2p data from NWB</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def load_suite2p_nwb(self, path):\n\"\"\"\n    Load suite2p data from NWB\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\nophys = nwbfile.processing[\"ophys\"]\n#################################################################\n# STATS, OPS and ISCELL\n#################################################################\ndims = nwbfile.acquisition[\"TwoPhotonSeries\"].dimension[:]\nself.ops = {\"Ly\": dims[0], \"Lx\": dims[1]}\nself.rate = nwbfile.acquisition[\n\"TwoPhotonSeries\"\n].imaging_plane.imaging_rate\nself.stats = {0: {}}\nself.iscell = ophys[\"ImageSegmentation\"][\"PlaneSegmentation\"][\n\"iscell\"\n].data[:]\ninfo = pd.DataFrame(\ndata=self.iscell[:, 0].astype(\"int\"), columns=[\"iscell\"]\n)\n#################################################################\n# ROIS\n#################################################################\ntry:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"pixel_mask\"]\nmultiplane = False\nexcept Exception:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"voxel_mask\"]\nmultiplane = True\nidx = np.where(self.iscell[:, 0])[0]\ninfo[\"plane\"] = 0\nfor n in range(len(rois)):\nroi = pd.DataFrame(rois[n])\nif \"z\" in roi.columns:\npl = roi[\"z\"][0]\nelse:\npl = 0\ninfo.loc[n, \"plane\"] = pl\nif pl not in self.stats.keys():\nself.stats[pl] = {}\nif n in idx:\nself.stats[pl][n] = {\n\"xpix\": roi[\"y\"].values,\n\"ypix\": roi[\"x\"].values,\n\"lam\": roi[\"weight\"].values,\n}\n#################################################################\n# Time Series\n#################################################################\nfields = np.intersect1d(\n[\"Fluorescence\", \"Neuropil\", \"Deconvolved\"],\nlist(ophys.fields[\"data_interfaces\"].keys()),\n)\nif len(fields) == 0:\nprint(\n\"No \" + \" or \".join([\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]),\n\"found in nwb {}\".format(self.nwbfilepath),\n)\nreturn False\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\ndata = {}\nif multiplane:\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\nelse:\nplanes = [0]\nfor k, name in zip(\n[\"F\", \"Fneu\", \"spks\"], [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n):\ntmp = []\ntimestamps = []\nfor i, n in enumerate(planes):\nif multiplane:\npl = \"plane{}\".format(n)\nelse:\npl = name  # This doesn't make sense\ntokeep = info[\"iscell\"][info[\"plane\"] == n].values == 1\nd = np.transpose(ophys[name][pl].data[:][tokeep])\nif ophys[name][pl].timestamps is not None:\nt = ophys[name][pl].timestamps[:]\nelse:\nt = (np.arange(0, len(d)) / self.rate) + ophys[name][\npl\n].starting_time\ntmp.append(d)\ntimestamps.append(t)\ndata[k] = nap.TsdFrame(t=timestamps[0], d=np.hstack(tmp))\nif \"F\" in data.keys():\nself.F = data[\"F\"]\nif \"Fneu\" in data.keys():\nself.Fneu = data[\"Fneu\"]\nif \"spks\" in data.keys():\nself.spks = data[\"spks\"]\nself.plane_info = pd.DataFrame(\ndata=info[\"plane\"][info[\"iscell\"] == 1].values, columns=[\"plane\"]\n)\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"old_pages/process.correlograms/","title":"Process.correlograms","text":""},{"location":"old_pages/process.correlograms/#pynapple.process.correlograms.cross_correlogram","title":"<code>cross_correlogram(t1, t2, binsize, windowsize)</code>","text":"<p>Performs the discrete cross-correlogram of two time series. The units should be in s for all arguments. Return the firing rate of the series t2 relative to the timings of t1. See compute_crosscorrelogram, compute_autocorrelogram and compute_eventcorrelogram for wrappers of this function.</p> <p>Parameters:</p> Name Type Description Default <code>t1</code> <code>numpy.ndarray</code> <p>The timestamps of the reference time series (in seconds)</p> required <code>t2</code> <code>numpy.ndarray</code> <p>The timestamps of the target time series (in seconds)</p> required <code>binsize</code> <code>float</code> <p>The bin size (in seconds)</p> required <code>windowsize</code> <code>float</code> <p>The window size (in seconds)</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>The cross-correlogram</p> <code>numpy.ndarray</code> <p>Center of the bins (in s)</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>@jit(nopython=True)\ndef cross_correlogram(t1, t2, binsize, windowsize):\n\"\"\"\n    Performs the discrete cross-correlogram of two time series.\n    The units should be in s for all arguments.\n    Return the firing rate of the series t2 relative to the timings of t1.\n    See compute_crosscorrelogram, compute_autocorrelogram and compute_eventcorrelogram\n    for wrappers of this function.\n    Parameters\n    ----------\n    t1 : numpy.ndarray\n        The timestamps of the reference time series (in seconds)\n    t2 : numpy.ndarray\n        The timestamps of the target time series (in seconds)\n    binsize : float\n        The bin size (in seconds)\n    windowsize : float\n        The window size (in seconds)\n    Returns\n    -------\n    numpy.ndarray\n        The cross-correlogram\n    numpy.ndarray\n        Center of the bins (in s)\n    \"\"\"\n# nbins = ((windowsize//binsize)*2)\nnt1 = len(t1)\nnt2 = len(t2)\nnbins = int((windowsize * 2) // binsize)\nif np.floor(nbins / 2) * 2 == nbins:\nnbins = nbins + 1\nw = (nbins / 2) * binsize\nC = np.zeros(nbins)\ni2 = 0\nfor i1 in range(nt1):\nlbound = t1[i1] - w\nwhile i2 &lt; nt2 and t2[i2] &lt; lbound:\ni2 = i2 + 1\nwhile i2 &gt; 0 and t2[i2 - 1] &gt; lbound:\ni2 = i2 - 1\nrbound = lbound\nleftb = i2\nfor j in range(nbins):\nk = 0\nrbound = rbound + binsize\nwhile leftb &lt; nt2 and t2[leftb] &lt; rbound:\nleftb = leftb + 1\nk = k + 1\nC[j] += k\nC = C / (nt1 * binsize)\nm = -w + binsize / 2\nB = np.zeros(nbins)\nfor j in range(nbins):\nB[j] = m + j * binsize\nreturn C, B\n</code></pre>"},{"location":"old_pages/process.correlograms/#pynapple.process.correlograms.compute_autocorrelogram","title":"<code>compute_autocorrelogram(group, binsize, windowsize, ep=None, norm=True, time_units='s')</code>","text":"<p>Computes the autocorrelogram of a group of Ts/Tsd objects. The group can be passed directly as a TsGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects to auto-correlate</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which auto-corrs are computed. If None, the epoch is the time support of the group.</p> <code>None</code> <code>norm</code> <code>bool, optional</code> <p>If True, autocorrelograms are normalized to baseline (i.e. divided by the average rate) If False, autoorrelograms are returned as the rate (Hz) of the time series (relative to itself)</p> <code>True</code> <code>time_units</code> <code>str, optional</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_autocorrelogram(\ngroup, binsize, windowsize, ep=None, norm=True, time_units=\"s\"\n):\n\"\"\"\n    Computes the autocorrelogram of a group of Ts/Tsd objects.\n    The group can be passed directly as a TsGroup object.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects to auto-correlate\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which auto-corrs are computed.\n        If None, the epoch is the time support of the group.\n    norm : bool, optional\n         If True, autocorrelograms are normalized to baseline (i.e. divided by the average rate)\n         If False, autoorrelograms are returned as the rate (Hz) of the time series (relative to itself)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup\n    \"\"\"\nif type(group) is nap.TsGroup:\nif isinstance(ep, nap.IntervalSet):\nnewgroup = group.restrict(ep)\nelse:\nnewgroup = group\nelse:\nraise RuntimeError(\"Unknown format for group\")\nautocorrs = {}\nbinsize = nap.format_timestamps(np.array([binsize], dtype=np.float64), time_units)[\n0\n]\nwindowsize = nap.format_timestamps(\nnp.array([windowsize], dtype=np.float64), time_units\n)[0]\nfor n in newgroup.keys():\nspk_time = newgroup[n].index.values\nauc, times = cross_correlogram(spk_time, spk_time, binsize, windowsize)\nautocorrs[n] = pd.Series(index=np.round(times, 6), data=auc, dtype=\"float\")\nautocorrs = pd.DataFrame.from_dict(autocorrs)\nif norm:\nautocorrs = autocorrs / newgroup.get_info(\"rate\")\n# Bug here\nif 0 in autocorrs.index.values:\nautocorrs.loc[0] = 0.0\nreturn autocorrs.astype(\"float\")\n</code></pre>"},{"location":"old_pages/process.correlograms/#pynapple.process.correlograms.compute_crosscorrelogram","title":"<code>compute_crosscorrelogram(group, binsize, windowsize, ep=None, norm=True, time_units='s', reverse=False)</code>","text":"<p>Computes all the pairwise cross-correlograms for TsGroup or list/tuple of two TsGroup.</p> <p>If input is TsGroup only, the reference Ts/Tsd and target are chosen based on the builtin itertools.combinations function. For example if indexes are [0,1,2], the function computes cross-correlograms for the pairs (0,1), (0, 2), and (1, 2). The left index gives the reference time series. To reverse the order, set reverse=True.</p> <p>If input is tuple/list of TsGroup, for example group=(group1, group2), the reference for each pairs comes from group1.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup or tuple</code> required <p>binsize : float     The bin size. Default is second.     If different, specify with the parameter time_units ('s' [default], 'ms', 'us'). windowsize : float     The window size. Default is second.     If different, specify with the parameter time_units ('s' [default], 'ms', 'us'). ep : IntervalSet     The epoch on which cross-corrs are computed.     If None, the epoch is the time support of the group. norm : bool, optional     If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)     If False, cross-orrelograms are returned as the rate (Hz) of the target time series ((relative to the reference time series) time_units : str, optional     The time units of the parameters. They have to be consistent for binsize and windowsize.     ('s' [default], 'ms', 'us'). reverse : bool, optional     To reverse the pair order if input is TsGroup</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup or tuple/list of two TsGroups</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_crosscorrelogram(\ngroup, binsize, windowsize, ep=None, norm=True, time_units=\"s\", reverse=False\n):\n\"\"\"\n    Computes all the pairwise cross-correlograms for TsGroup or list/tuple of two TsGroup.\n    If input is TsGroup only, the reference Ts/Tsd and target are chosen based on the builtin itertools.combinations function.\n    For example if indexes are [0,1,2], the function computes cross-correlograms\n    for the pairs (0,1), (0, 2), and (1, 2). The left index gives the reference time series.\n    To reverse the order, set reverse=True.\n    If input is tuple/list of TsGroup, for example group=(group1, group2), the reference for each pairs comes from group1.\n    Parameters\n    ----------\n    group : TsGroup or tuple/list of two TsGroups\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which cross-corrs are computed.\n        If None, the epoch is the time support of the group.\n    norm : bool, optional\n        If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)\n        If False, cross-orrelograms are returned as the rate (Hz) of the target time series ((relative to the reference time series)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    reverse : bool, optional\n        To reverse the pair order if input is TsGroup\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup or tuple/list of two TsGroups\n    \"\"\"\ncrosscorrs = {}\nbinsize = nap.format_timestamps(np.array([binsize], dtype=np.float64), time_units)[\n0\n]\nwindowsize = nap.format_timestamps(\nnp.array([windowsize], dtype=np.float64), time_units\n)[0]\nif isinstance(group, nap.TsGroup):\nif isinstance(ep, nap.IntervalSet):\nnewgroup = group.restrict(ep)\nelse:\nnewgroup = group\nneurons = list(newgroup.keys())\npairs = list(combinations(neurons, 2))\nif reverse:\npairs = list(map(lambda n: (n[1], n[0]), pairs))\nfor i, j in pairs:\nspk1 = newgroup[i].index.values\nspk2 = newgroup[j].index.values\nauc, times = cross_correlogram(spk1, spk2, binsize, windowsize)\ncrosscorrs[(i, j)] = pd.Series(index=times, data=auc, dtype=\"float\")\ncrosscorrs = pd.DataFrame.from_dict(crosscorrs)\nif norm:\nfreq = newgroup.get_info(\"rate\")\nfreq2 = pd.Series(\nindex=pairs, data=list(map(lambda n: freq.loc[n[1]], pairs))\n)\ncrosscorrs = crosscorrs / freq2\nelif (\nisinstance(group, (tuple, list))\nand len(group) == 2\nand all(map(lambda g: isinstance(g, nap.TsGroup), group))\n):\nif isinstance(ep, nap.IntervalSet):\nnewgroup = [group[i].restrict(ep) for i in range(2)]\nelse:\nnewgroup = group\npairs = product(list(newgroup[0].keys()), list(newgroup[1].keys()))\nfor i, j in pairs:\nspk1 = newgroup[0][i].index.values\nspk2 = newgroup[1][j].index.values\nauc, times = cross_correlogram(spk1, spk2, binsize, windowsize)\nif norm:\nauc /= newgroup[1][j].rate\ncrosscorrs[(i, j)] = pd.Series(index=times, data=auc, dtype=\"float\")\ncrosscorrs = pd.DataFrame.from_dict(crosscorrs)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nreturn crosscorrs.astype(\"float\")\n</code></pre>"},{"location":"old_pages/process.correlograms/#pynapple.process.correlograms.compute_eventcorrelogram","title":"<code>compute_eventcorrelogram(group, event, binsize, windowsize, ep=None, norm=True, time_units='s')</code>","text":"<p>Computes the correlograms of a group of Ts/Tsd objects with another single Ts/Tsd object The time of reference is the event times.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects to correlate with the event</p> required <code>event</code> <code>Ts</code> <p>The event to correlate the each of the time series in the group with.</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which cross-corrs are computed. If None, the epoch is the time support of the event.</p> <code>None</code> <code>norm</code> <code>bool, optional</code> <p>If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series) If False, cross-orrelograms are returned as the rate (Hz) of the target time series (relative to the event time series)</p> <code>True</code> <code>time_units</code> <code>str, optional</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_eventcorrelogram(\ngroup, event, binsize, windowsize, ep=None, norm=True, time_units=\"s\"\n):\n\"\"\"\n    Computes the correlograms of a group of Ts/Tsd objects with another single Ts/Tsd object\n    The time of reference is the event times.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects to correlate with the event\n    event : Ts/Tsd\n        The event to correlate the each of the time series in the group with.\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which cross-corrs are computed.\n        If None, the epoch is the time support of the event.\n    norm : bool, optional\n        If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)\n        If False, cross-orrelograms are returned as the rate (Hz) of the target time series (relative to the event time series)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup\n    \"\"\"\nif ep is None:\nep = event.time_support\ntsd1 = event.index.values\nelse:\ntsd1 = event.restrict(ep).index.values\nif type(group) is nap.TsGroup:\nnewgroup = group.restrict(ep)\nelse:\nraise RuntimeError(\"Unknown format for group\")\ncrosscorrs = {}\nbinsize = nap.format_timestamps(np.array([binsize], dtype=np.float64), time_units)[\n0\n]\nwindowsize = nap.format_timestamps(\nnp.array([windowsize], dtype=np.float64), time_units\n)[0]\nfor n in newgroup.keys():\nspk_time = newgroup[n].index.values\nauc, times = cross_correlogram(tsd1, spk_time, binsize, windowsize)\ncrosscorrs[n] = pd.Series(index=times, data=auc, dtype=\"float\")\ncrosscorrs = pd.DataFrame.from_dict(crosscorrs)\nif norm:\ncrosscorrs = crosscorrs / newgroup.get_info(\"rate\")\nreturn crosscorrs.astype(\"float\")\n</code></pre>"},{"location":"old_pages/process.decoding/","title":"Process.decoding","text":""},{"location":"old_pages/process.decoding/#pynapple.process.decoding.decode_1d","title":"<code>decode_1d(tuning_curves, group, ep, bin_size, time_units='s', feature=None)</code>","text":"<p>Performs Bayesian decoding over a one dimensional feature. See: Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.</p> <p>Parameters:</p> Name Type Description Default <code>tuning_curves</code> <code>pandas.DataFrame</code> <p>Each column is the tuning curve of one neuron relative to the feature. Index should be the center of the bin.</p> required <code>group</code> <code>TsGroup or dict of Ts</code> <p>A group of neurons with the same index as tuning curves column names.</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which decoding is computed</p> required <code>bin_size</code> <code>float</code> <p>Bin size. Default is second. Use the parameter time_units to change it.</p> required <code>time_units</code> <code>str, optional</code> <p>Time unit of the bin size ('s' [default], 'ms', 'us').</p> <code>'s'</code> <code>feature</code> <code>Tsd, optional</code> <p>The 1d feature used to compute the tuning curves. Used to correct for occupancy. If feature is not passed, the occupancy is uniform.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tsd</code> <p>The decoded feature</p> <code>TsdFrame</code> <p>The probability distribution of the decoded feature for each time bin</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a dict of Ts/Tsd or TsGroup. If different size of neurons for tuning_curves and group. If indexes don't match between tuning_curves and group.</p> Source code in <code>pynapple/process/decoding.py</code> <pre><code>def decode_1d(tuning_curves, group, ep, bin_size, time_units=\"s\", feature=None):\n\"\"\"\n    Performs Bayesian decoding over a one dimensional feature.\n    See:\n    Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J.\n    (1998). Interpreting neuronal population activity by\n    reconstruction: unified framework with application to\n    hippocampal place cells. Journal of neurophysiology, 79(2),\n    1017-1044.\n    Parameters\n    ----------\n    tuning_curves : pandas.DataFrame\n        Each column is the tuning curve of one neuron relative to the feature.\n        Index should be the center of the bin.\n    group : TsGroup or dict of Ts/Tsd object.\n        A group of neurons with the same index as tuning curves column names.\n    ep : IntervalSet\n        The epoch on which decoding is computed\n    bin_size : float\n        Bin size. Default is second. Use the parameter time_units to change it.\n    time_units : str, optional\n        Time unit of the bin size ('s' [default], 'ms', 'us').\n    feature : Tsd, optional\n        The 1d feature used to compute the tuning curves. Used to correct for occupancy.\n        If feature is not passed, the occupancy is uniform.\n    Returns\n    -------\n    Tsd\n        The decoded feature\n    TsdFrame\n        The probability distribution of the decoded feature for each time bin\n    Raises\n    ------\n    RuntimeError\n        If group is not a dict of Ts/Tsd or TsGroup.\n        If different size of neurons for tuning_curves and group.\n        If indexes don't match between tuning_curves and group.\n    \"\"\"\nif isinstance(group, dict):\nnewgroup = nap.TsGroup(group, time_support=ep)\nelif isinstance(group, nap.TsGroup):\nnewgroup = group.restrict(ep)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nif tuning_curves.shape[1] != len(newgroup):\nraise RuntimeError(\"Different shapes for tuning_curves and group\")\nif not np.all(tuning_curves.columns.values == np.array(newgroup.keys())):\nraise RuntimeError(\"Difference indexes for tuning curves and group keys\")\n# Bin spikes\ncount = newgroup.count(bin_size, ep, time_units)\n# Occupancy\nif feature is None:\noccupancy = np.ones(tuning_curves.shape[0])\nelif isinstance(feature, nap.Tsd):\ndiff = np.diff(tuning_curves.index.values)\nbins = tuning_curves.index.values[:-1] - diff / 2\nbins = np.hstack(\n(bins, [bins[-1] + diff[-1], bins[-1] + 2 * diff[-1]])\n)  # assuming the size of the last 2 bins is equal\noccupancy, _ = np.histogram(feature, bins)\nelse:\nraise RuntimeError(\"Unknown format for feature in decode_1d\")\n# Transforming to pure numpy array\ntc = tuning_curves.values\nct = count.values\nbin_size_s = nap.format_timestamps(\nnp.array([bin_size], dtype=np.float64), time_units\n)[0]\np1 = np.exp(-bin_size_s * tc.sum(1))\np2 = occupancy / occupancy.sum()\nct2 = np.tile(ct[:, np.newaxis, :], (1, tc.shape[0], 1))\np3 = np.prod(tc**ct2, -1)\np = p1 * p2 * p3\np = p / p.sum(1)[:, np.newaxis]\nidxmax = np.argmax(p, 1)\np = nap.TsdFrame(\nt=count.index.values, d=p, time_support=ep, columns=tuning_curves.index.values\n)\ndecoded = nap.Tsd(\nt=count.index.values, d=tuning_curves.index.values[idxmax], time_support=ep\n)\nreturn decoded, p\n</code></pre>"},{"location":"old_pages/process.decoding/#pynapple.process.decoding.decode_2d","title":"<code>decode_2d(tuning_curves, group, ep, bin_size, xy, time_units='s', features=None)</code>","text":"<p>Performs Bayesian decoding over a two dimensional feature. See: Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.</p> <p>Parameters:</p> Name Type Description Default <code>tuning_curves</code> <code>dict</code> <p>Dictionnay of 2d tuning curves (one for each neuron).</p> required <code>group</code> <code>TsGroup or dict of Ts</code> <p>A group of neurons with the same keys as tuning_curves dictionnary.</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which decoding is computed</p> required <code>bin_size</code> <code>float</code> <p>Bin size. Default is second. Use the parameter time_units to change it.</p> required <code>xy</code> <code>tuple</code> <p>A tuple of bin positions for the tuning curves i.e. xy=(x,y)</p> required <code>time_units</code> <code>str, optional</code> <p>Time unit of the bin size ('s' [default], 'ms', 'us').</p> <code>'s'</code> <code>features</code> <code>TsdFrame</code> <p>The 2 columns features used to compute the tuning curves. Used to correct for occupancy. If feature is not passed, the occupancy is uniform.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tsd</code> <p>The decoded feature in 2d</p> <code>numpy.ndarray</code> <p>The probability distribution of the decoded trajectory for each time bin</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a dict of Ts/Tsd or TsGroup. If different size of neurons for tuning_curves and group. If indexes don't match between tuning_curves and group.</p> Source code in <code>pynapple/process/decoding.py</code> <pre><code>def decode_2d(tuning_curves, group, ep, bin_size, xy, time_units=\"s\", features=None):\n\"\"\"\n    Performs Bayesian decoding over a two dimensional feature.\n    See:\n    Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J.\n    (1998). Interpreting neuronal population activity by\n    reconstruction: unified framework with application to\n    hippocampal place cells. Journal of neurophysiology, 79(2),\n    1017-1044.\n    Parameters\n    ----------\n    tuning_curves : dict\n        Dictionnay of 2d tuning curves (one for each neuron).\n    group : TsGroup or dict of Ts/Tsd object.\n        A group of neurons with the same keys as tuning_curves dictionnary.\n    ep : IntervalSet\n        The epoch on which decoding is computed\n    bin_size : float\n        Bin size. Default is second. Use the parameter time_units to change it.\n    xy : tuple\n        A tuple of bin positions for the tuning curves i.e. xy=(x,y)\n    time_units : str, optional\n        Time unit of the bin size ('s' [default], 'ms', 'us').\n    features : TsdFrame\n        The 2 columns features used to compute the tuning curves. Used to correct for occupancy.\n        If feature is not passed, the occupancy is uniform.\n    Returns\n    -------\n    Tsd\n        The decoded feature in 2d\n    numpy.ndarray\n        The probability distribution of the decoded trajectory for each time bin\n    Raises\n    ------\n    RuntimeError\n        If group is not a dict of Ts/Tsd or TsGroup.\n        If different size of neurons for tuning_curves and group.\n        If indexes don't match between tuning_curves and group.\n    \"\"\"\nif type(group) is dict:\nnewgroup = nap.TsGroup(group, time_support=ep)\nnumcells = len(newgroup)\nelif type(group) is nap.TsGroup:\nnewgroup = group.restrict(ep)\nnumcells = len(newgroup)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nif len(tuning_curves) != numcells:\nraise RuntimeError(\"Different shapes for tuning_curves and group\")\nif not np.all(np.array(list(tuning_curves.keys())) == np.array(newgroup.keys())):\nraise RuntimeError(\"Difference indexes for tuning curves and group keys\")\n# Bin spikes\n# if type(newgroup) is not nap.TsdFrame:\ncount = newgroup.count(bin_size, ep, time_units)\n# else:\n#     #Spikes already \"binned\" with continuous TsdFrame input\n#     count = newgroup\nindexes = list(tuning_curves.keys())\n# Occupancy\nif features is None:\noccupancy = np.ones_like(tuning_curves[indexes[0]]).flatten()\nelse:\nbinsxy = []\nfor i in range(len(xy)):\ndiff = np.diff(xy[i])\nbins = xy[i][:-1] - diff / 2\nbins = np.hstack(\n(bins, [bins[-1] + diff[-1], bins[-1] + 2 * diff[-1]])\n)  # assuming the size of the last 2 bins is equal\nbinsxy.append(bins)\noccupancy, _, _ = np.histogram2d(\nfeatures.iloc[:, 0], features.iloc[:, 1], [binsxy[0], binsxy[1]]\n)\noccupancy = occupancy.flatten()\n# Transforming to pure numpy array\ntc = np.array([tuning_curves[i] for i in tuning_curves.keys()])\ntc = tc.reshape(tc.shape[0], np.prod(tc.shape[1:]))\ntc = tc.T\nct = count.values\nbin_size_s = nap.format_timestamps(\nnp.array([bin_size], dtype=np.float64), time_units\n)[0]\np1 = np.exp(-bin_size_s * np.nansum(tc, 1))\np2 = occupancy / occupancy.sum()\nct2 = np.tile(ct[:, np.newaxis, :], (1, tc.shape[0], 1))\np3 = np.nanprod(tc**ct2, -1)\np = p1 * p2 * p3\np = p / p.sum(1)[:, np.newaxis]\nidxmax = np.argmax(p, 1)\np = p.reshape(p.shape[0], len(xy[0]), len(xy[1]))\nidxmax2d = np.unravel_index(idxmax, (len(xy[0]), len(xy[1])))\nif features is not None:\ncols = features.columns\nelse:\ncols = np.arange(2)\ndecoded = nap.TsdFrame(\nt=count.index.values,\nd=np.vstack((xy[0][idxmax2d[0]], xy[1][idxmax2d[1]])).T,\ntime_support=ep,\ncolumns=cols,\n)\nreturn decoded, p\n</code></pre>"},{"location":"old_pages/process.perievent/","title":"Process.perievent","text":""},{"location":"old_pages/process.perievent/#pynapple.process.perievent.compute_perievent","title":"<code>compute_perievent(data, tref, minmax, time_unit='s')</code>","text":"<p>Center ts/tsd/tsgroup object around the timestamps given by the tref argument. minmax indicates the start and end of the window.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Ts</code> <p>The data to align to tref. If Ts/Tsd, returns a TsGroup. If TsGroup, returns a dictionnary of TsGroup</p> required <code>tref</code> <code>Ts</code> <p>The timestamps of the event to align to</p> required <code>minmax</code> <code>tuple or int or float</code> <p>The window size. Can be unequal on each side i.e. (-500, 1000).</p> required <code>time_unit</code> <code>str, optional</code> <p>Time units of the minmax ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A TsGroup if data is a Ts/Tsd or a dictionnary of TsGroup if data is a TsGroup.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if tref is not a Ts/Tsd object or if data is not a Ts/Tsd or TsGroup</p> Source code in <code>pynapple/process/perievent.py</code> <pre><code>def compute_perievent(data, tref, minmax, time_unit=\"s\"):\n\"\"\"\n    Center ts/tsd/tsgroup object around the timestamps given by the tref argument.\n    minmax indicates the start and end of the window.\n    Parameters\n    ----------\n    data : Ts/Tsd/TsGroup\n        The data to align to tref.\n        If Ts/Tsd, returns a TsGroup.\n        If TsGroup, returns a dictionnary of TsGroup\n    tref : Ts/Tsd\n        The timestamps of the event to align to\n    minmax : tuple or int or float\n        The window size. Can be unequal on each side i.e. (-500, 1000).\n    time_unit : str, optional\n        Time units of the minmax ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    dict\n        A TsGroup if data is a Ts/Tsd or\n        a dictionnary of TsGroup if data is a TsGroup.\n    Raises\n    ------\n    RuntimeError\n        if tref is not a Ts/Tsd object or if data is not a Ts/Tsd or TsGroup\n    \"\"\"\nif not isinstance(tref, (nap.Ts, nap.Tsd)):\nraise RuntimeError(\"tref should be a Tsd object.\")\nif isinstance(minmax, float) or isinstance(minmax, int):\nminmax = np.array([minmax, minmax], dtype=np.float64)\nwindow = np.abs(nap.format_timestamps(np.array(minmax), time_unit))\ntime_support = nap.IntervalSet(start=-window[0], end=window[1])\nif isinstance(data, nap.TsGroup):\ntoreturn = {}\nfor n in data.index:\ntoreturn[n] = _align_tsd(data[n], tref, window, time_support)\nreturn toreturn\nelif isinstance(data, (nap.Ts, nap.Tsd)):\nreturn _align_tsd(data, tref, window, time_support)\nelse:\nraise RuntimeError(\"Unknown format for data\")\n</code></pre>"},{"location":"old_pages/process.perievent/#pynapple.process.perievent.compute_event_trigger_average","title":"<code>compute_event_trigger_average(group, feature, binsize, windowsize, ep, time_units='s')</code>","text":"<p>Bin the spike train in binsize and compute the Spike Trigger Average (STA) within windowsize. If C is the spike count matrix and feature is a Tsd array, the function computes the Hankel matrix H from windowsize=(-t1,+t2) by offseting the Tsd array.</p> <p>The STA is then defined as the dot product between H and C divided by the number of spikes.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects that hold the trigger time.</p> required <code>feature</code> <code>Tsd</code> <p>The 1-dimensional feature to average</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which STA are computed</p> required <code>time_units</code> <code>str, optional</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>TsdFrame</code> <p>A TsdFrame of Spike-Trigger Average. Each column is an element from the group.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if group is not a Ts/Tsd or TsGroup</p> Source code in <code>pynapple/process/perievent.py</code> <pre><code>def compute_event_trigger_average(\ngroup, feature, binsize, windowsize, ep, time_units=\"s\"\n):\n\"\"\"\n    Bin the spike train in binsize and compute the Spike Trigger Average (STA) within windowsize.\n    If C is the spike count matrix and feature is a Tsd array, the function computes\n    the Hankel matrix H from windowsize=(-t1,+t2) by offseting the Tsd array.\n    The STA is then defined as the dot product between H and C divided by the number of spikes.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects that hold the trigger time.\n    feature : Tsd\n        The 1-dimensional feature to average\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which STA are computed\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    TsdFrame\n        A TsdFrame of Spike-Trigger Average. Each column is an element from the group.\n    Raises\n    ------\n    RuntimeError\n        if group is not a Ts/Tsd or TsGroup\n    \"\"\"\nif type(group) is not nap.TsGroup:\nraise RuntimeError(\"Unknown format for group\")\nbinsize = nap.format_timestamps(np.array([binsize], dtype=np.float64), time_units)[\n0\n]\nstart = np.abs(\nnap.format_timestamps(np.array([windowsize[0]], dtype=np.float64), time_units)[\n0\n]\n)\nend = np.abs(\nnap.format_timestamps(np.array([windowsize[1]], dtype=np.float64), time_units)[\n0\n]\n)\nidx1 = -np.arange(0, start + binsize, binsize)[::-1][:-1]\nidx2 = np.arange(0, end + binsize, binsize)[1:]\ntime_idx = np.hstack((idx1, np.zeros(1), idx2))\ncount = group.count(binsize, ep)\ntmp = feature.bin_average(binsize, ep)\n# Build the Hankel matrix\nn_p = len(idx1)\nn_f = len(idx2)\npad_tmp = np.pad(tmp, (n_p, n_f))\noffset_tmp = hankel(pad_tmp, pad_tmp[-(n_p + n_f + 1) :])[0 : len(tmp)]\nsta = np.dot(offset_tmp.T, count.values)\nsta = sta / count.sum(0).values\nsta = nap.TsdFrame(t=time_idx, d=sta, columns=group.index)\nreturn sta\n</code></pre>"},{"location":"old_pages/process.randomize/","title":"Process.randomize","text":""},{"location":"old_pages/process.randomize/#pynapple.process.randomize.shift_timestamps","title":"<code>shift_timestamps(ts, min_shift=0.0, max_shift=None)</code>","text":"<p>Shifts all the time stamps of a random amount between min_shift and max_shift, wrapping the end of the time support to the beginning.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to shift. If TsGroup, shifts all Ts in the group independently.</p> required <code>min_shift</code> <code>float, optional</code> <p>minimum shift (default: 0 )</p> <code>0.0</code> <code>max_shift</code> <code>float, optional</code> <p>maximum shift, (default: length of time support)</p> <code>None</code> <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The randomly shifted timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def shift_timestamps(ts, min_shift=0.0, max_shift=None):\n\"\"\"\n    Shifts all the time stamps of a random amount between min_shift and max_shift, wrapping the\n    end of the time support to the beginning.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to shift. If TsGroup, shifts all Ts in the group independently.\n    min_shift : float, optional\n        minimum shift (default: 0 )\n    max_shift : float, optional\n        maximum shift, (default: length of time support)\n    Returns\n    -------\n    Ts or TsGroup\n        The randomly shifted timestamps\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _shift_ts,\nnap.ts_group.TsGroup: _shift_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nstrategy = strategies[type(ts)]\nreturn strategy(ts, min_shift, max_shift)\n</code></pre>"},{"location":"old_pages/process.randomize/#pynapple.process.randomize.shuffle_ts_intervals","title":"<code>shuffle_ts_intervals(ts, min_shift=0.0, max_shift=None)</code>","text":"<p>Randomizes the timestamps by shuffling the intervals between them.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to randomize. If TsGroup, randomizes all Ts in the group independently.</p> required <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The randomized timestamps, with shuffled intervals</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def shuffle_ts_intervals(ts, min_shift=0.0, max_shift=None):\n\"\"\"\n    Randomizes the timestamps by shuffling the intervals between them.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to randomize. If TsGroup, randomizes all Ts in the group independently.\n    Returns\n    -------\n    Ts or TsGroup\n        The randomized timestamps, with shuffled intervals\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _shuffle_intervals_ts,\nnap.ts_group.TsGroup: _shuffle_intervals_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nstrategy = strategies[type(ts)]\nreturn strategy(ts)\n</code></pre>"},{"location":"old_pages/process.randomize/#pynapple.process.randomize.jitter_timestamps","title":"<code>jitter_timestamps(ts, max_jitter=None, keep_tsupport=False)</code>","text":"<p>Jitters each time stamp independently of random amounts uniformly drawn between -max_jitter and max_jitter.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to jitter. If TsGroup, jitter is applied to each element of the group.</p> required <code>max_jitter</code> <code>float</code> <p>maximum jitter</p> <code>None</code> <code>keep_tsupport</code> <p>If True, keep time support of the input. The number of timestamps will not be conserved. If False, the time support is inferred from the jittered timestamps. The number of tmestamps is conserved. (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The jittered timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def jitter_timestamps(ts, max_jitter=None, keep_tsupport=False):\n\"\"\"\n    Jitters each time stamp independently of random amounts uniformly drawn between -max_jitter and max_jitter.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to jitter. If TsGroup, jitter is applied to each element of the group.\n    max_jitter : float\n        maximum jitter\n    keep_tsupport: bool, optional\n        If True, keep time support of the input. The number of timestamps will not be conserved.\n        If False, the time support is inferred from the jittered timestamps. The number of tmestamps\n        is conserved. (default: False)\n    Returns\n    -------\n    Ts or TsGroup\n        The jittered timestamps\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _jitter_ts,\nnap.ts_group.TsGroup: _jitter_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nif max_jitter is None:\nraise TypeError(\"missing required argument: max_jitter \")\nstrategy = strategies[type(ts)]\nreturn strategy(ts, max_jitter, keep_tsupport)\n</code></pre>"},{"location":"old_pages/process.randomize/#pynapple.process.randomize.resample_timestamps","title":"<code>resample_timestamps(ts)</code>","text":"<p>Resamples the timestamps in the time support, with uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to resample. If TsGroup, each Ts object in the group is independently resampled, in the time support of the whole group.</p> required <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The resampled timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def resample_timestamps(ts):\n\"\"\"\n    Resamples the timestamps in the time support, with uniform distribution.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to resample. If TsGroup, each Ts object in the group is independently\n        resampled, in the time support of the whole group.\n    Returns\n    -------\n    Ts or TsGroup\n        The resampled timestamps\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _resample_ts,\nnap.ts_group.TsGroup: _resample_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nstrategy = strategies[type(ts)]\nreturn strategy(ts)\n</code></pre>"},{"location":"old_pages/process.tuning_curves/","title":"Process.tuning curves","text":"<p>Summary</p>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_discrete_tuning_curves","title":"<code>compute_discrete_tuning_curves(group, dict_ep)</code>","text":"<pre><code>Compute discrete tuning curves of a TsGroup using a dictionnary of epochs.\n</code></pre> <p>The function returns a pandas DataFrame with each row being a key of the dictionnary of epochs and each column being a neurons.</p> <p>This function can typically being used for a set of stimulus being presented for multiple epochs. An example of the dictionnary is :</p> <pre><code>&gt;&gt;&gt; dict_ep =  {\n        \"stim0\": nap.IntervalSet(start=0, end=1),\n        \"stim1\":nap.IntervalSet(start=2, end=3)\n    }\n</code></pre> <p>In this case, the function will return a pandas DataFrame :</p> <pre><code>&gt;&gt;&gt; tc\n           neuron0    neuron1    neuron2\nstim0        0 Hz       1 Hz       2 Hz\nstim1        3 Hz       4 Hz       5 Hz\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>nap.TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>dict_ep</code> <code>dict</code> <p>Dictionary of IntervalSets</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>Table of firing rate for each neuron and each IntervalSet</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_discrete_tuning_curves(group, dict_ep):\n\"\"\"\n        Compute discrete tuning curves of a TsGroup using a dictionnary of epochs.\n    The function returns a pandas DataFrame with each row being a key of the dictionnary of epochs\n    and each column being a neurons.\n       This function can typically being used for a set of stimulus being presented for multiple epochs.\n    An example of the dictionnary is :\n        &gt;&gt;&gt; dict_ep =  {\n                \"stim0\": nap.IntervalSet(start=0, end=1),\n                \"stim1\":nap.IntervalSet(start=2, end=3)\n            }\n    In this case, the function will return a pandas DataFrame :\n        &gt;&gt;&gt; tc\n                   neuron0    neuron1    neuron2\n        stim0        0 Hz       1 Hz       2 Hz\n        stim1        3 Hz       4 Hz       5 Hz\n    Parameters\n    ----------\n    group : nap.TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    dict_ep : dict\n        Dictionary of IntervalSets\n    Returns\n    -------\n    pandas.DataFrame\n        Table of firing rate for each neuron and each IntervalSet\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object.\n    \"\"\"\nif not isinstance(group, nap.TsGroup):\nraise RuntimeError(\"Unknown format for group\")\nidx = np.sort(list(dict_ep.keys()))\ntuning_curves = pd.DataFrame(index=idx, columns=list(group.keys()), data=0)\nfor k in dict_ep.keys():\nif not isinstance(dict_ep[k], nap.IntervalSet):\nraise RuntimeError(\"Key {} in dict_ep is not an IntervalSet\".format(k))\nfor n in group.keys():\ntuning_curves.loc[k, n] = float(len(group[n].restrict(dict_ep[k])))\ntuning_curves.loc[k] = tuning_curves.loc[k] / dict_ep[k].tot_length(\"s\")\nreturn tuning_curves\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_1d_tuning_curves","title":"<code>compute_1d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None)</code>","text":"<p>Computes 1-dimensional tuning curves relative to a 1d feature.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>feature</code> <code>Tsd</code> <p>The 1-dimensional target feature (e.g. head-direction)</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curve</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>DataFrame to hold the tuning curves</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None):\n\"\"\"\n    Computes 1-dimensional tuning curves relative to a 1d feature.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    feature : Tsd\n        The 1-dimensional target feature (e.g. head-direction)\n    nb_bins : int\n        Number of bins in the tuning curve\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame to hold the tuning curves\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object.\n    \"\"\"\nif not isinstance(group, nap.TsGroup):\nraise RuntimeError(\"Unknown format for group\")\nif minmax is None:\nbins = np.linspace(np.min(feature), np.max(feature), nb_bins + 1)\nelse:\nbins = np.linspace(minmax[0], minmax[1], nb_bins + 1)\nidx = bins[0:-1] + np.diff(bins) / 2\ntuning_curves = pd.DataFrame(index=idx, columns=list(group.keys()))\nif isinstance(ep, nap.IntervalSet):\ngroup_value = group.value_from(feature, ep)\noccupancy, _ = np.histogram(feature.restrict(ep).values, bins)\nelse:\ngroup_value = group.value_from(feature)\noccupancy, _ = np.histogram(feature.values, bins)\nfor k in group_value:\ncount, _ = np.histogram(group_value[k].values, bins)\ncount = count / occupancy\ncount[np.isnan(count)] = 0.0\ntuning_curves[k] = count\ntuning_curves[k] = count * feature.rate\nreturn tuning_curves\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_2d_tuning_curves","title":"<code>compute_2d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None)</code>","text":"<p>Computes 2-dimensional tuning curves relative to a 2d feature</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>feature</code> <code>TsdFrame</code> <p>The 2d feature (i.e. 2 columns features).</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curves</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves given as: (minx, maxx, miny, maxy) If None, the boundaries are inferred from the target variable</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: </p> <p>tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).</p> <p>xy (list): List of bins center in the two dimensions</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object or if feature is not 2 columns only.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None):\n\"\"\"\n    Computes 2-dimensional tuning curves relative to a 2d feature\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    feature : TsdFrame\n        The 2d feature (i.e. 2 columns features).\n    nb_bins : int\n        Number of bins in the tuning curves\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves given as:\n        (minx, maxx, miny, maxy)\n        If None, the boundaries are inferred from the target variable\n    Returns\n    -------\n    tuple\n        A tuple containing: \\n\n        tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).\\n\n        xy (list): List of bins center in the two dimensions\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object or if feature is not 2 columns only.\n    \"\"\"\nif feature.shape[1] != 2:\nraise RuntimeError(\"feature should have 2 columns only.\")\nif type(group) is not nap.TsGroup:\nraise RuntimeError(\"Unknown format for group\")\nif isinstance(ep, nap.IntervalSet):\nfeature = feature.restrict(ep)\nelse:\nep = feature.time_support\ncols = list(feature.columns)\ngroups_value = {}\nbinsxy = {}\nfor i, c in enumerate(cols):\ngroups_value[c] = group.value_from(feature[c], ep)\nif minmax is None:\nbins = np.linspace(np.min(feature[c]), np.max(feature[c]), nb_bins + 1)\nelse:\nbins = np.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins + 1)\nbinsxy[c] = bins\noccupancy, _, _ = np.histogram2d(\nfeature[cols[0]].values,\nfeature[cols[1]].values,\n[binsxy[cols[0]], binsxy[cols[1]]],\n)\ntc = {}\nfor n in group.keys():\ncount, _, _ = np.histogram2d(\ngroups_value[cols[0]][n].values,\ngroups_value[cols[1]][n].values,\n[binsxy[cols[0]], binsxy[cols[1]]],\n)\ncount = count / occupancy\n# count[np.isnan(count)] = 0.0\ntc[n] = count * feature.rate\nxy = [binsxy[c][0:-1] + np.diff(binsxy[c]) / 2 for c in binsxy.keys()]\nreturn tc, xy\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_1d_mutual_info","title":"<code>compute_1d_mutual_info(tc, feature, ep=None, minmax=None, bitssec=False)</code>","text":"<p>Mutual information as defined in</p> <p>Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993). An information-theoretic approach to deciphering the hippocampal code. In Advances in neural information processing systems (pp. 1030-1037).</p> <p>Parameters:</p> Name Type Description Default <code>tc</code> <code>pandas.DataFrame or numpy.ndarray</code> <p>Tuning curves in columns</p> required <code>feature</code> <code>Tsd</code> <p>The feature that was used to compute the tuning curves</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch over which the tuning curves were computed If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <code>bitssec</code> <code>bool, optional</code> <p>By default, the function return bits per spikes. Set to true for bits per seconds</p> <code>False</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>Spatial Information (default is bits/spikes)</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_mutual_info(tc, feature, ep=None, minmax=None, bitssec=False):\n\"\"\"\n    Mutual information as defined in\n    Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993).\n    An information-theoretic approach to deciphering the hippocampal code.\n    In Advances in neural information processing systems (pp. 1030-1037).\n    Parameters\n    ----------\n    tc : pandas.DataFrame or numpy.ndarray\n        Tuning curves in columns\n    feature : Tsd\n        The feature that was used to compute the tuning curves\n    ep : IntervalSet, optional\n        The epoch over which the tuning curves were computed\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n    bitssec : bool, optional\n        By default, the function return bits per spikes.\n        Set to true for bits per seconds\n    Returns\n    -------\n    pandas.DataFrame\n        Spatial Information (default is bits/spikes)\n    \"\"\"\nif isinstance(tc, pd.DataFrame):\ncolumns = tc.columns.values\nfx = np.atleast_2d(tc.values)\nelif isinstance(tc, np.ndarray):\nfx = np.atleast_2d(tc)\ncolumns = np.arange(tc.shape[1])\nnb_bins = tc.shape[0] + 1\nif minmax is None:\nbins = np.linspace(np.min(feature), np.max(feature), nb_bins)\nelse:\nbins = np.linspace(minmax[0], minmax[1], nb_bins)\nif isinstance(ep, nap.IntervalSet):\noccupancy, _ = np.histogram(feature.restrict(ep).values, bins)\nelse:\noccupancy, _ = np.histogram(feature.values, bins)\noccupancy = occupancy / occupancy.sum()\noccupancy = occupancy[:, np.newaxis]\nfr = np.sum(fx * occupancy, 0)\nfxfr = fx / fr\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nlogfx = np.log2(fxfr)\nlogfx[np.isinf(logfx)] = 0.0\nSI = np.sum(occupancy * fx * logfx, 0)\nif bitssec:\nSI = pd.DataFrame(index=columns, columns=[\"SI\"], data=SI)\nreturn SI\nelse:\nSI = SI / fr\nSI = pd.DataFrame(index=columns, columns=[\"SI\"], data=SI)\nreturn SI\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_2d_mutual_info","title":"<code>compute_2d_mutual_info(tc, features, ep=None, minmax=None, bitssec=False)</code>","text":"<p>Mutual information as defined in</p> <p>Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993). An information-theoretic approach to deciphering the hippocampal code. In Advances in neural information processing systems (pp. 1030-1037).</p> <p>Parameters:</p> Name Type Description Default <code>tc</code> <code>dict or numpy.ndarray</code> <p>If array, first dimension should be the neuron</p> required <code>features</code> <code>TsdFrame</code> <p>The 2 columns features that were used to compute the tuning curves</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch over which the tuning curves were computed If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target features</p> <code>None</code> <code>bitssec</code> <code>bool, optional</code> <p>By default, the function return bits per spikes. Set to true for bits per seconds</p> <code>False</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>Spatial Information (default is bits/spikes)</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_mutual_info(tc, features, ep=None, minmax=None, bitssec=False):\n\"\"\"\n    Mutual information as defined in\n    Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993).\n    An information-theoretic approach to deciphering the hippocampal code.\n    In Advances in neural information processing systems (pp. 1030-1037).\n    Parameters\n    ----------\n    tc : dict or numpy.ndarray\n        If array, first dimension should be the neuron\n    features : TsdFrame\n        The 2 columns features that were used to compute the tuning curves\n    ep : IntervalSet, optional\n        The epoch over which the tuning curves were computed\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target features\n    bitssec : bool, optional\n        By default, the function return bits per spikes.\n        Set to true for bits per seconds\n    Returns\n    -------\n    pandas.DataFrame\n        Spatial Information (default is bits/spikes)\n    \"\"\"\n# A bit tedious here\nif type(tc) is dict:\nfx = np.array([tc[i] for i in tc.keys()])\nidx = list(tc.keys())\nelif type(tc) is np.ndarray:\nfx = tc\nidx = np.arange(len(tc))\nnb_bins = (fx.shape[1] + 1, fx.shape[2] + 1)\ncols = features.columns\nbins = []\nfor i, c in enumerate(cols):\nif minmax is None:\nbins.append(\nnp.linspace(np.min(features[c]), np.max(features[c]), nb_bins[i])\n)\nelse:\nbins.append(\nnp.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins[i])\n)\nif isinstance(ep, nap.IntervalSet):\nfeatures = features.restrict(ep)\noccupancy, _, _ = np.histogram2d(\nfeatures[cols[0]].values, features[cols[1]].values, [bins[0], bins[1]]\n)\noccupancy = occupancy / occupancy.sum()\nfr = np.nansum(fx * occupancy, (1, 2))\nfr = fr[:, np.newaxis, np.newaxis]\nfxfr = fx / fr\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nlogfx = np.log2(fxfr)\nlogfx[np.isinf(logfx)] = 0.0\nSI = np.nansum(occupancy * fx * logfx, (1, 2))\nif bitssec:\nSI = pd.DataFrame(index=idx, columns=[\"SI\"], data=SI)\nreturn SI\nelse:\nSI = SI / fr[:, 0, 0]\nSI = pd.DataFrame(index=idx, columns=[\"SI\"], data=SI)\nreturn SI\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_1d_tuning_curves_continous","title":"<code>compute_1d_tuning_curves_continous(tsdframe, feature, nb_bins, ep=None, minmax=None)</code>","text":"<p>Computes 1-dimensional tuning curves relative to a feature with continous data.</p> <p>Parameters:</p> Name Type Description Default <code>tsdframe</code> <code>Tsd or TsdFrame</code> <p>Input data (e.g. continus calcium data where each column is the calcium activity of one neuron)</p> required <code>feature</code> <code>Tsd</code> <p>The feature (one column)</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curves</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>DataFrame to hold the tuning curves</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tsdframe is not a Tsd or a TsdFrame object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_tuning_curves_continous(\ntsdframe, feature, nb_bins, ep=None, minmax=None\n):\n\"\"\"\n    Computes 1-dimensional tuning curves relative to a feature with continous data.\n    Parameters\n    ----------\n    tsdframe : Tsd or TsdFrame\n        Input data (e.g. continus calcium data\n        where each column is the calcium activity of one neuron)\n    feature : Tsd\n        The feature (one column)\n    nb_bins : int\n        Number of bins in the tuning curves\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame to hold the tuning curves\n    Raises\n    ------\n    RuntimeError\n        If tsdframe is not a Tsd or a TsdFrame object.\n    \"\"\"\nif not isinstance(tsdframe, (nap.Tsd, nap.TsdFrame)):\nraise RuntimeError(\"Unknown format for tsdframe.\")\nif isinstance(ep, nap.IntervalSet):\nfeature = feature.restrict(ep)\ntsdframe = tsdframe.restrict(ep)\nelse:\ntsdframe = tsdframe.restrict(feature.time_support)\nif minmax is None:\nbins = np.linspace(np.min(feature), np.max(feature), nb_bins + 1)\nelse:\nbins = np.linspace(minmax[0], minmax[1], nb_bins + 1)\nalign_times = tsdframe.value_from(feature)\nidx = np.digitize(align_times.values, bins) - 1\ntmp = pd.DataFrame(tsdframe).groupby(idx).mean()\ntmp = tmp.reindex(np.arange(0, len(bins) - 1))\ntmp.index = pd.Index(bins[0:-1] + np.diff(bins) / 2)\ntmp = tmp.fillna(0)\nreturn pd.DataFrame(tmp)\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_2d_tuning_curves_continuous","title":"<code>compute_2d_tuning_curves_continuous(tsdframe, features, nb_bins, ep=None, minmax=None)</code>","text":"<p>Computes 2-dimensional tuning curves relative to a 2d feature with continous data.</p> <p>Parameters:</p> Name Type Description Default <code>tsdframe</code> <code>Tsd or TsdFrame</code> <p>Input data (e.g. continuous calcium data where each column is the calcium activity of one neuron)</p> required <code>features</code> <code>TsdFrame</code> <p>The 2d feature (two columns)</p> required <code>nb_bins</code> <code>int or tuple</code> <p>Number of bins in the tuning curves (separate for 2 feature dimensions if tuple provided)</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. Should be a tuple of minx, maxx, miny, maxy If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: </p> <p>tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).</p> <p>xy (list): List of bins center in the two dimensions</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tsdframe is not a Tsd/TsdFrame or if features is not 2 columns</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_tuning_curves_continuous(\ntsdframe, features, nb_bins, ep=None, minmax=None\n):\n\"\"\"\n    Computes 2-dimensional tuning curves relative to a 2d feature with continous data.\n    Parameters\n    ----------\n    tsdframe : Tsd or TsdFrame\n        Input data (e.g. continuous calcium data\n        where each column is the calcium activity of one neuron)\n    features : TsdFrame\n        The 2d feature (two columns)\n    nb_bins : int or tuple\n        Number of bins in the tuning curves (separate for 2 feature dimensions if tuple provided)\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        Should be a tuple of minx, maxx, miny, maxy\n        If None, the boundaries are inferred from the target feature\n    Returns\n    -------\n    tuple\n        A tuple containing: \\n\n        tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).\\n\n        xy (list): List of bins center in the two dimensions\n    Raises\n    ------\n    RuntimeError\n        If tsdframe is not a Tsd/TsdFrame or if features is not 2 columns\n    \"\"\"\nif not isinstance(tsdframe, (nap.Tsd, nap.TsdFrame)):\nraise RuntimeError(\"Unknown format for tsdframe.\")\nif not isinstance(features, nap.TsdFrame):\nraise RuntimeError(\"Unknown format for features.\")\nif isinstance(ep, nap.IntervalSet):\nfeatures = features.restrict(ep)\ntsdframe = tsdframe.restrict(ep)\nelse:\ntsdframe = tsdframe.restrict(features.time_support)\nif features.shape[1] != 2:\nraise RuntimeError(\"features input is not 2 columns.\")\nif isinstance(nb_bins, int):\nnb_bins = (nb_bins, nb_bins)\nelif len(nb_bins) != 2:\nraise RuntimeError(\"nb_bins should be int or tuple of 2 ints\")\ncols = list(features.columns)\nbinsxy = {}\nidxs = {}\nfor i, c in enumerate(cols):\nif minmax is None:\nbins = np.linspace(np.min(features[c]), np.max(features[c]), nb_bins[i] + 1)\nelse:\nbins = np.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins[i] + 1)\nalign_times = tsdframe.value_from(features[c], ep)\nidxs[c] = np.digitize(align_times.values, bins) - 1\nbinsxy[c] = bins\nidxs = pd.DataFrame(idxs)\ntc_np = np.zeros((tsdframe.shape[1], nb_bins[0], nb_bins[1])) * np.nan\nfor k, tmp in idxs.groupby(cols):\nif (0 &lt;= k[0] &lt; nb_bins[0]) and (0 &lt;= k[1] &lt; nb_bins[1]):\ntc_np[:, k[0], k[1]] = tsdframe.iloc[tmp.index].mean(0).values\ntc_np[np.isnan(tc_np)] = 0.0\nxy = [binsxy[c][0:-1] + np.diff(binsxy[c]) / 2 for c in binsxy.keys()]\ntc = {c: tc_np[i] for i, c in enumerate(tsdframe.columns)}\nreturn tc, xy\n</code></pre>"},{"location":"old_pages/process.tuning_curves/#pynapple.process.tuning_curves.compute_1d_poisson_glm","title":"<code>compute_1d_poisson_glm(group, feature, binsize, windowsize, ep, time_units='s', niter=100, tolerance=1e-05)</code>","text":"<p>Poisson GLM</p> <p>Warning : this function is still experimental!</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>Spike trains</p> required <code>feature</code> <code>Tsd</code> <p>The regressors</p> required <code>binsize</code> <code>float</code> <p>Bin size</p> required <code>windowsize</code> <code>Float</code> <p>The window for offsetting the regressors</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>On which epoch to perfom the GLM</p> required <code>time_units</code> <code>str, optional</code> <p>Time units of binsize and windowsize</p> <code>'s'</code> <code>niter</code> <code>int, optional</code> <p>Number of iteration for fitting the GLM</p> <code>100</code> <code>tolerance</code> <code>float, optional</code> <p>Tolerance for stopping the IRLS</p> <code>1e-05</code> <p>Returns:</p> Type Description <code>tuple</code> <p>regressors : TsdFrame</p> <p>offset : pandas.Series</p> <p>prediction : TsdFrame</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if group is not a TsGroup</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_poisson_glm(\ngroup, feature, binsize, windowsize, ep, time_units=\"s\", niter=100, tolerance=1e-5\n):\n\"\"\"\n    Poisson GLM\n    Warning : this function is still experimental!\n    Parameters\n    ----------\n    group : TsGroup\n        Spike trains\n    feature : Tsd\n        The regressors\n    binsize : float\n        Bin size\n    windowsize : Float\n        The window for offsetting the regressors\n    ep : IntervalSet, optional\n        On which epoch to perfom the GLM\n    time_units : str, optional\n        Time units of binsize and windowsize\n    niter : int, optional\n        Number of iteration for fitting the GLM\n    tolerance : float, optional\n        Tolerance for stopping the IRLS\n    Returns\n    -------\n    tuple\n        regressors : TsdFrame\\n\n        offset : pandas.Series\\n\n        prediction : TsdFrame\\n\n    Raises\n    ------\n    RuntimeError\n        if group is not a TsGroup\n    \"\"\"\nif type(group) is nap.TsGroup:\nnewgroup = group.restrict(ep)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nbinsize = nap.format_timestamps(binsize, time_units)[0]\nwindowsize = nap.format_timestamps(windowsize, time_units)[0]\n# Bin the spike train\ncount = newgroup.count(binsize)\n# Downsample the feature to binsize\ntidx = []\ndfeat = []\nfor i in ep.index:\nbins = np.arange(ep.start[i], ep.end[i] + binsize, binsize)\nidx = np.digitize(feature.index.values, bins) - 1\ntmp = feature.groupby(idx).mean()\ntidx.append(bins[0:-1] + np.diff(bins) / 2)\ndfeat.append(tmp)\ndfeat = nap.Tsd(t=np.hstack(tidx), d=np.hstack(dfeat), time_support=ep)\n# Build the Hankel matrix\nnt = np.abs(windowsize // binsize).astype(\"int\") + 1\nX = hankel(\nnp.hstack((np.zeros(nt - 1), dfeat.values))[: -nt + 1], dfeat.values[-nt:]\n)\nX = np.hstack((np.ones((len(dfeat), 1)), X))\n# Fitting GLM for each neuron\nregressors = []\nfor i, n in enumerate(group.keys()):\nprint(\"Fitting Poisson GLM for unit %i\" % n)\nb = nap.jitted_functions.jit_poisson_IRLS(\nX, count[n].values, niter=niter, tolerance=tolerance\n)\nregressors.append(b)\nregressors = np.array(regressors).T\noffset = regressors[0]\nregressors = regressors[1:]\nregressors = nap.TsdFrame(\nt=np.arange(-nt + 1, 1) * binsize, d=regressors, columns=list(group.keys())\n)\noffset = pd.Series(index=group.keys(), data=offset)\nprediction = nap.TsdFrame(\nt=dfeat.index.values,\nd=np.exp(np.dot(X[:, 1:], regressors.values) + offset.values) * binsize,\n)\nreturn (regressors, offset, prediction)\n</code></pre>"},{"location":"reference/SUMMARY/","title":"Modules","text":"<ul> <li>core<ul> <li>interval_set</li> <li>jitted_functions</li> <li>time_series</li> <li>time_units</li> <li>ts_group</li> </ul> </li> <li>io<ul> <li>cnmfe</li> <li>ephys_gui</li> <li>folder</li> <li>interface_npz</li> <li>interface_nwb</li> <li>loader</li> <li>loader_gui</li> <li>misc</li> <li>neurosuite</li> <li>ophys_gui</li> <li>phy</li> <li>suite2p</li> </ul> </li> <li>process<ul> <li>correlograms</li> <li>decoding</li> <li>perievent</li> <li>randomize</li> <li>tuning_curves</li> </ul> </li> </ul>"},{"location":"reference/core/","title":"Core","text":""},{"location":"reference/core/interval_set/","title":"Interval set","text":""},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet","title":"<code>IntervalSet</code>","text":"<p>         Bases: <code>pd.DataFrame</code></p> <p>A subclass of pandas.DataFrame representing a (irregular) set of time intervals in elapsed time, with relative operations</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>class IntervalSet(pd.DataFrame):\n# class IntervalSet():\n\"\"\"\n    A subclass of pandas.DataFrame representing a (irregular) set of time intervals in elapsed time, with relative operations\n    \"\"\"\ndef __init__(self, start, end=None, time_units=\"s\", **kwargs):\n\"\"\"\n        IntervalSet initializer\n        If start and end and not aligned, meaning that \\n\n        1. len(start) != len(end)\n        2. end[i] &gt; start[i]\n        3. start[i+1] &gt; end[i]\n        4. start and end are not sorted,\n        IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point\n        Parameters\n        ----------\n        start : numpy.ndarray or number or pandas.DataFrame\n            Beginning of intervals\n        end : numpy.ndarray or number, optional\n            Ends of intervals\n        time_units : str, optional\n            Time unit of the intervals ('us', 'ms', 's' [default])\n        **kwargs\n            Additional parameters passed ot pandas.DataFrame\n        Returns\n        -------\n        IntervalSet\n            _\n        Raises\n        ------\n        RuntimeError\n            Description\n        ValueError\n            If a pandas.DataFrame is passed, it should contains\n            a column 'start' and a column 'end'.\n        \"\"\"\nif end is None:\ndf = pd.DataFrame(start)\nif \"start\" not in df.columns or \"end\" not in df.columns:\nraise ValueError(\"wrong columns name\")\nstart = df[\"start\"].values.astype(np.float64)\nend = df[\"end\"].values.astype(np.float64)\nstart = sort_timestamps(format_timestamps(start.ravel(), time_units))\nend = sort_timestamps(format_timestamps(end.ravel(), time_units))\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\nself._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\nreturn\nstart = np.array(start).astype(np.float64)\nend = np.array(end).astype(np.float64)\nstart = format_timestamps(np.array(start).ravel(), time_units)\nend = format_timestamps(np.array(end).ravel(), time_units)\nif len(start) != len(end):\nraise RuntimeError(\"Starts end ends are not of the same length\")\nif not (np.diff(start) &gt; 0).all():\nwarnings.warn(\"start is not sorted.\", stacklevel=2)\nstart = np.sort(start)\nif not (np.diff(end) &gt; 0).all():\nwarnings.warn(\"end is not sorted.\", stacklevel=2)\nend = np.sort(end)\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\n# self._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\ndef __repr__(self):\nreturn self.as_units(\"s\").__repr__()\ndef __str__(self):\nreturn self.__repr__()\ndef time_span(self):\n\"\"\"\n        Time span of the interval set.\n        Returns\n        -------\n        out: IntervalSet\n            an IntervalSet with a single interval encompassing the whole IntervalSet\n        \"\"\"\ns = self[\"start\"][0]\ne = self[\"end\"].iloc[-1]\nreturn IntervalSet(s, e)\ndef tot_length(self, time_units=\"s\"):\n\"\"\"\n        Total elapsed time in the set.\n        Parameters\n        ----------\n        time_units : None, optional\n            The time units to return the result in ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: float\n            _\n        \"\"\"\ntot_l = (self[\"end\"] - self[\"start\"]).sum()\nreturn return_timestamps(np.array([tot_l]), time_units)[0]\ndef intersect(self, a):\n\"\"\"\n        set intersection of IntervalSet\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to intersect self with\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitintersect(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\ndef union(self, a):\n\"\"\"\n        set union of IntervalSet\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to union self with\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitunion(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\ndef set_diff(self, a):\n\"\"\"\n        set difference of IntervalSet\n        Parameters\n        ----------\n        a : IntervalSet\n            the IntervalSet to set-substract from self\n        Returns\n        -------\n        out: IntervalSet\n            _\n        \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitdiff(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\ndef in_interval(self, tsd):\n\"\"\"\n        finds out in which element of the interval set each point in a time series fits.\n        NaNs for those that don't fit an interval\n        Parameters\n        ----------\n        tsd : Tsd\n            The tsd to be binned\n        Returns\n        -------\n        out: numpy.ndarray\n            an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet\n        \"\"\"\ntimes = tsd.index.values\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nreturn jitin_interval(times, starts, ends)\ndef drop_short_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Drops the short intervals in the interval set.\n        Parameters\n        ----------\n        threshold : numeric\n            Time threshold for \"short\" intervals\n        time_units : None, optional\n            The time units for the treshold ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: IntervalSet\n            A copied IntervalSet with the dropped intervals\n        \"\"\"\nthreshold = format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &gt; threshold].reset_index(\ndrop=True\n)\ndef drop_long_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Drops the long intervals in the interval set.\n        Parameters\n        ----------\n        threshold : numeric\n            Time threshold for \"long\" intervals\n        time_units : None, optional\n            The time units for the treshold ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: IntervalSet\n            A copied IntervalSet with the dropped intervals\n        \"\"\"\nthreshold = format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &lt; threshold].reset_index(\ndrop=True\n)\ndef as_units(self, units=\"s\"):\n\"\"\"\n        returns a DataFrame with time expressed in the desired unit\n        Parameters\n        ----------\n        units : None, optional\n            'us', 'ms', or 's' [default]\n        Returns\n        -------\n        out: pandas.DataFrame\n            DataFrame with adjusted times\n        \"\"\"\ndata = self.values.copy()\ndata = return_timestamps(data, units)\nif units == \"us\":\ndata = data.astype(np.int64)\ndf = pd.DataFrame(index=self.index.values, data=data, columns=self.columns)\nreturn df\ndef merge_close_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n        Merges intervals that are very close.\n        Parameters\n        ----------\n        threshold : numeric\n            time threshold for the closeness of the intervals\n        time_units : None, optional\n            time units for the threshold ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: IntervalSet\n            a copied IntervalSet with merged intervals\n        \"\"\"\nif len(self) == 0:\nreturn IntervalSet(start=[], end=[])\nthreshold = format_timestamps(\nnp.array((threshold,), dtype=np.float64).ravel(), time_units\n)[0]\nstart = self[\"start\"].values\nend = self[\"end\"].values\ntojoin = (start[1:] - end[0:-1]) &gt; threshold\nstart = np.hstack((start[0], start[1:][tojoin]))\nend = np.hstack((end[0:-1][tojoin], end[-1]))\nreturn IntervalSet(start=start, end=end)\ndef get_intervals_center(self, alpha=0.5):\n\"\"\"\n        Returns by default the centers of each intervals.\n        It is possible to bias the midpoint by changing the alpha parameter between [0, 1]\n        For each epoch:\n        t = start + (end-start)*alpha\n        Parameters\n        ----------\n        alpha : float, optional\n            The midpoint within each interval.\n        Returns\n        -------\n        Ts\n            Timestamps object\n        \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nif not isinstance(alpha, float):\nraise RuntimeError(\"Parameter alpha should be float type\")\nalpha = np.clip(alpha, 0, 1)\nt = starts + (ends - starts) * alpha\nreturn time_series.Ts(t=t, time_support=self)\ndef save(self, filename):\n\"\"\"\n        Save IntervalSet object in npz format. The file will contain the starts and ends.\n        The main purpose of this function is to save small/medium sized IntervalSet\n        objects. For example, you determined some epochs for one session that you want to save\n        to avoid recomputing them.\n        You can load the object with numpy.load. Keys are 'start', 'end' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n        &gt;&gt;&gt; ep.save(\"my_ep.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['start', 'end', 'type']\n        &gt;&gt;&gt; print(file['start'])\n        [0. 10. 20.]\n        It is then easy to recreate the IntervalSet object.\n        &gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n           start   end\n        0    0.0   5.0\n        1   10.0  12.0\n        2   20.0  33.0\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nstart=self.start.values,\nend=self.end.values,\ntype=np.array([\"IntervalSet\"], dtype=np.str_),\n)\nreturn\n@property\ndef _constructor(self):\nreturn IntervalSet\n@property\ndef starts(self):\n\"\"\"Return the starts of the IntervalSet as a Ts object\n        Returns\n        -------\n        Ts\n            The starts of the IntervalSet\n        \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nreturn time_series.Ts(t=self.values[:, 0], time_support=self)\n@property\ndef ends(self):\n\"\"\"Return the ends of the IntervalSet as a Ts object\n        Returns\n        -------\n        Ts\n            The ends of the IntervalSet\n        \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nreturn time_series.Ts(t=self.values[:, 1], time_support=self)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.starts","title":"<code>starts</code>  <code>property</code>","text":"<p>Return the starts of the IntervalSet as a Ts object</p> <p>Returns:</p> Type Description <code>Ts</code> <p>The starts of the IntervalSet</p>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.ends","title":"<code>ends</code>  <code>property</code>","text":"<p>Return the ends of the IntervalSet as a Ts object</p> <p>Returns:</p> Type Description <code>Ts</code> <p>The ends of the IntervalSet</p>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.__init__","title":"<code>__init__(start, end=None, time_units='s', **kwargs)</code>","text":"<p>IntervalSet initializer</p> <p>If start and end and not aligned, meaning that </p> <ol> <li>len(start) != len(end)</li> <li>end[i] &gt; start[i]</li> <li>start[i+1] &gt; end[i]</li> <li>start and end are not sorted,</li> </ol> <p>IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>numpy.ndarray or number or pandas.DataFrame</code> <p>Beginning of intervals</p> required <code>end</code> <code>numpy.ndarray or number, optional</code> <p>Ends of intervals</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>Time unit of the intervals ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>**kwargs</code> <p>Additional parameters passed ot pandas.DataFrame</p> <code>{}</code> <p>Returns:</p> Type Description <code>IntervalSet</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Description</p> <code>ValueError</code> <p>If a pandas.DataFrame is passed, it should contains a column 'start' and a column 'end'.</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def __init__(self, start, end=None, time_units=\"s\", **kwargs):\n\"\"\"\n    IntervalSet initializer\n    If start and end and not aligned, meaning that \\n\n    1. len(start) != len(end)\n    2. end[i] &gt; start[i]\n    3. start[i+1] &gt; end[i]\n    4. start and end are not sorted,\n    IntervalSet will try to \"fix\" the data by eliminating some of the start and end data point\n    Parameters\n    ----------\n    start : numpy.ndarray or number or pandas.DataFrame\n        Beginning of intervals\n    end : numpy.ndarray or number, optional\n        Ends of intervals\n    time_units : str, optional\n        Time unit of the intervals ('us', 'ms', 's' [default])\n    **kwargs\n        Additional parameters passed ot pandas.DataFrame\n    Returns\n    -------\n    IntervalSet\n        _\n    Raises\n    ------\n    RuntimeError\n        Description\n    ValueError\n        If a pandas.DataFrame is passed, it should contains\n        a column 'start' and a column 'end'.\n    \"\"\"\nif end is None:\ndf = pd.DataFrame(start)\nif \"start\" not in df.columns or \"end\" not in df.columns:\nraise ValueError(\"wrong columns name\")\nstart = df[\"start\"].values.astype(np.float64)\nend = df[\"end\"].values.astype(np.float64)\nstart = sort_timestamps(format_timestamps(start.ravel(), time_units))\nend = sort_timestamps(format_timestamps(end.ravel(), time_units))\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\nself._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\nreturn\nstart = np.array(start).astype(np.float64)\nend = np.array(end).astype(np.float64)\nstart = format_timestamps(np.array(start).ravel(), time_units)\nend = format_timestamps(np.array(end).ravel(), time_units)\nif len(start) != len(end):\nraise RuntimeError(\"Starts end ends are not of the same length\")\nif not (np.diff(start) &gt; 0).all():\nwarnings.warn(\"start is not sorted.\", stacklevel=2)\nstart = np.sort(start)\nif not (np.diff(end) &gt; 0).all():\nwarnings.warn(\"end is not sorted.\", stacklevel=2)\nend = np.sort(end)\ndata, to_warn = jitfix_iset(start, end)\nif np.any(to_warn):\nmsg = \"\\n\".join(all_warnings[to_warn])\nwarnings.warn(msg, stacklevel=2)\nsuper().__init__(data=data, columns=(\"start\", \"end\"), **kwargs)\nself.r_cache = None\n# self._metadata = [\"nap_class\"]\nself.nap_class = self.__class__.__name__\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.time_span","title":"<code>time_span()</code>","text":"<p>Time span of the interval set.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>an IntervalSet with a single interval encompassing the whole IntervalSet</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def time_span(self):\n\"\"\"\n    Time span of the interval set.\n    Returns\n    -------\n    out: IntervalSet\n        an IntervalSet with a single interval encompassing the whole IntervalSet\n    \"\"\"\ns = self[\"start\"][0]\ne = self[\"end\"].iloc[-1]\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.tot_length","title":"<code>tot_length(time_units='s')</code>","text":"<p>Total elapsed time in the set.</p> <p>Parameters:</p> Name Type Description Default <code>time_units</code> <code>None, optional</code> <p>The time units to return the result in ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>float</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def tot_length(self, time_units=\"s\"):\n\"\"\"\n    Total elapsed time in the set.\n    Parameters\n    ----------\n    time_units : None, optional\n        The time units to return the result in ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: float\n        _\n    \"\"\"\ntot_l = (self[\"end\"] - self[\"start\"]).sum()\nreturn return_timestamps(np.array([tot_l]), time_units)[0]\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.intersect","title":"<code>intersect(a)</code>","text":"<p>set intersection of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to intersect self with</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def intersect(self, a):\n\"\"\"\n    set intersection of IntervalSet\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to intersect self with\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitintersect(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.union","title":"<code>union(a)</code>","text":"<p>set union of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to union self with</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def union(self, a):\n\"\"\"\n    set union of IntervalSet\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to union self with\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitunion(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.set_diff","title":"<code>set_diff(a)</code>","text":"<p>set difference of IntervalSet</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>IntervalSet</code> <p>the IntervalSet to set-substract from self</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>_</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def set_diff(self, a):\n\"\"\"\n    set difference of IntervalSet\n    Parameters\n    ----------\n    a : IntervalSet\n        the IntervalSet to set-substract from self\n    Returns\n    -------\n    out: IntervalSet\n        _\n    \"\"\"\nstart1 = self.values[:, 0]\nend1 = self.values[:, 1]\nstart2 = a.values[:, 0]\nend2 = a.values[:, 1]\ns, e = jitdiff(start1, end1, start2, end2)\nreturn IntervalSet(s, e)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.in_interval","title":"<code>in_interval(tsd)</code>","text":"<p>finds out in which element of the interval set each point in a time series fits.</p> <p>NaNs for those that don't fit an interval</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>Tsd</code> <p>The tsd to be binned</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def in_interval(self, tsd):\n\"\"\"\n    finds out in which element of the interval set each point in a time series fits.\n    NaNs for those that don't fit an interval\n    Parameters\n    ----------\n    tsd : Tsd\n        The tsd to be binned\n    Returns\n    -------\n    out: numpy.ndarray\n        an array with the interval index labels for each time stamp (NaN) for timestamps not in IntervalSet\n    \"\"\"\ntimes = tsd.index.values\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nreturn jitin_interval(times, starts, ends)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.drop_short_intervals","title":"<code>drop_short_intervals(threshold, time_units='s')</code>","text":"<p>Drops the short intervals in the interval set.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>Time threshold for \"short\" intervals</p> required <code>time_units</code> <code>None, optional</code> <p>The time units for the treshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>A copied IntervalSet with the dropped intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def drop_short_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Drops the short intervals in the interval set.\n    Parameters\n    ----------\n    threshold : numeric\n        Time threshold for \"short\" intervals\n    time_units : None, optional\n        The time units for the treshold ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: IntervalSet\n        A copied IntervalSet with the dropped intervals\n    \"\"\"\nthreshold = format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &gt; threshold].reset_index(\ndrop=True\n)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.drop_long_intervals","title":"<code>drop_long_intervals(threshold, time_units='s')</code>","text":"<p>Drops the long intervals in the interval set.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>Time threshold for \"long\" intervals</p> required <code>time_units</code> <code>None, optional</code> <p>The time units for the treshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>A copied IntervalSet with the dropped intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def drop_long_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Drops the long intervals in the interval set.\n    Parameters\n    ----------\n    threshold : numeric\n        Time threshold for \"long\" intervals\n    time_units : None, optional\n        The time units for the treshold ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: IntervalSet\n        A copied IntervalSet with the dropped intervals\n    \"\"\"\nthreshold = format_timestamps(\nnp.array([threshold], dtype=np.float64), time_units\n)[0]\nreturn self.loc[(self[\"end\"] - self[\"start\"]) &lt; threshold].reset_index(\ndrop=True\n)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.as_units","title":"<code>as_units(units='s')</code>","text":"<p>returns a DataFrame with time expressed in the desired unit</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>None, optional</code> <p>'us', 'ms', or 's' [default]</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>pandas.DataFrame</code> <p>DataFrame with adjusted times</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    returns a DataFrame with time expressed in the desired unit\n    Parameters\n    ----------\n    units : None, optional\n        'us', 'ms', or 's' [default]\n    Returns\n    -------\n    out: pandas.DataFrame\n        DataFrame with adjusted times\n    \"\"\"\ndata = self.values.copy()\ndata = return_timestamps(data, units)\nif units == \"us\":\ndata = data.astype(np.int64)\ndf = pd.DataFrame(index=self.index.values, data=data, columns=self.columns)\nreturn df\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.merge_close_intervals","title":"<code>merge_close_intervals(threshold, time_units='s')</code>","text":"<p>Merges intervals that are very close.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>numeric</code> <p>time threshold for the closeness of the intervals</p> required <code>time_units</code> <code>None, optional</code> <p>time units for the threshold ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>IntervalSet</code> <p>a copied IntervalSet with merged intervals</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def merge_close_intervals(self, threshold, time_units=\"s\"):\n\"\"\"\n    Merges intervals that are very close.\n    Parameters\n    ----------\n    threshold : numeric\n        time threshold for the closeness of the intervals\n    time_units : None, optional\n        time units for the threshold ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: IntervalSet\n        a copied IntervalSet with merged intervals\n    \"\"\"\nif len(self) == 0:\nreturn IntervalSet(start=[], end=[])\nthreshold = format_timestamps(\nnp.array((threshold,), dtype=np.float64).ravel(), time_units\n)[0]\nstart = self[\"start\"].values\nend = self[\"end\"].values\ntojoin = (start[1:] - end[0:-1]) &gt; threshold\nstart = np.hstack((start[0], start[1:][tojoin]))\nend = np.hstack((end[0:-1][tojoin], end[-1]))\nreturn IntervalSet(start=start, end=end)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.get_intervals_center","title":"<code>get_intervals_center(alpha=0.5)</code>","text":"<p>Returns by default the centers of each intervals.</p> <p>It is possible to bias the midpoint by changing the alpha parameter between [0, 1] For each epoch: t = start + (end-start)*alpha</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float, optional</code> <p>The midpoint within each interval.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>Ts</code> <p>Timestamps object</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def get_intervals_center(self, alpha=0.5):\n\"\"\"\n    Returns by default the centers of each intervals.\n    It is possible to bias the midpoint by changing the alpha parameter between [0, 1]\n    For each epoch:\n    t = start + (end-start)*alpha\n    Parameters\n    ----------\n    alpha : float, optional\n        The midpoint within each interval.\n    Returns\n    -------\n    Ts\n        Timestamps object\n    \"\"\"\ntime_series = importlib.import_module(\".time_series\", \"pynapple.core\")\nstarts = self.values[:, 0]\nends = self.values[:, 1]\nif not isinstance(alpha, float):\nraise RuntimeError(\"Parameter alpha should be float type\")\nalpha = np.clip(alpha, 0, 1)\nt = starts + (ends - starts) * alpha\nreturn time_series.Ts(t=t, time_support=self)\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.IntervalSet.save","title":"<code>save(filename)</code>","text":"<p>Save IntervalSet object in npz format. The file will contain the starts and ends.</p> <p>The main purpose of this function is to save small/medium sized IntervalSet objects. For example, you determined some epochs for one session that you want to save to avoid recomputing them.</p> <p>You can load the object with numpy.load. Keys are 'start', 'end' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n&gt;&gt;&gt; ep.save(\"my_ep.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['start', 'end', 'type']\n&gt;&gt;&gt; print(file['start'])\n[0. 10. 20.]\n</code></pre> <p>It is then easy to recreate the IntervalSet object.</p> <pre><code>&gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n   start   end\n0    0.0   5.0\n1   10.0  12.0\n2   20.0  33.0\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save IntervalSet object in npz format. The file will contain the starts and ends.\n    The main purpose of this function is to save small/medium sized IntervalSet\n    objects. For example, you determined some epochs for one session that you want to save\n    to avoid recomputing them.\n    You can load the object with numpy.load. Keys are 'start', 'end' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=[0, 10, 20], end=[5, 12, 33])\n    &gt;&gt;&gt; ep.save(\"my_ep.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_ep.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['start', 'end', 'type']\n    &gt;&gt;&gt; print(file['start'])\n    [0. 10. 20.]\n    It is then easy to recreate the IntervalSet object.\n    &gt;&gt;&gt; nap.IntervalSet(file['start'], file['end'])\n       start   end\n    0    0.0   5.0\n    1   10.0  12.0\n    2   20.0  33.0\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nstart=self.start.values,\nend=self.end.values,\ntype=np.array([\"IntervalSet\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"reference/core/interval_set/#pynapple.core.interval_set.jitfix_iset","title":"<code>jitfix_iset(start, end)</code>","text":"<p>0 - &gt; \"Some starts and ends are equal. Removing 1 microsecond!\", 1 - &gt; \"Some ends precede the relative start. Dropping them!\", 2 - &gt; \"Some starts precede the previous end. Joining them!\", 3 - &gt; \"Some epochs have no duration\"</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>numpy.ndarray</code> <p>Description</p> required <code>end</code> <code>numpy.ndarray</code> <p>Description</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/core/interval_set.py</code> <pre><code>@jit(nopython=True)\ndef jitfix_iset(start, end):\n\"\"\"\n    0 - &gt; \"Some starts and ends are equal. Removing 1 microsecond!\",\n    1 - &gt; \"Some ends precede the relative start. Dropping them!\",\n    2 - &gt; \"Some starts precede the previous end. Joining them!\",\n    3 - &gt; \"Some epochs have no duration\"\n    Parameters\n    ----------\n    start : numpy.ndarray\n        Description\n    end : numpy.ndarray\n        Description\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nto_warn = np.zeros(4, dtype=np.bool_)\nm = start.shape[0]\ndata = np.zeros((m, 2), dtype=np.float64)\ni = 0\nct = 0\nwhile i &lt; m:\nnewstart = start[i]\nnewend = end[i]\nwhile i &lt; m:\nif end[i] == start[i]:\nto_warn[3] = True\ni += 1\nelse:\nnewstart = start[i]\nnewend = end[i]\nbreak\nwhile i &lt; m:\nif end[i] &lt; start[i]:\nto_warn[1] = True\ni += 1\nelse:\nnewstart = start[i]\nnewend = end[i]\nbreak\nwhile i &lt; m - 1:\nif start[i + 1] &lt; end[i]:\nto_warn[2] = True\ni += 1\nnewend = max(end[i - 1], end[i])\nelse:\nbreak\nif i &lt; m - 1:\nif newend == start[i + 1]:\nto_warn[0] = True\nnewend -= 1.0e-6\ndata[ct, 0] = newstart\ndata[ct, 1] = newend\nct += 1\ni += 1\ndata = data[0:ct]\nreturn (data, to_warn)\n</code></pre>"},{"location":"reference/core/jitted_functions/","title":"Jitted functions","text":""},{"location":"reference/core/jitted_functions/#pynapple.core.jitted_functions.jitrestrict","title":"<code>jitrestrict(time_array, data_array, starts, ends)</code>","text":"<p>Jitted version of restrict</p> <p>Parameters:</p> Name Type Description Default <code>time_array</code> <code>numpy.ndarray</code> <p>Description</p> required <code>data_array</code> <code>numpy.ndarray</code> <p>Description</p> required <code>starts</code> <code>numpy.ndarray</code> <p>Description</p> required <code>ends</code> <code>numpy.ndarray</code> <p>Description</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/core/jitted_functions.py</code> <pre><code>@jit(nopython=True)\ndef jitrestrict(time_array, data_array, starts, ends):\n\"\"\"\n    Jitted version of restrict\n    Parameters\n    ----------\n    time_array : numpy.ndarray\n        Description\n    data_array : numpy.ndarray\n        Description\n    starts : numpy.ndarray\n        Description\n    ends : numpy.ndarray\n        Description\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nn = len(time_array)\nm = len(starts)\nix = np.zeros(n, dtype=np.bool_)\nk = 0\nt = 0\nwhile ends[k] &lt; time_array[t]:\nk += 1\nwhile k &lt; m:\n# Outside\nwhile t &lt; n:\nif time_array[t] &gt;= starts[k]:\n# ix[t] = True\n# t += 1\nbreak\nt += 1\n# Inside\nwhile t &lt; n:\nif time_array[t] &gt; ends[k]:\nk += 1\nbreak\nelse:\nix[t] = True\nt += 1\nif k == m:\nbreak\nif t == n:\nbreak\nnew_time_array = time_array[ix]\nnew_data_array = data_array[ix]\nreturn (new_time_array, new_data_array)\n</code></pre>"},{"location":"reference/core/jitted_functions/#pynapple.core.jitted_functions.jitthreshold","title":"<code>jitthreshold(time_array, data_array, starts, ends, thr, method='above')</code>","text":"<p>Summary</p> <p>Parameters:</p> Name Type Description Default <code>time_array</code> <code>TYPE</code> <p>Description</p> required <code>data_array</code> <code>TYPE</code> <p>Description</p> required <code>starts</code> <code>TYPE</code> <p>Description</p> required <code>ends</code> <code>TYPE</code> <p>Description</p> required <code>thr</code> <code>TYPE</code> <p>Description</p> required <code>method</code> <code>str, optional</code> <p>Description</p> <code>'above'</code> <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/core/jitted_functions.py</code> <pre><code>@jit(nopython=True)\ndef jitthreshold(time_array, data_array, starts, ends, thr, method=\"above\"):\n\"\"\"Summary\n    Parameters\n    ----------\n    time_array : TYPE\n        Description\n    data_array : TYPE\n        Description\n    starts : TYPE\n        Description\n    ends : TYPE\n        Description\n    thr : TYPE\n        Description\n    method : str, optional\n        Description\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nn = time_array.shape[0]\nif method == \"above\":\nix = data_array &gt; thr\nelif method == \"below\":\nix = data_array &lt; thr\nelif method == \"aboveequal\":\nix = data_array &gt;= thr\nelif method == \"belowequal\":\nix = data_array &lt;= thr\nk = 0\nt = 0\nix_start = np.zeros(n, dtype=np.bool_)\nix_end = np.zeros(n, dtype=np.bool_)\nnew_start = np.zeros(n, dtype=np.float64)\nnew_end = np.zeros(n, dtype=np.float64)\nwhile time_array[t] &lt; starts[k]:\nk += 1\nif ix[t]:\nix_start[t] = 1\nnew_start[t] = time_array[t]\nt += 1\nwhile t &lt; n - 1:\n# transition\nif time_array[t] &gt; ends[k]:\nk += 1\nif ix[t - 1]:\nix_end[t - 1] = 1\nnew_end[t - 1] = time_array[t - 1]\nif ix[t]:\nix_start[t] = 1\nnew_start[t] = time_array[t]\nelse:\nif not ix[t - 1] and ix[t]:\nix_start[t] = 1\nnew_start[t] = time_array[t] - (time_array[t] - time_array[t - 1]) / 2\nif ix[t - 1] and not ix[t]:\nix_end[t] = 1\nnew_end[t] = time_array[t] - (time_array[t] - time_array[t - 1]) / 2\nt += 1\nif ix[t] and ix[t - 1]:\nix_end[t] = 1\nnew_end[t] = time_array[t]\nif ix[t] and not ix[t - 1]:\nix_start[t] = 1\nix_end[t] = 1\nnew_start[t] = time_array[t] - (time_array[t] - time_array[t - 1]) / 2\nnew_end[t] = time_array[t]\nelif ix[t - 1] and not ix[t]:\nix_end[t] = 1\nnew_end[t] = time_array[t] - (time_array[t] - time_array[t - 1]) / 2\nnew_time_array = time_array[ix]\nnew_data_array = data_array[ix]\nnew_starts = new_start[ix_start]\nnew_ends = new_end[ix_end]\nreturn (new_time_array, new_data_array, new_starts, new_ends)\n</code></pre>"},{"location":"reference/core/jitted_functions/#pynapple.core.jitted_functions.jitvaluefrom","title":"<code>jitvaluefrom(time_array, time_target_array, data_target_array, starts, ends)</code>","text":"<p>Summary</p> <p>Parameters:</p> Name Type Description Default <code>time_array</code> <code>TYPE</code> <p>Description</p> required <code>time_target_array</code> <code>TYPE</code> <p>Description</p> required <code>data_target_array</code> <code>TYPE</code> <p>Description</p> required <code>starts</code> <code>TYPE</code> <p>Description</p> required <code>ends</code> <code>TYPE</code> <p>Description</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/core/jitted_functions.py</code> <pre><code>@jit(nopython=True)\ndef jitvaluefrom(time_array, time_target_array, data_target_array, starts, ends):\n\"\"\"Summary\n    Parameters\n    ----------\n    time_array : TYPE\n        Description\n    time_target_array : TYPE\n        Description\n    data_target_array : TYPE\n        Description\n    starts : TYPE\n        Description\n    ends : TYPE\n        Description\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\ntime_array, _, count = jitrestrict_with_count(\ntime_array, np.zeros(time_array.shape[0]), starts, ends\n)\ntime_target_array, data_target_array, count_target = jitrestrict_with_count(\ntime_target_array, data_target_array, starts, ends\n)\nm = starts.shape[0]\nn = time_array.shape[0]\nd = time_target_array.shape[0]\nnew_data_array = np.zeros(n, dtype=data_target_array.dtype)\nif n &gt; 0 and d &gt; 0:\nfor k in range(m):\nif count[k] &gt; 0 and count_target[k] &gt; 0:\nt = np.sum(count[0:k])\ni = np.sum(count_target[0:k])\nmaxt = t + count[k]\nmaxi = i + count_target[k]\nwhile t &lt; maxt:\ninterval = abs(time_array[t] - time_target_array[i])\nnew_data_array[t] = data_target_array[i]\ni += 1\nwhile i &lt; maxi:\nnew_interval = abs(time_array[t] - time_target_array[i])\nif new_interval &gt; interval:\nbreak\nelse:\nnew_data_array[t] = data_target_array[i]\ninterval = new_interval\ni += 1\ni -= 1\nt += 1\nreturn (time_array, new_data_array, starts, ends)\n</code></pre>"},{"location":"reference/core/jitted_functions/#pynapple.core.jitted_functions.jitvaluefromtsdframe","title":"<code>jitvaluefromtsdframe(time_array, time_target_array, data_target_array, starts, ends)</code>","text":"<p>Summary</p> <p>Parameters:</p> Name Type Description Default <code>time_array</code> <code>TYPE</code> <p>Description</p> required <code>time_target_array</code> <code>TYPE</code> <p>Description</p> required <code>data_target_array</code> <code>TYPE</code> <p>Description</p> required <code>starts</code> <code>TYPE</code> <p>Description</p> required <code>ends</code> <code>TYPE</code> <p>Description</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/core/jitted_functions.py</code> <pre><code>@jit(nopython=True)\ndef jitvaluefromtsdframe(\ntime_array, time_target_array, data_target_array, starts, ends\n):\n\"\"\"Summary\n    Parameters\n    ----------\n    time_array : TYPE\n        Description\n    time_target_array : TYPE\n        Description\n    data_target_array : TYPE\n        Description\n    starts : TYPE\n        Description\n    ends : TYPE\n        Description\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\ntime_array, _, count = jitrestrict_with_count(\ntime_array, np.zeros(time_array.shape[0]), starts, ends\n)\ntime_target_array, data_target_array, count_target = jitrestrict_with_count(\ntime_target_array, data_target_array, starts, ends\n)\nm = starts.shape[0]\nn = time_array.shape[0]\nd = time_target_array.shape[0]\nnew_data_array = np.zeros(\n(n, data_target_array.shape[1]), dtype=data_target_array.dtype\n)\nif n &gt; 0 and d &gt; 0:\nfor k in range(m):\nif count[k] &gt; 0 and count_target[k] &gt; 0:\nt = np.sum(count[0:k])\ni = np.sum(count_target[0:k])\nmaxt = t + count[k]\nmaxi = i + count_target[k]\nwhile t &lt; maxt:\ninterval = abs(time_array[t] - time_target_array[i])\nnew_data_array[t] = data_target_array[i]\ni += 1\nwhile i &lt; maxi:\nnew_interval = abs(time_array[t] - time_target_array[i])\nif new_interval &gt; interval:\nbreak\nelse:\nnew_data_array[t] = data_target_array[i]\ninterval = new_interval\ni += 1\ni -= 1\nt += 1\nreturn (time_array, new_data_array, starts, ends)\n</code></pre>"},{"location":"reference/core/jitted_functions/#pynapple.core.jitted_functions.jitintersect","title":"<code>jitintersect(start1, end1, start2, end2)</code>","text":"<p>Summary</p> <p>Parameters:</p> Name Type Description Default <code>start1</code> <code>TYPE</code> <p>Description</p> required <code>end1</code> <code>TYPE</code> <p>Description</p> required <code>start2</code> <code>TYPE</code> <p>Description</p> required <code>end2</code> <code>TYPE</code> <p>Description</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/core/jitted_functions.py</code> <pre><code>@jit(nopython=True)\ndef jitintersect(start1, end1, start2, end2):\n\"\"\"Summary\n    Parameters\n    ----------\n    start1 : TYPE\n        Description\n    end1 : TYPE\n        Description\n    start2 : TYPE\n        Description\n    end2 : TYPE\n        Description\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nm = start1.shape[0]\nn = start2.shape[0]\ni = 0\nj = 0\nnewstart = np.zeros(m + n, dtype=np.float64)\nnewend = np.zeros(m + n, dtype=np.float64)\nct = 0\nwhile i &lt; m:\nwhile j &lt; n:\nif end2[j] &gt; start1[i]:\nbreak\nj += 1\nif j == n:\nbreak\nif start2[j] &lt; end1[i]:\nnewstart[ct] = max(start1[i], start2[j])\nnewend[ct] = min(end1[i], end2[j])\nct += 1\nif end2[j] &lt; end1[i]:\nj += 1\nelse:\ni += 1\nelse:\ni += 1\nnewstart = newstart[0:ct]\nnewend = newend[0:ct]\nreturn (newstart, newend)\n</code></pre>"},{"location":"reference/core/jitted_functions/#pynapple.core.jitted_functions.jitunion","title":"<code>jitunion(start1, end1, start2, end2)</code>","text":"<p>Summary</p> <p>Parameters:</p> Name Type Description Default <code>start1</code> <code>TYPE</code> <p>Description</p> required <code>end1</code> <code>TYPE</code> <p>Description</p> required <code>start2</code> <code>TYPE</code> <p>Description</p> required <code>end2</code> <code>TYPE</code> <p>Description</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/core/jitted_functions.py</code> <pre><code>@jit(nopython=True)\ndef jitunion(start1, end1, start2, end2):\n\"\"\"Summary\n    Parameters\n    ----------\n    start1 : TYPE\n        Description\n    end1 : TYPE\n        Description\n    start2 : TYPE\n        Description\n    end2 : TYPE\n        Description\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nm = start1.shape[0]\nn = start2.shape[0]\ni = 0\nj = 0\nnewstart = np.zeros(m + n, dtype=np.float64)\nnewend = np.zeros(m + n, dtype=np.float64)\nct = 0\nwhile i &lt; m:\nwhile j &lt; n:\nif end2[j] &gt; start1[i]:\nbreak\nnewstart[ct] = start2[j]\nnewend[ct] = end2[j]\nct += 1\nj += 1\nif j == n:\nbreak\n# overlap\nif start2[j] &lt; end1[i]:\nnewstart[ct] = min(start1[i], start2[j])\nwhile i &lt; m and j &lt; n:\nnewend[ct] = max(end1[i], end2[j])\nif end1[i] &lt; end2[j]:\ni += 1\nelse:\nj += 1\nif i == m:\nj += 1\nct += 1\nbreak\nif j == n:\ni += 1\nct += 1\nbreak\nif end2[j] &lt; start1[i]:\nj += 1\nct += 1\nbreak\nelif end1[i] &lt; start2[j]:\ni += 1\nct += 1\nbreak\nelse:\nnewstart[ct] = start1[i]\nnewend[ct] = end1[i]\nct += 1\ni += 1\nwhile i &lt; m:\nnewstart[ct] = start1[i]\nnewend[ct] = end1[i]\nct += 1\ni += 1\nwhile j &lt; n:\nnewstart[ct] = start2[j]\nnewend[ct] = end2[j]\nct += 1\nj += 1\nnewstart = newstart[0:ct]\nnewend = newend[0:ct]\nreturn (newstart, newend)\n</code></pre>"},{"location":"reference/core/jitted_functions/#pynapple.core.jitted_functions.jitin_interval","title":"<code>jitin_interval(time_array, starts, ends)</code>","text":"<p>In_interval</p> <p>Parameters:</p> Name Type Description Default <code>time_array</code> <code>numpy.ndarray</code> <p>Description</p> required <code>starts</code> <code>numpy.ndarray</code> <p>Description</p> required <code>ends</code> <code>numpy.ndarray</code> <p>Description</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/core/jitted_functions.py</code> <pre><code>@jit(nopython=True)\ndef jitin_interval(time_array, starts, ends):\n\"\"\"\n    In_interval\n    Parameters\n    ----------\n    time_array : numpy.ndarray\n        Description\n    starts : numpy.ndarray\n        Description\n    ends : numpy.ndarray\n        Description\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nn = len(time_array)\nm = len(starts)\ndata = np.ones(n, dtype=np.float64) * np.nan\nk = 0\nt = 0\nwhile ends[k] &lt; time_array[t]:\nk += 1\nwhile k &lt; m:\n# Outside\nwhile t &lt; n:\nif time_array[t] &gt;= starts[k]:\n# data[t] = k\n# t += 1\nbreak\n# data[t] = np.nan\nt += 1\n# Inside\nwhile t &lt; n:\nif time_array[t] &gt; ends[k]:\nk += 1\n# data[t] = np.nan\nbreak\nelse:\ndata[t] = k\nt += 1\nif k == m:\nbreak\nif t == n:\nbreak\nreturn data\n</code></pre>"},{"location":"reference/core/jitted_functions/#pynapple.core.jitted_functions.jit_poisson_IRLS","title":"<code>jit_poisson_IRLS(X, y, niter=100, tolerance=1e-05)</code>","text":"<p>Poisson Iteratively Reweighted Least Square for fitting Poisson GLM.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>numpy.ndarray</code> <p>Predictors</p> required <code>y</code> <code>numpy.ndarray</code> <p>Target</p> required <code>niter</code> <code>int, optional</code> <p>Number of iterations</p> <code>100</code> <code>tolerance</code> <code>float, optional</code> <p>Default is 10^-5</p> <code>1e-05</code> <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>Regression coefficients</p> Source code in <code>pynapple/core/jitted_functions.py</code> <pre><code>@jit(nopython=True)\ndef jit_poisson_IRLS(X, y, niter=100, tolerance=1e-5):\n\"\"\"Poisson Iteratively Reweighted Least Square\n    for fitting Poisson GLM.\n    Parameters\n    ----------\n    X : numpy.ndarray\n        Predictors\n    y : numpy.ndarray\n        Target\n    niter : int, optional\n        Number of iterations\n    tolerance : float, optional\n        Default is 10^-5\n    Returns\n    -------\n    numpy.ndarray\n        Regression coefficients\n    \"\"\"\ny = y.astype(np.float64)\nX = X.astype(np.float64)\nn, d = X.shape\nW = np.ones(n)\niXtWX = np.linalg.inv(np.dot(X.T * W, X))\nXtWY = np.dot(X.T * W, y)\nB = np.dot(iXtWX, XtWY)\nfor _ in range(niter):\nB_ = B\nL = np.exp(X.dot(B))  # Link function\nZ = L.reshape((-1, 1)) * X  # partial derivatives\ndelta = np.dot(np.linalg.inv(np.dot(Z.T * W, Z)), np.dot(Z.T * W, y))\nB = B + delta\ntol = np.sum(np.abs((B - B_) / B_))\nif tol &lt; tolerance:\nreturn B\nreturn B\n</code></pre>"},{"location":"reference/core/time_series/","title":"Time series","text":""},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd","title":"<code>Tsd</code>","text":"<p>         Bases: <code>pd.Series</code></p> <p>A subclass of pandas.Series specialized for neurophysiology time series.</p> <p>Tsd provides standardized time representation, plus various functions for manipulating times series.</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class Tsd(pd.Series):\n# class Tsd():\n\"\"\"\n    A subclass of pandas.Series specialized for neurophysiology time series.\n    Tsd provides standardized time representation, plus various functions for manipulating times series.\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, d=None, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n        Tsd Initializer.\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.Series\n            An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n        d : numpy.ndarray, optional\n            The data of the time series\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default])\n        time_support : IntervalSet, optional\n            The time support of the tsd object\n        **kwargs\n            Arguments that will be passed to the pandas.Series initializer.\n        \"\"\"\nif isinstance(t, SingleBlockManager):\nd = t.array\nt = t.index.values\nif \"index\" in kwargs:\nkwargs.pop(\"index\")\nelif isinstance(t, pd.Series):\nd = t.values\nt = t.index.values\nt = t.astype(np.float64).flatten()\nt = format_timestamps(t, time_units)\nt = sort_timestamps(t)\nif len(t):\nif time_support is not None:\nstarts = time_support.start.values\nends = time_support.end.values\nif d is not None:\nt, d = jitrestrict(t, d, starts, ends)\nsuper().__init__(index=t, data=d)\nelse:\nt = jittsrestrict(t, starts, ends)\nsuper().__init__(index=t, data=None, dtype=np.int8)\nelse:\ntime_support = IntervalSet(start=t[0], end=t[-1])\nif d is not None:\nsuper().__init__(index=t, data=d)\nelse:\nsuper().__init__(index=t, data=d, dtype=np.float64)\nself.time_support = time_support\nself.rate = t.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\ntime_support = IntervalSet(pd.DataFrame(columns=[\"start\", \"end\"]))\nsuper().__init__(index=t, data=d, dtype=np.float64)\nself.time_support = time_support\nself.rate = 0.0\nself.index.name = \"Time (s)\"\n# self._metadata.append(\"nap_class\")\nself.nap_class = self.__class__.__name__\ndef __add__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__add__(value), time_support=ts)\ndef __sub__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__sub__(value), time_support=ts)\ndef __truediv__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__truediv__(value), time_support=ts)\ndef __floordiv__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__floordiv__(value), time_support=ts)\ndef __mul__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__mul__(value), time_support=ts)\ndef __mod__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__mod__(value), time_support=ts)\ndef __pow__(self, value):\nts = self.time_support\nreturn Tsd(self.as_series().__pow__(value), time_support=ts)\ndef __lt__(self, value):\nreturn self.as_series().__lt__(value)\ndef __gt__(self, value):\nreturn self.as_series().__gt__(value)\ndef __le__(self, value):\nreturn self.as_series().__le__(value)\ndef __ge__(self, value):\nreturn self.as_series().__ge__(value)\ndef __ne__(self, value):\nreturn self.as_series().__ne__(value)\ndef __eq__(self, value):\nreturn self.as_series().__eq__(value)\ndef __repr__(self):\nreturn self.as_series().__repr__()\ndef __str__(self):\nreturn self.__repr__()\ndef times(self, units=\"s\"):\n\"\"\"\n        The time index of the Tsd, returned as np.double in the desired time units.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: numpy.ndarray\n            the time indexes\n        \"\"\"\nreturn return_timestamps(self.index.values, units)\ndef as_series(self):\n\"\"\"\n        Convert the Ts/Tsd object to a pandas.Series object.\n        Returns\n        -------\n        out: pandas.Series\n            _\n        \"\"\"\nreturn pd.Series(self, copy=True)\ndef as_units(self, units=\"s\"):\n\"\"\"\n        Returns a Series with time expressed in the desired unit.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        pandas.Series\n            the series object with adjusted times\n        \"\"\"\nss = self.as_series()\nt = self.index.values\nt = return_timestamps(t, units)\nif units == \"us\":\nt = t.astype(np.int64)\nss.index = t\nss.index.name = \"Time (\" + str(units) + \")\"\nreturn ss\ndef data(self):\n\"\"\"\n        The data in the Tsd object\n        Returns\n        -------\n        out: numpy.ndarray\n            _\n        \"\"\"\nreturn self.values\ndef value_from(self, data, ep=None):\n\"\"\"\n        Replace the value with the closest value from Tsd/TsdFrame argument\n        If data is TsdFrame, the output is also TsdFrame.\n        Parameters\n        ----------\n        data : Tsd/TsdFrame\n            The Tsd/TsdFrame object holding the values to replace.\n        ep : IntervalSet (optional)\n            The IntervalSet object to restrict the operation.\n            If None, the time support of the tsd input object is used.\n        Returns\n        -------\n        out : Tsd/TsdFrame\n            Object with the new values\n        Examples\n        --------\n        In this example, the ts object will receive the closest values in time from tsd.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n        &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n        The variable ts is a time series object containing only nan.\n        The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n        &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n        newts is the same size as ts restrict to ep.\n        &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n            52 52\n        \"\"\"\nif isinstance(data, Tsd):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\nelif isinstance(data, TsdFrame):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefromtsdframe(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nraise RuntimeError(\"The time series to align to should be Tsd/TsdFrame.\")\ndef restrict(self, ep):\n\"\"\"\n        Restricts a Tsd object to a set of time intervals delimited by an IntervalSet object\n        Parameters\n        ----------\n        ep : IntervalSet\n            the IntervalSet object\n        Returns\n        -------\n        out: Tsd\n            Tsd object restricted to ep\n        Examples\n        --------\n        The Ts object is restrict to the intervals defined by ep.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n        &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n        &gt;&gt;&gt; newts = ts.restrict(ep)\n        The time support of newts automatically inherit the epochs defined by ep.\n        &gt;&gt;&gt; newts.time_support\n        &gt;&gt;&gt;    start    end\n        &gt;&gt;&gt; 0    0.0  500.0\n        \"\"\"\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nreturn Tsd(t=t, d=d, time_support=ep)\ndef count(self, *args, **kwargs):\n\"\"\"\n        Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n        You can call this function in multiple ways :\n        1. *tsd.count(bin_size=1, time_units = 'ms')*\n        -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n        2. *tsd.count(1, ep=my_epochs)*\n        -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n        3. *tsd.count(ep=my_bins)*\n        -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n        4. *tsd.count()*\n        -&gt; Count occurent of events within each epoch of the time support.\n        bin_size should be seconds unless specified.\n        If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n        Parameters\n        ----------\n        bin_size : None or float, optional\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: Tsd\n            A Tsd object indexed by the center of the bins.\n        Examples\n        --------\n        This example shows how to count events within bins of 0.1 second.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n        &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n        &gt;&gt;&gt; bincount = ts.count(0.1)\n        An epoch can be specified:\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n        &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n        And bincount automatically inherit ep as time support:\n        &gt;&gt;&gt; bincount.time_support\n        &gt;&gt;&gt;    start    end\n        &gt;&gt;&gt; 0  100.0  800.0\n        \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\ntime_array = self.index.values\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = float(bin_size)\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\nt, d = jitcount(time_array, starts, ends, bin_size)\ntime_support = IntervalSet(start=starts, end=ends)\nreturn Tsd(t=t, d=d, time_support=time_support)\nelse:\n_, countin = jittsrestrict_with_count(time_array, starts, ends)\nt = starts + (ends - starts) / 2\nreturn Tsd(t=t, d=countin, time_support=ep)\ndef bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n        Bin the data by averaging points within bin_size\n        bin_size should be seconds unless specified.\n        If no epochs is passed, the data will be binned based on the time support.\n        Parameters\n        ----------\n        bin_size : float\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: Tsd\n            A Tsd object indexed by the center of the bins and holding the averaged data points.\n        Examples\n        --------\n        This example shows how to bin data within bins of 0.1 second.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n        &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n        An epoch can be specified:\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n        &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n        And bintsd automatically inherit ep as time support:\n        &gt;&gt;&gt; bintsd.time_support\n        &gt;&gt;&gt;    start    end\n        &gt;&gt;&gt; 0  10.0     80.0\n        \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nt, d = jitbin(time_array, data_array, starts, ends, bin_size)\ntime_support = IntervalSet(start=starts, end=ends)\nreturn Tsd(t=t, d=d, time_support=time_support)\ndef threshold(self, thr, method=\"above\"):\n\"\"\"\n        Apply a threshold function to the tsd to return a new tsd\n        with the time support being the epochs above/below/&gt;=/&lt;= the threshold\n        Parameters\n        ----------\n        thr : float\n            The threshold value\n        method : str, optional\n            The threshold method (above/below/aboveequal/belowequal)\n        Returns\n        -------\n        out: Tsd\n            All the time points below/ above/greater than equal to/less than equal to the threshold\n        Raises\n        ------\n        ValueError\n            Raise an error if method is not 'below' or 'above'\n        RuntimeError\n            Raise an error if thr is too high/low and no epochs is found.\n        Examples\n        --------\n        This example finds all epoch above 0.5 within the tsd object.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n        &gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n        The epochs with the times above/below the threshold can be accessed through the time support:\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n        &gt;&gt;&gt; tsd.threshold(50).time_support\n        &gt;&gt;&gt;    start   end\n        &gt;&gt;&gt; 0   50.5  99.0\n        \"\"\"\ntime_array = self.index.values\ndata_array = self.values\nstarts = self.time_support.start.values\nends = self.time_support.end.values\nif method not in [\"above\", \"below\", \"aboveequal\", \"belowequal\"]:\nraise ValueError(\n\"Method {} for thresholding is not accepted.\".format(method)\n)\nt, d, ns, ne = jitthreshold(time_array, data_array, starts, ends, thr, method)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\ndef to_tsgroup(self):\n\"\"\"\n        Convert Tsd to a TsGroup by grouping timestamps with the same values.\n        By default, the values are converted to integers.\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\n        Time (s)\n        0.0    0\n        1.0    2\n        2.0    0\n        3.0    1\n        dtype: int64\n        &gt;&gt;&gt; tsd.to_tsgroup()\n        Index    rate\n        -------  ------\n            0    0.67\n            1    0.33\n            2    0.33\n        The reverse operation can be done with the TsGroup.to_tsd function :\n        &gt;&gt;&gt; tsgroup.to_tsd()\n        Time (s)\n        0.0    0.0\n        1.0    2.0\n        2.0    0.0\n        3.0    1.0\n        dtype: float64\n        Returns\n        -------\n        TsGroup\n            Grouped timestamps\n        \"\"\"\nts_group = importlib.import_module(\".ts_group\", \"pynapple.core\")\nt = self.index.values\nd = self.values.astype(\"int\")\nidx = np.unique(d)\ngroup = {}\nfor k in idx:\ngroup[k] = Ts(t=t[d == k], time_support=self.time_support)\nreturn ts_group.TsGroup(group, time_support=self.time_support)\ndef save(self, filename):\n\"\"\"\n        Save Tsd object in npz format. The file will contain the timestamps, the\n        data and the time support.\n        The main purpose of this function is to save small/medium sized time series\n        objects. For example, you extracted one channel from your recording and\n        filtered it. You can save the filtered channel as a npz to avoid\n        reprocessing it.\n        You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n        &gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'd', 'start', 'end', 'type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1.]\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\n        Time (s)\n        0.0    2\n        1.0    3\n        dtype: int64\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([\"Tsd\"], dtype=np.str_),\n)\nreturn\n# def find_gaps(self, min_gap, method=\"absolute\"):\n#     \"\"\"\n#     finds gaps in a tsd larger than min_gap\n#     Parameters\n#     ----------\n#     min_gap : TYPE\n#         Description\n#     method : str, optional\n#         Description\n#     \"\"\"\n#     print(\"TODO\")\n#     return\n# def find_support(self, min_gap, method=\"absolute\"):\n#     \"\"\"\n#     find the smallest (to a min_gap resolution) IntervalSet containing all the times in the Tsd\n#     Parameters\n#     ----------\n#     min_gap : TYPE\n#         Description\n#     method : str, optional\n#         Description\n#     Returns\n#     -------\n#     TYPE\n#         Description\n#     \"\"\"\n#     print(\"TODO\")\n#     return\ndef start_time(self, units=\"s\"):\n\"\"\"\n        The first time index in the Ts/Tsd object\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: numpy.float64\n            _\n        \"\"\"\nreturn self.times(units=units)[0]\ndef end_time(self, units=\"s\"):\n\"\"\"\n        The last time index in the Ts/Tsd object\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: numpy.float64\n            _\n        \"\"\"\nreturn self.times(units=units)[-1]\n@property\ndef _constructor(self):\nreturn Tsd\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.__init__","title":"<code>__init__(t, d=None, time_units='s', time_support=None, **kwargs)</code>","text":"<p>Tsd Initializer.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>numpy.ndarray or pandas.Series</code> <p>An object transformable in a time series, or a pandas.Series equivalent (if d is None)</p> required <code>d</code> <code>numpy.ndarray, optional</code> <p>The data of the time series</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>The time units in which times are specified ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet, optional</code> <p>The time support of the tsd object</p> <code>None</code> <code>**kwargs</code> <p>Arguments that will be passed to the pandas.Series initializer.</p> <code>{}</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, d=None, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n    Tsd Initializer.\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.Series\n        An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n    d : numpy.ndarray, optional\n        The data of the time series\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default])\n    time_support : IntervalSet, optional\n        The time support of the tsd object\n    **kwargs\n        Arguments that will be passed to the pandas.Series initializer.\n    \"\"\"\nif isinstance(t, SingleBlockManager):\nd = t.array\nt = t.index.values\nif \"index\" in kwargs:\nkwargs.pop(\"index\")\nelif isinstance(t, pd.Series):\nd = t.values\nt = t.index.values\nt = t.astype(np.float64).flatten()\nt = format_timestamps(t, time_units)\nt = sort_timestamps(t)\nif len(t):\nif time_support is not None:\nstarts = time_support.start.values\nends = time_support.end.values\nif d is not None:\nt, d = jitrestrict(t, d, starts, ends)\nsuper().__init__(index=t, data=d)\nelse:\nt = jittsrestrict(t, starts, ends)\nsuper().__init__(index=t, data=None, dtype=np.int8)\nelse:\ntime_support = IntervalSet(start=t[0], end=t[-1])\nif d is not None:\nsuper().__init__(index=t, data=d)\nelse:\nsuper().__init__(index=t, data=d, dtype=np.float64)\nself.time_support = time_support\nself.rate = t.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\ntime_support = IntervalSet(pd.DataFrame(columns=[\"start\", \"end\"]))\nsuper().__init__(index=t, data=d, dtype=np.float64)\nself.time_support = time_support\nself.rate = 0.0\nself.index.name = \"Time (s)\"\n# self._metadata.append(\"nap_class\")\nself.nap_class = self.__class__.__name__\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.times","title":"<code>times(units='s')</code>","text":"<p>The time index of the Tsd, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>the time indexes</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the Tsd, returned as np.double in the desired time units.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.ndarray\n        the time indexes\n    \"\"\"\nreturn return_timestamps(self.index.values, units)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.as_series","title":"<code>as_series()</code>","text":"<p>Convert the Ts/Tsd object to a pandas.Series object.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>pandas.Series</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_series(self):\n\"\"\"\n    Convert the Ts/Tsd object to a pandas.Series object.\n    Returns\n    -------\n    out: pandas.Series\n        _\n    \"\"\"\nreturn pd.Series(self, copy=True)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.as_units","title":"<code>as_units(units='s')</code>","text":"<p>Returns a Series with time expressed in the desired unit.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Type Description <code>pandas.Series</code> <p>the series object with adjusted times</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    Returns a Series with time expressed in the desired unit.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    pandas.Series\n        the series object with adjusted times\n    \"\"\"\nss = self.as_series()\nt = self.index.values\nt = return_timestamps(t, units)\nif units == \"us\":\nt = t.astype(np.int64)\nss.index = t\nss.index.name = \"Time (\" + str(units) + \")\"\nreturn ss\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.data","title":"<code>data()</code>","text":"<p>The data in the Tsd object</p> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    The data in the Tsd object\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.value_from","title":"<code>value_from(data, ep=None)</code>","text":"<p>Replace the value with the closest value from Tsd/TsdFrame argument If data is TsdFrame, the output is also TsdFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tsd</code> <p>The Tsd/TsdFrame object holding the values to replace.</p> required <code>ep</code> <code>IntervalSet(optional)</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>Object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, data, ep=None):\n\"\"\"\n    Replace the value with the closest value from Tsd/TsdFrame argument\n    If data is TsdFrame, the output is also TsdFrame.\n    Parameters\n    ----------\n    data : Tsd/TsdFrame\n        The Tsd/TsdFrame object holding the values to replace.\n    ep : IntervalSet (optional)\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    out : Tsd/TsdFrame\n        Object with the new values\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n    newts is the same size as ts restrict to ep.\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\nif isinstance(data, Tsd):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\nelif isinstance(data, TsdFrame):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefromtsdframe(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nraise RuntimeError(\"The time series to align to should be Tsd/TsdFrame.\")\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.restrict","title":"<code>restrict(ep)</code>","text":"<p>Restricts a Tsd object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>ep</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>Tsd object restricted to ep</p> <p>Examples:</p> <p>The Ts object is restrict to the intervals defined by ep.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n&gt;&gt;&gt; newts = ts.restrict(ep)\n</code></pre> <p>The time support of newts automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newts.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0    0.0  500.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, ep):\n\"\"\"\n    Restricts a Tsd object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    ep : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    out: Tsd\n        Tsd object restricted to ep\n    Examples\n    --------\n    The Ts object is restrict to the intervals defined by ep.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=500, time_units='s')\n    &gt;&gt;&gt; newts = ts.restrict(ep)\n    The time support of newts automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newts.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0    0.0  500.0\n    \"\"\"\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nreturn Tsd(t=t, d=d, time_support=ep)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.count","title":"<code>count(*args, **kwargs)</code>","text":"<p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsd.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsd.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsd.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsd.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float, optional</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet, optional</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str, optional</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; bincount = ts.count(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n&gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n</code></pre> <p>And bincount automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bincount.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  100.0  800.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsd.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsd.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsd.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsd.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100)))\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; bincount = ts.count(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 100, end = 800, time_units = 's')\n    &gt;&gt;&gt; bincount = ts.count(0.1, ep=ep)\n    And bincount automatically inherit ep as time support:\n    &gt;&gt;&gt; bincount.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  100.0  800.0\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\ntime_array = self.index.values\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = float(bin_size)\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\nt, d = jitcount(time_array, starts, ends, bin_size)\ntime_support = IntervalSet(start=starts, end=ends)\nreturn Tsd(t=t, d=d, time_support=time_support)\nelse:\n_, countin = jittsrestrict_with_count(time_array, starts, ends)\nt = starts + (ends - starts) / 2\nreturn Tsd(t=t, d=countin, time_support=ep)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.bin_average","title":"<code>bin_average(bin_size, ep=None, time_units='s')</code>","text":"<p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet, optional</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>A Tsd object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsd automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsd.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: Tsd\n        A Tsd object indexed by the center of the bins and holding the averaged data points.\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsd = tsd.bin_average(0.1, ep=ep)\n    And bintsd automatically inherit ep as time support:\n    &gt;&gt;&gt; bintsd.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nt, d = jitbin(time_array, data_array, starts, ends, bin_size)\ntime_support = IntervalSet(start=starts, end=ends)\nreturn Tsd(t=t, d=d, time_support=time_support)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.threshold","title":"<code>threshold(thr, method='above')</code>","text":"<p>Apply a threshold function to the tsd to return a new tsd with the time support being the epochs above/below/&gt;=/&lt;= the threshold</p> <p>Parameters:</p> Name Type Description Default <code>thr</code> <code>float</code> <p>The threshold value</p> required <code>method</code> <code>str, optional</code> <p>The threshold method (above/below/aboveequal/belowequal)</p> <code>'above'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>All the time points below/ above/greater than equal to/less than equal to the threshold</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raise an error if method is not 'below' or 'above'</p> <code>RuntimeError</code> <p>Raise an error if thr is too high/low and no epochs is found.</p> <p>Examples:</p> <p>This example finds all epoch above 0.5 within the tsd object.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n&gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n</code></pre> <p>The epochs with the times above/below the threshold can be accessed through the time support:</p> <pre><code>&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n&gt;&gt;&gt; tsd.threshold(50).time_support\n&gt;&gt;&gt;    start   end\n&gt;&gt;&gt; 0   50.5  99.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def threshold(self, thr, method=\"above\"):\n\"\"\"\n    Apply a threshold function to the tsd to return a new tsd\n    with the time support being the epochs above/below/&gt;=/&lt;= the threshold\n    Parameters\n    ----------\n    thr : float\n        The threshold value\n    method : str, optional\n        The threshold method (above/below/aboveequal/belowequal)\n    Returns\n    -------\n    out: Tsd\n        All the time points below/ above/greater than equal to/less than equal to the threshold\n    Raises\n    ------\n    ValueError\n        Raise an error if method is not 'below' or 'above'\n    RuntimeError\n        Raise an error if thr is too high/low and no epochs is found.\n    Examples\n    --------\n    This example finds all epoch above 0.5 within the tsd object.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.random.rand(100))\n    &gt;&gt;&gt; newtsd = tsd.threshold(0.5)\n    The epochs with the times above/below the threshold can be accessed through the time support:\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(100), d=np.arange(100), time_units='s')\n    &gt;&gt;&gt; tsd.threshold(50).time_support\n    &gt;&gt;&gt;    start   end\n    &gt;&gt;&gt; 0   50.5  99.0\n    \"\"\"\ntime_array = self.index.values\ndata_array = self.values\nstarts = self.time_support.start.values\nends = self.time_support.end.values\nif method not in [\"above\", \"below\", \"aboveequal\", \"belowequal\"]:\nraise ValueError(\n\"Method {} for thresholding is not accepted.\".format(method)\n)\nt, d, ns, ne = jitthreshold(time_array, data_array, starts, ends, thr, method)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.to_tsgroup","title":"<code>to_tsgroup()</code>","text":"<p>Convert Tsd to a TsGroup by grouping timestamps with the same values. By default, the values are converted to integers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\nTime (s)\n0.0    0\n1.0    2\n2.0    0\n3.0    1\ndtype: int64\n</code></pre> <pre><code>&gt;&gt;&gt; tsd.to_tsgroup()\nIndex    rate\n-------  ------\n    0    0.67\n    1    0.33\n    2    0.33\n</code></pre> <p>The reverse operation can be done with the TsGroup.to_tsd function :</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd()\nTime (s)\n0.0    0.0\n1.0    2.0\n2.0    0.0\n3.0    1.0\ndtype: float64\n</code></pre> <p>Returns:</p> Type Description <code>TsGroup</code> <p>Grouped timestamps</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def to_tsgroup(self):\n\"\"\"\n    Convert Tsd to a TsGroup by grouping timestamps with the same values.\n    By default, the values are converted to integers.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t = np.array([0, 1, 2, 3]), d = np.array([0, 2, 0, 1]))\n    Time (s)\n    0.0    0\n    1.0    2\n    2.0    0\n    3.0    1\n    dtype: int64\n    &gt;&gt;&gt; tsd.to_tsgroup()\n    Index    rate\n    -------  ------\n        0    0.67\n        1    0.33\n        2    0.33\n    The reverse operation can be done with the TsGroup.to_tsd function :\n    &gt;&gt;&gt; tsgroup.to_tsd()\n    Time (s)\n    0.0    0.0\n    1.0    2.0\n    2.0    0.0\n    3.0    1.0\n    dtype: float64\n    Returns\n    -------\n    TsGroup\n        Grouped timestamps\n    \"\"\"\nts_group = importlib.import_module(\".ts_group\", \"pynapple.core\")\nt = self.index.values\nd = self.values.astype(\"int\")\nidx = np.unique(d)\ngroup = {}\nfor k in idx:\ngroup[k] = Ts(t=t[d == k], time_support=self.time_support)\nreturn ts_group.TsGroup(group, time_support=self.time_support)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.save","title":"<code>save(filename)</code>","text":"<p>Save Tsd object in npz format. The file will contain the timestamps, the data and the time support.</p> <p>The main purpose of this function is to save small/medium sized time series objects. For example, you extracted one channel from your recording and filtered it. You can save the filtered channel as a npz to avoid reprocessing it.</p> <p>You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n&gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'd', 'start', 'end', 'type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1.]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\nTime (s)\n0.0    2\n1.0    3\ndtype: int64\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save Tsd object in npz format. The file will contain the timestamps, the\n    data and the time support.\n    The main purpose of this function is to save small/medium sized time series\n    objects. For example, you extracted one channel from your recording and\n    filtered it. You can save the filtered channel as a npz to avoid\n    reprocessing it.\n    You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.array([0., 1.]), d = np.array([2, 3]))\n    &gt;&gt;&gt; tsd.save(\"my_path/my_tsd.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_tsd.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'd', 'start', 'end', 'type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1.]\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.Tsd(t=file['t'], d=file['d'], time_support=time_support)\n    Time (s)\n    0.0    2\n    1.0    3\n    dtype: int64\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([\"Tsd\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.start_time","title":"<code>start_time(units='s')</code>","text":"<p>The first time index in the Ts/Tsd object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def start_time(self, units=\"s\"):\n\"\"\"\n    The first time index in the Ts/Tsd object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nreturn self.times(units=units)[0]\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Tsd.end_time","title":"<code>end_time(units='s')</code>","text":"<p>The last time index in the Ts/Tsd object</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.float64</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def end_time(self, units=\"s\"):\n\"\"\"\n    The last time index in the Ts/Tsd object\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.float64\n        _\n    \"\"\"\nreturn self.times(units=units)[-1]\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame","title":"<code>TsdFrame</code>","text":"<p>         Bases: <code>pd.DataFrame</code></p> <p>A subclass of pandas.DataFrame specialized for neurophysiological time series.</p> <p>TsdFrame provides standardized time representation, plus various functions for manipulating times series with identical sampling frequency.</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class TsdFrame(pd.DataFrame):\n# class TsdFrame():\n\"\"\"\n    A subclass of pandas.DataFrame specialized for neurophysiological time series.\n    TsdFrame provides standardized time representation, plus various functions for manipulating times series with identical sampling frequency.\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, d=None, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n        TsdFrame initializer\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.DataFrame\n            the time index t,  or a pandas.DataFrame (if d is None)\n        d : numpy.ndarray\n            The data\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default]).\n        time_support : IntervalSet, optional\n            The time support of the TsdFrame object\n        **kwargs\n            Arguments that will be passed to the pandas.DataFrame initializer.\n        \"\"\"\nif isinstance(t, BlockManager):\nd = t.as_array()\nc = t.axes[0].values\nt = t.axes[1].values\nelif isinstance(t, pd.DataFrame):\nd = t.values\nc = t.columns.values\nt = t.index.values\nelse:\nif \"columns\" in kwargs:\nc = kwargs[\"columns\"]\nelse:\nif isinstance(d, np.ndarray):\nif len(d.shape) == 2:\nc = np.arange(d.shape[1])\nelif len(d.shape) == 1:\nc = np.zeros(1)\nelse:\nc = np.array([])\nelse:\nc = None\nt = t.astype(np.float64).flatten()\nt = format_timestamps(t, time_units)\nt = sort_timestamps(t)\nif len(t):\nif time_support is not None:\nstarts = time_support.start.values\nends = time_support.end.values\nif d is not None:\nt, d = jitrestrict(t, d, starts, ends)\nsuper().__init__(index=t, data=d, columns=c)\nelse:\nt = jittsrestrict(t, starts, ends)\nsuper().__init__(index=t, data=None, columns=c)\nelse:\ntime_support = IntervalSet(start=t[0], end=t[-1])\nsuper().__init__(index=t, data=d, columns=c)\nself.rate = t.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\ntime_support = IntervalSet(pd.DataFrame(columns=[\"start\", \"end\"]))\nsuper().__init__(index=np.array([]), dtype=np.float64)\nself.rate = 0.0\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nself.time_support = time_support\nself.index.name = \"Time (s)\"\n# self._metadata.append(\"nap_class\")\nself.nap_class = self.__class__.__name__\ndef __repr__(self):\nreturn self.as_units(\"s\").__repr__()\ndef __str__(self):\nreturn self.__repr__()\ndef __getitem__(self, key):\nresult = super().__getitem__(key)\ntime_support = self.time_support\nif isinstance(result, pd.Series):\nreturn Tsd(result, time_support=time_support)\nelif isinstance(result, pd.DataFrame):\nreturn TsdFrame(result, time_support=time_support)\ndef __add__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__add__(value), time_support=ts)\ndef __sub__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__sub__(value), time_support=ts)\ndef __truediv__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__truediv__(value), time_support=ts)\ndef __floordiv__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__floordiv__(value), time_support=ts)\ndef __mul__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__mul__(value), time_support=ts)\ndef __mod__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__mod__(value), time_support=ts)\ndef __pow__(self, value):\nts = self.time_support\nreturn TsdFrame(self.as_dataframe().__pow__(value), time_support=ts)\ndef __lt__(self, value):\nreturn self.as_dataframe().__lt__(value)\ndef __gt__(self, value):\nreturn self.as_dataframe().__gt__(value)\ndef __le__(self, value):\nreturn self.as_dataframe().__le__(value)\ndef __ge__(self, value):\nreturn self.as_dataframe().__ge__(value)\ndef __ne__(self, value):\nreturn self.as_dataframe().__ne__(value)\ndef __eq__(self, value):\nreturn self.as_dataframe().__eq__(value)\n@property\ndef _constructor(self):\nreturn TsdFrame\ndef times(self, units=\"s\"):\n\"\"\"\n        The time index of the TsdFrame, returned as np.double in the desired time units.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: numpy.ndarray\n            _\n        \"\"\"\nreturn return_timestamps(self.index.values, units)\ndef as_dataframe(self, copy=True):\n\"\"\"\n        Convert the TsdFrame object to a pandas.DataFrame object.\n        Returns\n        -------\n        out: pandas.DataFrame\n            _\n        \"\"\"\nreturn pd.DataFrame(self, copy=copy)\ndef as_units(self, units=\"s\"):\n\"\"\"\n        Returns a DataFrame with time expressed in the desired unit.\n        Parameters\n        ----------\n        units : str, optional\n            ('us', 'ms', 's' [default])\n        Returns\n        -------\n        pandas.DataFrame\n            the series object with adjusted times\n        \"\"\"\nt = self.index.values.copy()\nt = return_timestamps(t, units)\nif units == \"us\":\nt = t.astype(np.int64)\ndf = pd.DataFrame(index=t, data=self.values)\ndf.index.name = \"Time (\" + str(units) + \")\"\ndf.columns = self.columns.copy()\nreturn df\ndef data(self):\n\"\"\"\n        The data in the TsdFrame object\n        Returns\n        -------\n        out: numpy.ndarray\n            _\n        \"\"\"\nreturn self.values\ndef value_from(self, data, ep=None):\n\"\"\"\n        Replace the value with the closest value from Tsd/TsdFrame argument\n        If data is TsdFrame, the output is also TsdFrame.\n        Parameters\n        ----------\n        data : Tsd/TsdFrame\n            The Tsd/TsdFrame object holding the values to replace.\n        ep : IntervalSet (optional)\n            The IntervalSet object to restrict the operation.\n            If None, the time support of the tsd input object is used.\n        Returns\n        -------\n        out : Tsd/TsdFrame\n            Object with the new values\n        Examples\n        --------\n        In this example, the ts object will receive the closest values in time from tsd.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n        &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n        The variable ts is a time series object containing only nan.\n        The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n        &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n        newts is the same size as ts restrict to ep.\n        &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n            52 52\n        \"\"\"\nif isinstance(data, Tsd):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\nelif isinstance(data, TsdFrame):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefromtsdframe(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nraise RuntimeError(\"The time series to align to should be Tsd/TsdFrame.\")\ndef restrict(self, iset):\n\"\"\"\n        Restricts a TsdFrame object to a set of time intervals delimited by an IntervalSet object`\n        Parameters\n        ----------\n        iset : IntervalSet\n            the IntervalSet object\n        Returns\n        -------\n        TsdFrame\n            TsdFrame object restricted to ep\n        \"\"\"\nc = self.columns.values\ntime_array = self.index.values\ndata_array = self.values\nstarts = iset.start.values\nends = iset.end.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nreturn TsdFrame(t=t, d=d, columns=c, time_support=iset)\ndef bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n        Bin the data by averaging points within bin_size\n        bin_size should be seconds unless specified.\n        If no epochs is passed, the data will be binned based on the time support.\n        Parameters\n        ----------\n        bin_size : float\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: TsdFrame\n            A TsdFrame object indexed by the center of the bins and holding the averaged data points.\n        Examples\n        --------\n        This example shows how to bin data within bins of 0.1 second.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.arange(100), d=np.random.rand(100, 3))\n        &gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1)\n        An epoch can be specified:\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n        &gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1, ep=ep)\n        And bintsdframe automatically inherit ep as time support:\n        &gt;&gt;&gt; bintsdframe.time_support\n        &gt;&gt;&gt;    start    end\n        &gt;&gt;&gt; 0  10.0     80.0\n        \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nt, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\ntime_support = IntervalSet(start=starts, end=ends)\nreturn TsdFrame(t=t, d=d, time_support=time_support)\ndef save(self, filename):\n\"\"\"\n        Save TsdFrame object in npz format. The file will contain the timestamps, the\n        data and the time support.\n        The main purpose of this function is to save small/medium sized time series\n        objects. For example, you extracted several channels from your recording and\n        filtered them. You can save the filtered channels as a npz to avoid\n        reprocessing it.\n        You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type'\n        and 'columns' for columns names.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n        &gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'd', 'start', 'end', 'columns', 'type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1.]\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n                  a  b\n        Time (s)\n        0.0       2  3\n        1.0       4  5\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ncols_name = self.columns.values\nif cols_name.dtype == np.dtype(\"O\"):\ncols_name = cols_name.astype(str)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ncolumns=cols_name,\ntype=np.array([\"TsdFrame\"], dtype=np.str_),\n)\nreturn\n# def find_gaps(self, min_gap, time_units='s'):\n#     \"\"\"\n#     finds gaps in a tsd larger than min_gap. Return an IntervalSet.\n#     Epochs are defined by adding and removing 1 microsecond to the time index.\n#     Parameters\n#     ----------\n#     min_gap : float\n#         The minimum interval size considered to be a gap (default is second).\n#     time_units : str, optional\n#         Time units of min_gap ('us', 'ms', 's' [default])\n#     \"\"\"\n#     min_gap = format_timestamps(np.array([min_gap]), time_units)[0]\n#     time_array = self.index.values\n#     starts = self.time_support.start.values\n#     ends = self.time_support.end.values\n#     s, e = jitfind_gaps(time_array, starts, ends, min_gap)\n#     return nap.IntervalSet(s, e)\n# def find_support(self, min_gap, method=\"absolute\"):\n#     \"\"\"\n#     find the smallest (to a min_gap resolution) IntervalSet containing all the times in the Tsd\n#     Parameters\n#     ----------\n#     min_gap : float\n#         Description\n#     method : str, optional\n#         Description\n#     Returns\n#     -------\n#     TYPE\n#         Description\n#     \"\"\"\n#     print(\"TODO\")\n#     return\ndef start_time(self, units=\"s\"):\nreturn self.times(units=units)[0]\ndef end_time(self, units=\"s\"):\nreturn self.times(units=units)[-1]\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.__init__","title":"<code>__init__(t, d=None, time_units='s', time_support=None, **kwargs)</code>","text":"<p>TsdFrame initializer</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>numpy.ndarray or pandas.DataFrame</code> <p>the time index t,  or a pandas.DataFrame (if d is None)</p> required <code>d</code> <code>numpy.ndarray</code> <p>The data</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>The time units in which times are specified ('us', 'ms', 's' [default]).</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet, optional</code> <p>The time support of the TsdFrame object</p> <code>None</code> <code>**kwargs</code> <p>Arguments that will be passed to the pandas.DataFrame initializer.</p> <code>{}</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, d=None, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n    TsdFrame initializer\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.DataFrame\n        the time index t,  or a pandas.DataFrame (if d is None)\n    d : numpy.ndarray\n        The data\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default]).\n    time_support : IntervalSet, optional\n        The time support of the TsdFrame object\n    **kwargs\n        Arguments that will be passed to the pandas.DataFrame initializer.\n    \"\"\"\nif isinstance(t, BlockManager):\nd = t.as_array()\nc = t.axes[0].values\nt = t.axes[1].values\nelif isinstance(t, pd.DataFrame):\nd = t.values\nc = t.columns.values\nt = t.index.values\nelse:\nif \"columns\" in kwargs:\nc = kwargs[\"columns\"]\nelse:\nif isinstance(d, np.ndarray):\nif len(d.shape) == 2:\nc = np.arange(d.shape[1])\nelif len(d.shape) == 1:\nc = np.zeros(1)\nelse:\nc = np.array([])\nelse:\nc = None\nt = t.astype(np.float64).flatten()\nt = format_timestamps(t, time_units)\nt = sort_timestamps(t)\nif len(t):\nif time_support is not None:\nstarts = time_support.start.values\nends = time_support.end.values\nif d is not None:\nt, d = jitrestrict(t, d, starts, ends)\nsuper().__init__(index=t, data=d, columns=c)\nelse:\nt = jittsrestrict(t, starts, ends)\nsuper().__init__(index=t, data=None, columns=c)\nelse:\ntime_support = IntervalSet(start=t[0], end=t[-1])\nsuper().__init__(index=t, data=d, columns=c)\nself.rate = t.shape[0] / np.sum(\ntime_support.values[:, 1] - time_support.values[:, 0]\n)\nelse:\ntime_support = IntervalSet(pd.DataFrame(columns=[\"start\", \"end\"]))\nsuper().__init__(index=np.array([]), dtype=np.float64)\nself.rate = 0.0\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nself.time_support = time_support\nself.index.name = \"Time (s)\"\n# self._metadata.append(\"nap_class\")\nself.nap_class = self.__class__.__name__\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.times","title":"<code>times(units='s')</code>","text":"<p>The time index of the TsdFrame, returned as np.double in the desired time units.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def times(self, units=\"s\"):\n\"\"\"\n    The time index of the TsdFrame, returned as np.double in the desired time units.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn return_timestamps(self.index.values, units)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.as_dataframe","title":"<code>as_dataframe(copy=True)</code>","text":"<p>Convert the TsdFrame object to a pandas.DataFrame object.</p> <p>Returns:</p> Name Type Description <code>out</code> <code>pandas.DataFrame</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_dataframe(self, copy=True):\n\"\"\"\n    Convert the TsdFrame object to a pandas.DataFrame object.\n    Returns\n    -------\n    out: pandas.DataFrame\n        _\n    \"\"\"\nreturn pd.DataFrame(self, copy=copy)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.as_units","title":"<code>as_units(units='s')</code>","text":"<p>Returns a DataFrame with time expressed in the desired unit.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>str, optional</code> <p>('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>the series object with adjusted times</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def as_units(self, units=\"s\"):\n\"\"\"\n    Returns a DataFrame with time expressed in the desired unit.\n    Parameters\n    ----------\n    units : str, optional\n        ('us', 'ms', 's' [default])\n    Returns\n    -------\n    pandas.DataFrame\n        the series object with adjusted times\n    \"\"\"\nt = self.index.values.copy()\nt = return_timestamps(t, units)\nif units == \"us\":\nt = t.astype(np.int64)\ndf = pd.DataFrame(index=t, data=self.values)\ndf.index.name = \"Time (\" + str(units) + \")\"\ndf.columns = self.columns.copy()\nreturn df\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.data","title":"<code>data()</code>","text":"<p>The data in the TsdFrame object</p> <p>Returns:</p> Name Type Description <code>out</code> <code>numpy.ndarray</code> <p>_</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def data(self):\n\"\"\"\n    The data in the TsdFrame object\n    Returns\n    -------\n    out: numpy.ndarray\n        _\n    \"\"\"\nreturn self.values\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.value_from","title":"<code>value_from(data, ep=None)</code>","text":"<p>Replace the value with the closest value from Tsd/TsdFrame argument If data is TsdFrame, the output is also TsdFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Tsd</code> <p>The Tsd/TsdFrame object holding the values to replace.</p> required <code>ep</code> <code>IntervalSet(optional)</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>out</code> <code>Tsd</code> <p>Object with the new values</p> <p>Examples:</p> <p>In this example, the ts object will receive the closest values in time from tsd.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n&gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n</code></pre> <p>The variable ts is a time series object containing only nan. The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.</p> <pre><code>&gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n</code></pre> <p>newts is the same size as ts restrict to ep.</p> <pre><code>&gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n    52 52\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def value_from(self, data, ep=None):\n\"\"\"\n    Replace the value with the closest value from Tsd/TsdFrame argument\n    If data is TsdFrame, the output is also TsdFrame.\n    Parameters\n    ----------\n    data : Tsd/TsdFrame\n        The Tsd/TsdFrame object holding the values to replace.\n    ep : IntervalSet (optional)\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    out : Tsd/TsdFrame\n        Object with the new values\n    Examples\n    --------\n    In this example, the ts object will receive the closest values in time from tsd.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; t = np.unique(np.sort(np.random.randint(0, 1000, 100))) # random times\n    &gt;&gt;&gt; ts = nap.Ts(t=t, time_units='s')\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,1000), d=np.random.rand(1000), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 500, time_units = 's')\n    The variable ts is a time series object containing only nan.\n    The tsd object containing the values, for example the tracking data, and the epoch to restrict the operation.\n    &gt;&gt;&gt; newts = ts.value_from(tsd, ep)\n    newts is the same size as ts restrict to ep.\n    &gt;&gt;&gt; print(len(ts.restrict(ep)), len(newts))\n        52 52\n    \"\"\"\nif isinstance(data, Tsd):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefrom(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn Tsd(t=t, d=d, time_support=time_support)\nelif isinstance(data, TsdFrame):\nif ep is None:\nep = data.time_support\ntime_array = self.index.values\ntime_target_array = data.index.values\ndata_target_array = data.values\nstarts = ep.start.values\nends = ep.end.values\nt, d, ns, ne = jitvaluefromtsdframe(\ntime_array, time_target_array, data_target_array, starts, ends\n)\ntime_support = IntervalSet(start=ns, end=ne)\nreturn TsdFrame(t=t, d=d, time_support=time_support, columns=data.columns)\nelse:\nraise RuntimeError(\"The time series to align to should be Tsd/TsdFrame.\")\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.restrict","title":"<code>restrict(iset)</code>","text":"<p>Restricts a TsdFrame object to a set of time intervals delimited by an IntervalSet object`</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Type Description <code>TsdFrame</code> <p>TsdFrame object restricted to ep</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def restrict(self, iset):\n\"\"\"\n    Restricts a TsdFrame object to a set of time intervals delimited by an IntervalSet object`\n    Parameters\n    ----------\n    iset : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    TsdFrame\n        TsdFrame object restricted to ep\n    \"\"\"\nc = self.columns.values\ntime_array = self.index.values\ndata_array = self.values\nstarts = iset.start.values\nends = iset.end.values\nt, d = jitrestrict(time_array, data_array, starts, ends)\nreturn TsdFrame(t=t, d=d, columns=c, time_support=iset)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.bin_average","title":"<code>bin_average(bin_size, ep=None, time_units='s')</code>","text":"<p>Bin the data by averaging points within bin_size bin_size should be seconds unless specified. If no epochs is passed, the data will be binned based on the time support.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>float</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet, optional</code> <p>IntervalSet to restrict the operation</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> <code>'s'</code> <p>Returns:</p> Name Type Description <code>out</code> <code>TsdFrame</code> <p>A TsdFrame object indexed by the center of the bins and holding the averaged data points.</p> <p>Examples:</p> <p>This example shows how to bin data within bins of 0.1 second.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.arange(100), d=np.random.rand(100, 3))\n&gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1)\n</code></pre> <p>An epoch can be specified:</p> <pre><code>&gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n&gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1, ep=ep)\n</code></pre> <p>And bintsdframe automatically inherit ep as time support:</p> <pre><code>&gt;&gt;&gt; bintsdframe.time_support\n&gt;&gt;&gt;    start    end\n&gt;&gt;&gt; 0  10.0     80.0\n</code></pre> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def bin_average(self, bin_size, ep=None, time_units=\"s\"):\n\"\"\"\n    Bin the data by averaging points within bin_size\n    bin_size should be seconds unless specified.\n    If no epochs is passed, the data will be binned based on the time support.\n    Parameters\n    ----------\n    bin_size : float\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: TsdFrame\n        A TsdFrame object indexed by the center of the bins and holding the averaged data points.\n    Examples\n    --------\n    This example shows how to bin data within bins of 0.1 second.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.arange(100), d=np.random.rand(100, 3))\n    &gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1)\n    An epoch can be specified:\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 10, end = 80, time_units = 's')\n    &gt;&gt;&gt; bintsdframe = tsdframe.bin_average(0.1, ep=ep)\n    And bintsdframe automatically inherit ep as time support:\n    &gt;&gt;&gt; bintsdframe.time_support\n    &gt;&gt;&gt;    start    end\n    &gt;&gt;&gt; 0  10.0     80.0\n    \"\"\"\nif not isinstance(ep, IntervalSet):\nep = self.time_support\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\ntime_array = self.index.values\ndata_array = self.values\nstarts = ep.start.values\nends = ep.end.values\nt, d = jitbin_array(time_array, data_array, starts, ends, bin_size)\ntime_support = IntervalSet(start=starts, end=ends)\nreturn TsdFrame(t=t, d=d, time_support=time_support)\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.TsdFrame.save","title":"<code>save(filename)</code>","text":"<p>Save TsdFrame object in npz format. The file will contain the timestamps, the data and the time support.</p> <p>The main purpose of this function is to save small/medium sized time series objects. For example, you extracted several channels from your recording and filtered them. You can save the filtered channels as a npz to avoid reprocessing it.</p> <p>You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type' and 'columns' for columns names.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n&gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'd', 'start', 'end', 'columns', 'type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1.]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n          a  b\nTime (s)\n0.0       2  3\n1.0       4  5\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save TsdFrame object in npz format. The file will contain the timestamps, the\n    data and the time support.\n    The main purpose of this function is to save small/medium sized time series\n    objects. For example, you extracted several channels from your recording and\n    filtered them. You can save the filtered channels as a npz to avoid\n    reprocessing it.\n    You can load the object with numpy.load. Keys are 't', 'd', 'start', 'end', 'type'\n    and 'columns' for columns names.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsdframe = nap.TsdFrame(t=np.array([0., 1.]), d = np.array([[2, 3],[4,5]]), columns=['a', 'b'])\n    &gt;&gt;&gt; tsdframe.save(\"my_path/my_tsdframe.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_tsdframe.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'd', 'start', 'end', 'columns', 'type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1.]\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.TsdFrame(t=file['t'], d=file['d'], time_support=time_support, columns=file['columns'])\n              a  b\n    Time (s)\n    0.0       2  3\n    1.0       4  5\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ncols_name = self.columns.values\nif cols_name.dtype == np.dtype(\"O\"):\ncols_name = cols_name.astype(str)\nnp.savez(\nfilename,\nt=self.index.values,\nd=self.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ncolumns=cols_name,\ntype=np.array([\"TsdFrame\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts","title":"<code>Ts</code>","text":"<p>         Bases: <code>Tsd</code></p> <p>A subclass of the Tsd object for a time series with only time index, By default, the values are set to nan. All the functions of a Tsd object are available in a Ts object.</p> <p>Attributes:</p> Name Type Description <code>rate</code> <code>float</code> <p>Frequency of the time series (Hz) computed over the time support</p> <code>time_support</code> <code>IntervalSet</code> <p>The time support of the time series</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>class Ts(Tsd):\n\"\"\"\n    A subclass of the Tsd object for a time series with only time index,\n    By default, the values are set to nan.\n    All the functions of a Tsd object are available in a Ts object.\n    Attributes\n    ----------\n    rate : float\n        Frequency of the time series (Hz) computed over the time support\n    time_support : IntervalSet\n        The time support of the time series\n    \"\"\"\ndef __init__(self, t, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n        Ts Initializer\n        Parameters\n        ----------\n        t : numpy.ndarray or pandas.Series\n            An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n        time_units : str, optional\n            The time units in which times are specified ('us', 'ms', 's' [default])\n        time_support : IntervalSet, optional\n            The time support of the Ts object\n        **kwargs\n            Arguments that will be passed to the pandas.Series initializer.\n        \"\"\"\nsuper().__init__(\nt,\nNone,\ntime_units=time_units,\ntime_support=time_support,\ndtype=np.float64,\n**kwargs,\n)\nself.nts_class = self.__class__.__name__\ndef __repr__(self):\nreturn self.as_series().fillna(\"\").__repr__()\ndef __str__(self):\nreturn self.__repr__()\ndef save(self, filename):\n\"\"\"\n        Save Ts object in npz format. The file will contain the timestamps and\n        the time support.\n        The main purpose of this function is to save small/medium sized timestamps\n        object.\n        You can load the object with numpy.load. Keys are 't', 'start' and 'end' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n        &gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['t', 'start', 'end', 'type']\n        &gt;&gt;&gt; print(file['t'])\n        [0. 1. 1.5]\n        It is then easy to recreate the Tsd object.\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\n        Time (s)\n        0.0\n        1.0\n        1.5\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([\"Ts\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.__init__","title":"<code>__init__(t, time_units='s', time_support=None, **kwargs)</code>","text":"<p>Ts Initializer</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>numpy.ndarray or pandas.Series</code> <p>An object transformable in a time series, or a pandas.Series equivalent (if d is None)</p> required <code>time_units</code> <code>str, optional</code> <p>The time units in which times are specified ('us', 'ms', 's' [default])</p> <code>'s'</code> <code>time_support</code> <code>IntervalSet, optional</code> <p>The time support of the Ts object</p> <code>None</code> <code>**kwargs</code> <p>Arguments that will be passed to the pandas.Series initializer.</p> <code>{}</code> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def __init__(self, t, time_units=\"s\", time_support=None, **kwargs):\n\"\"\"\n    Ts Initializer\n    Parameters\n    ----------\n    t : numpy.ndarray or pandas.Series\n        An object transformable in a time series, or a pandas.Series equivalent (if d is None)\n    time_units : str, optional\n        The time units in which times are specified ('us', 'ms', 's' [default])\n    time_support : IntervalSet, optional\n        The time support of the Ts object\n    **kwargs\n        Arguments that will be passed to the pandas.Series initializer.\n    \"\"\"\nsuper().__init__(\nt,\nNone,\ntime_units=time_units,\ntime_support=time_support,\ndtype=np.float64,\n**kwargs,\n)\nself.nts_class = self.__class__.__name__\n</code></pre>"},{"location":"reference/core/time_series/#pynapple.core.time_series.Ts.save","title":"<code>save(filename)</code>","text":"<p>Save Ts object in npz format. The file will contain the timestamps and the time support.</p> <p>The main purpose of this function is to save small/medium sized timestamps object.</p> <p>You can load the object with numpy.load. Keys are 't', 'start' and 'end' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n&gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['t', 'start', 'end', 'type']\n&gt;&gt;&gt; print(file['t'])\n[0. 1. 1.5]\n</code></pre> <p>It is then easy to recreate the Tsd object.</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\nTime (s)\n0.0\n1.0\n1.5\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/time_series.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save Ts object in npz format. The file will contain the timestamps and\n    the time support.\n    The main purpose of this function is to save small/medium sized timestamps\n    object.\n    You can load the object with numpy.load. Keys are 't', 'start' and 'end' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; ts = nap.Ts(t=np.array([0., 1., 1.5]))\n    &gt;&gt;&gt; ts.save(\"my_path/my_ts.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_path/my_ts.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['t', 'start', 'end', 'type']\n    &gt;&gt;&gt; print(file['t'])\n    [0. 1. 1.5]\n    It is then easy to recreate the Tsd object.\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; nap.Ts(t=file['t'], time_support=time_support)\n    Time (s)\n    0.0\n    1.0\n    1.5\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\nnp.savez(\nfilename,\nt=self.index.values,\nstart=self.time_support.start.values,\nend=self.time_support.end.values,\ntype=np.array([\"Ts\"], dtype=np.str_),\n)\nreturn\n</code></pre>"},{"location":"reference/core/time_units/","title":"Time units","text":"<p>This class deals with conversion between different time units for all pynapple objects. It also provides a context manager that tweaks the default time units to the supported units: - 'us': microseconds - 'ms': milliseconds - 's': seconds  (overall default)</p>"},{"location":"reference/core/time_units/#pynapple.core.time_units.format_timestamps","title":"<code>format_timestamps(t, units='s')</code>","text":"<p>Converts time index in pynapple in a default format</p> <p>Args:     t: a vector (or scalar) of times     units: the units in which times are given</p> <p>Returns:     t: times in standard pynapple format</p> Source code in <code>pynapple/core/time_units.py</code> <pre><code>def format_timestamps(t, units=\"s\"):\n\"\"\"\n    Converts time index in pynapple in a default format\n    Args:\n        t: a vector (or scalar) of times\n        units: the units in which times are given\n    Returns:\n        t: times in standard pynapple format\n    \"\"\"\nif units == \"s\":\nt = np.around(t, 9)\nelif units == \"ms\":\nt = np.around(t / 1.0e3, 9)\nelif units == \"us\":\nt = np.around(t / 1.0e6, 9)\nelse:\nraise ValueError(\"unrecognized time units type\")\nreturn t\n</code></pre>"},{"location":"reference/core/time_units/#pynapple.core.time_units.return_timestamps","title":"<code>return_timestamps(t, units='s')</code>","text":"<p>Converts time index in pynapple in a particular format</p> <p>Args:     t: a vector (or scalar) of times     units: the units in which times are given</p> <p>Returns:     t: times in standard pynapple format</p> Source code in <code>pynapple/core/time_units.py</code> <pre><code>def return_timestamps(t, units=\"s\"):\n\"\"\"\n    Converts time index in pynapple in a particular format\n    Args:\n        t: a vector (or scalar) of times\n        units: the units in which times are given\n    Returns:\n        t: times in standard pynapple format\n    \"\"\"\nif units == \"s\":\nt = np.around(t, 9)\nelif units == \"ms\":\nt = np.around(t * 1.0e3, 9)\nelif units == \"us\":\nt = np.around(t * 1.0e6, 9)\nelse:\nraise ValueError(\"unrecognized time units type\")\nreturn t\n</code></pre>"},{"location":"reference/core/ts_group/","title":"Ts group","text":""},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup","title":"<code>TsGroup</code>","text":"<p>         Bases: <code>UserDict</code></p> <p>The TsGroup is a dictionnary-like object to hold multiple <code>Ts</code> or <code>Tsd</code> objects with different time index.</p> <p>Attributes:</p> Name Type Description <code>time_support</code> <code>IntervalSet</code> <p>The time support of the TsGroup</p> <code>rates</code> <code>pandas.Series</code> <p>The rate of each element of the TsGroup</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>class TsGroup(UserDict):\n\"\"\"\n    The TsGroup is a dictionnary-like object to hold multiple [`Ts`][pynapple.core.time_series.Ts] or [`Tsd`][pynapple.core.time_series.Tsd] objects with different time index.\n    Attributes\n    ----------\n    time_support: IntervalSet\n        The time support of the TsGroup\n    rates : pandas.Series\n        The rate of each element of the TsGroup\n    \"\"\"\ndef __init__(\nself, data, time_support=None, time_units=\"s\", bypass_check=False, **kwargs\n):\n\"\"\"\n        TsGroup Initializer\n        Parameters\n        ----------\n        data : dict\n            Dictionnary containing Ts/Tsd objects\n        time_support : IntervalSet, optional\n            The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed.\n            If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.\n        time_units : str, optional\n            Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).\n        bypass_check: bool, optional\n            To avoid checking that each element is within time_support.\n            Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand\n        **kwargs\n            Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray.\n            Note that the index should match the index of the input dictionnary.\n        Raises\n        ------\n        RuntimeError\n            Raise error if the union of time support of Ts/Tsd object is empty.\n        \"\"\"\nself._initialized = False\nself.index = np.sort(list(data.keys()))\nself._metadata = pd.DataFrame(index=self.index, columns=[\"rate\"], dtype=\"float\")\n# Transform elements to Ts/Tsd objects\nfor k in self.index:\nif isinstance(data[k], (np.ndarray, list)):\nwarnings.warn(\n\"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\nstacklevel=2,\n)\ndata[k] = Ts(\nt=data[k], time_support=time_support, time_units=time_units\n)\n# If time_support is passed, all elements of data are restricted prior to init\nif isinstance(time_support, IntervalSet):\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nelse:\n# Otherwise do the union of all time supports\ntime_support = union_intervals([data[k].time_support for k in self.index])\nif len(time_support) == 0:\nraise RuntimeError(\n\"Union of time supports is empty. Consider passing a time support as argument.\"\n)\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nUserDict.__init__(self, data)\n# Making the TsGroup non mutable\nself._initialized = True\n# Trying to add argument as metainfo\nself.set_info(**kwargs)\n\"\"\"\n    Base functions\n    \"\"\"\ndef __setitem__(self, key, value):\nif self._initialized:\nraise RuntimeError(\"TsGroup object is not mutable.\")\nself._metadata.loc[int(key), \"rate\"] = float(value.rate)\nsuper().__setitem__(int(key), value)\n# if self.__contains__(key):\n#     raise KeyError(\"Key {} already in group index.\".format(key))\n# else:\n# if isinstance(value, (Ts, Tsd)):\n#     self._metadata.loc[int(key), \"rate\"] = value.rate\n#     super().__setitem__(int(key), value)\n# elif isinstance(value, (np.ndarray, list)):\n#     warnings.warn(\n#         \"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\n#         stacklevel=2,\n#     )\n#     tmp = Ts(t=value, time_units=\"s\")\n#     self._metadata.loc[int(key), \"rate\"] = tmp.rate\n#     super().__setitem__(int(key), tmp)\n# else:\n#     raise ValueError(\"Value with key {} is not an iterable.\".format(key))\ndef __getitem__(self, key):\nif key.__hash__:\nif self.__contains__(key):\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\nelse:\nmetadata = self._metadata.loc[key, self._metadata.columns.drop(\"rate\")]\nreturn TsGroup(\n{k: self[k] for k in key}, time_support=self.time_support, **metadata\n)\ndef __repr__(self):\ncols = self._metadata.columns.drop(\"rate\")\nheaders = [\"Index\", \"rate\"] + [c for c in cols]\nlines = []\nfor i in self.data.keys():\nlines.append(\n[str(i), \"%.2f\" % self._metadata.loc[i, \"rate\"]]\n+ [self._metadata.loc[i, c] for c in cols]\n)\nreturn tabulate(lines, headers=headers)\ndef __str__(self):\nreturn self.__repr__()\ndef keys(self):\n\"\"\"\n        Return index/keys of TsGroup\n        Returns\n        -------\n        list\n            List of keys\n        \"\"\"\nreturn list(self.data.keys())\ndef items(self):\n\"\"\"\n        Return a list of key/object.\n        Returns\n        -------\n        list\n            List of tuples\n        \"\"\"\nreturn list(self.data.items())\ndef values(self):\n\"\"\"\n        Return a list of all the Ts/Tsd objects in the TsGroup\n        Returns\n        -------\n        list\n            List of Ts/Tsd objects\n        \"\"\"\nreturn list(self.data.values())\n@property\ndef rates(self):\n\"\"\"\n        Return the rates of each element of the group in Hz\n        \"\"\"\nreturn self._metadata[\"rate\"]\n#######################\n# Metadata\n#######################\n@property\ndef metadata_columns(self):\n\"\"\"\n        Returns list of metadata columns\n        -------\n        \"\"\"\nreturn list(self._metadata.columns)\ndef set_info(self, *args, **kwargs):\n\"\"\"\n        Add metadata informations about the TsGroup.\n        Metadata are saved as a DataFrame.\n        Parameters\n        ----------\n        *args\n            pandas.Dataframe or list of pandas.DataFrame\n        **kwargs\n            Can be either pandas.Series or numpy.ndarray\n        Raises\n        ------\n        RuntimeError\n            Raise an error if\n                no column labels are found when passing simple arguments,\n                indexes are not equals for a pandas series,\n                not the same length when passing numpy array.\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        To add metadata with a pandas.DataFrame:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n        &gt;&gt;&gt; tsgroup.set_info(structs)\n        &gt;&gt;&gt; tsgroup\n          Index    Freq. (Hz)  struct\n        -------  ------------  --------\n              0             1  pfc\n              1             2  pfc\n              2             4  ca1\n        To add metadata with a pd.Series or numpy.ndarray:\n        &gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n        &gt;&gt;&gt; tsgroup.set_info(hd=hd)\n        &gt;&gt;&gt; tsgroup\n          Index    Freq. (Hz)  struct      hd\n        -------  ------------  --------  ----\n              0             1  pfc          0\n              1             2  pfc          1\n              2             4  ca1          1\n        \"\"\"\nif len(args):\nfor arg in args:\nif isinstance(arg, pd.DataFrame):\nif pd.Index.equals(self._metadata.index, arg.index):\nself._metadata = self._metadata.join(arg)\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(arg, (pd.Series, np.ndarray)):\nraise RuntimeError(\"Columns needs to be labelled for metadata\")\nif len(kwargs):\nfor k, v in kwargs.items():\nif isinstance(v, pd.Series):\nif pd.Index.equals(self._metadata.index, v.index):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(v, np.ndarray):\nif len(self._metadata) == len(v):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Array is not the same length.\")\nreturn\ndef get_info(self, key):\n\"\"\"\n        Returns the metainfo located in one column.\n        The key for the column frequency is \"rate\".\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        Returns\n        -------\n        pandas.Series\n            The metainfo\n        \"\"\"\nif key in [\"freq\", \"frequency\"]:\nkey = \"rate\"\nreturn self._metadata[key]\n#################################\n# Generic functions of Tsd objects\n#################################\ndef restrict(self, ep):\n\"\"\"\n        Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object\n        Parameters\n        ----------\n        ep : IntervalSet\n            the IntervalSet object\n        Returns\n        -------\n        TsGroup\n            TsGroup object restricted to ep\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        &gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n        All objects within the TsGroup automatically inherit the epochs defined by ep.\n        &gt;&gt;&gt; newtsgroup.time_support\n           start    end\n        0    0.0  100.0\n        &gt;&gt;&gt; newtsgroup[0].time_support\n           start    end\n        0    0.0  100.0\n        \"\"\"\nnewgr = {}\nfor k in self.index:\nnewgr[k] = self.data[k].restrict(ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(\nnewgr, time_support=ep, bypass_check=True, **self._metadata[cols]\n)\ndef value_from(self, tsd, ep=None):\n\"\"\"\n        Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument\n        Parameters\n        ----------\n        tsd : Tsd\n            The Tsd object holding the values to replace\n        ep : IntervalSet\n            The IntervalSet object to restrict the operation.\n            If None, the time support of the tsd input object is used.\n        Returns\n        -------\n        TsGroup\n            TsGroup object with the new values\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        The variable tsd is a time series object containing the values to assign, for example the tracking data:\n        &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n        &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n        &gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n        \"\"\"\nif ep is None:\nep = tsd.time_support\nnewgr = {}\nfor k in self.data:\nnewgr[k] = self.data[k].value_from(tsd, ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(newgr, time_support=ep, **self._metadata[cols])\ndef count(self, *args, **kwargs):\n\"\"\"\n        Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n        You can call this function in multiple ways :\n        1. *tsgroup.count(bin_size=1, time_units = 'ms')*\n        -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n        2. *tsgroup.count(1, ep=my_epochs)*\n        -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n        3. *tsgroup.count(ep=my_bins)*\n        -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n        4. *tsgroup.count()*\n        -&gt; Count occurent of events within each epoch of the time support.\n        bin_size should be seconds unless specified.\n        If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n        Parameters\n        ----------\n        bin_size : None or float, optional\n            The bin size (default is second)\n        ep : None or IntervalSet, optional\n            IntervalSet to restrict the operation\n        time_units : str, optional\n            Time units of bin size ('us', 'ms', 's' [default])\n        Returns\n        -------\n        out: TsdFrame\n            A TsdFrame with the columns being the index of each item in the TsGroup.\n        Examples\n        --------\n        This example shows how to count events within bins of 0.1 second for the first 100 seconds.\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n        &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n        &gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n        &gt;&gt;&gt; bincount\n                  0  1  2\n        Time (s)\n        0.05      0  0  0\n        0.15      0  0  0\n        0.25      0  0  1\n        0.35      0  0  0\n        0.45      0  0  0\n        ...      .. .. ..\n        99.55     0  1  1\n        99.65     0  0  0\n        99.75     0  0  1\n        99.85     0  0  0\n        99.95     1  1  1\n        [1000 rows x 3 columns]\n        \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = float(bin_size)\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\ntime_index, _ = jitcount(np.array([]), starts, ends, bin_size)\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jitcount(\nself.data[self.index[i]].index.values, starts, ends, bin_size\n)[1]\nelse:\ntime_index = starts + (ends - starts) / 2\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jittsrestrict_with_count(\nself.data[self.index[i]].index.values, starts, ends\n)[1]\ntoreturn = TsdFrame(t=time_index, d=count, time_support=ep, columns=self.index)\nreturn toreturn\ndef to_tsd(self, *args):\n\"\"\"\n        Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.\n        Parameters\n        ----------\n        *args\n            string, list, numpy.ndarray or pandas.Series\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\n        Index    rate\n        -------  ------\n        0       1\n        5       1\n        By default, the values of the Tsd is the index of the timestamp in the TsGroup:\n        &gt;&gt;&gt; tsgroup.to_tsd()\n        Time (s)\n        0.0    0.0\n        1.0    0.0\n        2.0    5.0\n        3.0    5.0\n        dtype: float64\n        Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.\n        &gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n        &gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\n        Time (s)\n        0.0    3.141593\n        1.0    3.141593\n        2.0    6.283185\n        3.0    6.283185\n        dtype: float64\n        Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :\n        &gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\n        Time (s)\n        0.0   -1.0\n        1.0   -1.0\n        2.0    1.0\n        3.0    1.0\n        dtype: float64\n        The reverse operation can be done with the Tsd.to_tsgroup function :\n        &gt;&gt;&gt; my_tsd\n        Time (s)\n        0.0    0.0\n        1.0    0.0\n        2.0    5.0\n        3.0    5.0\n        dtype: float64\n        &gt;&gt;&gt; my_tsd.to_tsgroup()\n          Index    rate\n        -------  ------\n              0       1\n              5       1\n        Returns\n        -------\n        Tsd\n        Raises\n        ------\n        RuntimeError\n            \"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes\n            \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object\n            \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata,\n            \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series\n        \"\"\"\nif len(args):\nif isinstance(args[0], pd.Series):\nif pd.Index.equals(self._metadata.index, args[0].index):\n_values = args[0].values.flatten()\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(args[0], (np.ndarray, list)):\nif len(self._metadata) == len(args[0]):\n_values = np.array(args[0])\nelse:\nraise RuntimeError(\"Values is not the same length.\")\nelif isinstance(args[0], str):\nif args[0] in self._metadata.columns:\n_values = self._metadata[args[0]].values\nelse:\nraise RuntimeError(\n\"Key {} not in metadata of TsGroup\".format(args[0])\n)\nelse:\npossible_keys = []\nfor k, d in self._metadata.dtypes.items():\nif \"int\" in str(d) or \"float\" in str(d):\npossible_keys.append(k)\nraise RuntimeError(\n\"Unknown argument format. Must be pandas.Series, numpy.ndarray or a string from one of the following values : [{}]\".format(\n\", \".join(possible_keys)\n)\n)\nelse:\n_values = self.index\nnt = 0\nfor n in self.index:\nnt += self[n].shape[0]\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nk = 0\nfor n, v in zip(self.index, _values):\nkl = self[n].shape[0]\ntimes[k : k + kl] = self[n].index.values\ndata[k : k + kl] = v\nk += kl\nidx = np.argsort(times)\ntoreturn = Tsd(t=times[idx], d=data[idx], time_support=self.time_support)\nreturn toreturn\n\"\"\"\n    Special slicing of metadata\n    \"\"\"\ndef getby_threshold(self, key, thr, op=\"&gt;\"):\n\"\"\"\n        Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        thr : float\n            THe value for thresholding\n        op : str, optional\n            The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.\n        Returns\n        -------\n        TsGroup\n            The new TsGroup\n        Raises\n        ------\n        RuntimeError\n            Raise eror is operation is not recognized.\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n          Index    Freq. (Hz)\n        -------  ------------\n              0             1\n              1             2\n              2             4\n        This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.\n        &gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n          Index    Freq. (Hz)\n        -------  ------------\n              1             2\n              2             4\n        \"\"\"\nif op == \"&gt;\":\nix = list(self._metadata.index[self._metadata[key] &gt; thr])\nreturn self[ix]\nelif op == \"&lt;\":\nix = list(self._metadata.index[self._metadata[key] &lt; thr])\nreturn self[ix]\nelif op == \"&gt;=\":\nix = list(self._metadata.index[self._metadata[key] &gt;= thr])\nreturn self[ix]\nelif op == \"&lt;=\":\nix = list(self._metadata.index[self._metadata[key] &lt;= thr])\nreturn self[ix]\nelse:\nraise RuntimeError(\"Operation {} not recognized.\".format(op))\ndef getby_intervals(self, key, bins):\n\"\"\"\n        Return a list of TsGroup binned.\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        bins : numpy.ndarray or list\n            The bin intervals\n        Returns\n        -------\n        list\n            A list of TsGroup\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n          Index    Freq. (Hz)    alpha\n        -------  ------------  -------\n              0             1        0\n              1             2        1\n              2             4        2\n        This exemple shows how to bin the TsGroup according to one metainfo key.\n        &gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n        &gt;&gt;&gt; newtsgroup\n        [  Index    Freq. (Hz)    alpha\n         -------  ------------  -------\n               0             1        0,\n           Index    Freq. (Hz)    alpha\n         -------  ------------  -------\n               1             2        1]\n        By default, the function returns the center of the bins.\n        &gt;&gt;&gt; bincenter\n        array([0.5, 1.5])\n        \"\"\"\nidx = np.digitize(self._metadata[key], bins) - 1\ngroups = self._metadata.index.groupby(idx)\nix = np.unique(list(groups.keys()))\nix = ix[ix &gt;= 0]\nix = ix[ix &lt; len(bins) - 1]\nxb = bins[0:-1] + np.diff(bins) / 2\nsliced = [self[list(groups[i])] for i in ix]\nreturn sliced, xb[ix]\ndef getby_category(self, key):\n\"\"\"\n        Return a list of TsGroup grouped by category.\n        Parameters\n        ----------\n        key : str\n            One of the metainfo columns name\n        Returns\n        -------\n        dict\n            A dictionnary of TsGroup\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n        1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n        2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n        }\n        &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n          Index    Freq. (Hz)    group\n        -------  ------------  -------\n              0             1        0\n              1             2        1\n              2             4        1\n        This exemple shows how to group the TsGroup according to one metainfo key.\n        &gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n        &gt;&gt;&gt; newtsgroup\n        {0:   Index    Freq. (Hz)    group\n         -------  ------------  -------\n               0             1        0,\n         1:   Index    Freq. (Hz)    group\n         -------  ------------  -------\n               1             2        1\n               2             4        1}\n        \"\"\"\ngroups = self._metadata.groupby(key).groups\nsliced = {k: self[list(groups[k])] for k in groups.keys()}\nreturn sliced\ndef save(self, filename):\n\"\"\"\n        Save TsGroup object in npz format. The file will contain the timestamps,\n        the data (if group of Tsd), group index, the time support and the metadata\n        The main purpose of this function is to save small/medium sized TsGroup\n        objects.\n        The function will \"flatten\" the TsGroup by sorting all the timestamps\n        and assigning to each the corresponding index. Typically, a TsGroup like\n        this :\n            TsGroup({\n                0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n                1 : Tsd(t=[1, 5], d=[5, 6])\n            })\n        will be saved as npz with the following keys:\n            {\n                't' : [0, 1, 2, 4, 5],\n                'd' : [1, 5, 2, 3, 5],\n                'index' : [0, 1, 0, 0, 1],\n                'start' : [0],\n                'end' : [5],\n                'type' : 'TsGroup'\n            }\n        Metadata are saved by columns with the column name as the npz key. To avoid\n        potential conflicts, make sure the columns name of the metadata are different\n        from ['t', 'd', 'start', 'end', 'index']\n        You can load the object with numpy.load. Default keys are 't', 'd'(optional),\n        'start', 'end', 'index' and 'type'.\n        See the example below.\n        Parameters\n        ----------\n        filename : str\n            The filename\n        Examples\n        --------\n        &gt;&gt;&gt; import pynapple as nap\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; tsgroup = nap.TsGroup({\n            0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n            6 : nap.Ts(t=np.array([1.0, 5.0]))\n            },\n            group = np.array([0, 1]),\n            location = np.array(['right foot', 'left foot'])\n            )\n        &gt;&gt;&gt; tsgroup\n          Index    rate    group  location\n        -------  ------  -------  ----------\n              0     0.6        0  right foot\n              6     0.4        1  left foot\n        &gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n        Here I can retrieve my data with numpy directly:\n        &gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n        &gt;&gt;&gt; print(list(file.keys()))\n        ['rate', 'group', 'location', 't', 'index', 'start', 'end', 'type']\n        &gt;&gt;&gt; print(file['index'])\n        [0 6 0 0 6]\n        In the case where TsGroup is a set of Ts objects, it is very direct to\n        recreate the TsGroup by using the function to_tsgroup :\n        &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n        &gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n        &gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n        &gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n        &gt;&gt;&gt; tsgroup\n          Index    rate    group  location\n        -------  ------  -------  ----------\n              0     0.6        0  right foot\n              6     0.4        1  left foot\n        Raises\n        ------\n        RuntimeError\n            If filename is not str, path does not exist or filename is a directory.\n        \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ndicttosave = {\"type\": np.array([\"TsGroup\"], dtype=np.str_)}\nfor k in self._metadata.columns:\nif k not in [\"t\", \"d\", \"start\", \"end\", \"index\"]:\ntmp = self._metadata[k].values\nif tmp.dtype == np.dtype(\"O\"):\ntmp = tmp.astype(np.str_)\ndicttosave[k] = tmp\n# We can't use to_tsd here in case tsgroup contains Tsd and not only Ts.\nnt = 0\nfor n in self.index:\nnt += self[n].shape[0]\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nindex = np.zeros(nt, dtype=np.int64)\nk = 0\nfor n in self.index:\nkl = self[n].shape[0]\ntimes[k : k + kl] = self[n].index.values\ndata[k : k + kl] = self[n].values\nindex[k : k + kl] = int(n)\nk += kl\nidx = np.argsort(times)\ntimes = times[idx]\nindex = index[idx]\ndicttosave[\"t\"] = times\ndicttosave[\"index\"] = index\nif not np.all(np.isnan(data)):\ndicttosave[\"d\"] = data[idx]\ndicttosave[\"start\"] = self.time_support.start.values\ndicttosave[\"end\"] = self.time_support.end.values\nnp.savez(filename, **dicttosave)\nreturn\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.rates","title":"<code>rates</code>  <code>property</code>","text":"<p>Return the rates of each element of the group in Hz</p>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.metadata_columns","title":"<code>metadata_columns</code>  <code>property</code>","text":""},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.metadata_columns--returns-list-of-metadata-columns","title":"Returns list of metadata columns","text":""},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.__init__","title":"<code>__init__(data, time_support=None, time_units='s', bypass_check=False, **kwargs)</code>","text":"<p>TsGroup Initializer</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict</code> <p>Dictionnary containing Ts/Tsd objects</p> required <code>time_support</code> <code>IntervalSet, optional</code> <p>The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed. If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.</p> <code>None</code> <code>time_units</code> <code>str, optional</code> <p>Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).</p> <code>'s'</code> <code>bypass_check</code> <p>To avoid checking that each element is within time_support. Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand</p> <code>False</code> <code>**kwargs</code> <p>Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray. Note that the index should match the index of the input dictionnary.</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise error if the union of time support of Ts/Tsd object is empty.</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def __init__(\nself, data, time_support=None, time_units=\"s\", bypass_check=False, **kwargs\n):\n\"\"\"\n    TsGroup Initializer\n    Parameters\n    ----------\n    data : dict\n        Dictionnary containing Ts/Tsd objects\n    time_support : IntervalSet, optional\n        The time support of the TsGroup. Ts/Tsd objects will be restricted to the time support if passed.\n        If no time support is specified, TsGroup will merge time supports from all the Ts/Tsd objects in data.\n    time_units : str, optional\n        Time units if data does not contain Ts/Tsd objects ('us', 'ms', 's' [default]).\n    bypass_check: bool, optional\n        To avoid checking that each element is within time_support.\n        Useful to speed up initialization of TsGroup when Ts/Tsd objects have already been restricted beforehand\n    **kwargs\n        Meta-info about the Ts/Tsd objects. Can be either pandas.Series or numpy.ndarray.\n        Note that the index should match the index of the input dictionnary.\n    Raises\n    ------\n    RuntimeError\n        Raise error if the union of time support of Ts/Tsd object is empty.\n    \"\"\"\nself._initialized = False\nself.index = np.sort(list(data.keys()))\nself._metadata = pd.DataFrame(index=self.index, columns=[\"rate\"], dtype=\"float\")\n# Transform elements to Ts/Tsd objects\nfor k in self.index:\nif isinstance(data[k], (np.ndarray, list)):\nwarnings.warn(\n\"Elements should not be passed as numpy array. Default time units is seconds when creating the Ts object.\",\nstacklevel=2,\n)\ndata[k] = Ts(\nt=data[k], time_support=time_support, time_units=time_units\n)\n# If time_support is passed, all elements of data are restricted prior to init\nif isinstance(time_support, IntervalSet):\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nelse:\n# Otherwise do the union of all time supports\ntime_support = union_intervals([data[k].time_support for k in self.index])\nif len(time_support) == 0:\nraise RuntimeError(\n\"Union of time supports is empty. Consider passing a time support as argument.\"\n)\nself.time_support = time_support\nif not bypass_check:\ndata = {k: data[k].restrict(self.time_support) for k in self.index}\nUserDict.__init__(self, data)\n# Making the TsGroup non mutable\nself._initialized = True\n# Trying to add argument as metainfo\nself.set_info(**kwargs)\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.keys","title":"<code>keys()</code>","text":"<p>Return index/keys of TsGroup</p> <p>Returns:</p> Type Description <code>list</code> <p>List of keys</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def keys(self):\n\"\"\"\n    Return index/keys of TsGroup\n    Returns\n    -------\n    list\n        List of keys\n    \"\"\"\nreturn list(self.data.keys())\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.items","title":"<code>items()</code>","text":"<p>Return a list of key/object.</p> <p>Returns:</p> Type Description <code>list</code> <p>List of tuples</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def items(self):\n\"\"\"\n    Return a list of key/object.\n    Returns\n    -------\n    list\n        List of tuples\n    \"\"\"\nreturn list(self.data.items())\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.values","title":"<code>values()</code>","text":"<p>Return a list of all the Ts/Tsd objects in the TsGroup</p> <p>Returns:</p> Type Description <code>list</code> <p>List of Ts/Tsd objects</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def values(self):\n\"\"\"\n    Return a list of all the Ts/Tsd objects in the TsGroup\n    Returns\n    -------\n    list\n        List of Ts/Tsd objects\n    \"\"\"\nreturn list(self.data.values())\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.set_info","title":"<code>set_info(*args, **kwargs)</code>","text":"<p>Add metadata informations about the TsGroup. Metadata are saved as a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>pandas.Dataframe or list of pandas.DataFrame</p> <code>()</code> <code>**kwargs</code> <p>Can be either pandas.Series or numpy.ndarray</p> <code>{}</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise an error if no column labels are found when passing simple arguments, indexes are not equals for a pandas series, not the same length when passing numpy array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n</code></pre> <p>To add metadata with a pandas.DataFrame:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n&gt;&gt;&gt; tsgroup.set_info(structs)\n&gt;&gt;&gt; tsgroup\n  Index    Freq. (Hz)  struct\n-------  ------------  --------\n      0             1  pfc\n      1             2  pfc\n      2             4  ca1\n</code></pre> <p>To add metadata with a pd.Series or numpy.ndarray:</p> <pre><code>&gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n&gt;&gt;&gt; tsgroup.set_info(hd=hd)\n&gt;&gt;&gt; tsgroup\n  Index    Freq. (Hz)  struct      hd\n-------  ------------  --------  ----\n      0             1  pfc          0\n      1             2  pfc          1\n      2             4  ca1          1\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def set_info(self, *args, **kwargs):\n\"\"\"\n    Add metadata informations about the TsGroup.\n    Metadata are saved as a DataFrame.\n    Parameters\n    ----------\n    *args\n        pandas.Dataframe or list of pandas.DataFrame\n    **kwargs\n        Can be either pandas.Series or numpy.ndarray\n    Raises\n    ------\n    RuntimeError\n        Raise an error if\n            no column labels are found when passing simple arguments,\n            indexes are not equals for a pandas series,\n            not the same length when passing numpy array.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    To add metadata with a pandas.DataFrame:\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; structs = pd.DataFrame(index = [0,1,2], data=['pfc','pfc','ca1'], columns=['struct'])\n    &gt;&gt;&gt; tsgroup.set_info(structs)\n    &gt;&gt;&gt; tsgroup\n      Index    Freq. (Hz)  struct\n    -------  ------------  --------\n          0             1  pfc\n          1             2  pfc\n          2             4  ca1\n    To add metadata with a pd.Series or numpy.ndarray:\n    &gt;&gt;&gt; hd = pd.Series(index = [0,1,2], data = [0,1,1])\n    &gt;&gt;&gt; tsgroup.set_info(hd=hd)\n    &gt;&gt;&gt; tsgroup\n      Index    Freq. (Hz)  struct      hd\n    -------  ------------  --------  ----\n          0             1  pfc          0\n          1             2  pfc          1\n          2             4  ca1          1\n    \"\"\"\nif len(args):\nfor arg in args:\nif isinstance(arg, pd.DataFrame):\nif pd.Index.equals(self._metadata.index, arg.index):\nself._metadata = self._metadata.join(arg)\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(arg, (pd.Series, np.ndarray)):\nraise RuntimeError(\"Columns needs to be labelled for metadata\")\nif len(kwargs):\nfor k, v in kwargs.items():\nif isinstance(v, pd.Series):\nif pd.Index.equals(self._metadata.index, v.index):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(v, np.ndarray):\nif len(self._metadata) == len(v):\nself._metadata[k] = v\nelse:\nraise RuntimeError(\"Array is not the same length.\")\nreturn\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.get_info","title":"<code>get_info(key)</code>","text":"<p>Returns the metainfo located in one column. The key for the column frequency is \"rate\".</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <p>Returns:</p> Type Description <code>pandas.Series</code> <p>The metainfo</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def get_info(self, key):\n\"\"\"\n    Returns the metainfo located in one column.\n    The key for the column frequency is \"rate\".\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    Returns\n    -------\n    pandas.Series\n        The metainfo\n    \"\"\"\nif key in [\"freq\", \"frequency\"]:\nkey = \"rate\"\nreturn self._metadata[key]\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.restrict","title":"<code>restrict(ep)</code>","text":"<p>Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object</p> <p>Parameters:</p> Name Type Description Default <code>ep</code> <code>IntervalSet</code> <p>the IntervalSet object</p> required <p>Returns:</p> Type Description <code>TsGroup</code> <p>TsGroup object restricted to ep</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n&gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n</code></pre> <p>All objects within the TsGroup automatically inherit the epochs defined by ep.</p> <pre><code>&gt;&gt;&gt; newtsgroup.time_support\n   start    end\n0    0.0  100.0\n&gt;&gt;&gt; newtsgroup[0].time_support\n   start    end\n0    0.0  100.0\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def restrict(self, ep):\n\"\"\"\n    Restricts a TsGroup object to a set of time intervals delimited by an IntervalSet object\n    Parameters\n    ----------\n    ep : IntervalSet\n        the IntervalSet object\n    Returns\n    -------\n    TsGroup\n        TsGroup object restricted to ep\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    &gt;&gt;&gt; newtsgroup = tsgroup.restrict(ep)\n    All objects within the TsGroup automatically inherit the epochs defined by ep.\n    &gt;&gt;&gt; newtsgroup.time_support\n       start    end\n    0    0.0  100.0\n    &gt;&gt;&gt; newtsgroup[0].time_support\n       start    end\n    0    0.0  100.0\n    \"\"\"\nnewgr = {}\nfor k in self.index:\nnewgr[k] = self.data[k].restrict(ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(\nnewgr, time_support=ep, bypass_check=True, **self._metadata[cols]\n)\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.value_from","title":"<code>value_from(tsd, ep=None)</code>","text":"<p>Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>Tsd</code> <p>The Tsd object holding the values to replace</p> required <code>ep</code> <code>IntervalSet</code> <p>The IntervalSet object to restrict the operation. If None, the time support of the tsd input object is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>TsGroup</code> <p>TsGroup object with the new values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n</code></pre> <p>The variable tsd is a time series object containing the values to assign, for example the tracking data:</p> <pre><code>&gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n&gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n&gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def value_from(self, tsd, ep=None):\n\"\"\"\n    Replace the value of each Ts/Tsd object within the Ts group with the closest value from tsd argument\n    Parameters\n    ----------\n    tsd : Tsd\n        The Tsd object holding the values to replace\n    ep : IntervalSet\n        The IntervalSet object to restrict the operation.\n        If None, the time support of the tsd input object is used.\n    Returns\n    -------\n    TsGroup\n        TsGroup object with the new values\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    The variable tsd is a time series object containing the values to assign, for example the tracking data:\n    &gt;&gt;&gt; tsd = nap.Tsd(t=np.arange(0,100), d=np.random.rand(100), time_units='s')\n    &gt;&gt;&gt; ep = nap.IntervalSet(start = 0, end = 100, time_units = 's')\n    &gt;&gt;&gt; newtsgroup = tsgroup.value_from(tsd, ep)\n    \"\"\"\nif ep is None:\nep = tsd.time_support\nnewgr = {}\nfor k in self.data:\nnewgr[k] = self.data[k].value_from(tsd, ep)\ncols = self._metadata.columns.drop(\"rate\")\nreturn TsGroup(newgr, time_support=ep, **self._metadata[cols])\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.count","title":"<code>count(*args, **kwargs)</code>","text":"<p>Count occurences of events within bin_size or within a set of bins defined as an IntervalSet. You can call this function in multiple ways :</p> <ol> <li> <p>tsgroup.count(bin_size=1, time_units = 'ms') -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.</p> </li> <li> <p>tsgroup.count(1, ep=my_epochs) -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.</p> </li> <li> <p>tsgroup.count(ep=my_bins) -&gt; Count occurent of events within each epoch of the intervalSet object my_bins</p> </li> <li> <p>tsgroup.count() -&gt; Count occurent of events within each epoch of the time support.</p> </li> </ol> <p>bin_size should be seconds unless specified. If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.</p> <p>Parameters:</p> Name Type Description Default <code>bin_size</code> <code>None or float, optional</code> <p>The bin size (default is second)</p> required <code>ep</code> <code>None or IntervalSet, optional</code> <p>IntervalSet to restrict the operation</p> required <code>time_units</code> <code>str, optional</code> <p>Time units of bin size ('us', 'ms', 's' [default])</p> required <p>Returns:</p> Name Type Description <code>out</code> <code>TsdFrame</code> <p>A TsdFrame with the columns being the index of each item in the TsGroup.</p> <p>Examples:</p> <p>This example shows how to count events within bins of 0.1 second for the first 100 seconds.</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n&gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n&gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n&gt;&gt;&gt; bincount\n          0  1  2\nTime (s)\n0.05      0  0  0\n0.15      0  0  0\n0.25      0  0  1\n0.35      0  0  0\n0.45      0  0  0\n...      .. .. ..\n99.55     0  1  1\n99.65     0  0  0\n99.75     0  0  1\n99.85     0  0  0\n99.95     1  1  1\n[1000 rows x 3 columns]\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def count(self, *args, **kwargs):\n\"\"\"\n    Count occurences of events within bin_size or within a set of bins defined as an IntervalSet.\n    You can call this function in multiple ways :\n    1. *tsgroup.count(bin_size=1, time_units = 'ms')*\n    -&gt; Count occurence of events within a 1 ms bin defined on the time support of the object.\n    2. *tsgroup.count(1, ep=my_epochs)*\n    -&gt; Count occurent of events within a 1 second bin defined on the IntervalSet my_epochs.\n    3. *tsgroup.count(ep=my_bins)*\n    -&gt; Count occurent of events within each epoch of the intervalSet object my_bins\n    4. *tsgroup.count()*\n    -&gt; Count occurent of events within each epoch of the time support.\n    bin_size should be seconds unless specified.\n    If bin_size is used and no epochs is passed, the data will be binned based on the time support of the object.\n    Parameters\n    ----------\n    bin_size : None or float, optional\n        The bin size (default is second)\n    ep : None or IntervalSet, optional\n        IntervalSet to restrict the operation\n    time_units : str, optional\n        Time units of bin size ('us', 'ms', 's' [default])\n    Returns\n    -------\n    out: TsdFrame\n        A TsdFrame with the columns being the index of each item in the TsGroup.\n    Examples\n    --------\n    This example shows how to count events within bins of 0.1 second for the first 100 seconds.\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n    &gt;&gt;&gt; ep = nap.IntervalSet(start=0, end=100, time_units='s')\n    &gt;&gt;&gt; bincount = tsgroup.count(0.1, ep)\n    &gt;&gt;&gt; bincount\n              0  1  2\n    Time (s)\n    0.05      0  0  0\n    0.15      0  0  0\n    0.25      0  0  1\n    0.35      0  0  0\n    0.45      0  0  0\n    ...      .. .. ..\n    99.55     0  1  1\n    99.65     0  0  0\n    99.75     0  0  1\n    99.85     0  0  0\n    99.95     1  1  1\n    [1000 rows x 3 columns]\n    \"\"\"\nbin_size = None\nif \"bin_size\" in kwargs:\nbin_size = kwargs[\"bin_size\"]\nif isinstance(bin_size, int):\nbin_size = float(bin_size)\nif not isinstance(bin_size, float):\nraise ValueError(\"bin_size argument should be float.\")\nelse:\nfor a in args:\nif isinstance(a, (float, int)):\nbin_size = float(a)\ntime_units = \"s\"\nif \"time_units\" in kwargs:\ntime_units = kwargs[\"time_units\"]\nif not isinstance(time_units, str):\nraise ValueError(\"time_units argument should be 's', 'ms' or 'us'.\")\nelse:\nfor a in args:\nif isinstance(a, str) and a in [\"s\", \"ms\", \"us\"]:\ntime_units = a\nep = self.time_support\nif \"ep\" in kwargs:\nep = kwargs[\"ep\"]\nif not isinstance(ep, IntervalSet):\nraise ValueError(\"ep argument should be IntervalSet\")\nelse:\nfor a in args:\nif isinstance(a, IntervalSet):\nep = a\nstarts = ep.start.values\nends = ep.end.values\nif isinstance(bin_size, (float, int)):\nbin_size = float(bin_size)\nbin_size = format_timestamps(np.array([bin_size]), time_units)[0]\ntime_index, _ = jitcount(np.array([]), starts, ends, bin_size)\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jitcount(\nself.data[self.index[i]].index.values, starts, ends, bin_size\n)[1]\nelse:\ntime_index = starts + (ends - starts) / 2\nn = len(self.index)\ncount = np.zeros((time_index.shape[0], n), dtype=np.int64)\nfor i in range(n):\ncount[:, i] = jittsrestrict_with_count(\nself.data[self.index[i]].index.values, starts, ends\n)[1]\ntoreturn = TsdFrame(t=time_index, d=count, time_support=ep, columns=self.index)\nreturn toreturn\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.to_tsd","title":"<code>to_tsd(*args)</code>","text":"<p>Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>string, list, numpy.ndarray or pandas.Series</p> <code>()</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\nIndex    rate\n-------  ------\n0       1\n5       1\n</code></pre> <p>By default, the values of the Tsd is the index of the timestamp in the TsGroup:</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd()\nTime (s)\n0.0    0.0\n1.0    0.0\n2.0    5.0\n3.0    5.0\ndtype: float64\n</code></pre> <p>Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.</p> <pre><code>&gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n&gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\nTime (s)\n0.0    3.141593\n1.0    3.141593\n2.0    6.283185\n3.0    6.283185\ndtype: float64\n</code></pre> <p>Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :</p> <pre><code>&gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\nTime (s)\n0.0   -1.0\n1.0   -1.0\n2.0    1.0\n3.0    1.0\ndtype: float64\n</code></pre> <p>The reverse operation can be done with the Tsd.to_tsgroup function :</p> <pre><code>&gt;&gt;&gt; my_tsd\nTime (s)\n0.0    0.0\n1.0    0.0\n2.0    5.0\n3.0    5.0\ndtype: float64\n&gt;&gt;&gt; my_tsd.to_tsgroup()\n  Index    rate\n-------  ------\n      0       1\n      5       1\n</code></pre> <p>Returns:</p> Type Description <code>Tsd</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>\"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata, \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def to_tsd(self, *args):\n\"\"\"\n    Convert TsGroup to a Tsd. The timestamps of the TsGroup are merged together and sorted.\n    Parameters\n    ----------\n    *args\n        string, list, numpy.ndarray or pandas.Series\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsgroup = nap.TsGroup({0:nap.Ts(t=np.array([0, 1])), 5:nap.Ts(t=np.array([2, 3]))})\n    Index    rate\n    -------  ------\n    0       1\n    5       1\n    By default, the values of the Tsd is the index of the timestamp in the TsGroup:\n    &gt;&gt;&gt; tsgroup.to_tsd()\n    Time (s)\n    0.0    0.0\n    1.0    0.0\n    2.0    5.0\n    3.0    5.0\n    dtype: float64\n    Values can be inherited from the metadata of the TsGroup by giving the key of the corresponding columns.\n    &gt;&gt;&gt; tsgroup.set_info( phase=np.array([np.pi, 2*np.pi]) ) # assigning a phase to my 2 elements of the TsGroup\n    &gt;&gt;&gt; tsgroup.to_tsd(\"phase\")\n    Time (s)\n    0.0    3.141593\n    1.0    3.141593\n    2.0    6.283185\n    3.0    6.283185\n    dtype: float64\n    Values can also be passed directly to the function from a list, numpy.ndarray or pandas.Series of values as long as the length matches :\n    &gt;&gt;&gt; tsgroup.to_tsd([-1, 1])\n    Time (s)\n    0.0   -1.0\n    1.0   -1.0\n    2.0    1.0\n    3.0    1.0\n    dtype: float64\n    The reverse operation can be done with the Tsd.to_tsgroup function :\n    &gt;&gt;&gt; my_tsd\n    Time (s)\n    0.0    0.0\n    1.0    0.0\n    2.0    5.0\n    3.0    5.0\n    dtype: float64\n    &gt;&gt;&gt; my_tsd.to_tsgroup()\n      Index    rate\n    -------  ------\n          0       1\n          5       1\n    Returns\n    -------\n    Tsd\n    Raises\n    ------\n    RuntimeError\n        \"Index are not equals\" : if pandas.Series indexes don't match the TsGroup indexes\n        \"Values is not the same length\" : if numpy.ndarray/list object is not the same size as the TsGroup object\n        \"Key not in metadata of TsGroup\" : if string argument does not match any column names of the metadata,\n        \"Unknown argument format\" ; if argument is not a string, list, numpy.ndarray or pandas.Series\n    \"\"\"\nif len(args):\nif isinstance(args[0], pd.Series):\nif pd.Index.equals(self._metadata.index, args[0].index):\n_values = args[0].values.flatten()\nelse:\nraise RuntimeError(\"Index are not equals\")\nelif isinstance(args[0], (np.ndarray, list)):\nif len(self._metadata) == len(args[0]):\n_values = np.array(args[0])\nelse:\nraise RuntimeError(\"Values is not the same length.\")\nelif isinstance(args[0], str):\nif args[0] in self._metadata.columns:\n_values = self._metadata[args[0]].values\nelse:\nraise RuntimeError(\n\"Key {} not in metadata of TsGroup\".format(args[0])\n)\nelse:\npossible_keys = []\nfor k, d in self._metadata.dtypes.items():\nif \"int\" in str(d) or \"float\" in str(d):\npossible_keys.append(k)\nraise RuntimeError(\n\"Unknown argument format. Must be pandas.Series, numpy.ndarray or a string from one of the following values : [{}]\".format(\n\", \".join(possible_keys)\n)\n)\nelse:\n_values = self.index\nnt = 0\nfor n in self.index:\nnt += self[n].shape[0]\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nk = 0\nfor n, v in zip(self.index, _values):\nkl = self[n].shape[0]\ntimes[k : k + kl] = self[n].index.values\ndata[k : k + kl] = v\nk += kl\nidx = np.argsort(times)\ntoreturn = Tsd(t=times[idx], d=data[idx], time_support=self.time_support)\nreturn toreturn\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.getby_threshold","title":"<code>getby_threshold(key, thr, op='&gt;')</code>","text":"<p>Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <code>thr</code> <code>float</code> <p>THe value for thresholding</p> required <code>op</code> <code>str, optional</code> <p>The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.</p> <code>'&gt;'</code> <p>Returns:</p> Type Description <code>TsGroup</code> <p>The new TsGroup</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raise eror is operation is not recognized.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n  Index    Freq. (Hz)\n-------  ------------\n      0             1\n      1             2\n      2             4\n</code></pre> <p>This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.</p> <pre><code>&gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n  Index    Freq. (Hz)\n-------  ------------\n      1             2\n      2             4\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_threshold(self, key, thr, op=\"&gt;\"):\n\"\"\"\n    Return a TsGroup with all Ts/Tsd objects with values above threshold for metainfo under key.\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    thr : float\n        THe value for thresholding\n    op : str, optional\n        The type of operation. Possibilities are '&gt;', '&lt;', '&gt;=' or '&lt;='.\n    Returns\n    -------\n    TsGroup\n        The new TsGroup\n    Raises\n    ------\n    RuntimeError\n        Raise eror is operation is not recognized.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp)\n      Index    Freq. (Hz)\n    -------  ------------\n          0             1\n          1             2\n          2             4\n    This exemple shows how to get a new TsGroup with all elements for which the metainfo frequency is above 1.\n    &gt;&gt;&gt; newtsgroup = tsgroup.getby_threshold('freq', 1, op = '&gt;')\n      Index    Freq. (Hz)\n    -------  ------------\n          1             2\n          2             4\n    \"\"\"\nif op == \"&gt;\":\nix = list(self._metadata.index[self._metadata[key] &gt; thr])\nreturn self[ix]\nelif op == \"&lt;\":\nix = list(self._metadata.index[self._metadata[key] &lt; thr])\nreturn self[ix]\nelif op == \"&gt;=\":\nix = list(self._metadata.index[self._metadata[key] &gt;= thr])\nreturn self[ix]\nelif op == \"&lt;=\":\nix = list(self._metadata.index[self._metadata[key] &lt;= thr])\nreturn self[ix]\nelse:\nraise RuntimeError(\"Operation {} not recognized.\".format(op))\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.getby_intervals","title":"<code>getby_intervals(key, bins)</code>","text":"<p>Return a list of TsGroup binned.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <code>bins</code> <code>numpy.ndarray or list</code> <p>The bin intervals</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of TsGroup</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n  Index    Freq. (Hz)    alpha\n-------  ------------  -------\n      0             1        0\n      1             2        1\n      2             4        2\n</code></pre> <p>This exemple shows how to bin the TsGroup according to one metainfo key.</p> <pre><code>&gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n&gt;&gt;&gt; newtsgroup\n[  Index    Freq. (Hz)    alpha\n -------  ------------  -------\n       0             1        0,\n   Index    Freq. (Hz)    alpha\n -------  ------------  -------\n       1             2        1]\n</code></pre> <p>By default, the function returns the center of the bins.</p> <pre><code>&gt;&gt;&gt; bincenter\narray([0.5, 1.5])\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_intervals(self, key, bins):\n\"\"\"\n    Return a list of TsGroup binned.\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    bins : numpy.ndarray or list\n        The bin intervals\n    Returns\n    -------\n    list\n        A list of TsGroup\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, alpha = np.arange(3))\n      Index    Freq. (Hz)    alpha\n    -------  ------------  -------\n          0             1        0\n          1             2        1\n          2             4        2\n    This exemple shows how to bin the TsGroup according to one metainfo key.\n    &gt;&gt;&gt; newtsgroup, bincenter = tsgroup.getby_intervals('alpha', [0, 1, 2])\n    &gt;&gt;&gt; newtsgroup\n    [  Index    Freq. (Hz)    alpha\n     -------  ------------  -------\n           0             1        0,\n       Index    Freq. (Hz)    alpha\n     -------  ------------  -------\n           1             2        1]\n    By default, the function returns the center of the bins.\n    &gt;&gt;&gt; bincenter\n    array([0.5, 1.5])\n    \"\"\"\nidx = np.digitize(self._metadata[key], bins) - 1\ngroups = self._metadata.index.groupby(idx)\nix = np.unique(list(groups.keys()))\nix = ix[ix &gt;= 0]\nix = ix[ix &lt; len(bins) - 1]\nxb = bins[0:-1] + np.diff(bins) / 2\nsliced = [self[list(groups[i])] for i in ix]\nreturn sliced, xb[ix]\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.getby_category","title":"<code>getby_category(key)</code>","text":"<p>Return a list of TsGroup grouped by category.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>One of the metainfo columns name</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionnary of TsGroup</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n}\n&gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n  Index    Freq. (Hz)    group\n-------  ------------  -------\n      0             1        0\n      1             2        1\n      2             4        1\n</code></pre> <p>This exemple shows how to group the TsGroup according to one metainfo key.</p> <pre><code>&gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n&gt;&gt;&gt; newtsgroup\n{0:   Index    Freq. (Hz)    group\n -------  ------------  -------\n       0             1        0,\n 1:   Index    Freq. (Hz)    group\n -------  ------------  -------\n       1             2        1\n       2             4        1}\n</code></pre> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def getby_category(self, key):\n\"\"\"\n    Return a list of TsGroup grouped by category.\n    Parameters\n    ----------\n    key : str\n        One of the metainfo columns name\n    Returns\n    -------\n    dict\n        A dictionnary of TsGroup\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tmp = { 0:nap.Ts(t=np.arange(0,200), time_units='s'),\n    1:nap.Ts(t=np.arange(0,200,0.5), time_units='s'),\n    2:nap.Ts(t=np.arange(0,300,0.25), time_units='s'),\n    }\n    &gt;&gt;&gt; tsgroup = nap.TsGroup(tmp, group = [0,1,1])\n      Index    Freq. (Hz)    group\n    -------  ------------  -------\n          0             1        0\n          1             2        1\n          2             4        1\n    This exemple shows how to group the TsGroup according to one metainfo key.\n    &gt;&gt;&gt; newtsgroup = tsgroup.getby_category('group')\n    &gt;&gt;&gt; newtsgroup\n    {0:   Index    Freq. (Hz)    group\n     -------  ------------  -------\n           0             1        0,\n     1:   Index    Freq. (Hz)    group\n     -------  ------------  -------\n           1             2        1\n           2             4        1}\n    \"\"\"\ngroups = self._metadata.groupby(key).groups\nsliced = {k: self[list(groups[k])] for k in groups.keys()}\nreturn sliced\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.TsGroup.save","title":"<code>save(filename)</code>","text":"<p>Save TsGroup object in npz format. The file will contain the timestamps, the data (if group of Tsd), group index, the time support and the metadata</p> <p>The main purpose of this function is to save small/medium sized TsGroup objects.</p> <p>The function will \"flatten\" the TsGroup by sorting all the timestamps and assigning to each the corresponding index. Typically, a TsGroup like this :</p> <pre><code>TsGroup({\n    0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n    1 : Tsd(t=[1, 5], d=[5, 6])\n})\n</code></pre> <p>will be saved as npz with the following keys:</p> <pre><code>{\n    't' : [0, 1, 2, 4, 5],\n    'd' : [1, 5, 2, 3, 5],\n    'index' : [0, 1, 0, 0, 1],\n    'start' : [0],\n    'end' : [5],\n    'type' : 'TsGroup'\n}\n</code></pre> <p>Metadata are saved by columns with the column name as the npz key. To avoid potential conflicts, make sure the columns name of the metadata are different from ['t', 'd', 'start', 'end', 'index']</p> <p>You can load the object with numpy.load. Default keys are 't', 'd'(optional), 'start', 'end', 'index' and 'type'. See the example below.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The filename</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; tsgroup = nap.TsGroup({\n    0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n    6 : nap.Ts(t=np.array([1.0, 5.0]))\n    },\n    group = np.array([0, 1]),\n    location = np.array(['right foot', 'left foot'])\n    )\n&gt;&gt;&gt; tsgroup\n  Index    rate    group  location\n-------  ------  -------  ----------\n      0     0.6        0  right foot\n      6     0.4        1  left foot\n&gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n</code></pre> <p>Here I can retrieve my data with numpy directly:</p> <pre><code>&gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n&gt;&gt;&gt; print(list(file.keys()))\n['rate', 'group', 'location', 't', 'index', 'start', 'end', 'type']\n&gt;&gt;&gt; print(file['index'])\n[0 6 0 0 6]\n</code></pre> <p>In the case where TsGroup is a set of Ts objects, it is very direct to recreate the TsGroup by using the function to_tsgroup :</p> <pre><code>&gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n&gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n&gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n&gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n&gt;&gt;&gt; tsgroup\n  Index    rate    group  location\n-------  ------  -------  ----------\n      0     0.6        0  right foot\n      6     0.4        1  left foot\n</code></pre> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If filename is not str, path does not exist or filename is a directory.</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def save(self, filename):\n\"\"\"\n    Save TsGroup object in npz format. The file will contain the timestamps,\n    the data (if group of Tsd), group index, the time support and the metadata\n    The main purpose of this function is to save small/medium sized TsGroup\n    objects.\n    The function will \"flatten\" the TsGroup by sorting all the timestamps\n    and assigning to each the corresponding index. Typically, a TsGroup like\n    this :\n        TsGroup({\n            0 : Tsd(t=[0, 2, 4], d=[1, 2, 3])\n            1 : Tsd(t=[1, 5], d=[5, 6])\n        })\n    will be saved as npz with the following keys:\n        {\n            't' : [0, 1, 2, 4, 5],\n            'd' : [1, 5, 2, 3, 5],\n            'index' : [0, 1, 0, 0, 1],\n            'start' : [0],\n            'end' : [5],\n            'type' : 'TsGroup'\n        }\n    Metadata are saved by columns with the column name as the npz key. To avoid\n    potential conflicts, make sure the columns name of the metadata are different\n    from ['t', 'd', 'start', 'end', 'index']\n    You can load the object with numpy.load. Default keys are 't', 'd'(optional),\n    'start', 'end', 'index' and 'type'.\n    See the example below.\n    Parameters\n    ----------\n    filename : str\n        The filename\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; import numpy as np\n    &gt;&gt;&gt; tsgroup = nap.TsGroup({\n        0 : nap.Ts(t=np.array([0.0, 2.0, 4.0])),\n        6 : nap.Ts(t=np.array([1.0, 5.0]))\n        },\n        group = np.array([0, 1]),\n        location = np.array(['right foot', 'left foot'])\n        )\n    &gt;&gt;&gt; tsgroup\n      Index    rate    group  location\n    -------  ------  -------  ----------\n          0     0.6        0  right foot\n          6     0.4        1  left foot\n    &gt;&gt;&gt; tsgroup.save(\"my_tsgroup.npz\")\n    Here I can retrieve my data with numpy directly:\n    &gt;&gt;&gt; file = np.load(\"my_tsgroup.npz\")\n    &gt;&gt;&gt; print(list(file.keys()))\n    ['rate', 'group', 'location', 't', 'index', 'start', 'end', 'type']\n    &gt;&gt;&gt; print(file['index'])\n    [0 6 0 0 6]\n    In the case where TsGroup is a set of Ts objects, it is very direct to\n    recreate the TsGroup by using the function to_tsgroup :\n    &gt;&gt;&gt; time_support = nap.IntervalSet(file['start'], file['end'])\n    &gt;&gt;&gt; tsd = nap.Tsd(t=file['t'], d=file['index'], time_support = time_support)\n    &gt;&gt;&gt; tsgroup = tsd.to_tsgroup()\n    &gt;&gt;&gt; tsgroup.set_info(group = file['group'], location = file['location'])\n    &gt;&gt;&gt; tsgroup\n      Index    rate    group  location\n    -------  ------  -------  ----------\n          0     0.6        0  right foot\n          6     0.4        1  left foot\n    Raises\n    ------\n    RuntimeError\n        If filename is not str, path does not exist or filename is a directory.\n    \"\"\"\nif not isinstance(filename, str):\nraise RuntimeError(\"Invalid type; please provide filename as string\")\nif os.path.isdir(filename):\nraise RuntimeError(\n\"Invalid filename input. {} is directory.\".format(filename)\n)\nif not filename.lower().endswith(\".npz\"):\nfilename = filename + \".npz\"\ndirname = os.path.dirname(filename)\nif len(dirname) and not os.path.exists(dirname):\nraise RuntimeError(\n\"Path {} does not exist.\".format(os.path.dirname(filename))\n)\ndicttosave = {\"type\": np.array([\"TsGroup\"], dtype=np.str_)}\nfor k in self._metadata.columns:\nif k not in [\"t\", \"d\", \"start\", \"end\", \"index\"]:\ntmp = self._metadata[k].values\nif tmp.dtype == np.dtype(\"O\"):\ntmp = tmp.astype(np.str_)\ndicttosave[k] = tmp\n# We can't use to_tsd here in case tsgroup contains Tsd and not only Ts.\nnt = 0\nfor n in self.index:\nnt += self[n].shape[0]\ntimes = np.zeros(nt)\ndata = np.zeros(nt)\nindex = np.zeros(nt, dtype=np.int64)\nk = 0\nfor n in self.index:\nkl = self[n].shape[0]\ntimes[k : k + kl] = self[n].index.values\ndata[k : k + kl] = self[n].values\nindex[k : k + kl] = int(n)\nk += kl\nidx = np.argsort(times)\ntimes = times[idx]\nindex = index[idx]\ndicttosave[\"t\"] = times\ndicttosave[\"index\"] = index\nif not np.all(np.isnan(data)):\ndicttosave[\"d\"] = data[idx]\ndicttosave[\"start\"] = self.time_support.start.values\ndicttosave[\"end\"] = self.time_support.end.values\nnp.savez(filename, **dicttosave)\nreturn\n</code></pre>"},{"location":"reference/core/ts_group/#pynapple.core.ts_group.union_intervals","title":"<code>union_intervals(i_sets)</code>","text":"<p>Helper to merge intervals from ts_group</p> Source code in <code>pynapple/core/ts_group.py</code> <pre><code>def union_intervals(i_sets):\n\"\"\"\n    Helper to merge intervals from ts_group\n    \"\"\"\nn = len(i_sets)\nif n == 1:\nreturn i_sets[0]\nnew_start = np.zeros(0)\nnew_end = np.zeros(0)\nif n == 2:\nnew_start, new_end = jitunion(\ni_sets[0].start.values,\ni_sets[0].end.values,\ni_sets[1].start.values,\ni_sets[1].end.values,\n)\nif n &gt; 2:\nsizes = np.array([i_sets[i].shape[0] for i in range(n)])\nstartends = np.zeros((np.sum(sizes), 2))\nct = 0\nfor i in range(sizes.shape[0]):\nstartends[ct : ct + sizes[i], :] = i_sets[i].values\nct += sizes[i]\nnew_start, new_end = jitunion_isets(startends[:, 0], startends[:, 1])\nreturn IntervalSet(new_start, new_end)\n</code></pre>"},{"location":"reference/io/","title":"Io","text":""},{"location":"reference/io/cnmfe/","title":"Cnmfe","text":"<p>Loaders for calcium imaging data with miniscope. Support CNMF-E in matlab, inscopix-cnmfe and minian.</p>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E","title":"<code>CNMF_E</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E). The path folder should contain a file ending in .mat when calling Source2d.save_neurons</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>numpy.ndarray</code> <p>Spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class CNMF_E(BaseLoader):\n\"\"\"Loader for data processed with matlab CNMF-E(https://github.com/zhoupc/CNMF_E).\n    The path folder should contain a file ending in .mat\n    when calling Source2d.save_neurons\n    Attributes\n    ----------\n    A : numpy.ndarray\n        Spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_cnmf_e(path)\nself.save_cnmfe_nwb(path)\ndef load_cnmf_e(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nfiles = os.listdir(path)\nmatfiles = [f for f in files if f.endswith(\".mat\")]\nif len(matfiles):\ndata = loadmat(os.path.join(path, matfiles[0]), struct_as_record=False)\nelse:\nraise RuntimeError(\"No mat file found in {}\".format(path))\nself.struct = data[\"neuron_results\"][0][0]\nC = self.struct.C.T\nself.A = self.struct.A.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = np.atleast_2d(self.A[i])\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_cnmf_e(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmf_e","title":"<code>load_cnmf_e(path)</code>","text":"<p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmf_e(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nfiles = os.listdir(path)\nmatfiles = [f for f in files if f.endswith(\".mat\")]\nif len(matfiles):\ndata = loadmat(os.path.join(path, matfiles[0]), struct_as_record=False)\nelse:\nraise RuntimeError(\"No mat file found in {}\".format(path))\nself.struct = data[\"neuron_results\"][0][0]\nC = self.struct.C.T\nself.A = self.struct.A.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nreturn None\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.save_cnmfe_nwb","title":"<code>save_cnmfe_nwb(path)</code>","text":"<p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = np.atleast_2d(self.A[i])\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.CNMF_E.load_cnmfe_nwb","title":"<code>load_cnmfe_nwb(path)</code>","text":"<p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian","title":"<code>Minian</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for data processed with Minian (https://github.com/denisecailab/minian). The path folder should contain a subfolder name minian.</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>numpy.ndarray</code> <p>Spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class Minian(BaseLoader):\n\"\"\"Loader for data processed with Minian (https://github.com/denisecailab/minian).\n    The path folder should contain a subfolder name minian.\n    Attributes\n    ----------\n    A : numpy.ndarray\n        Spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_minian(path)\nself.save_cnmfe_nwb(path)\ndef load_minian(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nminian_folder = os.path.join(path, \"minian\")\nif not os.path.exists(minian_folder):\nraise RuntimeError(\"Path {} does not contain a minian folder\".format(path))\ntry:\nimport zarr\nexcept ImportError as ie:\nprint(\"Please install module zarr for loading minian data\", ie)\nsys.exit()\ndata = zarr.open(minian_folder, \"r\")\nC = data[\"C.zarr\"][\"C\"][:]\nC = C.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nself.A = data[\"A.zarr\"][\"A\"][:]\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_minian(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.load_minian","title":"<code>load_minian(path)</code>","text":"<p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_minian(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nminian_folder = os.path.join(path, \"minian\")\nif not os.path.exists(minian_folder):\nraise RuntimeError(\"Path {} does not contain a minian folder\".format(path))\ntry:\nimport zarr\nexcept ImportError as ie:\nprint(\"Please install module zarr for loading minian data\", ie)\nsys.exit()\ndata = zarr.open(minian_folder, \"r\")\nC = data[\"C.zarr\"][\"C\"][:]\nC = C.T\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C)\nself.A = data[\"A.zarr\"][\"A\"][:]\nreturn None\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.save_cnmfe_nwb","title":"<code>save_cnmfe_nwb(path)</code>","text":"<p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.Minian.load_cnmfe_nwb","title":"<code>load_cnmfe_nwb(path)</code>","text":"<p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE","title":"<code>InscopixCNMFE</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe). The folder should contain a file ending with '_traces.csv' and a tiff file for spatial footprints.</p> <p>Attributes:</p> Name Type Description <code>A</code> <code>np.ndarray</code> <p>The spatial footprints</p> <code>C</code> <code>TsdFrame</code> <p>The calcium transients</p> <code>sampling_rate</code> <code>float</code> <p>Sampling rate of the data (default is 30 Hz).</p> Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>class InscopixCNMFE(BaseLoader):\n\"\"\"Loader for Inscopix-cnmfe (https://github.com/inscopix/inscopix-cnmfe).\n    The folder should contain a file ending with '_traces.csv'\n    and a tiff file for spatial footprints.\n    Attributes\n    ----------\n    A : np.ndarray\n        The spatial footprints\n    C : TsdFrame\n        The calcium transients\n    sampling_rate : float\n        Sampling rate of the data (default is 30 Hz).\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_inscopix_cnmfe(path)\nself.save_cnmfe_nwb(path)\ndef load_inscopix_cnmfe(self, path):\n\"\"\"\n        Load the calcium transients and the spatial footprints.\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nfiles = os.listdir(path)\ntracefile = [f for f in files if f.endswith(\"_traces.csv\")]\nif len(tracefile):\nC = pd.read_csv(os.path.join(path, tracefile[0]), index_col=0)\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*_traces.csv\")\n)\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C.values)\ntry:\nimport tifffile as tiff\nexcept ImportError as ie:\nprint(\"Please install module tifffile for loading inscopix-cnmfe data\", ie)\nsys.exit()\ntifffile = [f for f in files if f.endswith(\".tiff\")]\nif len(tifffile):\nself.A = tiff.imread(os.path.join(path, tifffile[0]))\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*.tiff\")\n)\nreturn None\ndef save_cnmfe_nwb(self, path):\n\"\"\"\n        Save the data to NWB.\n        Since there is no one-photon field in nwb, it uses the two-photon field.\n        Parameters\n        ----------\n        path : TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_cnmfe_nwb(self, path):\n\"\"\"\n        Load the calcium transient and spatial footprint from nwb\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_cnmfe_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_inscopix_cnmfe(path)\nself.save_cnmfe_nwb(path)\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_inscopix_cnmfe","title":"<code>load_inscopix_cnmfe(path)</code>","text":"<p>Load the calcium transients and the spatial footprints.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_inscopix_cnmfe(self, path):\n\"\"\"\n    Load the calcium transients and the spatial footprints.\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nfiles = os.listdir(path)\ntracefile = [f for f in files if f.endswith(\"_traces.csv\")]\nif len(tracefile):\nC = pd.read_csv(os.path.join(path, tracefile[0]), index_col=0)\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*_traces.csv\")\n)\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ntime_index = np.arange(0, len(C)) / self.sampling_rate\nself.C = nap.TsdFrame(t=time_index, d=C.values)\ntry:\nimport tifffile as tiff\nexcept ImportError as ie:\nprint(\"Please install module tifffile for loading inscopix-cnmfe data\", ie)\nsys.exit()\ntifffile = [f for f in files if f.endswith(\".tiff\")]\nif len(tifffile):\nself.A = tiff.imread(os.path.join(path, tifffile[0]))\nelse:\nraise RuntimeError(\n\"Path {} does not contain the file {}\".format(path, \"*.tiff\")\n)\nreturn None\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.save_cnmfe_nwb","title":"<code>save_cnmfe_nwb(path)</code>","text":"<p>Save the data to NWB. Since there is no one-photon field in nwb, it uses the two-photon field.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>TYPE</code> <p>Description</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def save_cnmfe_nwb(self, path):\n\"\"\"\n    Save the data to NWB.\n    Since there is no one-photon field in nwb, it uses the two-photon field.\n    Parameters\n    ----------\n    path : TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice_info = self.ophys_information[\"device\"]\ndevice = nwbfile.create_device(\nname=device_info[\"name\"],\ndescription=device_info[\"description\"],\nmanufacturer=device_info[\"manufacturer\"],\n)\noptical_info = self.ophys_information[\"OpticalChannel\"]\noptical_info[\"emission_lambda\"] = float(optical_info[\"emission_lambda\"])\noptical_channel = OpticalChannel(\nname=optical_info[\"name\"],\ndescription=optical_info[\"description\"],\nemission_lambda=optical_info[\"emission_lambda\"],\n)\nimaging_info = self.ophys_information[\"ImagingPlane\"]\nimaging_info[\"excitation_lambda\"] = float(imaging_info[\"excitation_lambda\"])\nimaging_plane = nwbfile.create_imaging_plane(\nname=imaging_info[\"name\"],\noptical_channel=optical_channel,\nimaging_rate=self.sampling_rate,\ndescription=imaging_info[\"description\"],\ndevice=device,\nexcitation_lambda=imaging_info[\"excitation_lambda\"],\nindicator=imaging_info[\"indicator\"],\nlocation=imaging_info[\"location\"],\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nseg_info = self.ophys_information[\"PlaneSegmentation\"]\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=seg_info[\"name\"],\ndescription=seg_info[\"description\"],\nimaging_plane=imaging_plane,\n)\nfor i in range(self.C.shape[1]):\nimage_mask = self.A[i]\n# add image mask to plane segmentation\nps.add_roi(image_mask=image_mask)\nophys_module.add(img_seg)\nrt_region = ps.create_roi_table_region(\nregion=list(np.arange(self.C.shape[1])), description=\"ROIs\"\n)\nroi_resp_series = RoiResponseSeries(\nname=\"RoiResponseSeries\",\ndata=self.C.values,\nrois=rt_region,\nunit=\"lumens\",\ntimestamps=self.C.index.values,\n)\nfl = Fluorescence(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/cnmfe/#pynapple.io.cnmfe.InscopixCNMFE.load_cnmfe_nwb","title":"<code>load_cnmfe_nwb(path)</code>","text":"<p>Load the calcium transient and spatial footprint from nwb</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/cnmfe.py</code> <pre><code>def load_cnmfe_nwb(self, path):\n\"\"\"\n    Load the calcium transient and spatial footprint from nwb\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\ndata = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].data[:]\nt = nwbfile.processing[\"ophys\"][\"Fluorescence\"][\n\"RoiResponseSeries\"\n].timestamps[:]\nself.C = nap.TsdFrame(t=t, d=data)\nself.A = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"image_mask\"].data[:]\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/ephys_gui/","title":"Ephys gui","text":"<p>Summary</p>"},{"location":"reference/io/folder/","title":"Folder","text":"<p>The Folder class helps to navigate a hierarchical data tree.</p>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder","title":"<code>Folder</code>","text":"<p>         Bases: <code>UserDict</code></p> <p>Base class for all type of folders (i.e. Project, Subject, Sessions, ...). Handles files and sub-folders discovery</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>dict</code> <p>Dictionnary holidng all the pynapple objects found in the folder.</p> <code>name</code> <code>str</code> <p>Name of the folder</p> <code>npz_files</code> <code>list</code> <p>List of npz files found in the folder</p> <code>nwb_files</code> <code>list</code> <p>List of nwb files found in the folder</p> <code>path</code> <code>str</code> <p>Absolute path of the folder</p> <code>subfolds</code> <code>dict</code> <p>Dictionnary of all the subfolders</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>class Folder(UserDict):\n\"\"\"\n    Base class for all type of folders (i.e. Project, Subject, Sessions, ...).\n    Handles files and sub-folders discovery\n    Attributes\n    ----------\n    data : dict\n        Dictionnary holidng all the pynapple objects found in the folder.\n    name : str\n        Name of the folder\n    npz_files : list\n        List of npz files found in the folder\n    nwb_files : list\n        List of nwb files found in the folder\n    path : str\n        Absolute path of the folder\n    subfolds : dict\n        Dictionnary of all the subfolders\n    \"\"\"\ndef __init__(self, path):  # , exclude=(), max_depth=4):\n\"\"\"Initialize the Folder object\n        Parameters\n        ----------\n        path : str\n            Path to the folder\n        \"\"\"\npath = path.rstrip(\"/\")\nself.path = path\nself.name = os.path.basename(path)\nself._basic_view = Tree(\n\":open_file_folder: {}\".format(self.name), guide_style=\"blue\"\n)\nself._full_view = None\n# Search sub-folders\nsubfolds = [\nf.path\nfor f in os.scandir(path)\nif f.is_dir() and not f.name.startswith(\".\")\n]\nsubfolds.sort()\nself.subfolds = {}\nfor s in subfolds:\nsub = os.path.basename(s)\nself.subfolds[sub] = Folder(s)\nself._basic_view.add(\":open_file_folder: [blue]\" + sub)\n# Search files\nself.npz_files = _find_files(path, \"npz\")\nself.nwb_files = _find_files(path, \"nwb\")\nfor filename, file in self.npz_files.items():\nself._basic_view.add(\"[green]\" + file.name + \" \\t|\\t \" + file.type)\nfor file in self.nwb_files.values():\nself._basic_view.add(\"[magenta]\" + file.name + \" \\t|\\t NWB file\")\n# Putting everything together\nself.data = {**self.npz_files, **self.nwb_files, **self.subfolds}\nUserDict.__init__(self, self.data)\ndef __str__(self):\n\"\"\"View of the object\"\"\"\nwith Console() as console:\nconsole.print(self._basic_view)\nreturn \"\"\n# def __repr__(self):\n#     \"\"\"View of the object\"\"\"\n#     print(self._basic_view)\ndef __getitem__(self, key):\n\"\"\"Get subfolder or load file.\n        Parameters\n        ----------\n        key : str\n        Returns\n        -------\n        (Ts, Tsd, TsdFrame, TsGroup, IntervalSet, Folder or NWBFile)\n        Raises\n        ------\n        KeyError\n            If key is not in the dictionnary\n        \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], NPZFile):\ndata = self.data[key].load()\nself.data[key] = data\n# setattr(self, key, data)\nreturn data\nelif isinstance(self.data[key], NWBFile):\nreturn self.data[key]\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n# # # Gets called when an attribute is accessed\n# def __getattribute__(self, item):\n#     value = super(Folder, self).__getattribute__(item)\n#     if isinstance(value, NPZFile):\n#         data = value.load()\n#         setattr(self, item, data)\n#         self.data[item] = data\n#         return data\n#     else:\n#         return value\ndef _generate_tree_view(self):\ntree = Tree(\":open_file_folder: {}\".format(self.name), guide_style=\"blue\")\n# Folder\nfor fold in self.subfolds.keys():\ntree.add(\":open_file_folder: \" + fold)\n_walk_folder(tree.children[-1], self.subfolds[fold])\n# NPZ files\nfor file in self.npz_files.values():\ntree.add(\"[green]\" + file.name + \" \\t|\\t \" + file.type)\n# NWB files\nfor file in self.nwb_files.values():\ntree.add(\"[magenta]\" + file.name + \" \\t|\\t NWB file\")\nself._full_view = tree\ndef expand(self):\n\"\"\"Display the full tree view. Equivalent to Folder.view\"\"\"\nif not isinstance(self._full_view, Tree):\nself._generate_tree_view()\nwith Console() as console:\nconsole.print(self._full_view)\nreturn None\n@property\ndef view(self):\n\"\"\"Summary\"\"\"\nreturn self.expand()\ndef save(self, name, obj, description=\"\"):\n\"\"\"Save a pynapple object in the folder in a single file in uncompressed ``.npz`` format.\n        By default, the save function overwrite previously save file with the same name.\n        Parameters\n        ----------\n        name : str\n            Filename\n        obj : Ts, Tsd, TsdFrame, TsGroup or IntervalSet\n            Pynapple object.\n        description : str, optional\n            Metainformation added as a json sidecar.\n        \"\"\"\nfilepath = os.path.join(self.path, name)\nobj.save(filepath)\nself.npz_files[name] = NPZFile(filepath + \".npz\")\nself.data[name] = obj\nmetadata = {\"time\": str(datetime.now()), \"info\": str(description)}\nwith open(os.path.join(self.path, name + \".json\"), \"w\") as ff:\njson.dump(metadata, ff, indent=2)\n# regenerate the tree view\nself._generate_tree_view()\ndef load(self):\n\"\"\"Load all compatible NPZ files.\"\"\"\nfor k in self.npz_files.keys():\nself[k] = self.npz_files[k].load()\n# def add_metadata(self):\n#     \"\"\"Summary\"\"\"\n#     pass\ndef info(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n        Parameters\n        ----------\n        name : str\n            Name of the npz file\n        \"\"\"\nself.metadata(name)\ndef doc(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n        Parameters\n        ----------\n        name : str\n            Name of the npz file\n        \"\"\"\nself.metadata(name)\ndef metadata(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n        Parameters\n        ----------\n        name : str\n            Name of the npz file\n        \"\"\"\n# Search for json first\njson_filename = os.path.join(self.path, name + \".json\")\nif os.path.isfile(json_filename):\nwith open(json_filename, \"r\") as ff:\nmetadata = json.load(ff)\ntext = \"\\n\".join([\" : \".join(it) for it in metadata.items()])\npanel = Panel.fit(\ntext, border_style=\"green\", title=os.path.join(self.path, name + \".npz\")\n)\nelse:\npanel = Panel.fit(\n\"No metadata\",\nborder_style=\"red\",\ntitle=os.path.join(self.path, name + \".npz\"),\n)\nwith Console() as console:\nconsole.print(panel)\nreturn None\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.view","title":"<code>view</code>  <code>property</code>","text":"<p>Summary</p>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.__init__","title":"<code>__init__(path)</code>","text":"<p>Initialize the Folder object</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the folder</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def __init__(self, path):  # , exclude=(), max_depth=4):\n\"\"\"Initialize the Folder object\n    Parameters\n    ----------\n    path : str\n        Path to the folder\n    \"\"\"\npath = path.rstrip(\"/\")\nself.path = path\nself.name = os.path.basename(path)\nself._basic_view = Tree(\n\":open_file_folder: {}\".format(self.name), guide_style=\"blue\"\n)\nself._full_view = None\n# Search sub-folders\nsubfolds = [\nf.path\nfor f in os.scandir(path)\nif f.is_dir() and not f.name.startswith(\".\")\n]\nsubfolds.sort()\nself.subfolds = {}\nfor s in subfolds:\nsub = os.path.basename(s)\nself.subfolds[sub] = Folder(s)\nself._basic_view.add(\":open_file_folder: [blue]\" + sub)\n# Search files\nself.npz_files = _find_files(path, \"npz\")\nself.nwb_files = _find_files(path, \"nwb\")\nfor filename, file in self.npz_files.items():\nself._basic_view.add(\"[green]\" + file.name + \" \\t|\\t \" + file.type)\nfor file in self.nwb_files.values():\nself._basic_view.add(\"[magenta]\" + file.name + \" \\t|\\t NWB file\")\n# Putting everything together\nself.data = {**self.npz_files, **self.nwb_files, **self.subfolds}\nUserDict.__init__(self, self.data)\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.__str__","title":"<code>__str__()</code>","text":"<p>View of the object</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def __str__(self):\n\"\"\"View of the object\"\"\"\nwith Console() as console:\nconsole.print(self._basic_view)\nreturn \"\"\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get subfolder or load file.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required <p>Returns:</p> Type Description <code>(Ts, Tsd, TsdFrame, TsGroup, IntervalSet, Folder or NWBFile)</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If key is not in the dictionnary</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def __getitem__(self, key):\n\"\"\"Get subfolder or load file.\n    Parameters\n    ----------\n    key : str\n    Returns\n    -------\n    (Ts, Tsd, TsdFrame, TsGroup, IntervalSet, Folder or NWBFile)\n    Raises\n    ------\n    KeyError\n        If key is not in the dictionnary\n    \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], NPZFile):\ndata = self.data[key].load()\nself.data[key] = data\n# setattr(self, key, data)\nreturn data\nelif isinstance(self.data[key], NWBFile):\nreturn self.data[key]\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.expand","title":"<code>expand()</code>","text":"<p>Display the full tree view. Equivalent to Folder.view</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def expand(self):\n\"\"\"Display the full tree view. Equivalent to Folder.view\"\"\"\nif not isinstance(self._full_view, Tree):\nself._generate_tree_view()\nwith Console() as console:\nconsole.print(self._full_view)\nreturn None\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.save","title":"<code>save(name, obj, description='')</code>","text":"<p>Save a pynapple object in the folder in a single file in uncompressed <code>.npz</code> format. By default, the save function overwrite previously save file with the same name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Filename</p> required <code>obj</code> <code>Ts, Tsd, TsdFrame, TsGroup or IntervalSet</code> <p>Pynapple object.</p> required <code>description</code> <code>str, optional</code> <p>Metainformation added as a json sidecar.</p> <code>''</code> Source code in <code>pynapple/io/folder.py</code> <pre><code>def save(self, name, obj, description=\"\"):\n\"\"\"Save a pynapple object in the folder in a single file in uncompressed ``.npz`` format.\n    By default, the save function overwrite previously save file with the same name.\n    Parameters\n    ----------\n    name : str\n        Filename\n    obj : Ts, Tsd, TsdFrame, TsGroup or IntervalSet\n        Pynapple object.\n    description : str, optional\n        Metainformation added as a json sidecar.\n    \"\"\"\nfilepath = os.path.join(self.path, name)\nobj.save(filepath)\nself.npz_files[name] = NPZFile(filepath + \".npz\")\nself.data[name] = obj\nmetadata = {\"time\": str(datetime.now()), \"info\": str(description)}\nwith open(os.path.join(self.path, name + \".json\"), \"w\") as ff:\njson.dump(metadata, ff, indent=2)\n# regenerate the tree view\nself._generate_tree_view()\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.load","title":"<code>load()</code>","text":"<p>Load all compatible NPZ files.</p> Source code in <code>pynapple/io/folder.py</code> <pre><code>def load(self):\n\"\"\"Load all compatible NPZ files.\"\"\"\nfor k in self.npz_files.keys():\nself[k] = self.npz_files[k].load()\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.info","title":"<code>info(name)</code>","text":"<p>Display the metadata within the json sidecar of a NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the npz file</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def info(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n    Parameters\n    ----------\n    name : str\n        Name of the npz file\n    \"\"\"\nself.metadata(name)\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.doc","title":"<code>doc(name)</code>","text":"<p>Display the metadata within the json sidecar of a NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the npz file</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def doc(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n    Parameters\n    ----------\n    name : str\n        Name of the npz file\n    \"\"\"\nself.metadata(name)\n</code></pre>"},{"location":"reference/io/folder/#pynapple.io.folder.Folder.metadata","title":"<code>metadata(name)</code>","text":"<p>Display the metadata within the json sidecar of a NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the npz file</p> required Source code in <code>pynapple/io/folder.py</code> <pre><code>def metadata(self, name):\n\"\"\"Display the metadata within the json sidecar of a NPZ file\n    Parameters\n    ----------\n    name : str\n        Name of the npz file\n    \"\"\"\n# Search for json first\njson_filename = os.path.join(self.path, name + \".json\")\nif os.path.isfile(json_filename):\nwith open(json_filename, \"r\") as ff:\nmetadata = json.load(ff)\ntext = \"\\n\".join([\" : \".join(it) for it in metadata.items()])\npanel = Panel.fit(\ntext, border_style=\"green\", title=os.path.join(self.path, name + \".npz\")\n)\nelse:\npanel = Panel.fit(\n\"No metadata\",\nborder_style=\"red\",\ntitle=os.path.join(self.path, name + \".npz\"),\n)\nwith Console() as console:\nconsole.print(panel)\nreturn None\n</code></pre>"},{"location":"reference/io/interface_npz/","title":"Interface npz","text":"<p>File classes help to validate and load pynapple objects or NWB files. Data are always lazy-loaded. Both classes behaves like dictionnary.</p>"},{"location":"reference/io/interface_npz/#pynapple.io.interface_npz.NPZFile","title":"<code>NPZFile</code>","text":"<p>         Bases: <code>object</code></p> <p>Class that points to a NPZ file that can be loaded as a pynapple object. Objects have a save function in npz format as well as the Folder class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; tsd = nap.load_file(\"path/to/my_tsd.npz\")\n&gt;&gt;&gt; tsd\nTime (s)\n0.0    0\n0.1    1\n0.2    2\ndtype: int64\n</code></pre> Source code in <code>pynapple/io/interface_npz.py</code> <pre><code>class NPZFile(object):\n\"\"\"Class that points to a NPZ file that can be loaded as a pynapple object.\n    Objects have a save function in npz format as well as the Folder class.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; tsd = nap.load_file(\"path/to/my_tsd.npz\")\n    &gt;&gt;&gt; tsd\n    Time (s)\n    0.0    0\n    0.1    1\n    0.2    2\n    dtype: int64\n    \"\"\"\ndef __init__(self, path):\n\"\"\"Initialization of the NPZ file\n        Parameters\n        ----------\n        path : str\n            Valid path to a NPZ file\n        \"\"\"\nself.path = path\nself.name = os.path.basename(path)\nself.file = np.load(self.path, allow_pickle=True)\nself.type = \"\"\n# First check if type is explicitely defined\npossible = [\"Ts\", \"Tsd\", \"TsdFrame\", \"TsGroup\", \"IntervalSet\"]\nif \"type\" in self.file.keys():\nif len(self.file[\"type\"]) == 1:\nif isinstance(self.file[\"type\"][0], np.str_):\nif self.file[\"type\"] in possible:\nself.type = self.file[\"type\"][0]\n# Second check manually\nif self.type == \"\":\nk = set(self.file.keys())\nif {\"t\", \"start\", \"end\", \"index\"}.issubset(k):\nself.type = \"TsGroup\"\nelif {\"t\", \"d\", \"start\", \"end\", \"columns\"}.issubset(k):\nself.type = \"TsdFrame\"\nelif {\"t\", \"d\", \"start\", \"end\"}.issubset(k):\nself.type = \"Tsd\"\nelif {\"t\", \"start\", \"end\"}.issubset(k):\nself.type = \"Ts\"\nelif {\"start\", \"end\"}.issubset(k):\nself.type = \"IntervalSet\"\nelse:\nself.type = \"npz\"\ndef load(self):\n\"\"\"Load the NPZ file\n        Returns\n        -------\n        (Tsd, Ts, TsdFrame, TsGroup, IntervalSet)\n            A pynapple object\n        \"\"\"\nif self.type == \"npz\":\nreturn self.file\nelse:\ntime_support = nap.IntervalSet(self.file[\"start\"], self.file[\"end\"])\nif self.type == \"TsGroup\":\ntsd = nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"index\"], time_support=time_support\n)\ntsgroup = tsd.to_tsgroup()\nif \"d\" in self.file.keys():\nprint(\"TODO\")\nmetainfo = {}\nfor k in set(self.file.keys()) - {\n\"start\",\n\"end\",\n\"t\",\n\"index\",\n\"d\",\n\"rate\",\n}:\ntmp = self.file[k]\nif len(tmp) == len(tsgroup):\nmetainfo[k] = tmp\ntsgroup.set_info(**metainfo)\nreturn tsgroup\nelif self.type == \"TsdFrame\":\nreturn nap.TsdFrame(\nt=self.file[\"t\"],\nd=self.file[\"d\"],\ntime_support=time_support,\ncolumns=self.file[\"columns\"],\n)\nelif self.type == \"Tsd\":\nreturn nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"d\"], time_support=time_support\n)\nelif self.type == \"Ts\":\nreturn nap.Ts(t=self.file[\"t\"], time_support=time_support)\nelif self.type == \"IntervalSet\":\nreturn time_support\nelse:\nreturn self.file\n</code></pre>"},{"location":"reference/io/interface_npz/#pynapple.io.interface_npz.NPZFile.__init__","title":"<code>__init__(path)</code>","text":"<p>Initialization of the NPZ file</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Valid path to a NPZ file</p> required Source code in <code>pynapple/io/interface_npz.py</code> <pre><code>def __init__(self, path):\n\"\"\"Initialization of the NPZ file\n    Parameters\n    ----------\n    path : str\n        Valid path to a NPZ file\n    \"\"\"\nself.path = path\nself.name = os.path.basename(path)\nself.file = np.load(self.path, allow_pickle=True)\nself.type = \"\"\n# First check if type is explicitely defined\npossible = [\"Ts\", \"Tsd\", \"TsdFrame\", \"TsGroup\", \"IntervalSet\"]\nif \"type\" in self.file.keys():\nif len(self.file[\"type\"]) == 1:\nif isinstance(self.file[\"type\"][0], np.str_):\nif self.file[\"type\"] in possible:\nself.type = self.file[\"type\"][0]\n# Second check manually\nif self.type == \"\":\nk = set(self.file.keys())\nif {\"t\", \"start\", \"end\", \"index\"}.issubset(k):\nself.type = \"TsGroup\"\nelif {\"t\", \"d\", \"start\", \"end\", \"columns\"}.issubset(k):\nself.type = \"TsdFrame\"\nelif {\"t\", \"d\", \"start\", \"end\"}.issubset(k):\nself.type = \"Tsd\"\nelif {\"t\", \"start\", \"end\"}.issubset(k):\nself.type = \"Ts\"\nelif {\"start\", \"end\"}.issubset(k):\nself.type = \"IntervalSet\"\nelse:\nself.type = \"npz\"\n</code></pre>"},{"location":"reference/io/interface_npz/#pynapple.io.interface_npz.NPZFile.load","title":"<code>load()</code>","text":"<p>Load the NPZ file</p> <p>Returns:</p> Type Description <code>Tsd, Ts, TsdFrame, TsGroup, IntervalSet</code> <p>A pynapple object</p> Source code in <code>pynapple/io/interface_npz.py</code> <pre><code>def load(self):\n\"\"\"Load the NPZ file\n    Returns\n    -------\n    (Tsd, Ts, TsdFrame, TsGroup, IntervalSet)\n        A pynapple object\n    \"\"\"\nif self.type == \"npz\":\nreturn self.file\nelse:\ntime_support = nap.IntervalSet(self.file[\"start\"], self.file[\"end\"])\nif self.type == \"TsGroup\":\ntsd = nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"index\"], time_support=time_support\n)\ntsgroup = tsd.to_tsgroup()\nif \"d\" in self.file.keys():\nprint(\"TODO\")\nmetainfo = {}\nfor k in set(self.file.keys()) - {\n\"start\",\n\"end\",\n\"t\",\n\"index\",\n\"d\",\n\"rate\",\n}:\ntmp = self.file[k]\nif len(tmp) == len(tsgroup):\nmetainfo[k] = tmp\ntsgroup.set_info(**metainfo)\nreturn tsgroup\nelif self.type == \"TsdFrame\":\nreturn nap.TsdFrame(\nt=self.file[\"t\"],\nd=self.file[\"d\"],\ntime_support=time_support,\ncolumns=self.file[\"columns\"],\n)\nelif self.type == \"Tsd\":\nreturn nap.Tsd(\nt=self.file[\"t\"], d=self.file[\"d\"], time_support=time_support\n)\nelif self.type == \"Ts\":\nreturn nap.Ts(t=self.file[\"t\"], time_support=time_support)\nelif self.type == \"IntervalSet\":\nreturn time_support\nelse:\nreturn self.file\n</code></pre>"},{"location":"reference/io/interface_nwb/","title":"Interface nwb","text":"<p>Pynapple class to interface with NWB files. Data are always lazy-loaded. Object behaves like dictionnary.</p>"},{"location":"reference/io/interface_nwb/#pynapple.io.interface_nwb.NWBFile","title":"<code>NWBFile</code>","text":"<p>         Bases: <code>UserDict</code></p> <p>Class for reading NWB Files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pynapple as nap\n&gt;&gt;&gt; data = nap.load_file(\"my_file.nwb\")\n&gt;&gt;&gt; data[\"units\"]\n  Index    rate  location      group\n-------  ------  ----------  -------\n      0    1.0  brain        0\n      1    1.0  brain        0\n      2    1.0  brain        0\n</code></pre> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>class NWBFile(UserDict):\n\"\"\"Class for reading NWB Files.\n    Examples\n    --------\n    &gt;&gt;&gt; import pynapple as nap\n    &gt;&gt;&gt; data = nap.load_file(\"my_file.nwb\")\n    &gt;&gt;&gt; data[\"units\"]\n      Index    rate  location      group\n    -------  ------  ----------  -------\n          0    1.0  brain        0\n          1    1.0  brain        0\n          2    1.0  brain        0\n    \"\"\"\n_f_eval = {\n\"IntervalSet\": _make_interval_set,\n\"Tsd\": _make_tsd,\n\"Ts\": _make_ts,\n\"TsdFrame\": _make_tsd_frame,\n\"TsGroup\": _make_tsgroup,\n}\ndef __init__(self, file):\n\"\"\"\n        Parameters\n        ----------\n        file : str or pynwb.file.NWBFile\n            Valid file to a NWB file\n        Raises\n        ------\n        FileNotFoundError\n            If path is invalid\n        RuntimeError\n            If file is not an instance of NWBFile\n        \"\"\"\nif isinstance(file, str):\nif os.path.exists(file):\nself.path = file\nself.name = os.path.basename(file).split(\".\")[0]\nself.io = NWBHDF5IO(file, \"r\")\nself.nwb = self.io.read()\nelse:\nraise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), file)\nelif isinstance(file, pynwb.file.NWBFile):\nself.nwb = file\nself.name = self.nwb.subject.subject_id\nelse:\nraise RuntimeError(\n\"unrecognized argument. Please provide path to a valid NWB file or open NWB file.\"\n)\nself.data = _extract_compatible_data_from_nwbfile(self.nwb)\nself.key_to_id = {k: self.data[k][\"id\"] for k in self.data.keys()}\nself._view = Table(title=self.name)\nself._view.add_column(\"Keys\", justify=\"left\", style=\"cyan\", no_wrap=True)\nself._view.add_column(\"Type\", style=\"green\")\n# self._view.add_column(\"NWB module\", justify=\"right\", style=\"magenta\")\nfor k in self.data.keys():\nself._view.add_row(\nk,\nself.data[k][\"type\"],\n# self.data[k]['top_module']\n)\nUserDict.__init__(self, self.data)\ndef __str__(self):\n\"\"\"View of the object\"\"\"\nwith Console() as console:\nconsole.print(self._view)\nreturn \"\"\n# def __repr__(self):\n#     \"\"\"View of the object\"\"\"\n#     return \"\"\ndef __getitem__(self, key):\n\"\"\"Get object from NWB\n        Parameters\n        ----------\n        key : str\n        Returns\n        -------\n        (Ts, Tsd, TsdFrame, TsGroup, IntervalSet or dict of IntervalSet)\n        Raises\n        ------\n        KeyError\n            If key is not in the dictionnary\n        \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], dict) and \"id\" in self.data[key]:\nobj = self.nwb.objects[self.data[key][\"id\"]]\ntry:\ndata = self._f_eval[self.data[key][\"type\"]](obj)\nexcept Exception:\nwarnings.warn(\n\"Failed to build {}.\\n Returning the NWB object for manual inspection\".format(\nself.data[key][\"type\"]\n),\nstacklevel=2,\n)\ndata = obj\nself.data[key] = data\nreturn data\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n</code></pre>"},{"location":"reference/io/interface_nwb/#pynapple.io.interface_nwb.NWBFile.__init__","title":"<code>__init__(file)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>file</code> <code>str or pynwb.file.NWBFile</code> <p>Valid file to a NWB file</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If path is invalid</p> <code>RuntimeError</code> <p>If file is not an instance of NWBFile</p> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>def __init__(self, file):\n\"\"\"\n    Parameters\n    ----------\n    file : str or pynwb.file.NWBFile\n        Valid file to a NWB file\n    Raises\n    ------\n    FileNotFoundError\n        If path is invalid\n    RuntimeError\n        If file is not an instance of NWBFile\n    \"\"\"\nif isinstance(file, str):\nif os.path.exists(file):\nself.path = file\nself.name = os.path.basename(file).split(\".\")[0]\nself.io = NWBHDF5IO(file, \"r\")\nself.nwb = self.io.read()\nelse:\nraise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), file)\nelif isinstance(file, pynwb.file.NWBFile):\nself.nwb = file\nself.name = self.nwb.subject.subject_id\nelse:\nraise RuntimeError(\n\"unrecognized argument. Please provide path to a valid NWB file or open NWB file.\"\n)\nself.data = _extract_compatible_data_from_nwbfile(self.nwb)\nself.key_to_id = {k: self.data[k][\"id\"] for k in self.data.keys()}\nself._view = Table(title=self.name)\nself._view.add_column(\"Keys\", justify=\"left\", style=\"cyan\", no_wrap=True)\nself._view.add_column(\"Type\", style=\"green\")\n# self._view.add_column(\"NWB module\", justify=\"right\", style=\"magenta\")\nfor k in self.data.keys():\nself._view.add_row(\nk,\nself.data[k][\"type\"],\n# self.data[k]['top_module']\n)\nUserDict.__init__(self, self.data)\n</code></pre>"},{"location":"reference/io/interface_nwb/#pynapple.io.interface_nwb.NWBFile.__str__","title":"<code>__str__()</code>","text":"<p>View of the object</p> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>def __str__(self):\n\"\"\"View of the object\"\"\"\nwith Console() as console:\nconsole.print(self._view)\nreturn \"\"\n</code></pre>"},{"location":"reference/io/interface_nwb/#pynapple.io.interface_nwb.NWBFile.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Get object from NWB</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> required <p>Returns:</p> Type Description <code>(Ts, Tsd, TsdFrame, TsGroup, IntervalSet or dict of IntervalSet)</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If key is not in the dictionnary</p> Source code in <code>pynapple/io/interface_nwb.py</code> <pre><code>def __getitem__(self, key):\n\"\"\"Get object from NWB\n    Parameters\n    ----------\n    key : str\n    Returns\n    -------\n    (Ts, Tsd, TsdFrame, TsGroup, IntervalSet or dict of IntervalSet)\n    Raises\n    ------\n    KeyError\n        If key is not in the dictionnary\n    \"\"\"\nif key.__hash__:\nif self.__contains__(key):\nif isinstance(self.data[key], dict) and \"id\" in self.data[key]:\nobj = self.nwb.objects[self.data[key][\"id\"]]\ntry:\ndata = self._f_eval[self.data[key][\"type\"]](obj)\nexcept Exception:\nwarnings.warn(\n\"Failed to build {}.\\n Returning the NWB object for manual inspection\".format(\nself.data[key][\"type\"]\n),\nstacklevel=2,\n)\ndata = obj\nself.data[key] = data\nreturn data\nelse:\nreturn self.data[key]\nelse:\nraise KeyError(\"Can't find key {} in group index.\".format(key))\n</code></pre>"},{"location":"reference/io/loader/","title":"Loader","text":"<p>BaseLoader is the general class for loading session with pynapple.</p> <p>@author: Guillaume Viejo</p>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader","title":"<code>BaseLoader</code>","text":"<p>         Bases: <code>object</code></p> <p>General loader for epochs and tracking data</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>class BaseLoader(object):\n\"\"\"\n    General loader for epochs and tracking data\n    \"\"\"\ndef __init__(self, path=None):\nself.path = path\nstart_gui = True\n# Check if a pynapplenwb folder exist to bypass GUI\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nstart_gui = False\nself.load_data(path)\n# Starting the GUI\nif start_gui:\nwarnings.warn(\nget_deprecation_text(), category=DeprecationWarning, stacklevel=2\n)\napp = App()\nwindow = BaseLoaderGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# Extracting all the information from gui loader\nif window.status:\nself.session_information = window.session_information\nself.subject_information = window.subject_information\nself.name = self.session_information[\"name\"]\nself.tracking_frequency = window.tracking_frequency\nself.position = self._make_position(\nwindow.tracking_parameters,\nwindow.tracking_method,\nwindow.tracking_frequency,\nwindow.epochs,\nwindow.time_units_epochs,\nwindow.tracking_alignment,\n)\nself.epochs = self._make_epochs(window.epochs, window.time_units_epochs)\nself.time_support = self._join_epochs(\nwindow.epochs, window.time_units_epochs\n)\n# Save the data\nself.create_nwb_file(path)\ndef load_default_csv(self, csv_file):\n\"\"\"\n        Load tracking data. The default csv should have the time index in the first column in seconds.\n        If no header is provided, the column names will be the column index.\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\ndef load_optitrack_csv(self, csv_file):\n\"\"\"\n        Load tracking data exported with Optitrack.\n        By default, the function reads rows 4 and 5 to build the column names.\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Raises\n        ------\n        RuntimeError\n            If header names are unknown. Should be 'Position' and 'Rotation'\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\ndef load_dlc_csv(self, csv_file):\n\"\"\"\n        Load tracking data exported with DeepLabCut\n        Parameters\n        ----------\n        csv_file : str\n            path to the csv file\n        Returns\n        -------\n        pandas.DataFrame\n            _\n        \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\ndef load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n        Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n        Parameters\n        ----------\n        ttl_file : str\n            File name\n        n_channels : int, optional\n            The number of channels in the binary file.\n        channel : int, optional\n            Which channel contains the TTL\n        bytes_size : int, optional\n            Bytes size of the binary file.\n        fs : float, optional\n            Sampling frequency of the binary file\n        Returns\n        -------\n        pd.Series\n            A series containing the time index of the TTL.\n        \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\ndef _make_position(\nself, parameters, method, frequency, epochs, time_units, alignment\n):\n\"\"\"\n        Make the position TSDFrame with the parameters extracted from the GUI.\n        \"\"\"\nif len(parameters.index) == 0:\nreturn None\nelse:\nif len(epochs) == 0:\nepochs.loc[0, \"start\"] = 0.0\nframes = []\ntime_supports_starts = []\ntime_support_ends = []\nfor i in range(len(parameters)):\nif method.lower() == \"optitrack\":\nposition = self.load_optitrack_csv(parameters.loc[i, \"csv\"])\nelif method.lower() == \"deep lab cut\":\nposition = self.load_dlc_csv(parameters.loc[i, \"csv\"])\nelif method.lower() == \"default\":\nposition = self.load_default_csv(parameters.loc[i, \"csv\"])\nif alignment.lower() == \"local\":\nstart_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"start\"], time_units\n)\nend_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"end\"], time_units\n)\ntimestamps = (\nposition.index.values\n+ nap.return_timestamps(start_epoch, \"s\")[0]\n)\n# Make sure timestamps are within the epochs\nidx = np.where(timestamps &lt; end_epoch)[0]\nposition = position.iloc[idx]\nposition.index = pd.Index(timestamps[idx])\nif alignment.lower() == \"ttl\":\nttl = self.load_ttl_pulse(\nttl_file=parameters.loc[i, \"ttl\"],\ntracking_frequency=frequency,\nn_channels=int(parameters.loc[i, \"n_channels\"]),\nchannel=int(parameters.loc[i, \"tracking_channel\"]),\nbytes_size=int(parameters.loc[i, \"bytes_size\"]),\nfs=float(parameters.loc[i, \"fs\"]),\nthreshold=float(parameters.loc[i, \"threshold\"]),\n)\nif len(ttl):\nlength = np.minimum(len(ttl), len(position))\nttl = ttl.iloc[0:length]\nposition = position.iloc[0:length]\nelse:\nraise RuntimeError(\n\"No ttl detected for {}\".format(parameters.loc[i, \"ttl\"])\n)\n# Make sure start epochs in seconds\n# start_epoch = format_timestamp(\n#     epochs.loc[parameters.loc[f, \"epoch\"], \"start\"], time_units\n# )\nstart_epoch = nap.format_timestamps(\nepochs.loc[int(parameters.loc[i, \"epoch\"]), \"start\"], time_units\n)\ntimestamps = start_epoch + ttl.index.values\nposition.index = pd.Index(timestamps)\nframes.append(position)\ntime_supports_starts.append(position.index[0])\ntime_support_ends.append(position.index[-1])\nposition = pd.concat(frames)\ntime_supports = nap.IntervalSet(\nstart=time_supports_starts, end=time_support_ends, time_units=\"s\"\n)\n# Specific to optitrACK\nif set([\"rx\", \"ry\", \"rz\"]).issubset(position.columns):\nposition[[\"ry\", \"rx\", \"rz\"]] *= np.pi / 180\nposition[[\"ry\", \"rx\", \"rz\"]] += 2 * np.pi\nposition[[\"ry\", \"rx\", \"rz\"]] %= 2 * np.pi\nposition = nap.TsdFrame(\nt=position.index.values,\nd=position.values,\ncolumns=position.columns.values,\ntime_support=time_supports,\ntime_units=\"s\",\n)\nreturn position\ndef _make_epochs(self, epochs, time_units=\"s\"):\n\"\"\"\n        Split GUI epochs into dict of epochs\n        \"\"\"\nlabels = epochs.groupby(\"label\").groups\nisets = {}\nfor lbs in labels.keys():\ntmp = epochs.loc[labels[lbs]]\nisets[lbs] = nap.IntervalSet(\nstart=tmp[\"start\"], end=tmp[\"end\"], time_units=time_units\n)\nreturn isets\ndef _join_epochs(self, epochs, time_units=\"s\"):\n\"\"\"\n        To create the global time support of the data\n        \"\"\"\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nisets = nap.IntervalSet(\nstart=epochs[\"start\"].sort_values(),\nend=epochs[\"end\"].sort_values(),\ntime_units=time_units,\n)\niset = isets.merge_close_intervals(1, time_units=\"us\")\nif len(iset):\nreturn iset\nelse:\nreturn None\ndef create_nwb_file(self, path):\n\"\"\"\n        Initialize the NWB file in the folder pynapplenwb within the data folder.\n        Parameters\n        ----------\n        path : str\n            The path to save the data\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\ndef load_data(self, path):\n\"\"\"\n        Load NWB data save with pynapple in the pynapplenwb folder\n        Parameters\n        ----------\n        path : str\n            Path to the session folder\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\ndef save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n        Add epochs to the NWB file (e.g. ripples epochs)\n        See pynwb.epoch.TimeIntervals\n        Parameters\n        ----------\n        iset : IntervalSet\n            The intervalSet to save\n        name : str\n            The name in the nwb file\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\ndef save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n        Save timestamps in the NWB file (e.g. ripples time) with the time support.\n        See pynwb.base.TimeSeries\n        Parameters\n        ----------\n        tsd : TsdFrame\n            _\n        name : str\n            _\n        description : str, optional\n            _\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_intervals(self, name):\n\"\"\"\n        Load epochs from the NWB file (e.g. 'ripples')\n        Parameters\n        ----------\n        name : str\n            The name in the nwb file\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\ndef load_nwb_timeseries(self, name):\n\"\"\"\n        Load timestamps in the NWB file (e.g. ripples time)\n        Parameters\n        ----------\n        name : str\n            _\n        Returns\n        -------\n        Tsd\n            _\n        \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_default_csv","title":"<code>load_default_csv(csv_file)</code>","text":"<p>Load tracking data. The default csv should have the time index in the first column in seconds. If no header is provided, the column names will be the column index.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_default_csv(self, csv_file):\n\"\"\"\n    Load tracking data. The default csv should have the time index in the first column in seconds.\n    If no header is provided, the column names will be the column index.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[0], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nreturn position\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_optitrack_csv","title":"<code>load_optitrack_csv(csv_file)</code>","text":"<p>Load tracking data exported with Optitrack. By default, the function reads rows 4 and 5 to build the column names.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If header names are unknown. Should be 'Position' and 'Rotation'</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_optitrack_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with Optitrack.\n    By default, the function reads rows 4 and 5 to build the column names.\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Raises\n    ------\n    RuntimeError\n        If header names are unknown. Should be 'Position' and 'Rotation'\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[4, 5], index_col=1)\nif 1 in position.columns:\nposition = position.drop(labels=1, axis=1)\nposition = position[~position.index.duplicated(keep=\"first\")]\norder = []\ncols = []\nfor n in position.columns:\nif n[0] == \"Rotation\":\norder.append(\"r\" + n[1].lower())\ncols.append(n)\nelif n[0] == \"Position\":\norder.append(n[1].lower())\ncols.append(n)\nif len(order) == 0:\nraise RuntimeError(\n\"Unknow tracking format for csv file {}\".format(csv_file)\n)\nposition = position[cols]\nposition.columns = order\nreturn position\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_dlc_csv","title":"<code>load_dlc_csv(csv_file)</code>","text":"<p>Load tracking data exported with DeepLabCut</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>path to the csv file</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_dlc_csv(self, csv_file):\n\"\"\"\n    Load tracking data exported with DeepLabCut\n    Parameters\n    ----------\n    csv_file : str\n        path to the csv file\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    \"\"\"\nposition = pd.read_csv(csv_file, header=[1, 2], index_col=0)\nposition = position[~position.index.duplicated(keep=\"first\")]\nposition.columns = list(map(lambda x: \"_\".join(x), position.columns.values))\nreturn position\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_ttl_pulse","title":"<code>load_ttl_pulse(ttl_file, tracking_frequency, n_channels=1, channel=0, bytes_size=2, fs=20000.0, threshold=0.3)</code>","text":"<p>Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.</p> <p>Parameters:</p> Name Type Description Default <code>ttl_file</code> <code>str</code> <p>File name</p> required <code>n_channels</code> <code>int, optional</code> <p>The number of channels in the binary file.</p> <code>1</code> <code>channel</code> <code>int, optional</code> <p>Which channel contains the TTL</p> <code>0</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the binary file.</p> <code>2</code> <code>fs</code> <code>float, optional</code> <p>Sampling frequency of the binary file</p> <code>20000.0</code> <p>Returns:</p> Type Description <code>pd.Series</code> <p>A series containing the time index of the TTL.</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_ttl_pulse(\nself,\nttl_file,\ntracking_frequency,\nn_channels=1,\nchannel=0,\nbytes_size=2,\nfs=20000.0,\nthreshold=0.3,\n):\n\"\"\"\n    Load TTLs from a binary file. Each TTLs is then used to reaassign the time index of tracking frames.\n    Parameters\n    ----------\n    ttl_file : str\n        File name\n    n_channels : int, optional\n        The number of channels in the binary file.\n    channel : int, optional\n        Which channel contains the TTL\n    bytes_size : int, optional\n        Bytes size of the binary file.\n    fs : float, optional\n        Sampling frequency of the binary file\n    Returns\n    -------\n    pd.Series\n        A series containing the time index of the TTL.\n    \"\"\"\nf = open(ttl_file, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\nwith open(ttl_file, \"rb\") as f:\ndata = np.fromfile(f, np.uint16).reshape((n_samples, n_channels))\nif n_channels == 1:\ndata = data.flatten().astype(np.int32)\nelse:\ndata = data[:, channel].flatten().astype(np.int32)\ndata = data / data.max()\npeaks, _ = scipy.signal.find_peaks(\nnp.diff(data), height=threshold, distance=int(fs / (tracking_frequency * 2))\n)\ntimestep = np.arange(0, len(data)) / fs\npeaks += 1\nttl = pd.Series(index=timestep[peaks], data=data[peaks])\nreturn ttl\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.create_nwb_file","title":"<code>create_nwb_file(path)</code>","text":"<p>Initialize the NWB file in the folder pynapplenwb within the data folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def create_nwb_file(self, path):\n\"\"\"\n    Initialize the NWB file in the folder pynapplenwb within the data folder.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nos.makedirs(self.nwb_path)\nself.nwbfilepath = os.path.join(\nself.nwb_path, self.session_information[\"name\"] + \".nwb\"\n)\nself.subject_information[\"date_of_birth\"] = None\nnwbfile = NWBFile(\nsession_description=self.session_information[\"description\"],\nidentifier=self.session_information[\"name\"],\nsession_start_time=datetime.datetime.now(datetime.timezone.utc),\nexperimenter=self.session_information[\"experimenter\"],\nlab=self.session_information[\"lab\"],\ninstitution=self.session_information[\"institution\"],\nsubject=Subject(**self.subject_information),\n)\n# Tracking\nif self.position is not None:\ndata = self.position.as_units(\"s\")\n# specific to optitrack\nif set([\"x\", \"y\", \"z\", \"rx\", \"ry\", \"rz\"]).issubset(data.columns):\nposition = Position()\nfor c in [\"x\", \"y\", \"z\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\ndirection = CompassDirection()\nfor c in [\"rx\", \"ry\", \"rz\"]:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"radian\",\nreference_frame=\"\",\n)\ndirection.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\nnwbfile.add_acquisition(direction)\n# Other types\nelse:\nposition = Position()\nfor c in data.columns:\ntmp = SpatialSeries(\nname=c,\ndata=data[c].values,\ntimestamps=data.index.values,\nunit=\"\",\nreference_frame=\"\",\n)\nposition.add_spatial_series(tmp)\nnwbfile.add_acquisition(position)\n# Adding time support of position as TimeIntervals\nepochs = self.position.time_support.as_units(\"s\")\nposition_time_support = TimeIntervals(\nname=\"position_time_support\",\ndescription=\"The time support of the position i.e the real start and end of the tracking\",\n)\nfor i in self.position.time_support.index:\nposition_time_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(position_time_support)\n# Epochs\nfor ep in self.epochs.keys():\nepochs = self.epochs[ep].as_units(\"s\")\nfor i in self.epochs[ep].index:\nnwbfile.add_epoch(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=[ep],  # This is stupid nwb who tries to parse the string\n)\nwith NWBHDF5IO(self.nwbfilepath, \"w\") as io:\nio.write(nwbfile)\nreturn\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_data","title":"<code>load_data(path)</code>","text":"<p>Load NWB data save with pynapple in the pynapplenwb folder</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session folder</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_data(self, path):\n\"\"\"\n    Load NWB data save with pynapple in the pynapplenwb folder\n    Parameters\n    ----------\n    path : str\n        Path to the session folder\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nposition = {}\nacq_keys = nwbfile.acquisition.keys()\nif \"CompassDirection\" in acq_keys:\ncompass = nwbfile.acquisition[\"CompassDirection\"]\nfor k in compass.spatial_series.keys():\nposition[k] = pd.Series(\nindex=compass.get_spatial_series(k).timestamps[:],\ndata=compass.get_spatial_series(k).data[:],\n)\nif \"Position\" in acq_keys:\ntracking = nwbfile.acquisition[\"Position\"]\nfor k in tracking.spatial_series.keys():\nposition[k] = pd.Series(\nindex=tracking.get_spatial_series(k).timestamps[:],\ndata=tracking.get_spatial_series(k).data[:],\n)\nif len(position):\nposition = pd.DataFrame.from_dict(position)\n# retrieveing time support position if in epochs\nif \"position_time_support\" in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[\"position_time_support\"].to_dataframe()\ntime_support = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nself.position = nap.TsdFrame(\nposition, time_units=\"s\", time_support=time_support\n)\nif nwbfile.epochs is not None:\nepochs = nwbfile.epochs.to_dataframe()\n# NWB is dumb and cannot take a single string for labels\nepochs[\"label\"] = [epochs.loc[i, \"tags\"][0] for i in epochs.index]\nepochs = epochs.drop(labels=\"tags\", axis=1)\nepochs = epochs.rename(columns={\"start_time\": \"start\", \"stop_time\": \"end\"})\nself.epochs = self._make_epochs(epochs)\nself.time_support = self._join_epochs(epochs, \"s\")\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.save_nwb_intervals","title":"<code>save_nwb_intervals(iset, name, description='')</code>","text":"<p>Add epochs to the NWB file (e.g. ripples epochs) See pynwb.epoch.TimeIntervals</p> <p>Parameters:</p> Name Type Description Default <code>iset</code> <code>IntervalSet</code> <p>The intervalSet to save</p> required <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_intervals(self, iset, name, description=\"\"):\n\"\"\"\n    Add epochs to the NWB file (e.g. ripples epochs)\n    See pynwb.epoch.TimeIntervals\n    Parameters\n    ----------\n    iset : IntervalSet\n        The intervalSet to save\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nepochs = iset.as_units(\"s\")\ntime_intervals = TimeIntervals(name=name, description=description)\nfor i in epochs.index:\ntime_intervals.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_intervals)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.save_nwb_timeseries","title":"<code>save_nwb_timeseries(tsd, name, description='')</code>","text":"<p>Save timestamps in the NWB file (e.g. ripples time) with the time support. See pynwb.base.TimeSeries</p> <p>Parameters:</p> Name Type Description Default <code>tsd</code> <code>TsdFrame</code> <p>_</p> required <code>name</code> <code>str</code> <p>_</p> required <code>description</code> <code>str, optional</code> <p>_</p> <code>''</code> Source code in <code>pynapple/io/loader.py</code> <pre><code>def save_nwb_timeseries(self, tsd, name, description=\"\"):\n\"\"\"\n    Save timestamps in the NWB file (e.g. ripples time) with the time support.\n    See pynwb.base.TimeSeries\n    Parameters\n    ----------\n    tsd : TsdFrame\n        _\n    name : str\n        _\n    description : str, optional\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nts = TimeSeries(\nname=name,\nunit=\"s\",\ndata=tsd.values,\ntimestamps=tsd.as_units(\"s\").index.values,\n)\ntime_support = TimeIntervals(\nname=name + \"_timesupport\", description=\"The time support of the object\"\n)\nepochs = tsd.time_support.as_units(\"s\")\nfor i in epochs.index:\ntime_support.add_interval(\nstart_time=epochs.loc[i, \"start\"],\nstop_time=epochs.loc[i, \"end\"],\ntags=str(i),\n)\nnwbfile.add_time_intervals(time_support)\nnwbfile.add_acquisition(ts)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_nwb_intervals","title":"<code>load_nwb_intervals(name)</code>","text":"<p>Load epochs from the NWB file (e.g. 'ripples')</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name in the nwb file</p> required Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_intervals(self, name):\n\"\"\"\n    Load epochs from the NWB file (e.g. 'ripples')\n    Parameters\n    ----------\n    name : str\n        The name in the nwb file\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif name in nwbfile.intervals.keys():\nepochs = nwbfile.intervals[name].to_dataframe()\nisets = nap.IntervalSet(\nstart=epochs[\"start_time\"], end=epochs[\"stop_time\"], time_units=\"s\"\n)\nio.close()\nreturn isets\nelse:\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/loader/#pynapple.io.loader.BaseLoader.load_nwb_timeseries","title":"<code>load_nwb_timeseries(name)</code>","text":"<p>Load timestamps in the NWB file (e.g. ripples time)</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>_</p> required <p>Returns:</p> Type Description <code>Tsd</code> <p>_</p> Source code in <code>pynapple/io/loader.py</code> <pre><code>def load_nwb_timeseries(self, name):\n\"\"\"\n    Load timestamps in the NWB file (e.g. ripples time)\n    Parameters\n    ----------\n    name : str\n        _\n    Returns\n    -------\n    Tsd\n        _\n    \"\"\"\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nts = nwbfile.acquisition[name]\ntime_support = self.load_nwb_intervals(name + \"_timesupport\")\ntsd = nap.Tsd(\nt=ts.timestamps[:], d=ts.data[:], time_units=\"s\", time_support=time_support\n)\nio.close()\nreturn tsd\n</code></pre>"},{"location":"reference/io/loader_gui/","title":"Loader gui","text":""},{"location":"reference/io/loader_gui/#pynapple.io.loader_gui.EntryPopup","title":"<code>EntryPopup</code>","text":"<p>         Bases: <code>ttk.Entry</code></p> Source code in <code>pynapple/io/loader_gui.py</code> <pre><code>class EntryPopup(ttk.Entry):\ndef __init__(self, parent, iid, column, text, table, **kw):\nttk.Style().configure(\"pad.TEntry\", padding=\"1 1 1 1\")\nsuper().__init__(parent, style=\"pad.TEntry\", **kw)\nself.tv = parent\nself.iid = iid\nself.column = column\nself.table = table\nself.insert(0, text)\nself[\"exportselection\"] = False\nself.focus_force()\nself.select_all()\nself.bind(\"&lt;Return&gt;\", self.on_return)\nself.bind(\"&lt;Control-a&gt;\", self.select_all)\nself.bind(\"&lt;Escape&gt;\", lambda *ignore: self.destroy())\ndef on_return(self, event):\nrowid = self.tv.focus()\nvals = self.tv.item(rowid, \"values\")\nvals = list(vals)\nvals[self.column] = self.get()\nself.tv.item(rowid, values=vals)\nself.destroy()\nself.table.loc[int(rowid), self.table.columns[int(self.column)]] = vals[\nself.column\n]\ndef select_all(self, *ignore):\n\"\"\"Set selection on the whole text\"\"\"\nself.selection_range(0, \"end\")\n# returns 'break' to interrupt default key-bindings\nreturn \"break\"\n</code></pre>"},{"location":"reference/io/loader_gui/#pynapple.io.loader_gui.EntryPopup.select_all","title":"<code>select_all(*ignore)</code>","text":"<p>Set selection on the whole text</p> Source code in <code>pynapple/io/loader_gui.py</code> <pre><code>def select_all(self, *ignore):\n\"\"\"Set selection on the whole text\"\"\"\nself.selection_range(0, \"end\")\n# returns 'break' to interrupt default key-bindings\nreturn \"break\"\n</code></pre>"},{"location":"reference/io/misc/","title":"Misc","text":"<p>Various io functions</p>"},{"location":"reference/io/misc/#pynapple.io.misc.load_file","title":"<code>load_file(path)</code>","text":"<p>Load file. Current format supported is (npz,nwb,)</p> <p>.npz -&gt; If the file is compatible with a pynapple format, the function will return a pynapple object. Otherwise, the function will return the output of numpy.load</p> <p>.nwb -&gt; Return the pynapple.io.NWBFile class wrapping the NWBFile</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the file</p> required <p>Returns:</p> Type Description <code>Tsd, TsdFrame, Ts, IntervalSet, TsGroup, pynapple.io.NWBFile</code> <p>One of the 5 pynapple objects or pynapple.io.NWBFile</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If file is missing</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_file(path):\n\"\"\"Load file. Current format supported is (npz,nwb,)\n    .npz -&gt; If the file is compatible with a pynapple format, the function will return a pynapple object.\n    Otherwise, the function will return the output of numpy.load\n    .nwb -&gt; Return the pynapple.io.NWBFile class wrapping the NWBFile\n    Parameters\n    ----------\n    path : str\n        Path to the file\n    Returns\n    -------\n    (Tsd, TsdFrame, Ts, IntervalSet, TsGroup, pynapple.io.NWBFile)\n        One of the 5 pynapple objects or pynapple.io.NWBFile\n    Raises\n    ------\n    FileNotFoundError\n        If file is missing\n    \"\"\"\nif os.path.isfile(path):\nif path.endswith(\".npz\"):\nreturn NPZFile(path).load()\nelif path.endswith(\".nwb\"):\nreturn NWBFile(path)\nelse:\nraise RuntimeError(\"File format not supported\")\nelse:\nraise FileNotFoundError(\"File {} does not exist\".format(path))\n</code></pre>"},{"location":"reference/io/misc/#pynapple.io.misc.load_folder","title":"<code>load_folder(path)</code>","text":"<p>Load folder containing files or other folder. Pynapple will walk throught the subfolders to detect compatible npz files or nwb files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the folder</p> required <p>Returns:</p> Type Description <code>Folder</code> <p>A dictionnary-like class containing all the sub-folders and compatible files (i.e. npz, nwb)</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If folder is missing</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_folder(path):\n\"\"\"Load folder containing files or other folder.\n    Pynapple will walk throught the subfolders to detect compatible npz files\n    or nwb files.\n    Parameters\n    ----------\n    path : str\n        Path to the folder\n    Returns\n    -------\n    Folder\n        A dictionnary-like class containing all the sub-folders and compatible files (i.e. npz, nwb)\n    Raises\n    ------\n    RuntimeError\n        If folder is missing\n    \"\"\"\nif os.path.isdir(path):\nreturn Folder(path)\nelse:\nraise RuntimeError(\"Folder {} does not exist\".format(path))\n</code></pre>"},{"location":"reference/io/misc/#pynapple.io.misc.load_session","title":"<code>load_session(path=None, session_type=None)</code>","text":"<p>%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % WARNING : THIS FUNCTION IS DEPRECATED % %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% General Loader for</p> <ul> <li> <p>Neurosuite</p> </li> <li> <p>Phy</p> </li> <li> <p>Minian</p> </li> <li> <p>Inscopix-cnmfe</p> </li> <li> <p>Matlab-cnmfe</p> </li> <li> <p>Suite2p</p> </li> <li>None for default session.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str, optional</code> <p>The path to load the data</p> <code>None</code> <code>session_type</code> <code>str, optional</code> <p>Can be 'neurosuite', 'phy', 'minian', 'inscopix-cnmfe', 'cnmfe-matlab', 'suite2p' or None for default loader.</p> <code>None</code> <p>Returns:</p> Type Description <code>Session</code> <p>A class holding all the data from the session.</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_session(path=None, session_type=None):\n\"\"\"\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    % WARNING : THIS FUNCTION IS DEPRECATED %\n    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n    General Loader for\n    - Neurosuite\\n\n    - Phy\\n\n    - Minian\\n\n    - Inscopix-cnmfe\\n\n    - Matlab-cnmfe\\n\n    - Suite2p\n    - None for default session.\n    Parameters\n    ----------\n    path : str, optional\n        The path to load the data\n    session_type : str, optional\n        Can be 'neurosuite', 'phy',\n        'minian', 'inscopix-cnmfe', 'cnmfe-matlab',\n        'suite2p' or None for default loader.\n    Returns\n    -------\n    Session\n        A class holding all the data from the session.\n    \"\"\"\nif path:\nif not os.path.isdir(path):\nraise RuntimeError(\"Path {} is not found.\".format(path))\nif isinstance(session_type, str):\nsession_type = session_type.lower()\nif session_type == \"neurosuite\":\nreturn NeuroSuite(path)\nelif session_type == \"phy\":\nreturn Phy(path)\nelif session_type == \"inscopix-cnmfe\":\nreturn InscopixCNMFE(path)\nelif session_type == \"minian\":\nreturn Minian(path)\nelif session_type == \"cnmfe-matlab\":\nreturn CNMF_E(path)\nelif session_type == \"suite2p\":\nreturn Suite2P(path)\nelse:\nreturn BaseLoader(path)\n</code></pre>"},{"location":"reference/io/misc/#pynapple.io.misc.load_eeg","title":"<code>load_eeg(filepath, channel=None, n_channels=None, frequency=None, precision='int16', bytes_size=2)</code>","text":"<p>Standalone function to load eeg/lfp/dat file in binary format.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>The path to the eeg file</p> required <code>channel</code> <code>int or list of int, optional</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>n_channels</code> <code>int, optional</code> <p>Number of channels</p> <code>None</code> <code>frequency</code> <code>float, optional</code> <p>Sampling rate of the file</p> <code>None</code> <code>precision</code> <code>str, optional</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the binary file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p>"},{"location":"reference/io/misc/#pynapple.io.misc.load_eeg--deleted-parameters","title":"Deleted Parameters","text":"<p>extension : str, optional     The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def load_eeg(\nfilepath,\nchannel=None,\nn_channels=None,\nfrequency=None,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Standalone function to load eeg/lfp/dat file in binary format.\n    Parameters\n    ----------\n    filepath : str\n        The path to the eeg file\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    n_channels : int, optional\n        Number of channels\n    frequency : float, optional\n        Sampling rate of the file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the binary file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    Deleted Parameters\n    ------------------\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    \"\"\"\n# Need to check if a xml file exists\npath = os.path.dirname(filepath)\nbasename = os.path.basename(filepath).split(\".\")[0]\nlistdir = os.listdir(path)\nif frequency is None or n_channels is None:\nif basename + \".xml\" in listdir:\nxmlpath = os.path.join(path, basename + \".xml\")\nxmldoc = minidom.parse(xmlpath)\nelse:\nraise RuntimeError(\n\"Can't find xml file; please specify sampling frequency or number of channels\"\n)\nif frequency is None:\nif filepath.endswith(\".dat\"):\nfs_dat = int(\nxmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nfrequency = fs_dat\nelif filepath.endswith((\".lfp\", \".eeg\")):\nfs_eeg = int(\nxmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nfrequency = fs_eeg\nif n_channels is None:\nn_channels = int(\nxmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn fp\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"reference/io/misc/#pynapple.io.misc.append_NWB_LFP","title":"<code>append_NWB_LFP(path, lfp, channel=None)</code>","text":"<p>Standalone function for adding lfp/eeg to already existing nwb files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data. The function will looks for a nwb file in path or in path/pynapplenwb.</p> required <code>lfp</code> <code>Tsd or TsdFrame</code> <p>Description</p> required <code>channel</code> <code>None, optional</code> <p>channel number in int ff lfp is a Tsd</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the nwb file </p> <p>If no channel is specify when passing a Tsd</p> Source code in <code>pynapple/io/misc.py</code> <pre><code>def append_NWB_LFP(path, lfp, channel=None):\n\"\"\"Standalone function for adding lfp/eeg to already existing nwb files.\n    Parameters\n    ----------\n    path : str\n        The path to the data. The function will looks for a nwb file in path\n        or in path/pynapplenwb.\n    lfp : Tsd or TsdFrame\n        Description\n    channel : None, optional\n        channel number in int ff lfp is a Tsd\n    Raises\n    ------\n    RuntimeError\n        If can't find the nwb file \\n\n        If no channel is specify when passing a Tsd\n    \"\"\"\nnew_path = os.path.join(path, \"pynapplenwb\")\nnwb_path = \"\"\nif os.path.exists(new_path):\nnwbfilename = [f for f in os.listdir(new_path) if f.endswith(\".nwb\")]\nif len(nwbfilename):\nnwb_path = os.path.join(path, \"pynapplenwb\", nwbfilename[0])\nelse:\nnwbfilename = [f for f in os.listdir(path) if f.endswith(\".nwb\")]\nif len(nwbfilename):\nnwb_path = os.path.join(path, \"pynapplenwb\", nwbfilename[0])\nif len(nwb_path) == 0:\nraise RuntimeError(\"Can't find nwb file in {}\".format(path))\nif isinstance(lfp, nap.TsdFrame):\nchannels = lfp.columns.values\nelif isinstance(lfp, nap.Tsd):\nif isinstance(channel, int):\nchannels = [channel]\nelse:\nraise RuntimeError(\"Please specify which channel it is.\")\nio = NWBHDF5IO(nwb_path, \"r+\")\nnwbfile = io.read()\nall_table_region = nwbfile.create_electrode_table_region(\nregion=channels, description=\"\", name=\"electrodes\"\n)\nlfp_electrical_series = ElectricalSeries(\nname=\"ElectricalSeries\",\ndata=lfp.values,\ntimestamps=lfp.index.values,\nelectrodes=all_table_region,\n)\nlfp = LFP(electrical_series=lfp_electrical_series)\necephys_module = nwbfile.create_processing_module(\nname=\"ecephys\", description=\"processed extracellular electrophysiology data\"\n)\necephys_module.add(lfp)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/","title":"Neurosuite","text":"<p>Class and functions for loading data processed with the Neurosuite (Klusters, Neuroscope, NDmanager)</p> <p>@author: Guillaume Viejo</p>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite","title":"<code>NeuroSuite</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for kluster data</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>class NeuroSuite(BaseLoader):\n\"\"\"\n    Loader for kluster data\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Instantiate the data class from a neurosuite folder.\n        Parameters\n        ----------\n        path : str\n            The path to the data.\n        \"\"\"\nself.basename = os.path.basename(path)\nself.time_support = None\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_neurosuite = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_nwb_spikes(path)\nif success:\nloading_neurosuite = False\n# Bypass if data have already been transfered to nwb\nif loading_neurosuite:\nself.load_neurosuite_xml(path)\n# print(\"XML loaded\")\n# To label the electrodes groups\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.group_to_channel)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# print(\"GUI DONE\")\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_neurosuite_spikes(path, self.basename, self.time_support)\nself.save_data(path)\ndef load_neurosuite_spikes(self, path, basename, time_support=None, fs=20000.0):\n\"\"\"\n        Read the clus and res files and convert to NWB.\n        Instantiate automatically a TsGroup object.\n        Parameters\n        ----------\n        path : str\n            The path to the data\n        basename : str\n            Basename of the clu and res files.\n        time_support : IntevalSet, optional\n            The time support of the data\n        fs : float, optional\n            Sampling rate of the recording.\n        Raises\n        ------\n        RuntimeError\n            If number of clu and res are not equal.\n        \"\"\"\nfiles = os.listdir(path)\nclu_files = np.sort([f for f in files if \".clu.\" in f and f[0] != \".\"])\nres_files = np.sort([f for f in files if \".res.\" in f and f[0] != \".\"])\nclu1 = np.sort([int(f.split(\".\")[-1]) for f in clu_files])\nclu2 = np.sort([int(f.split(\".\")[-1]) for f in res_files])\nif len(clu_files) != len(res_files) or not (clu1 == clu2).any():\nraise RuntimeError(\n\"Not the same number of clu and res files in \" + path + \"; Exiting ...\"\n)\ncount = 0\nspikes = {}\ngroup = pd.Series(dtype=np.int32)\nfor i, s in zip(range(len(clu_files)), clu1):\nclu = np.genfromtxt(\nos.path.join(path, basename + \".clu.\" + str(s)), dtype=np.int32\n)[1:]\nif np.max(clu) &gt; 1:  # getting rid of mua and noise\nres = np.genfromtxt(os.path.join(path, basename + \".res.\" + str(s)))\ntmp = np.unique(clu).astype(int)\nidx_clu = tmp[tmp &gt; 1]\nidx_out = np.arange(count, count + len(idx_clu))\nfor j, k in zip(idx_clu, idx_out):\nt = res[clu == j] / fs\nspikes[k] = nap.Ts(t=t, time_units=\"s\")\ngroup.loc[k] = s\ncount += len(idx_clu)\ngroup = group - 1  # better to start it a 0\nself.spikes = nap.TsGroup(\nspikes, time_support=time_support, time_units=\"s\", group=group\n)\n# adding some information to help parse the neurons\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\ndef load_neurosuite_xml(self, path):\n\"\"\"\n        path should be the folder session containing the XML file\n        Function reads :\n        1. the number of channels\n        2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder\n            eeg file first if both are present or both are absent\n        3. the mappings shanks to channels as a dict\n        Parameters\n        ----------\n        path: str\n            The path to the data\n        Raises\n        ------\n        RuntimeError\n            If path does not contain the xml file.\n        \"\"\"\nlistdir = os.listdir(path)\nxmlfiles = [f for f in listdir if f.endswith(\".xml\")]\nif not len(xmlfiles):\nraise RuntimeError(\"Path {} contains no xml files;\".format(path))\nsys.exit()\nnew_path = os.path.join(path, xmlfiles[0])\nself.xmldoc = minidom.parse(new_path)\nself.nChannels = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nself.fs_dat = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nself.fs_eeg = int(\nself.xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nself.group_to_channel = {}\ngroups = (\nself.xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n.getElementsByTagName(\"channelGroups\")[0]\n.getElementsByTagName(\"group\")\n)\nfor i in range(len(groups)):\nself.group_to_channel[i] = np.array(\n[\nint(child.firstChild.data)\nfor child in groups[i].getElementsByTagName(\"channel\")\n]\n)\nreturn\ndef save_data(self, path):\n\"\"\"\n        Save the data to NWB format.\n        Parameters\n        ----------\n        path : str\n            The path to save the data\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.group_to_channel:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.group_to_channel[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_spikes(self, path):\n\"\"\"\n        Read the NWB spikes to extract the spike times.\n        Parameters\n        ----------\n        path : str\n            The path to the data\n        Returns\n        -------\n        TYPE\n            Description\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn False\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\ndef load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n        Load the LFP.\n        Parameters\n        ----------\n        filename : str, optional\n            The filename of the lfp file.\n            It can be useful it multiple dat files are present in the data directory\n        channel : int or list of int, optional\n            The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n        extension : str, optional\n            The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n        frequency : float, optional\n            Default 1250 Hz for the eeg file\n        precision : str, optional\n            The precision of the binary file\n        bytes_size : int, optional\n            Bytes size of the lfp file\n        Raises\n        ------\n        RuntimeError\n            If can't find the lfp/eeg/dat file\n        Returns\n        -------\n        Tsd or TsdFrame\n            The lfp in a time series format\n        \"\"\"\nif filename is not None:\nfilepath = os.path.join(self.path, filename)\nelse:\nlistdir = os.listdir(self.path)\neegfile = [f for f in listdir if f.endswith(extension)]\nif not len(eegfile):\nraise RuntimeError(\n\"Path {} contains no {} files;\".format(self.path, extension)\n)\nfilepath = os.path.join(self.path, eegfile[0])\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\ndef read_neuroscope_intervals(self, name=None, path2file=None):\n\"\"\"\n        This function reads .evt files in which odd raws indicate the beginning\n        of the time series and the even raws are the ends.\n        If the file is present in the nwb, provide the just the name. If the file\n        is not present in the nwb, it loads the events from the nwb directory.\n        If just the path is provided but not the name, it takes the name from the file.\n        Parameters\n        ----------\n        name: str\n            name of the epoch in the nwb file, e.g. \"rem\" or desired name save\n            the data in the nwb.\n        path2file: str\n            Path of the file you want to load.\n        Returns\n        -------\n        IntervalSet\n            Contains two columns corresponding to the start and end of the intervals.\n        \"\"\"\nif name:\nisets = self.load_nwb_intervals(name)\nif isinstance(isets, nap.IntervalSet):\nreturn isets\nif name is not None and path2file is None:\npath2file = os.path.join(self.path, self.basename + \".\" + name + \".evt\")\nif path2file is not None:\ntry:\n# df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None)\ntmp = np.genfromtxt(path2file)[:, 0]\ndf = tmp.reshape(len(tmp) // 2, 2)\nexcept ValueError:\nprint(\"specify a valid name\")\nisets = nap.IntervalSet(df[:, 0], df[:, 1], time_units=\"ms\")\nif name is None:\nname = path2file.split(\".\")[-2]\nprint(\"*** saving file in the nwb as\", name)\nself.save_nwb_intervals(isets, name)\nelse:\nraise ValueError(\"specify a valid path\")\nreturn isets\ndef write_neuroscope_intervals(self, extension, isets, name):\n\"\"\"Write events to load with neuroscope (e.g. ripples start and ends)\n        Parameters\n        ----------\n        extension : str\n            The extension of the file (e.g. basename.evt.py.rip)\n        isets : IntervalSet\n            The IntervalSet to write\n        name : str\n            The name of the events (e.g. Ripples)\n        \"\"\"\nstart = isets.as_units(\"ms\")[\"start\"].values\nends = isets.as_units(\"ms\")[\"end\"].values\ndatatowrite = np.vstack((start, ends)).T.flatten()\nn = len(isets)\ntexttowrite = np.vstack(\n(\n(np.repeat(np.array([name + \" start\"]), n)),\n(np.repeat(np.array([name + \" end\"]), n)),\n)\n).T.flatten()\nevt_file = os.path.join(self.path, self.basename + extension)\nf = open(evt_file, \"w\")\nfor t, n in zip(datatowrite, texttowrite):\nf.writelines(\"{:1.6f}\".format(t) + \"\\t\" + n + \"\\n\")\nf.close()\nreturn\ndef load_mean_waveforms(self, epoch=None, waveform_window=None, spike_count=1000):\n\"\"\"\n        Load the mean waveforms from a dat file.\n        Parameters\n        ----------\n        epoch : IntervalSet\n            default = None\n            Restrict spikes to an epoch.\n        waveform_window : IntervalSet\n            default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms')\n            Limit waveform extraction before and after spike time\n        spike_count : int\n            default = 1000\n            Number of spikes used per neuron for the calculation of waveforms\n        Returns\n        -------\n        dictionary\n            the waveforms for all neurons\n        pandas.Series\n            the channel with the maximum waveform for each neuron\n        \"\"\"\nif not isinstance(waveform_window, nap.IntervalSet):\nwaveform_window = nap.IntervalSet(start=-0.5, end=1, time_units=\"ms\")\nspikes = self.spikes\nif not os.path.exists(self.path):  # check if path exists\nprint(\"The path \" + self.path + \" doesn't exist; Exiting ...\")\nsys.exit()\n# Load XML INFO\nself.load_neurosuite_xml(self.path)\nn_channels = self.nChannels\nfs = self.fs_dat\ngroup_to_channel = self.group_to_channel\ngroup = spikes.get_info(\"group\")\n# Check if there is an epoch, restrict spike times to epoch\nif epoch is not None:\nif type(epoch) is not nap.IntervalSet:\nprint(\"Epoch must be an IntervalSet\")\nsys.exit()\nelse:\nprint(\"Restricting spikes to epoch\")\nspikes = spikes.restrict(epoch)\nepstart = int(epoch.as_units(\"s\")[\"start\"].values[0] * fs)\nepend = int(epoch.as_units(\"s\")[\"end\"].values[0] * fs)\n# Find dat file\nfiles = os.listdir(self.path)\ndat_files = np.sort([f for f in files if \"dat\" in f and f[0] != \".\"])\n# Need n_samples collected in the entire recording from dat file to load\nfile = os.path.join(self.path, dat_files[0])\nf = open(\nfile, \"rb\"\n)  # open file to get number of samples collected in the entire recording\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\n# map to memory all samples for all channels, channels are numbered according to neuroscope number\nfp = np.memmap(file, np.int16, \"r\", shape=(n_samples, n_channels))\n# convert spike times to spikes in sample number\nsample_spikes = {\nneuron: (spikes[neuron].as_units(\"s\").index.values * fs).astype(\"int\")\nfor neuron in spikes\n}\n# prep for waveforms\noverlap = int(\nwaveform_window.tot_length(time_units=\"s\")\n)  # one spike's worth of overlap between windows\nwaveform_window = abs(np.array(waveform_window.as_units(\"s\"))[0] * fs).astype(\nint\n)  # convert time to sample number\nneuron_waveforms = {\nn: np.zeros([np.sum(waveform_window), len(group_to_channel[group[n]])])\nfor n in sample_spikes\n}\n# divide dat file into batches that slightly overlap for faster loading\nbatch_size = 3000000\nwindows = np.arange(0, int(endoffile / n_channels / bytes_size), batch_size)\nif epoch is not None:\nprint(\"Restricting dat file to epoch\")\nwindows = windows[(windows &gt;= epstart) &amp; (windows &lt;= epend)]\nbatches = []\nfor (\ni\n) in windows:  # make overlapping batches from the beginning to end of recording\nif i == windows[-1]:  # the last batch cannot overlap with the next one\nbatches.append([i, n_samples])\nelse:\nbatches.append([i, i + batch_size + overlap])\nbatches = [np.int32(batch) for batch in batches]\nsample_counted_spikes = {}\nfor index, neuron in enumerate(sample_spikes):\nif len(sample_spikes[neuron]) &gt;= spike_count:\nsample_counted_spikes[neuron] = np.array(\nnp.random.choice(list(sample_spikes[neuron]), spike_count)\n)\nelif len(sample_spikes[neuron]) &lt; spike_count:\nprint(\n\"Not enough spikes in neuron \" + str(index) + \"... using all spikes\"\n)\nsample_counted_spikes[neuron] = sample_spikes[neuron]\n# Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file\nspike_check = np.array(\n[\nint(spikes_neuron)\nfor spikes_neuron in sample_counted_spikes[neuron]\nfor neuron in sample_counted_spikes\n]\n)\nfor index, timestep in enumerate(batches):\nprint(\nf\"Extracting waveforms from dat file: window {index+1} / {len(windows)}\",\nend=\"\\r\",\n)\nif (\nlen(\nspike_check[\n(timestep[0] &lt; spike_check) &amp; (timestep[1] &gt; spike_check)\n]\n)\n== 0\n):\ncontinue  # if there are no spikes for any neurons in this batch, skip and go to the next one\n# Load dat file for timestep\ntmp = pd.DataFrame(\ndata=fp[timestep[0] : timestep[1], :],\ncolumns=np.arange(n_channels),\nindex=range(timestep[0], timestep[1]),\n)  # load dat file\n# Check if any spikes are present\nfor neuron in sample_counted_spikes:\nneurontmp = sample_counted_spikes[neuron]\ntmp2 = neurontmp[(timestep[0] &lt; neurontmp) &amp; (timestep[1] &gt; neurontmp)]\nif len(neurontmp) == 0:\ncontinue  # skip neuron if it has no spikes in this batch\ntmpn = tmp[\ngroup_to_channel[group[neuron]]\n]  # restrict dat file to the channel group of the neuron\nfor time in tmp2:  # add each spike waveform to neuron_waveform\nspikewindow = tmpn.loc[\ntime - waveform_window[0] : time + waveform_window[1] - 1\n]  # waveform for this spike time\ntry:\nneuron_waveforms[neuron] += spikewindow.values\nexcept (\nException\n):  # ignore if full waveform is not present in this batch\npass\nmeanwf = {\nn: pd.DataFrame(\ndata=np.array(neuron_waveforms[n]) / spike_count,\ncolumns=np.arange(len(group_to_channel[group[n]])),\nindex=np.array(np.arange(-waveform_window[0], waveform_window[1])) / fs,\n)\nfor n in sample_counted_spikes\n}\n# find the max channel for each neuron\nmaxch = pd.Series(\ndata=[meanwf[n][meanwf[n].loc[0].idxmin()].name for n in meanwf],\nindex=spikes.keys(),\n)\nreturn meanwf, maxch\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.__init__","title":"<code>__init__(path)</code>","text":"<p>Instantiate the data class from a neurosuite folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Instantiate the data class from a neurosuite folder.\n    Parameters\n    ----------\n    path : str\n        The path to the data.\n    \"\"\"\nself.basename = os.path.basename(path)\nself.time_support = None\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_neurosuite = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_nwb_spikes(path)\nif success:\nloading_neurosuite = False\n# Bypass if data have already been transfered to nwb\nif loading_neurosuite:\nself.load_neurosuite_xml(path)\n# print(\"XML loaded\")\n# To label the electrodes groups\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.group_to_channel)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\n# print(\"GUI DONE\")\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_neurosuite_spikes(path, self.basename, self.time_support)\nself.save_data(path)\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_spikes","title":"<code>load_neurosuite_spikes(path, basename, time_support=None, fs=20000.0)</code>","text":"<p>Read the clus and res files and convert to NWB. Instantiate automatically a TsGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <code>basename</code> <code>str</code> <p>Basename of the clu and res files.</p> required <code>time_support</code> <code>IntevalSet, optional</code> <p>The time support of the data</p> <code>None</code> <code>fs</code> <code>float, optional</code> <p>Sampling rate of the recording.</p> <code>20000.0</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If number of clu and res are not equal.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_neurosuite_spikes(self, path, basename, time_support=None, fs=20000.0):\n\"\"\"\n    Read the clus and res files and convert to NWB.\n    Instantiate automatically a TsGroup object.\n    Parameters\n    ----------\n    path : str\n        The path to the data\n    basename : str\n        Basename of the clu and res files.\n    time_support : IntevalSet, optional\n        The time support of the data\n    fs : float, optional\n        Sampling rate of the recording.\n    Raises\n    ------\n    RuntimeError\n        If number of clu and res are not equal.\n    \"\"\"\nfiles = os.listdir(path)\nclu_files = np.sort([f for f in files if \".clu.\" in f and f[0] != \".\"])\nres_files = np.sort([f for f in files if \".res.\" in f and f[0] != \".\"])\nclu1 = np.sort([int(f.split(\".\")[-1]) for f in clu_files])\nclu2 = np.sort([int(f.split(\".\")[-1]) for f in res_files])\nif len(clu_files) != len(res_files) or not (clu1 == clu2).any():\nraise RuntimeError(\n\"Not the same number of clu and res files in \" + path + \"; Exiting ...\"\n)\ncount = 0\nspikes = {}\ngroup = pd.Series(dtype=np.int32)\nfor i, s in zip(range(len(clu_files)), clu1):\nclu = np.genfromtxt(\nos.path.join(path, basename + \".clu.\" + str(s)), dtype=np.int32\n)[1:]\nif np.max(clu) &gt; 1:  # getting rid of mua and noise\nres = np.genfromtxt(os.path.join(path, basename + \".res.\" + str(s)))\ntmp = np.unique(clu).astype(int)\nidx_clu = tmp[tmp &gt; 1]\nidx_out = np.arange(count, count + len(idx_clu))\nfor j, k in zip(idx_clu, idx_out):\nt = res[clu == j] / fs\nspikes[k] = nap.Ts(t=t, time_units=\"s\")\ngroup.loc[k] = s\ncount += len(idx_clu)\ngroup = group - 1  # better to start it a 0\nself.spikes = nap.TsGroup(\nspikes, time_support=time_support, time_units=\"s\", group=group\n)\n# adding some information to help parse the neurons\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_neurosuite_xml","title":"<code>load_neurosuite_xml(path)</code>","text":"<p>path should be the folder session containing the XML file</p> <p>Function reads : 1. the number of channels 2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder     eeg file first if both are present or both are absent 3. the mappings shanks to channels as a dict</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>The path to the data</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If path does not contain the xml file.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_neurosuite_xml(self, path):\n\"\"\"\n    path should be the folder session containing the XML file\n    Function reads :\n    1. the number of channels\n    2. the sampling frequency of the dat file or the eeg file depending of what is present in the folder\n        eeg file first if both are present or both are absent\n    3. the mappings shanks to channels as a dict\n    Parameters\n    ----------\n    path: str\n        The path to the data\n    Raises\n    ------\n    RuntimeError\n        If path does not contain the xml file.\n    \"\"\"\nlistdir = os.listdir(path)\nxmlfiles = [f for f in listdir if f.endswith(\".xml\")]\nif not len(xmlfiles):\nraise RuntimeError(\"Path {} contains no xml files;\".format(path))\nsys.exit()\nnew_path = os.path.join(path, xmlfiles[0])\nself.xmldoc = minidom.parse(new_path)\nself.nChannels = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"nChannels\")[0]\n.firstChild.data\n)\nself.fs_dat = int(\nself.xmldoc.getElementsByTagName(\"acquisitionSystem\")[0]\n.getElementsByTagName(\"samplingRate\")[0]\n.firstChild.data\n)\nself.fs_eeg = int(\nself.xmldoc.getElementsByTagName(\"fieldPotentials\")[0]\n.getElementsByTagName(\"lfpSamplingRate\")[0]\n.firstChild.data\n)\nself.group_to_channel = {}\ngroups = (\nself.xmldoc.getElementsByTagName(\"anatomicalDescription\")[0]\n.getElementsByTagName(\"channelGroups\")[0]\n.getElementsByTagName(\"group\")\n)\nfor i in range(len(groups)):\nself.group_to_channel[i] = np.array(\n[\nint(child.firstChild.data)\nfor child in groups[i].getElementsByTagName(\"channel\")\n]\n)\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.save_data","title":"<code>save_data(path)</code>","text":"<p>Save the data to NWB format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the data</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def save_data(self, path):\n\"\"\"\n    Save the data to NWB format.\n    Parameters\n    ----------\n    path : str\n        The path to save the data\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.group_to_channel:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.group_to_channel[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_nwb_spikes","title":"<code>load_nwb_spikes(path)</code>","text":"<p>Read the NWB spikes to extract the spike times.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the data</p> required <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_nwb_spikes(self, path):\n\"\"\"\n    Read the NWB spikes to extract the spike times.\n    Parameters\n    ----------\n    path : str\n        The path to the data\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn False\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_lfp","title":"<code>load_lfp(filename=None, channel=None, extension='.eeg', frequency=1250.0, precision='int16', bytes_size=2)</code>","text":"<p>Load the LFP.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str, optional</code> <p>The filename of the lfp file. It can be useful it multiple dat files are present in the data directory</p> <code>None</code> <code>channel</code> <code>int or list of int, optional</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>extension</code> <code>str, optional</code> <p>The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> <code>'.eeg'</code> <code>frequency</code> <code>float, optional</code> <p>Default 1250 Hz for the eeg file</p> <code>1250.0</code> <code>precision</code> <code>str, optional</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the lfp file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Load the LFP.\n    Parameters\n    ----------\n    filename : str, optional\n        The filename of the lfp file.\n        It can be useful it multiple dat files are present in the data directory\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    frequency : float, optional\n        Default 1250 Hz for the eeg file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the lfp file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    \"\"\"\nif filename is not None:\nfilepath = os.path.join(self.path, filename)\nelse:\nlistdir = os.listdir(self.path)\neegfile = [f for f in listdir if f.endswith(extension)]\nif not len(eegfile):\nraise RuntimeError(\n\"Path {} contains no {} files;\".format(self.path, extension)\n)\nfilepath = os.path.join(self.path, eegfile[0])\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.read_neuroscope_intervals","title":"<code>read_neuroscope_intervals(name=None, path2file=None)</code>","text":"<p>This function reads .evt files in which odd raws indicate the beginning of the time series and the even raws are the ends. If the file is present in the nwb, provide the just the name. If the file is not present in the nwb, it loads the events from the nwb directory. If just the path is provided but not the name, it takes the name from the file.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <p>name of the epoch in the nwb file, e.g. \"rem\" or desired name save the data in the nwb.</p> <code>None</code> <p>path2file: str     Path of the file you want to load.</p> <p>Returns:</p> Type Description <code>IntervalSet</code> <p>Contains two columns corresponding to the start and end of the intervals.</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def read_neuroscope_intervals(self, name=None, path2file=None):\n\"\"\"\n    This function reads .evt files in which odd raws indicate the beginning\n    of the time series and the even raws are the ends.\n    If the file is present in the nwb, provide the just the name. If the file\n    is not present in the nwb, it loads the events from the nwb directory.\n    If just the path is provided but not the name, it takes the name from the file.\n    Parameters\n    ----------\n    name: str\n        name of the epoch in the nwb file, e.g. \"rem\" or desired name save\n        the data in the nwb.\n    path2file: str\n        Path of the file you want to load.\n    Returns\n    -------\n    IntervalSet\n        Contains two columns corresponding to the start and end of the intervals.\n    \"\"\"\nif name:\nisets = self.load_nwb_intervals(name)\nif isinstance(isets, nap.IntervalSet):\nreturn isets\nif name is not None and path2file is None:\npath2file = os.path.join(self.path, self.basename + \".\" + name + \".evt\")\nif path2file is not None:\ntry:\n# df = pd.read_csv(path2file, delimiter=' ', usecols = [0], header = None)\ntmp = np.genfromtxt(path2file)[:, 0]\ndf = tmp.reshape(len(tmp) // 2, 2)\nexcept ValueError:\nprint(\"specify a valid name\")\nisets = nap.IntervalSet(df[:, 0], df[:, 1], time_units=\"ms\")\nif name is None:\nname = path2file.split(\".\")[-2]\nprint(\"*** saving file in the nwb as\", name)\nself.save_nwb_intervals(isets, name)\nelse:\nraise ValueError(\"specify a valid path\")\nreturn isets\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.write_neuroscope_intervals","title":"<code>write_neuroscope_intervals(extension, isets, name)</code>","text":"<p>Write events to load with neuroscope (e.g. ripples start and ends)</p> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>The extension of the file (e.g. basename.evt.py.rip)</p> required <code>isets</code> <code>IntervalSet</code> <p>The IntervalSet to write</p> required <code>name</code> <code>str</code> <p>The name of the events (e.g. Ripples)</p> required Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def write_neuroscope_intervals(self, extension, isets, name):\n\"\"\"Write events to load with neuroscope (e.g. ripples start and ends)\n    Parameters\n    ----------\n    extension : str\n        The extension of the file (e.g. basename.evt.py.rip)\n    isets : IntervalSet\n        The IntervalSet to write\n    name : str\n        The name of the events (e.g. Ripples)\n    \"\"\"\nstart = isets.as_units(\"ms\")[\"start\"].values\nends = isets.as_units(\"ms\")[\"end\"].values\ndatatowrite = np.vstack((start, ends)).T.flatten()\nn = len(isets)\ntexttowrite = np.vstack(\n(\n(np.repeat(np.array([name + \" start\"]), n)),\n(np.repeat(np.array([name + \" end\"]), n)),\n)\n).T.flatten()\nevt_file = os.path.join(self.path, self.basename + extension)\nf = open(evt_file, \"w\")\nfor t, n in zip(datatowrite, texttowrite):\nf.writelines(\"{:1.6f}\".format(t) + \"\\t\" + n + \"\\n\")\nf.close()\nreturn\n</code></pre>"},{"location":"reference/io/neurosuite/#pynapple.io.neurosuite.NeuroSuite.load_mean_waveforms","title":"<code>load_mean_waveforms(epoch=None, waveform_window=None, spike_count=1000)</code>","text":"<p>Load the mean waveforms from a dat file.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>IntervalSet</code> <p>default = None Restrict spikes to an epoch.</p> <code>None</code> <code>waveform_window</code> <code>IntervalSet</code> <p>default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms') Limit waveform extraction before and after spike time</p> <code>None</code> <code>spike_count</code> <code>int</code> <p>default = 1000 Number of spikes used per neuron for the calculation of waveforms</p> <code>1000</code> <p>Returns:</p> Type Description <code>dictionary</code> <p>the waveforms for all neurons</p> <code>pandas.Series</code> <p>the channel with the maximum waveform for each neuron</p> Source code in <code>pynapple/io/neurosuite.py</code> <pre><code>def load_mean_waveforms(self, epoch=None, waveform_window=None, spike_count=1000):\n\"\"\"\n    Load the mean waveforms from a dat file.\n    Parameters\n    ----------\n    epoch : IntervalSet\n        default = None\n        Restrict spikes to an epoch.\n    waveform_window : IntervalSet\n        default interval nap.IntervalSet(start = -0.0005, end = 0.001, time_units = 'ms')\n        Limit waveform extraction before and after spike time\n    spike_count : int\n        default = 1000\n        Number of spikes used per neuron for the calculation of waveforms\n    Returns\n    -------\n    dictionary\n        the waveforms for all neurons\n    pandas.Series\n        the channel with the maximum waveform for each neuron\n    \"\"\"\nif not isinstance(waveform_window, nap.IntervalSet):\nwaveform_window = nap.IntervalSet(start=-0.5, end=1, time_units=\"ms\")\nspikes = self.spikes\nif not os.path.exists(self.path):  # check if path exists\nprint(\"The path \" + self.path + \" doesn't exist; Exiting ...\")\nsys.exit()\n# Load XML INFO\nself.load_neurosuite_xml(self.path)\nn_channels = self.nChannels\nfs = self.fs_dat\ngroup_to_channel = self.group_to_channel\ngroup = spikes.get_info(\"group\")\n# Check if there is an epoch, restrict spike times to epoch\nif epoch is not None:\nif type(epoch) is not nap.IntervalSet:\nprint(\"Epoch must be an IntervalSet\")\nsys.exit()\nelse:\nprint(\"Restricting spikes to epoch\")\nspikes = spikes.restrict(epoch)\nepstart = int(epoch.as_units(\"s\")[\"start\"].values[0] * fs)\nepend = int(epoch.as_units(\"s\")[\"end\"].values[0] * fs)\n# Find dat file\nfiles = os.listdir(self.path)\ndat_files = np.sort([f for f in files if \"dat\" in f and f[0] != \".\"])\n# Need n_samples collected in the entire recording from dat file to load\nfile = os.path.join(self.path, dat_files[0])\nf = open(\nfile, \"rb\"\n)  # open file to get number of samples collected in the entire recording\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nf.close()\n# map to memory all samples for all channels, channels are numbered according to neuroscope number\nfp = np.memmap(file, np.int16, \"r\", shape=(n_samples, n_channels))\n# convert spike times to spikes in sample number\nsample_spikes = {\nneuron: (spikes[neuron].as_units(\"s\").index.values * fs).astype(\"int\")\nfor neuron in spikes\n}\n# prep for waveforms\noverlap = int(\nwaveform_window.tot_length(time_units=\"s\")\n)  # one spike's worth of overlap between windows\nwaveform_window = abs(np.array(waveform_window.as_units(\"s\"))[0] * fs).astype(\nint\n)  # convert time to sample number\nneuron_waveforms = {\nn: np.zeros([np.sum(waveform_window), len(group_to_channel[group[n]])])\nfor n in sample_spikes\n}\n# divide dat file into batches that slightly overlap for faster loading\nbatch_size = 3000000\nwindows = np.arange(0, int(endoffile / n_channels / bytes_size), batch_size)\nif epoch is not None:\nprint(\"Restricting dat file to epoch\")\nwindows = windows[(windows &gt;= epstart) &amp; (windows &lt;= epend)]\nbatches = []\nfor (\ni\n) in windows:  # make overlapping batches from the beginning to end of recording\nif i == windows[-1]:  # the last batch cannot overlap with the next one\nbatches.append([i, n_samples])\nelse:\nbatches.append([i, i + batch_size + overlap])\nbatches = [np.int32(batch) for batch in batches]\nsample_counted_spikes = {}\nfor index, neuron in enumerate(sample_spikes):\nif len(sample_spikes[neuron]) &gt;= spike_count:\nsample_counted_spikes[neuron] = np.array(\nnp.random.choice(list(sample_spikes[neuron]), spike_count)\n)\nelif len(sample_spikes[neuron]) &lt; spike_count:\nprint(\n\"Not enough spikes in neuron \" + str(index) + \"... using all spikes\"\n)\nsample_counted_spikes[neuron] = sample_spikes[neuron]\n# Make one array containing all selected spike times of all neurons - will be used to check for spikes before loading dat file\nspike_check = np.array(\n[\nint(spikes_neuron)\nfor spikes_neuron in sample_counted_spikes[neuron]\nfor neuron in sample_counted_spikes\n]\n)\nfor index, timestep in enumerate(batches):\nprint(\nf\"Extracting waveforms from dat file: window {index+1} / {len(windows)}\",\nend=\"\\r\",\n)\nif (\nlen(\nspike_check[\n(timestep[0] &lt; spike_check) &amp; (timestep[1] &gt; spike_check)\n]\n)\n== 0\n):\ncontinue  # if there are no spikes for any neurons in this batch, skip and go to the next one\n# Load dat file for timestep\ntmp = pd.DataFrame(\ndata=fp[timestep[0] : timestep[1], :],\ncolumns=np.arange(n_channels),\nindex=range(timestep[0], timestep[1]),\n)  # load dat file\n# Check if any spikes are present\nfor neuron in sample_counted_spikes:\nneurontmp = sample_counted_spikes[neuron]\ntmp2 = neurontmp[(timestep[0] &lt; neurontmp) &amp; (timestep[1] &gt; neurontmp)]\nif len(neurontmp) == 0:\ncontinue  # skip neuron if it has no spikes in this batch\ntmpn = tmp[\ngroup_to_channel[group[neuron]]\n]  # restrict dat file to the channel group of the neuron\nfor time in tmp2:  # add each spike waveform to neuron_waveform\nspikewindow = tmpn.loc[\ntime - waveform_window[0] : time + waveform_window[1] - 1\n]  # waveform for this spike time\ntry:\nneuron_waveforms[neuron] += spikewindow.values\nexcept (\nException\n):  # ignore if full waveform is not present in this batch\npass\nmeanwf = {\nn: pd.DataFrame(\ndata=np.array(neuron_waveforms[n]) / spike_count,\ncolumns=np.arange(len(group_to_channel[group[n]])),\nindex=np.array(np.arange(-waveform_window[0], waveform_window[1])) / fs,\n)\nfor n in sample_counted_spikes\n}\n# find the max channel for each neuron\nmaxch = pd.Series(\ndata=[meanwf[n][meanwf[n].loc[0].idxmin()].name for n in meanwf],\nindex=spikes.keys(),\n)\nreturn meanwf, maxch\n</code></pre>"},{"location":"reference/io/ophys_gui/","title":"Ophys gui","text":""},{"location":"reference/io/phy/","title":"Phy","text":"<p>Class and functions for loading data processed with Phy2</p> <p>@author: Sara Mahallati, Guillaume Viejo</p>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy","title":"<code>Phy</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for Phy data</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>class Phy(BaseLoader):\n\"\"\"\n    Loader for Phy data\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Instantiate the data class from a Phy folder.\n        Parameters\n        ----------\n        path : str or Path object\n            The path to the data.\n        \"\"\"\nself.time_support = None\nself.sample_rate = None\nself.n_channels_dat = None\nself.channel_map = None\nself.ch_to_sh = None\nself.spikes = None\nself.channel_positions = None\nsuper().__init__(path)\n# This path stuff should happen only once in the parent class\nself.path = Path(path)\nself.basename = self.path.name\nself.nwb_path = self.path / \"pynapplenwb\"\n# from what I can see in the loading function, only one nwb file per folder:\ntry:\nself.nwb_file = list(self.nwb_path.glob(\"*.nwb\"))[0]\nexcept IndexError:\nself.nwb_file = None\n# Need to check if nwb file exists and if data are there\n# if self.path is not None:  -&gt; are there any cases where this is None?\nif self.nwb_file is not None:\nloaded_spikes = self.load_nwb_spikes()\nif loaded_spikes is not None:\nreturn\n# Bypass if data have already been transferred to nwb\nself.load_phy_params()\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.channel_map)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_phy_spikes(self.time_support)\nself.save_data()\napp.quit()\ndef load_phy_params(self):\n\"\"\"\n        path should be the folder session containing the params.py file\n        Function reads :\n        1. the number of channels\n        2. the sampling frequency of the dat file\n        Raises\n        ------\n        AssertionError\n            If path does not contain the params file or channel_map.npy\n        \"\"\"\nassert (\nself.path / \"params.py\"\n).exists(), f\"Can't find params.py in {self.path}\"\n# It is strongly recommended not to conflate parameters and code! Also, there's a library called params.\n# I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py!\n# In this way we just read the file, and we don't have to add to sys to import...\n# TODO maybe remove this\nsys.path.append(str(self.path))\nimport params as params\nself.sample_rate = params.sample_rate\nself.n_channels_dat = params.n_channels_dat\nassert (\nself.path / \"channel_map.npy\"\n).exists(), f\"Can't find channel_map.npy in {self.path}\"\nchannel_map = np.load(self.path / \"channel_map.npy\")\nif (self.path / \"channel_shanks.npy\").exists():\nchannel_shank = np.load(self.path / \"channel_shanks.npy\")\nn_shanks = len(np.unique(channel_shank))\nself.channel_map = {\ni: channel_map[channel_shank == i] for i in range(n_shanks)\n}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=channel_shank.flatten(),\n)\nelse:\nself.channel_map = {i: channel_map[i] for i in range(len(channel_map))}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=np.hstack(\n[\nnp.ones(len(channel_map[i]), dtype=int) * i\nfor i in range(len(channel_map))\n]\n),\n)\nreturn\ndef load_phy_spikes(self, time_support=None):\n\"\"\"\n        Load Phy spike times and convert to NWB.\n        Instantiate automatically a TsGroup object.\n        The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv\n        Parameters\n        ----------\n        path : Path object\n            The path to the data\n        time_support : IntevalSet, optional\n            The time support of the data\n        Raises\n        ------\n        RuntimeError\n            If files are missing.\n            The function needs :\n            - cluster_info.tsv or cluster_group.tsv\n            - spike_times.npy\n            - spike_clusters.npy\n            - channel_positions.npy\n            - templates.npy\n        \"\"\"\n# Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used:\nhas_cluster_info = False\nif (self.path / \"cluster_info.tsv\").exists():\ncluster_info_file = self.path / \"cluster_info.tsv\"\nhas_cluster_info = True\nelif (self.path / \"cluster_group.tsv\").exists():\ncluster_info_file = self.path / \"cluster_group.tsv\"\nelse:\nraise RuntimeError(\n\"Can't find cluster_info.tsv or cluster_group.tsv in {};\".format(\nself.path\n)\n)\ncluster_info = pd.read_csv(cluster_info_file, sep=\"\\t\", index_col=\"cluster_id\")\n# In my processed data with KiloSort 3.0, the column is named KSLabel\nif \"group\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.group == \"good\"].index.values\nelif \"KSLabel\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.KSLabel == \"good\"].index.values\nelse:\nraise RuntimeError(\n\"Can't find column group or KSLabel in {};\".format(cluster_info_file)\n)\nspike_times = np.load(self.path / \"spike_times.npy\")\nspike_clusters = np.load(self.path / \"spike_clusters.npy\")\nspikes = {}\nfor n in cluster_id_good:\nspikes[n] = nap.Ts(\nt=spike_times[spike_clusters == n] / self.sample_rate,\ntime_support=time_support,\n)\nself.spikes = nap.TsGroup(spikes, time_support=time_support)\n# Adding the position of the electrodes in case\nself.channel_positions = np.load(self.path / \"channel_positions.npy\")\n# Adding shank group info from cluster_info if present\nif has_cluster_info:\ngroup = cluster_info.loc[cluster_id_good, \"sh\"]\nself.spikes.set_info(group=group)\nelse:\ntemplate = np.load(self.path / \"templates.npy\")\ntemplate = template[cluster_id_good]\nch = np.power(template, 2).max(1).argmax(1)\ngroup = pd.Series(index=cluster_id_good, data=self.ch_to_sh[ch].values)\nself.spikes.set_info(group=group)\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\ndef save_data(self):\n\"\"\"Save the data to NWB format.\"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.channel_map:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.channel_map[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_nwb_spikes(self):\n\"\"\"Read the NWB spikes to extract the spike times.\n        Returns\n        -------\n        TYPE\n            Description\n        \"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn None\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\ndef load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n        Load the LFP.\n        Parameters\n        ----------\n        filename : str, optional\n            The filename of the lfp file.\n            It can be useful it multiple dat files are present in the data directory\n        channel : int or list of int, optional\n            The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n        extension : str, optional\n            The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n        frequency : float, optional\n            Default 1250 Hz for the eeg file\n        precision : str, optional\n            The precision of the binary file\n        bytes_size : int, optional\n            Bytes size of the lfp file\n        Raises\n        ------\n        RuntimeError\n            If can't find the lfp/eeg/dat file\n        Returns\n        -------\n        Tsd or TsdFrame\n            The lfp in a time series format\n        \"\"\"\nif filename is not None:\nfilepath = self.path / filename\nelse:\ntry:\nfilepath = list(self.path.glob(f\"*{extension}\"))[0]\nexcept IndexError:\nraise RuntimeError(f\"Path {self.path} contains no {extension} files;\")\n# is it possible that this is a leftover from neurosuite data?\n# This is not implemented for this class.\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.__init__","title":"<code>__init__(path)</code>","text":"<p>Instantiate the data class from a Phy folder.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path object</code> <p>The path to the data.</p> required Source code in <code>pynapple/io/phy.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Instantiate the data class from a Phy folder.\n    Parameters\n    ----------\n    path : str or Path object\n        The path to the data.\n    \"\"\"\nself.time_support = None\nself.sample_rate = None\nself.n_channels_dat = None\nself.channel_map = None\nself.ch_to_sh = None\nself.spikes = None\nself.channel_positions = None\nsuper().__init__(path)\n# This path stuff should happen only once in the parent class\nself.path = Path(path)\nself.basename = self.path.name\nself.nwb_path = self.path / \"pynapplenwb\"\n# from what I can see in the loading function, only one nwb file per folder:\ntry:\nself.nwb_file = list(self.nwb_path.glob(\"*.nwb\"))[0]\nexcept IndexError:\nself.nwb_file = None\n# Need to check if nwb file exists and if data are there\n# if self.path is not None:  -&gt; are there any cases where this is None?\nif self.nwb_file is not None:\nloaded_spikes = self.load_nwb_spikes()\nif loaded_spikes is not None:\nreturn\n# Bypass if data have already been transferred to nwb\nself.load_phy_params()\napp = App()\nwindow = EphysGUI(app, path=path, groups=self.channel_map)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ephys_information = window.ephys_information\nself.load_phy_spikes(self.time_support)\nself.save_data()\napp.quit()\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_phy_params","title":"<code>load_phy_params()</code>","text":"<p>path should be the folder session containing the params.py file</p> <p>Function reads : 1. the number of channels 2. the sampling frequency of the dat file</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If path does not contain the params file or channel_map.npy</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_phy_params(self):\n\"\"\"\n    path should be the folder session containing the params.py file\n    Function reads :\n    1. the number of channels\n    2. the sampling frequency of the dat file\n    Raises\n    ------\n    AssertionError\n        If path does not contain the params file or channel_map.npy\n    \"\"\"\nassert (\nself.path / \"params.py\"\n).exists(), f\"Can't find params.py in {self.path}\"\n# It is strongly recommended not to conflate parameters and code! Also, there's a library called params.\n# I would recommend putting in the folder a file called params.json, or .txt, or .yml, but not .py!\n# In this way we just read the file, and we don't have to add to sys to import...\n# TODO maybe remove this\nsys.path.append(str(self.path))\nimport params as params\nself.sample_rate = params.sample_rate\nself.n_channels_dat = params.n_channels_dat\nassert (\nself.path / \"channel_map.npy\"\n).exists(), f\"Can't find channel_map.npy in {self.path}\"\nchannel_map = np.load(self.path / \"channel_map.npy\")\nif (self.path / \"channel_shanks.npy\").exists():\nchannel_shank = np.load(self.path / \"channel_shanks.npy\")\nn_shanks = len(np.unique(channel_shank))\nself.channel_map = {\ni: channel_map[channel_shank == i] for i in range(n_shanks)\n}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=channel_shank.flatten(),\n)\nelse:\nself.channel_map = {i: channel_map[i] for i in range(len(channel_map))}\nself.ch_to_sh = pd.Series(\nindex=channel_map.flatten(),\ndata=np.hstack(\n[\nnp.ones(len(channel_map[i]), dtype=int) * i\nfor i in range(len(channel_map))\n]\n),\n)\nreturn\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_phy_spikes","title":"<code>load_phy_spikes(time_support=None)</code>","text":"<p>Load Phy spike times and convert to NWB. Instantiate automatically a TsGroup object. The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path object</code> <p>The path to the data</p> required <code>time_support</code> <code>IntevalSet, optional</code> <p>The time support of the data</p> <code>None</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If files are missing. The function needs : - cluster_info.tsv or cluster_group.tsv - spike_times.npy - spike_clusters.npy - channel_positions.npy - templates.npy</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_phy_spikes(self, time_support=None):\n\"\"\"\n    Load Phy spike times and convert to NWB.\n    Instantiate automatically a TsGroup object.\n    The cluster group is taken first from cluster_info.tsv and second from cluster_group.tsv\n    Parameters\n    ----------\n    path : Path object\n        The path to the data\n    time_support : IntevalSet, optional\n        The time support of the data\n    Raises\n    ------\n    RuntimeError\n        If files are missing.\n        The function needs :\n        - cluster_info.tsv or cluster_group.tsv\n        - spike_times.npy\n        - spike_clusters.npy\n        - channel_positions.npy\n        - templates.npy\n    \"\"\"\n# Check if cluster_info.tsv or cluster_group.tsv exists. If both exist, cluster_info.tsv is used:\nhas_cluster_info = False\nif (self.path / \"cluster_info.tsv\").exists():\ncluster_info_file = self.path / \"cluster_info.tsv\"\nhas_cluster_info = True\nelif (self.path / \"cluster_group.tsv\").exists():\ncluster_info_file = self.path / \"cluster_group.tsv\"\nelse:\nraise RuntimeError(\n\"Can't find cluster_info.tsv or cluster_group.tsv in {};\".format(\nself.path\n)\n)\ncluster_info = pd.read_csv(cluster_info_file, sep=\"\\t\", index_col=\"cluster_id\")\n# In my processed data with KiloSort 3.0, the column is named KSLabel\nif \"group\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.group == \"good\"].index.values\nelif \"KSLabel\" in cluster_info.columns:\ncluster_id_good = cluster_info[cluster_info.KSLabel == \"good\"].index.values\nelse:\nraise RuntimeError(\n\"Can't find column group or KSLabel in {};\".format(cluster_info_file)\n)\nspike_times = np.load(self.path / \"spike_times.npy\")\nspike_clusters = np.load(self.path / \"spike_clusters.npy\")\nspikes = {}\nfor n in cluster_id_good:\nspikes[n] = nap.Ts(\nt=spike_times[spike_clusters == n] / self.sample_rate,\ntime_support=time_support,\n)\nself.spikes = nap.TsGroup(spikes, time_support=time_support)\n# Adding the position of the electrodes in case\nself.channel_positions = np.load(self.path / \"channel_positions.npy\")\n# Adding shank group info from cluster_info if present\nif has_cluster_info:\ngroup = cluster_info.loc[cluster_id_good, \"sh\"]\nself.spikes.set_info(group=group)\nelse:\ntemplate = np.load(self.path / \"templates.npy\")\ntemplate = template[cluster_id_good]\nch = np.power(template, 2).max(1).argmax(1)\ngroup = pd.Series(index=cluster_id_good, data=self.ch_to_sh[ch].values)\nself.spikes.set_info(group=group)\nnames = pd.Series(\nindex=group.index,\ndata=[self.ephys_information[group.loc[i]][\"name\"] for i in group.index],\n)\nif ~np.all(names.values == \"\"):\nself.spikes.set_info(name=names)\nlocations = pd.Series(\nindex=group.index,\ndata=[\nself.ephys_information[group.loc[i]][\"location\"] for i in group.index\n],\n)\nif ~np.all(locations.values == \"\"):\nself.spikes.set_info(location=locations)\nreturn\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.save_data","title":"<code>save_data()</code>","text":"<p>Save the data to NWB format.</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def save_data(self):\n\"\"\"Save the data to NWB format.\"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r+\")\nnwbfile = io.read()\nelectrode_groups = {}\nfor g in self.channel_map:\ndevice = nwbfile.create_device(\nname=self.ephys_information[g][\"device\"][\"name\"] + \"-\" + str(g),\ndescription=self.ephys_information[g][\"device\"][\"description\"],\nmanufacturer=self.ephys_information[g][\"device\"][\"manufacturer\"],\n)\nif (\nlen(self.ephys_information[g][\"position\"])\nand type(self.ephys_information[g][\"position\"]) is str\n):\nself.ephys_information[g][\"position\"] = re.split(\n\";|,| \", self.ephys_information[g][\"position\"]\n)\nelif self.ephys_information[g][\"position\"] == \"\":\nself.ephys_information[g][\"position\"] = None\nelectrode_groups[g] = nwbfile.create_electrode_group(\nname=\"group\" + str(g) + \"_\" + self.ephys_information[g][\"name\"],\ndescription=self.ephys_information[g][\"description\"],\nposition=self.ephys_information[g][\"position\"],\nlocation=self.ephys_information[g][\"location\"],\ndevice=device,\n)\nfor idx in self.channel_map[g]:\nnwbfile.add_electrode(\nid=idx,\nx=0.0,\ny=0.0,\nz=0.0,\nimp=0.0,\nlocation=self.ephys_information[g][\"location\"],\nfiltering=\"none\",\ngroup=electrode_groups[g],\n)\n# Adding units\nnwbfile.add_unit_column(\"location\", \"the anatomical location of this unit\")\nnwbfile.add_unit_column(\"group\", \"the group of the unit\")\nfor u in self.spikes.keys():\nnwbfile.add_unit(\nid=u,\nspike_times=self.spikes[u].as_units(\"s\").index.values,\nelectrode_group=electrode_groups[self.spikes.get_info(\"group\").loc[u]],\nlocation=self.ephys_information[self.spikes.get_info(\"group\").loc[u]][\n\"location\"\n],\ngroup=self.spikes.get_info(\"group\").loc[u],\n)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_nwb_spikes","title":"<code>load_nwb_spikes()</code>","text":"<p>Read the NWB spikes to extract the spike times.</p> <p>Returns:</p> Type Description <code>TYPE</code> <p>Description</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_nwb_spikes(self):\n\"\"\"Read the NWB spikes to extract the spike times.\n    Returns\n    -------\n    TYPE\n        Description\n    \"\"\"\nio = NWBHDF5IO(self.nwb_file, \"r\")\nnwbfile = io.read()\nif nwbfile.units is None:\nio.close()\nreturn None\nelse:\nunits = nwbfile.units.to_dataframe()\nspikes = {\nn: nap.Ts(t=units.loc[n, \"spike_times\"], time_units=\"s\")\nfor n in units.index\n}\nself.spikes = nap.TsGroup(\nspikes,\ntime_support=self.time_support,\ntime_units=\"s\",\ngroup=units[\"group\"],\n)\nif ~np.all(units[\"location\"] == \"\"):\nself.spikes.set_info(location=units[\"location\"])\nio.close()\nreturn True\n</code></pre>"},{"location":"reference/io/phy/#pynapple.io.phy.Phy.load_lfp","title":"<code>load_lfp(filename=None, channel=None, extension='.eeg', frequency=1250.0, precision='int16', bytes_size=2)</code>","text":"<p>Load the LFP.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str, optional</code> <p>The filename of the lfp file. It can be useful it multiple dat files are present in the data directory</p> <code>None</code> <code>channel</code> <code>int or list of int, optional</code> <p>The channel(s) to load. If None return a memory map of the dat file to avoid memory error</p> <code>None</code> <code>extension</code> <code>str, optional</code> <p>The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match</p> <code>'.eeg'</code> <code>frequency</code> <code>float, optional</code> <p>Default 1250 Hz for the eeg file</p> <code>1250.0</code> <code>precision</code> <code>str, optional</code> <p>The precision of the binary file</p> <code>'int16'</code> <code>bytes_size</code> <code>int, optional</code> <p>Bytes size of the lfp file</p> <code>2</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If can't find the lfp/eeg/dat file</p> <p>Returns:</p> Type Description <code>Tsd or TsdFrame</code> <p>The lfp in a time series format</p> Source code in <code>pynapple/io/phy.py</code> <pre><code>def load_lfp(\nself,\nfilename=None,\nchannel=None,\nextension=\".eeg\",\nfrequency=1250.0,\nprecision=\"int16\",\nbytes_size=2,\n):\n\"\"\"\n    Load the LFP.\n    Parameters\n    ----------\n    filename : str, optional\n        The filename of the lfp file.\n        It can be useful it multiple dat files are present in the data directory\n    channel : int or list of int, optional\n        The channel(s) to load. If None return a memory map of the dat file to avoid memory error\n    extension : str, optional\n        The file extenstion (.eeg, .dat, .lfp). Make sure the frequency match\n    frequency : float, optional\n        Default 1250 Hz for the eeg file\n    precision : str, optional\n        The precision of the binary file\n    bytes_size : int, optional\n        Bytes size of the lfp file\n    Raises\n    ------\n    RuntimeError\n        If can't find the lfp/eeg/dat file\n    Returns\n    -------\n    Tsd or TsdFrame\n        The lfp in a time series format\n    \"\"\"\nif filename is not None:\nfilepath = self.path / filename\nelse:\ntry:\nfilepath = list(self.path.glob(f\"*{extension}\"))[0]\nexcept IndexError:\nraise RuntimeError(f\"Path {self.path} contains no {extension} files;\")\n# is it possible that this is a leftover from neurosuite data?\n# This is not implemented for this class.\nself.load_neurosuite_xml(self.path)\nn_channels = int(self.nChannels)\nf = open(filepath, \"rb\")\nstartoffile = f.seek(0, 0)\nendoffile = f.seek(0, 2)\nbytes_size = 2\nn_samples = int((endoffile - startoffile) / n_channels / bytes_size)\nduration = n_samples / frequency\nf.close()\nfp = np.memmap(filepath, np.int16, \"r\", shape=(n_samples, n_channels))\ntimestep = np.arange(0, n_samples) / frequency\ntime_support = nap.IntervalSet(start=0, end=duration, time_units=\"s\")\nif channel is None:\nreturn nap.TsdFrame(\nt=timestep, d=fp, time_units=\"s\", time_support=time_support\n)\nelif type(channel) is int:\nreturn nap.Tsd(\nt=timestep, d=fp[:, channel], time_units=\"s\", time_support=time_support\n)\nelif type(channel) is list:\nreturn nap.TsdFrame(\nt=timestep,\nd=fp[:, channel],\ntime_units=\"s\",\ntime_support=time_support,\ncolumns=channel,\n)\n</code></pre>"},{"location":"reference/io/suite2p/","title":"Suite2p","text":"<p>Loader for Suite2P https://github.com/MouseLand/suite2p</p>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P","title":"<code>Suite2P</code>","text":"<p>         Bases: <code>BaseLoader</code></p> <p>Loader for data processed with Suite2P.</p> <p>Pynapple will try to look for data in this order :</p> <ol> <li> <p>pynapplenwb/session_name.nwb</p> </li> <li> <p>suite2p/plane/.npy</p> </li> </ol> <p>Attributes:</p> Name Type Description <code>F</code> <code>TsdFrame</code> <p>Fluorescence traces (timepoints x ROIs) for all planes</p> <code>Fneu</code> <code>TsdFrame</code> <p>Neuropil fluorescence traces (timepoints x ROIs) for all planes</p> <code>spks</code> <code>TsdFrame</code> <p>Deconvolved traces (timepoints x ROIS) for all planes</p> <code>plane_info</code> <code>pandas.DataFrame</code> <p>Contains plane identity of each cell</p> <code>stats</code> <code>dict</code> <p>dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells (Can be smaller when loading from the NWB file)</p> <code>ops</code> <code>dict</code> <p>Parameters from Suite2p. (Can be smaller when loading from the NWB file)</p> <code>iscell</code> <code>numpy.ndarray</code> <p>Cell classification</p> Source code in <code>pynapple/io/suite2p.py</code> <pre><code>class Suite2P(BaseLoader):\n\"\"\"Loader for data processed with Suite2P.\n    Pynapple will try to look for data in this order :\n    1. pynapplenwb/session_name.nwb\n    2. suite2p/plane*/*.npy\n    Attributes\n    ----------\n    F : TsdFrame\n        Fluorescence traces (timepoints x ROIs) for all planes\n    Fneu : TsdFrame\n        Neuropil fluorescence traces (timepoints x ROIs) for all planes\n    spks : TsdFrame\n        Deconvolved traces (timepoints x ROIS) for all planes\n    plane_info : pandas.DataFrame\n        Contains plane identity of each cell\n    stats : dict\n        dictionnay of statistics from stat.npy for each planes only for the neurons that were classified as cells\n        (Can be smaller when loading from the NWB file)\n    ops : dict\n        Parameters from Suite2p. (Can be smaller when loading from the NWB file)\n    iscell : numpy.ndarray\n        Cell classification\n    \"\"\"\ndef __init__(self, path):\n\"\"\"\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_suite2p_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_suite2p(path)\nself.save_suite2p_nwb(path)\ndef load_suite2p(self, path):\n\"\"\"\n        Looking for suite2/plane*\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.path_suite2p = os.path.join(path, \"suite2p\")\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ndata = {\n\"F\": [],\n\"Fneu\": [],\n\"spks\": [],\n}\nplane_info = []\nself.stats = {}\nself.pops = {}\nself.iscells = {}\nself.planes = []\nif os.path.exists(self.path_suite2p):\nplanes = glob.glob(os.path.join(self.path_suite2p, \"plane*\"))\nif len(planes):\n# count = 0\nfor plane_dir in planes:\nn = int(os.path.basename(plane_dir)[-1])\nself.planes.append(n)\n# Loading iscell.npy\ntry:\niscell = np.load(\nos.path.join(plane_dir, \"iscell.npy\"), allow_pickle=True\n)\nidx = np.where(iscell.astype(\"int\")[:, 0])[0]\nplane_info.append(np.ones(len(idx), dtype=\"int\") * n)\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading F.npy, Fneu.py and spks.npy\nfor obj in [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]:\ntry:\nname = obj.split(\".\")[0]\ntmp = np.load(\nos.path.join(plane_dir, obj), allow_pickle=True\n)\ndata[name].append(tmp[idx])\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading stat.npy and ops.npy\ntry:\nstat = np.load(\nos.path.join(plane_dir, \"stat.npy\"), allow_pickle=True\n)\nops = np.load(\nos.path.join(plane_dir, \"ops.npy\"), allow_pickle=True\n).item()\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Saving stat, ops and iscell\nself.stats[n] = stat\nself.pops[n] = ops\nself.iscells[n] = iscell\n# count += len(idx)\nelse:\nwarnings.warn(\n\"Couldn't find planes in %s\" % self.path_suite2p, stacklevel=2\n)\nsys.exit()\nelse:\nwarnings.warn(\"No suite2p folder in %s\" % path, stacklevel=2)\nsys.exit()\n# Calcium transients\ndata[\"F\"] = np.transpose(np.vstack(data[\"F\"]))\ndata[\"Fneu\"] = np.transpose(np.vstack(data[\"Fneu\"]))\ndata[\"spks\"] = np.transpose(np.vstack(data[\"spks\"]))\ntime_index = np.arange(0, len(data[\"F\"])) / self.sampling_rate\nself.F = nap.TsdFrame(t=time_index, d=data[\"F\"])\nself.Fneu = nap.TsdFrame(t=time_index, d=data[\"Fneu\"])\nself.spks = nap.TsdFrame(t=time_index, d=data[\"spks\"])\nself.ops = self.pops[0]\nself.iscell = np.vstack([self.iscells[k] for k in self.iscells.keys()])\n# Metadata\nself.plane_info = pd.DataFrame.from_dict({\"plane\": np.hstack(plane_info)})\nreturn\ndef save_suite2p_nwb(self, path):\n\"\"\"\n        Save the data to NWB. To ensure continuity, this function is based on :\n        https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.\n        Parameters\n        ----------\n        path : str\n            The path of the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nmultiplane = True if len(self.planes) &gt; 1 else False\nops = self.pops[list(self.pops.keys())[0]]\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice = nwbfile.create_device(\nname=self.ophys_information[\"device\"][\"name\"],\ndescription=self.ophys_information[\"device\"][\"description\"],\nmanufacturer=self.ophys_information[\"device\"][\"manufacturer\"],\n)\nimaging_plane = nwbfile.create_imaging_plane(\nname=self.ophys_information[\"ImagingPlane\"][\"name\"],\noptical_channel=OpticalChannel(\nname=self.ophys_information[\"OpticalChannel\"][\"name\"],\ndescription=self.ophys_information[\"OpticalChannel\"][\"description\"],\nemission_lambda=float(\nself.ophys_information[\"OpticalChannel\"][\"emission_lambda\"]\n),\n),\nimaging_rate=self.sampling_rate,\ndescription=self.ophys_information[\"ImagingPlane\"][\"description\"],\ndevice=device,\nexcitation_lambda=float(\nself.ophys_information[\"ImagingPlane\"][\"excitation_lambda\"]\n),\nindicator=self.ophys_information[\"ImagingPlane\"][\"indicator\"],\nlocation=self.ophys_information[\"ImagingPlane\"][\"location\"],\ngrid_spacing=([2.0, 2.0, 30.0] if multiplane else [2.0, 2.0]),\ngrid_spacing_unit=\"microns\",\n)\n# link to external data\nimage_series = TwoPhotonSeries(\nname=\"TwoPhotonSeries\",\ndimension=[ops[\"Ly\"], ops[\"Lx\"]],\nexternal_file=(ops[\"filelist\"] if \"filelist\" in ops else [\"\"]),\nimaging_plane=imaging_plane,\nstarting_frame=[0],\nformat=\"external\",\nstarting_time=0.0,\nrate=ops[\"fs\"] * ops[\"nplanes\"],\n)\nnwbfile.add_acquisition(image_series)\n# processing\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=self.ophys_information[\"PlaneSegmentation\"][\"name\"],\ndescription=self.ophys_information[\"PlaneSegmentation\"][\"description\"],\nimaging_plane=imaging_plane,\n# reference_images=image_series,\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nophys_module.add(img_seg)\nfile_strs = [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]\ntraces = []\nncells = np.zeros(len(self.pops), dtype=np.int_)\nNfr = np.array([self.pops[k][\"nframes\"] for k in self.pops.keys()]).max()\nfor iplane, ops in self.pops.items():\nif iplane == 0:\niscell = self.iscells[iplane]\nfor fstr in file_strs:\ntraces.append(np.load(os.path.join(ops[\"save_path\"], fstr)))\nPlaneCellsIdx = iplane * np.ones(len(iscell))\nelse:\niscell = np.append(\niscell,\nself.iscells[iplane],\naxis=0,\n)\nfor i, fstr in enumerate(file_strs):\ntrace = np.load(os.path.join(ops[\"save_path\"], fstr))\nif trace.shape[1] &lt; Nfr:\nfcat = np.zeros(\n(trace.shape[0], Nfr - trace.shape[1]), \"float32\"\n)\ntrace = np.concatenate((trace, fcat), axis=1)\ntraces[i] = np.append(traces[i], trace, axis=0)\nPlaneCellsIdx = np.append(\nPlaneCellsIdx, iplane * np.ones(len(iscell) - len(PlaneCellsIdx))\n)\nstat = self.stats[iplane]\nncells[iplane] = len(stat)\nfor n in range(ncells[iplane]):\nif multiplane:\npixel_mask = np.array(\n[\nstat[n][\"ypix\"],\nstat[n][\"xpix\"],\niplane * np.ones(stat[n][\"npix\"]),\nstat[n][\"lam\"],\n]\n)\nps.add_roi(voxel_mask=pixel_mask.T)\nelse:\npixel_mask = np.array(\n[stat[n][\"ypix\"], stat[n][\"xpix\"], stat[n][\"lam\"]]\n)\nps.add_roi(pixel_mask=pixel_mask.T)\nps.add_column(\"iscell\", \"two columns - iscell &amp; probcell\", iscell)\nrt_region = []\nfor iplane, ops in self.pops.items():\nif iplane == 0:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(0, ncells[iplane]),\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\nelse:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(\nnp.sum(ncells[:iplane]),\nncells[iplane] + np.sum(ncells[:iplane]),\n)\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\n# FLUORESCENCE (all are required)\nname_strs = [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\nfor i, (fstr, nstr) in enumerate(zip(file_strs, name_strs)):\nfor iplane, ops in self.pops.items():\nroi_resp_series = RoiResponseSeries(\nname=f\"plane{int(iplane)}\",\ndata=traces[i][PlaneCellsIdx == iplane],\nrois=rt_region[iplane],\nunit=\"lumens\",\nrate=ops[\"fs\"],\n)\nif iplane == 0:\nfl = Fluorescence(roi_response_series=roi_resp_series, name=nstr)\nelse:\nfl.add_roi_response_series(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\ndef load_suite2p_nwb(self, path):\n\"\"\"\n        Load suite2p data from NWB\n        Parameters\n        ----------\n        path : str\n            Path to the session\n        \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\nophys = nwbfile.processing[\"ophys\"]\n#################################################################\n# STATS, OPS and ISCELL\n#################################################################\ndims = nwbfile.acquisition[\"TwoPhotonSeries\"].dimension[:]\nself.ops = {\"Ly\": dims[0], \"Lx\": dims[1]}\nself.rate = nwbfile.acquisition[\n\"TwoPhotonSeries\"\n].imaging_plane.imaging_rate\nself.stats = {0: {}}\nself.iscell = ophys[\"ImageSegmentation\"][\"PlaneSegmentation\"][\n\"iscell\"\n].data[:]\ninfo = pd.DataFrame(\ndata=self.iscell[:, 0].astype(\"int\"), columns=[\"iscell\"]\n)\n#################################################################\n# ROIS\n#################################################################\ntry:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"pixel_mask\"]\nmultiplane = False\nexcept Exception:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"voxel_mask\"]\nmultiplane = True\nidx = np.where(self.iscell[:, 0])[0]\ninfo[\"plane\"] = 0\nfor n in range(len(rois)):\nroi = pd.DataFrame(rois[n])\nif \"z\" in roi.columns:\npl = roi[\"z\"][0]\nelse:\npl = 0\ninfo.loc[n, \"plane\"] = pl\nif pl not in self.stats.keys():\nself.stats[pl] = {}\nif n in idx:\nself.stats[pl][n] = {\n\"xpix\": roi[\"y\"].values,\n\"ypix\": roi[\"x\"].values,\n\"lam\": roi[\"weight\"].values,\n}\n#################################################################\n# Time Series\n#################################################################\nfields = np.intersect1d(\n[\"Fluorescence\", \"Neuropil\", \"Deconvolved\"],\nlist(ophys.fields[\"data_interfaces\"].keys()),\n)\nif len(fields) == 0:\nprint(\n\"No \" + \" or \".join([\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]),\n\"found in nwb {}\".format(self.nwbfilepath),\n)\nreturn False\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\ndata = {}\nif multiplane:\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\nelse:\nplanes = [0]\nfor k, name in zip(\n[\"F\", \"Fneu\", \"spks\"], [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n):\ntmp = []\ntimestamps = []\nfor i, n in enumerate(planes):\nif multiplane:\npl = \"plane{}\".format(n)\nelse:\npl = name  # This doesn't make sense\ntokeep = info[\"iscell\"][info[\"plane\"] == n].values == 1\nd = np.transpose(ophys[name][pl].data[:][tokeep])\nif ophys[name][pl].timestamps is not None:\nt = ophys[name][pl].timestamps[:]\nelse:\nt = (np.arange(0, len(d)) / self.rate) + ophys[name][\npl\n].starting_time\ntmp.append(d)\ntimestamps.append(t)\ndata[k] = nap.TsdFrame(t=timestamps[0], d=np.hstack(tmp))\nif \"F\" in data.keys():\nself.F = data[\"F\"]\nif \"Fneu\" in data.keys():\nself.Fneu = data[\"Fneu\"]\nif \"spks\" in data.keys():\nself.spks = data[\"spks\"]\nself.plane_info = pd.DataFrame(\ndata=info[\"plane\"][info[\"iscell\"] == 1].values, columns=[\"plane\"]\n)\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.__init__","title":"<code>__init__(path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def __init__(self, path):\n\"\"\"\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.basename = os.path.basename(path)\nsuper().__init__(path)\n# Need to check if nwb file exists and if data are there\nloading_my_data = True\nif self.path is not None:\nnwb_path = os.path.join(self.path, \"pynapplenwb\")\nif os.path.exists(nwb_path):\nfiles = os.listdir(nwb_path)\nif len([f for f in files if f.endswith(\".nwb\")]):\nsuccess = self.load_suite2p_nwb(path)\nif success:\nloading_my_data = False\n# Bypass if data have already been transfered to nwb\nif loading_my_data:\napp = App()\nwindow = OphysGUI(app, path=path)\napp.mainloop()\ntry:\napp.update()\nexcept Exception:\npass\nif window.status:\nself.ophys_information = window.ophys_information\nself.load_suite2p(path)\nself.save_suite2p_nwb(path)\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p","title":"<code>load_suite2p(path)</code>","text":"<p>Looking for suite2/plane*</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def load_suite2p(self, path):\n\"\"\"\n    Looking for suite2/plane*\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.path_suite2p = os.path.join(path, \"suite2p\")\nself.sampling_rate = float(\nself.ophys_information[\"ImagingPlane\"][\"imaging_rate\"]\n)\ndata = {\n\"F\": [],\n\"Fneu\": [],\n\"spks\": [],\n}\nplane_info = []\nself.stats = {}\nself.pops = {}\nself.iscells = {}\nself.planes = []\nif os.path.exists(self.path_suite2p):\nplanes = glob.glob(os.path.join(self.path_suite2p, \"plane*\"))\nif len(planes):\n# count = 0\nfor plane_dir in planes:\nn = int(os.path.basename(plane_dir)[-1])\nself.planes.append(n)\n# Loading iscell.npy\ntry:\niscell = np.load(\nos.path.join(plane_dir, \"iscell.npy\"), allow_pickle=True\n)\nidx = np.where(iscell.astype(\"int\")[:, 0])[0]\nplane_info.append(np.ones(len(idx), dtype=\"int\") * n)\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading F.npy, Fneu.py and spks.npy\nfor obj in [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]:\ntry:\nname = obj.split(\".\")[0]\ntmp = np.load(\nos.path.join(plane_dir, obj), allow_pickle=True\n)\ndata[name].append(tmp[idx])\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Loading stat.npy and ops.npy\ntry:\nstat = np.load(\nos.path.join(plane_dir, \"stat.npy\"), allow_pickle=True\n)\nops = np.load(\nos.path.join(plane_dir, \"ops.npy\"), allow_pickle=True\n).item()\nexcept OSError as e:\nprint(e)\nsys.exit()\n# Saving stat, ops and iscell\nself.stats[n] = stat\nself.pops[n] = ops\nself.iscells[n] = iscell\n# count += len(idx)\nelse:\nwarnings.warn(\n\"Couldn't find planes in %s\" % self.path_suite2p, stacklevel=2\n)\nsys.exit()\nelse:\nwarnings.warn(\"No suite2p folder in %s\" % path, stacklevel=2)\nsys.exit()\n# Calcium transients\ndata[\"F\"] = np.transpose(np.vstack(data[\"F\"]))\ndata[\"Fneu\"] = np.transpose(np.vstack(data[\"Fneu\"]))\ndata[\"spks\"] = np.transpose(np.vstack(data[\"spks\"]))\ntime_index = np.arange(0, len(data[\"F\"])) / self.sampling_rate\nself.F = nap.TsdFrame(t=time_index, d=data[\"F\"])\nself.Fneu = nap.TsdFrame(t=time_index, d=data[\"Fneu\"])\nself.spks = nap.TsdFrame(t=time_index, d=data[\"spks\"])\nself.ops = self.pops[0]\nself.iscell = np.vstack([self.iscells[k] for k in self.iscells.keys()])\n# Metadata\nself.plane_info = pd.DataFrame.from_dict({\"plane\": np.hstack(plane_info)})\nreturn\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.save_suite2p_nwb","title":"<code>save_suite2p_nwb(path)</code>","text":"<p>Save the data to NWB. To ensure continuity, this function is based on : https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def save_suite2p_nwb(self, path):\n\"\"\"\n    Save the data to NWB. To ensure continuity, this function is based on :\n    https://github.com/MouseLand/suite2p/blob/main/suite2p/io/nwb.py.\n    Parameters\n    ----------\n    path : str\n        The path of the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nmultiplane = True if len(self.planes) &gt; 1 else False\nops = self.pops[list(self.pops.keys())[0]]\nio = NWBHDF5IO(self.nwbfilepath, \"r+\")\nnwbfile = io.read()\ndevice = nwbfile.create_device(\nname=self.ophys_information[\"device\"][\"name\"],\ndescription=self.ophys_information[\"device\"][\"description\"],\nmanufacturer=self.ophys_information[\"device\"][\"manufacturer\"],\n)\nimaging_plane = nwbfile.create_imaging_plane(\nname=self.ophys_information[\"ImagingPlane\"][\"name\"],\noptical_channel=OpticalChannel(\nname=self.ophys_information[\"OpticalChannel\"][\"name\"],\ndescription=self.ophys_information[\"OpticalChannel\"][\"description\"],\nemission_lambda=float(\nself.ophys_information[\"OpticalChannel\"][\"emission_lambda\"]\n),\n),\nimaging_rate=self.sampling_rate,\ndescription=self.ophys_information[\"ImagingPlane\"][\"description\"],\ndevice=device,\nexcitation_lambda=float(\nself.ophys_information[\"ImagingPlane\"][\"excitation_lambda\"]\n),\nindicator=self.ophys_information[\"ImagingPlane\"][\"indicator\"],\nlocation=self.ophys_information[\"ImagingPlane\"][\"location\"],\ngrid_spacing=([2.0, 2.0, 30.0] if multiplane else [2.0, 2.0]),\ngrid_spacing_unit=\"microns\",\n)\n# link to external data\nimage_series = TwoPhotonSeries(\nname=\"TwoPhotonSeries\",\ndimension=[ops[\"Ly\"], ops[\"Lx\"]],\nexternal_file=(ops[\"filelist\"] if \"filelist\" in ops else [\"\"]),\nimaging_plane=imaging_plane,\nstarting_frame=[0],\nformat=\"external\",\nstarting_time=0.0,\nrate=ops[\"fs\"] * ops[\"nplanes\"],\n)\nnwbfile.add_acquisition(image_series)\n# processing\nimg_seg = ImageSegmentation()\nps = img_seg.create_plane_segmentation(\nname=self.ophys_information[\"PlaneSegmentation\"][\"name\"],\ndescription=self.ophys_information[\"PlaneSegmentation\"][\"description\"],\nimaging_plane=imaging_plane,\n# reference_images=image_series,\n)\nophys_module = nwbfile.create_processing_module(\nname=\"ophys\", description=\"optical physiology processed data\"\n)\nophys_module.add(img_seg)\nfile_strs = [\"F.npy\", \"Fneu.npy\", \"spks.npy\"]\ntraces = []\nncells = np.zeros(len(self.pops), dtype=np.int_)\nNfr = np.array([self.pops[k][\"nframes\"] for k in self.pops.keys()]).max()\nfor iplane, ops in self.pops.items():\nif iplane == 0:\niscell = self.iscells[iplane]\nfor fstr in file_strs:\ntraces.append(np.load(os.path.join(ops[\"save_path\"], fstr)))\nPlaneCellsIdx = iplane * np.ones(len(iscell))\nelse:\niscell = np.append(\niscell,\nself.iscells[iplane],\naxis=0,\n)\nfor i, fstr in enumerate(file_strs):\ntrace = np.load(os.path.join(ops[\"save_path\"], fstr))\nif trace.shape[1] &lt; Nfr:\nfcat = np.zeros(\n(trace.shape[0], Nfr - trace.shape[1]), \"float32\"\n)\ntrace = np.concatenate((trace, fcat), axis=1)\ntraces[i] = np.append(traces[i], trace, axis=0)\nPlaneCellsIdx = np.append(\nPlaneCellsIdx, iplane * np.ones(len(iscell) - len(PlaneCellsIdx))\n)\nstat = self.stats[iplane]\nncells[iplane] = len(stat)\nfor n in range(ncells[iplane]):\nif multiplane:\npixel_mask = np.array(\n[\nstat[n][\"ypix\"],\nstat[n][\"xpix\"],\niplane * np.ones(stat[n][\"npix\"]),\nstat[n][\"lam\"],\n]\n)\nps.add_roi(voxel_mask=pixel_mask.T)\nelse:\npixel_mask = np.array(\n[stat[n][\"ypix\"], stat[n][\"xpix\"], stat[n][\"lam\"]]\n)\nps.add_roi(pixel_mask=pixel_mask.T)\nps.add_column(\"iscell\", \"two columns - iscell &amp; probcell\", iscell)\nrt_region = []\nfor iplane, ops in self.pops.items():\nif iplane == 0:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(0, ncells[iplane]),\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\nelse:\nrt_region.append(\nps.create_roi_table_region(\nregion=list(\nnp.arange(\nnp.sum(ncells[:iplane]),\nncells[iplane] + np.sum(ncells[:iplane]),\n)\n),\ndescription=f\"ROIs for plane{int(iplane)}\",\n)\n)\n# FLUORESCENCE (all are required)\nname_strs = [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\nfor i, (fstr, nstr) in enumerate(zip(file_strs, name_strs)):\nfor iplane, ops in self.pops.items():\nroi_resp_series = RoiResponseSeries(\nname=f\"plane{int(iplane)}\",\ndata=traces[i][PlaneCellsIdx == iplane],\nrois=rt_region[iplane],\nunit=\"lumens\",\nrate=ops[\"fs\"],\n)\nif iplane == 0:\nfl = Fluorescence(roi_response_series=roi_resp_series, name=nstr)\nelse:\nfl.add_roi_response_series(roi_response_series=roi_resp_series)\nophys_module.add(fl)\nio.write(nwbfile)\nio.close()\nreturn\n</code></pre>"},{"location":"reference/io/suite2p/#pynapple.io.suite2p.Suite2P.load_suite2p_nwb","title":"<code>load_suite2p_nwb(path)</code>","text":"<p>Load suite2p data from NWB</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the session</p> required Source code in <code>pynapple/io/suite2p.py</code> <pre><code>def load_suite2p_nwb(self, path):\n\"\"\"\n    Load suite2p data from NWB\n    Parameters\n    ----------\n    path : str\n        Path to the session\n    \"\"\"\nself.nwb_path = os.path.join(path, \"pynapplenwb\")\nif not os.path.exists(self.nwb_path):\nraise RuntimeError(\"Path {} does not exist.\".format(self.nwb_path))\nself.nwbfilename = [f for f in os.listdir(self.nwb_path) if \"nwb\" in f][0]\nself.nwbfilepath = os.path.join(self.nwb_path, self.nwbfilename)\nio = NWBHDF5IO(self.nwbfilepath, \"r\")\nnwbfile = io.read()\nif \"ophys\" in nwbfile.processing.keys():\nophys = nwbfile.processing[\"ophys\"]\n#################################################################\n# STATS, OPS and ISCELL\n#################################################################\ndims = nwbfile.acquisition[\"TwoPhotonSeries\"].dimension[:]\nself.ops = {\"Ly\": dims[0], \"Lx\": dims[1]}\nself.rate = nwbfile.acquisition[\n\"TwoPhotonSeries\"\n].imaging_plane.imaging_rate\nself.stats = {0: {}}\nself.iscell = ophys[\"ImageSegmentation\"][\"PlaneSegmentation\"][\n\"iscell\"\n].data[:]\ninfo = pd.DataFrame(\ndata=self.iscell[:, 0].astype(\"int\"), columns=[\"iscell\"]\n)\n#################################################################\n# ROIS\n#################################################################\ntry:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"pixel_mask\"]\nmultiplane = False\nexcept Exception:\nrois = nwbfile.processing[\"ophys\"][\"ImageSegmentation\"][\n\"PlaneSegmentation\"\n][\"voxel_mask\"]\nmultiplane = True\nidx = np.where(self.iscell[:, 0])[0]\ninfo[\"plane\"] = 0\nfor n in range(len(rois)):\nroi = pd.DataFrame(rois[n])\nif \"z\" in roi.columns:\npl = roi[\"z\"][0]\nelse:\npl = 0\ninfo.loc[n, \"plane\"] = pl\nif pl not in self.stats.keys():\nself.stats[pl] = {}\nif n in idx:\nself.stats[pl][n] = {\n\"xpix\": roi[\"y\"].values,\n\"ypix\": roi[\"x\"].values,\n\"lam\": roi[\"weight\"].values,\n}\n#################################################################\n# Time Series\n#################################################################\nfields = np.intersect1d(\n[\"Fluorescence\", \"Neuropil\", \"Deconvolved\"],\nlist(ophys.fields[\"data_interfaces\"].keys()),\n)\nif len(fields) == 0:\nprint(\n\"No \" + \" or \".join([\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]),\n\"found in nwb {}\".format(self.nwbfilepath),\n)\nreturn False\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\ndata = {}\nif multiplane:\nkeys = ophys[fields[0]].roi_response_series.keys()\nplanes = [int(k[-1]) for k in keys if \"plane\" in k]\nelse:\nplanes = [0]\nfor k, name in zip(\n[\"F\", \"Fneu\", \"spks\"], [\"Fluorescence\", \"Neuropil\", \"Deconvolved\"]\n):\ntmp = []\ntimestamps = []\nfor i, n in enumerate(planes):\nif multiplane:\npl = \"plane{}\".format(n)\nelse:\npl = name  # This doesn't make sense\ntokeep = info[\"iscell\"][info[\"plane\"] == n].values == 1\nd = np.transpose(ophys[name][pl].data[:][tokeep])\nif ophys[name][pl].timestamps is not None:\nt = ophys[name][pl].timestamps[:]\nelse:\nt = (np.arange(0, len(d)) / self.rate) + ophys[name][\npl\n].starting_time\ntmp.append(d)\ntimestamps.append(t)\ndata[k] = nap.TsdFrame(t=timestamps[0], d=np.hstack(tmp))\nif \"F\" in data.keys():\nself.F = data[\"F\"]\nif \"Fneu\" in data.keys():\nself.Fneu = data[\"Fneu\"]\nif \"spks\" in data.keys():\nself.spks = data[\"spks\"]\nself.plane_info = pd.DataFrame(\ndata=info[\"plane\"][info[\"iscell\"] == 1].values, columns=[\"plane\"]\n)\nio.close()\nreturn True\nelse:\nio.close()\nreturn False\n</code></pre>"},{"location":"reference/process/","title":"Process","text":""},{"location":"reference/process/correlograms/","title":"Correlograms","text":""},{"location":"reference/process/correlograms/#pynapple.process.correlograms.cross_correlogram","title":"<code>cross_correlogram(t1, t2, binsize, windowsize)</code>","text":"<p>Performs the discrete cross-correlogram of two time series. The units should be in s for all arguments. Return the firing rate of the series t2 relative to the timings of t1. See compute_crosscorrelogram, compute_autocorrelogram and compute_eventcorrelogram for wrappers of this function.</p> <p>Parameters:</p> Name Type Description Default <code>t1</code> <code>numpy.ndarray</code> <p>The timestamps of the reference time series (in seconds)</p> required <code>t2</code> <code>numpy.ndarray</code> <p>The timestamps of the target time series (in seconds)</p> required <code>binsize</code> <code>float</code> <p>The bin size (in seconds)</p> required <code>windowsize</code> <code>float</code> <p>The window size (in seconds)</p> required <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>The cross-correlogram</p> <code>numpy.ndarray</code> <p>Center of the bins (in s)</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>@jit(nopython=True)\ndef cross_correlogram(t1, t2, binsize, windowsize):\n\"\"\"\n    Performs the discrete cross-correlogram of two time series.\n    The units should be in s for all arguments.\n    Return the firing rate of the series t2 relative to the timings of t1.\n    See compute_crosscorrelogram, compute_autocorrelogram and compute_eventcorrelogram\n    for wrappers of this function.\n    Parameters\n    ----------\n    t1 : numpy.ndarray\n        The timestamps of the reference time series (in seconds)\n    t2 : numpy.ndarray\n        The timestamps of the target time series (in seconds)\n    binsize : float\n        The bin size (in seconds)\n    windowsize : float\n        The window size (in seconds)\n    Returns\n    -------\n    numpy.ndarray\n        The cross-correlogram\n    numpy.ndarray\n        Center of the bins (in s)\n    \"\"\"\n# nbins = ((windowsize//binsize)*2)\nnt1 = len(t1)\nnt2 = len(t2)\nnbins = int((windowsize * 2) // binsize)\nif np.floor(nbins / 2) * 2 == nbins:\nnbins = nbins + 1\nw = (nbins / 2) * binsize\nC = np.zeros(nbins)\ni2 = 0\nfor i1 in range(nt1):\nlbound = t1[i1] - w\nwhile i2 &lt; nt2 and t2[i2] &lt; lbound:\ni2 = i2 + 1\nwhile i2 &gt; 0 and t2[i2 - 1] &gt; lbound:\ni2 = i2 - 1\nrbound = lbound\nleftb = i2\nfor j in range(nbins):\nk = 0\nrbound = rbound + binsize\nwhile leftb &lt; nt2 and t2[leftb] &lt; rbound:\nleftb = leftb + 1\nk = k + 1\nC[j] += k\nC = C / (nt1 * binsize)\nm = -w + binsize / 2\nB = np.zeros(nbins)\nfor j in range(nbins):\nB[j] = m + j * binsize\nreturn C, B\n</code></pre>"},{"location":"reference/process/correlograms/#pynapple.process.correlograms.compute_autocorrelogram","title":"<code>compute_autocorrelogram(group, binsize, windowsize, ep=None, norm=True, time_units='s')</code>","text":"<p>Computes the autocorrelogram of a group of Ts/Tsd objects. The group can be passed directly as a TsGroup object.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects to auto-correlate</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which auto-corrs are computed. If None, the epoch is the time support of the group.</p> <code>None</code> <code>norm</code> <code>bool, optional</code> <p>If True, autocorrelograms are normalized to baseline (i.e. divided by the average rate) If False, autoorrelograms are returned as the rate (Hz) of the time series (relative to itself)</p> <code>True</code> <code>time_units</code> <code>str, optional</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_autocorrelogram(\ngroup, binsize, windowsize, ep=None, norm=True, time_units=\"s\"\n):\n\"\"\"\n    Computes the autocorrelogram of a group of Ts/Tsd objects.\n    The group can be passed directly as a TsGroup object.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects to auto-correlate\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which auto-corrs are computed.\n        If None, the epoch is the time support of the group.\n    norm : bool, optional\n         If True, autocorrelograms are normalized to baseline (i.e. divided by the average rate)\n         If False, autoorrelograms are returned as the rate (Hz) of the time series (relative to itself)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup\n    \"\"\"\nif type(group) is nap.TsGroup:\nif isinstance(ep, nap.IntervalSet):\nnewgroup = group.restrict(ep)\nelse:\nnewgroup = group\nelse:\nraise RuntimeError(\"Unknown format for group\")\nautocorrs = {}\nbinsize = nap.format_timestamps(np.array([binsize], dtype=np.float64), time_units)[\n0\n]\nwindowsize = nap.format_timestamps(\nnp.array([windowsize], dtype=np.float64), time_units\n)[0]\nfor n in newgroup.keys():\nspk_time = newgroup[n].index.values\nauc, times = cross_correlogram(spk_time, spk_time, binsize, windowsize)\nautocorrs[n] = pd.Series(index=np.round(times, 6), data=auc, dtype=\"float\")\nautocorrs = pd.DataFrame.from_dict(autocorrs)\nif norm:\nautocorrs = autocorrs / newgroup.get_info(\"rate\")\n# Bug here\nif 0 in autocorrs.index.values:\nautocorrs.loc[0] = 0.0\nreturn autocorrs.astype(\"float\")\n</code></pre>"},{"location":"reference/process/correlograms/#pynapple.process.correlograms.compute_crosscorrelogram","title":"<code>compute_crosscorrelogram(group, binsize, windowsize, ep=None, norm=True, time_units='s', reverse=False)</code>","text":"<p>Computes all the pairwise cross-correlograms for TsGroup or list/tuple of two TsGroup.</p> <p>If input is TsGroup only, the reference Ts/Tsd and target are chosen based on the builtin itertools.combinations function. For example if indexes are [0,1,2], the function computes cross-correlograms for the pairs (0,1), (0, 2), and (1, 2). The left index gives the reference time series. To reverse the order, set reverse=True.</p> <p>If input is tuple/list of TsGroup, for example group=(group1, group2), the reference for each pairs comes from group1.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup or tuple</code> required <p>binsize : float     The bin size. Default is second.     If different, specify with the parameter time_units ('s' [default], 'ms', 'us'). windowsize : float     The window size. Default is second.     If different, specify with the parameter time_units ('s' [default], 'ms', 'us'). ep : IntervalSet     The epoch on which cross-corrs are computed.     If None, the epoch is the time support of the group. norm : bool, optional     If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)     If False, cross-orrelograms are returned as the rate (Hz) of the target time series ((relative to the reference time series) time_units : str, optional     The time units of the parameters. They have to be consistent for binsize and windowsize.     ('s' [default], 'ms', 'us'). reverse : bool, optional     To reverse the pair order if input is TsGroup</p> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup or tuple/list of two TsGroups</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_crosscorrelogram(\ngroup, binsize, windowsize, ep=None, norm=True, time_units=\"s\", reverse=False\n):\n\"\"\"\n    Computes all the pairwise cross-correlograms for TsGroup or list/tuple of two TsGroup.\n    If input is TsGroup only, the reference Ts/Tsd and target are chosen based on the builtin itertools.combinations function.\n    For example if indexes are [0,1,2], the function computes cross-correlograms\n    for the pairs (0,1), (0, 2), and (1, 2). The left index gives the reference time series.\n    To reverse the order, set reverse=True.\n    If input is tuple/list of TsGroup, for example group=(group1, group2), the reference for each pairs comes from group1.\n    Parameters\n    ----------\n    group : TsGroup or tuple/list of two TsGroups\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which cross-corrs are computed.\n        If None, the epoch is the time support of the group.\n    norm : bool, optional\n        If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)\n        If False, cross-orrelograms are returned as the rate (Hz) of the target time series ((relative to the reference time series)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    reverse : bool, optional\n        To reverse the pair order if input is TsGroup\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup or tuple/list of two TsGroups\n    \"\"\"\ncrosscorrs = {}\nbinsize = nap.format_timestamps(np.array([binsize], dtype=np.float64), time_units)[\n0\n]\nwindowsize = nap.format_timestamps(\nnp.array([windowsize], dtype=np.float64), time_units\n)[0]\nif isinstance(group, nap.TsGroup):\nif isinstance(ep, nap.IntervalSet):\nnewgroup = group.restrict(ep)\nelse:\nnewgroup = group\nneurons = list(newgroup.keys())\npairs = list(combinations(neurons, 2))\nif reverse:\npairs = list(map(lambda n: (n[1], n[0]), pairs))\nfor i, j in pairs:\nspk1 = newgroup[i].index.values\nspk2 = newgroup[j].index.values\nauc, times = cross_correlogram(spk1, spk2, binsize, windowsize)\ncrosscorrs[(i, j)] = pd.Series(index=times, data=auc, dtype=\"float\")\ncrosscorrs = pd.DataFrame.from_dict(crosscorrs)\nif norm:\nfreq = newgroup.get_info(\"rate\")\nfreq2 = pd.Series(\nindex=pairs, data=list(map(lambda n: freq.loc[n[1]], pairs))\n)\ncrosscorrs = crosscorrs / freq2\nelif (\nisinstance(group, (tuple, list))\nand len(group) == 2\nand all(map(lambda g: isinstance(g, nap.TsGroup), group))\n):\nif isinstance(ep, nap.IntervalSet):\nnewgroup = [group[i].restrict(ep) for i in range(2)]\nelse:\nnewgroup = group\npairs = product(list(newgroup[0].keys()), list(newgroup[1].keys()))\nfor i, j in pairs:\nspk1 = newgroup[0][i].index.values\nspk2 = newgroup[1][j].index.values\nauc, times = cross_correlogram(spk1, spk2, binsize, windowsize)\nif norm:\nauc /= newgroup[1][j].rate\ncrosscorrs[(i, j)] = pd.Series(index=times, data=auc, dtype=\"float\")\ncrosscorrs = pd.DataFrame.from_dict(crosscorrs)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nreturn crosscorrs.astype(\"float\")\n</code></pre>"},{"location":"reference/process/correlograms/#pynapple.process.correlograms.compute_eventcorrelogram","title":"<code>compute_eventcorrelogram(group, event, binsize, windowsize, ep=None, norm=True, time_units='s')</code>","text":"<p>Computes the correlograms of a group of Ts/Tsd objects with another single Ts/Tsd object The time of reference is the event times.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects to correlate with the event</p> required <code>event</code> <code>Ts</code> <p>The event to correlate the each of the time series in the group with.</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which cross-corrs are computed. If None, the epoch is the time support of the event.</p> <code>None</code> <code>norm</code> <code>bool, optional</code> <p>If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series) If False, cross-orrelograms are returned as the rate (Hz) of the target time series (relative to the event time series)</p> <code>True</code> <code>time_units</code> <code>str, optional</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>_</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>group must be TsGroup</p> Source code in <code>pynapple/process/correlograms.py</code> <pre><code>def compute_eventcorrelogram(\ngroup, event, binsize, windowsize, ep=None, norm=True, time_units=\"s\"\n):\n\"\"\"\n    Computes the correlograms of a group of Ts/Tsd objects with another single Ts/Tsd object\n    The time of reference is the event times.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects to correlate with the event\n    event : Ts/Tsd\n        The event to correlate the each of the time series in the group with.\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which cross-corrs are computed.\n        If None, the epoch is the time support of the event.\n    norm : bool, optional\n        If True (default), cross-correlograms are normalized to baseline (i.e. divided by the average rate of the target time series)\n        If False, cross-orrelograms are returned as the rate (Hz) of the target time series (relative to the event time series)\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    pandas.DataFrame\n        _\n    Raises\n    ------\n    RuntimeError\n        group must be TsGroup\n    \"\"\"\nif ep is None:\nep = event.time_support\ntsd1 = event.index.values\nelse:\ntsd1 = event.restrict(ep).index.values\nif type(group) is nap.TsGroup:\nnewgroup = group.restrict(ep)\nelse:\nraise RuntimeError(\"Unknown format for group\")\ncrosscorrs = {}\nbinsize = nap.format_timestamps(np.array([binsize], dtype=np.float64), time_units)[\n0\n]\nwindowsize = nap.format_timestamps(\nnp.array([windowsize], dtype=np.float64), time_units\n)[0]\nfor n in newgroup.keys():\nspk_time = newgroup[n].index.values\nauc, times = cross_correlogram(tsd1, spk_time, binsize, windowsize)\ncrosscorrs[n] = pd.Series(index=times, data=auc, dtype=\"float\")\ncrosscorrs = pd.DataFrame.from_dict(crosscorrs)\nif norm:\ncrosscorrs = crosscorrs / newgroup.get_info(\"rate\")\nreturn crosscorrs.astype(\"float\")\n</code></pre>"},{"location":"reference/process/decoding/","title":"Decoding","text":""},{"location":"reference/process/decoding/#pynapple.process.decoding.decode_1d","title":"<code>decode_1d(tuning_curves, group, ep, bin_size, time_units='s', feature=None)</code>","text":"<p>Performs Bayesian decoding over a one dimensional feature. See: Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.</p> <p>Parameters:</p> Name Type Description Default <code>tuning_curves</code> <code>pandas.DataFrame</code> <p>Each column is the tuning curve of one neuron relative to the feature. Index should be the center of the bin.</p> required <code>group</code> <code>TsGroup or dict of Ts</code> <p>A group of neurons with the same index as tuning curves column names.</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which decoding is computed</p> required <code>bin_size</code> <code>float</code> <p>Bin size. Default is second. Use the parameter time_units to change it.</p> required <code>time_units</code> <code>str, optional</code> <p>Time unit of the bin size ('s' [default], 'ms', 'us').</p> <code>'s'</code> <code>feature</code> <code>Tsd, optional</code> <p>The 1d feature used to compute the tuning curves. Used to correct for occupancy. If feature is not passed, the occupancy is uniform.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tsd</code> <p>The decoded feature</p> <code>TsdFrame</code> <p>The probability distribution of the decoded feature for each time bin</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a dict of Ts/Tsd or TsGroup. If different size of neurons for tuning_curves and group. If indexes don't match between tuning_curves and group.</p> Source code in <code>pynapple/process/decoding.py</code> <pre><code>def decode_1d(tuning_curves, group, ep, bin_size, time_units=\"s\", feature=None):\n\"\"\"\n    Performs Bayesian decoding over a one dimensional feature.\n    See:\n    Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J.\n    (1998). Interpreting neuronal population activity by\n    reconstruction: unified framework with application to\n    hippocampal place cells. Journal of neurophysiology, 79(2),\n    1017-1044.\n    Parameters\n    ----------\n    tuning_curves : pandas.DataFrame\n        Each column is the tuning curve of one neuron relative to the feature.\n        Index should be the center of the bin.\n    group : TsGroup or dict of Ts/Tsd object.\n        A group of neurons with the same index as tuning curves column names.\n    ep : IntervalSet\n        The epoch on which decoding is computed\n    bin_size : float\n        Bin size. Default is second. Use the parameter time_units to change it.\n    time_units : str, optional\n        Time unit of the bin size ('s' [default], 'ms', 'us').\n    feature : Tsd, optional\n        The 1d feature used to compute the tuning curves. Used to correct for occupancy.\n        If feature is not passed, the occupancy is uniform.\n    Returns\n    -------\n    Tsd\n        The decoded feature\n    TsdFrame\n        The probability distribution of the decoded feature for each time bin\n    Raises\n    ------\n    RuntimeError\n        If group is not a dict of Ts/Tsd or TsGroup.\n        If different size of neurons for tuning_curves and group.\n        If indexes don't match between tuning_curves and group.\n    \"\"\"\nif isinstance(group, dict):\nnewgroup = nap.TsGroup(group, time_support=ep)\nelif isinstance(group, nap.TsGroup):\nnewgroup = group.restrict(ep)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nif tuning_curves.shape[1] != len(newgroup):\nraise RuntimeError(\"Different shapes for tuning_curves and group\")\nif not np.all(tuning_curves.columns.values == np.array(newgroup.keys())):\nraise RuntimeError(\"Difference indexes for tuning curves and group keys\")\n# Bin spikes\ncount = newgroup.count(bin_size, ep, time_units)\n# Occupancy\nif feature is None:\noccupancy = np.ones(tuning_curves.shape[0])\nelif isinstance(feature, nap.Tsd):\ndiff = np.diff(tuning_curves.index.values)\nbins = tuning_curves.index.values[:-1] - diff / 2\nbins = np.hstack(\n(bins, [bins[-1] + diff[-1], bins[-1] + 2 * diff[-1]])\n)  # assuming the size of the last 2 bins is equal\noccupancy, _ = np.histogram(feature, bins)\nelse:\nraise RuntimeError(\"Unknown format for feature in decode_1d\")\n# Transforming to pure numpy array\ntc = tuning_curves.values\nct = count.values\nbin_size_s = nap.format_timestamps(\nnp.array([bin_size], dtype=np.float64), time_units\n)[0]\np1 = np.exp(-bin_size_s * tc.sum(1))\np2 = occupancy / occupancy.sum()\nct2 = np.tile(ct[:, np.newaxis, :], (1, tc.shape[0], 1))\np3 = np.prod(tc**ct2, -1)\np = p1 * p2 * p3\np = p / p.sum(1)[:, np.newaxis]\nidxmax = np.argmax(p, 1)\np = nap.TsdFrame(\nt=count.index.values, d=p, time_support=ep, columns=tuning_curves.index.values\n)\ndecoded = nap.Tsd(\nt=count.index.values, d=tuning_curves.index.values[idxmax], time_support=ep\n)\nreturn decoded, p\n</code></pre>"},{"location":"reference/process/decoding/#pynapple.process.decoding.decode_2d","title":"<code>decode_2d(tuning_curves, group, ep, bin_size, xy, time_units='s', features=None)</code>","text":"<p>Performs Bayesian decoding over a two dimensional feature. See: Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J. (1998). Interpreting neuronal population activity by reconstruction: unified framework with application to hippocampal place cells. Journal of neurophysiology, 79(2), 1017-1044.</p> <p>Parameters:</p> Name Type Description Default <code>tuning_curves</code> <code>dict</code> <p>Dictionnay of 2d tuning curves (one for each neuron).</p> required <code>group</code> <code>TsGroup or dict of Ts</code> <p>A group of neurons with the same keys as tuning_curves dictionnary.</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which decoding is computed</p> required <code>bin_size</code> <code>float</code> <p>Bin size. Default is second. Use the parameter time_units to change it.</p> required <code>xy</code> <code>tuple</code> <p>A tuple of bin positions for the tuning curves i.e. xy=(x,y)</p> required <code>time_units</code> <code>str, optional</code> <p>Time unit of the bin size ('s' [default], 'ms', 'us').</p> <code>'s'</code> <code>features</code> <code>TsdFrame</code> <p>The 2 columns features used to compute the tuning curves. Used to correct for occupancy. If feature is not passed, the occupancy is uniform.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tsd</code> <p>The decoded feature in 2d</p> <code>numpy.ndarray</code> <p>The probability distribution of the decoded trajectory for each time bin</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a dict of Ts/Tsd or TsGroup. If different size of neurons for tuning_curves and group. If indexes don't match between tuning_curves and group.</p> Source code in <code>pynapple/process/decoding.py</code> <pre><code>def decode_2d(tuning_curves, group, ep, bin_size, xy, time_units=\"s\", features=None):\n\"\"\"\n    Performs Bayesian decoding over a two dimensional feature.\n    See:\n    Zhang, K., Ginzburg, I., McNaughton, B. L., &amp; Sejnowski, T. J.\n    (1998). Interpreting neuronal population activity by\n    reconstruction: unified framework with application to\n    hippocampal place cells. Journal of neurophysiology, 79(2),\n    1017-1044.\n    Parameters\n    ----------\n    tuning_curves : dict\n        Dictionnay of 2d tuning curves (one for each neuron).\n    group : TsGroup or dict of Ts/Tsd object.\n        A group of neurons with the same keys as tuning_curves dictionnary.\n    ep : IntervalSet\n        The epoch on which decoding is computed\n    bin_size : float\n        Bin size. Default is second. Use the parameter time_units to change it.\n    xy : tuple\n        A tuple of bin positions for the tuning curves i.e. xy=(x,y)\n    time_units : str, optional\n        Time unit of the bin size ('s' [default], 'ms', 'us').\n    features : TsdFrame\n        The 2 columns features used to compute the tuning curves. Used to correct for occupancy.\n        If feature is not passed, the occupancy is uniform.\n    Returns\n    -------\n    Tsd\n        The decoded feature in 2d\n    numpy.ndarray\n        The probability distribution of the decoded trajectory for each time bin\n    Raises\n    ------\n    RuntimeError\n        If group is not a dict of Ts/Tsd or TsGroup.\n        If different size of neurons for tuning_curves and group.\n        If indexes don't match between tuning_curves and group.\n    \"\"\"\nif type(group) is dict:\nnewgroup = nap.TsGroup(group, time_support=ep)\nnumcells = len(newgroup)\nelif type(group) is nap.TsGroup:\nnewgroup = group.restrict(ep)\nnumcells = len(newgroup)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nif len(tuning_curves) != numcells:\nraise RuntimeError(\"Different shapes for tuning_curves and group\")\nif not np.all(np.array(list(tuning_curves.keys())) == np.array(newgroup.keys())):\nraise RuntimeError(\"Difference indexes for tuning curves and group keys\")\n# Bin spikes\n# if type(newgroup) is not nap.TsdFrame:\ncount = newgroup.count(bin_size, ep, time_units)\n# else:\n#     #Spikes already \"binned\" with continuous TsdFrame input\n#     count = newgroup\nindexes = list(tuning_curves.keys())\n# Occupancy\nif features is None:\noccupancy = np.ones_like(tuning_curves[indexes[0]]).flatten()\nelse:\nbinsxy = []\nfor i in range(len(xy)):\ndiff = np.diff(xy[i])\nbins = xy[i][:-1] - diff / 2\nbins = np.hstack(\n(bins, [bins[-1] + diff[-1], bins[-1] + 2 * diff[-1]])\n)  # assuming the size of the last 2 bins is equal\nbinsxy.append(bins)\noccupancy, _, _ = np.histogram2d(\nfeatures.iloc[:, 0], features.iloc[:, 1], [binsxy[0], binsxy[1]]\n)\noccupancy = occupancy.flatten()\n# Transforming to pure numpy array\ntc = np.array([tuning_curves[i] for i in tuning_curves.keys()])\ntc = tc.reshape(tc.shape[0], np.prod(tc.shape[1:]))\ntc = tc.T\nct = count.values\nbin_size_s = nap.format_timestamps(\nnp.array([bin_size], dtype=np.float64), time_units\n)[0]\np1 = np.exp(-bin_size_s * np.nansum(tc, 1))\np2 = occupancy / occupancy.sum()\nct2 = np.tile(ct[:, np.newaxis, :], (1, tc.shape[0], 1))\np3 = np.nanprod(tc**ct2, -1)\np = p1 * p2 * p3\np = p / p.sum(1)[:, np.newaxis]\nidxmax = np.argmax(p, 1)\np = p.reshape(p.shape[0], len(xy[0]), len(xy[1]))\nidxmax2d = np.unravel_index(idxmax, (len(xy[0]), len(xy[1])))\nif features is not None:\ncols = features.columns\nelse:\ncols = np.arange(2)\ndecoded = nap.TsdFrame(\nt=count.index.values,\nd=np.vstack((xy[0][idxmax2d[0]], xy[1][idxmax2d[1]])).T,\ntime_support=ep,\ncolumns=cols,\n)\nreturn decoded, p\n</code></pre>"},{"location":"reference/process/perievent/","title":"Perievent","text":""},{"location":"reference/process/perievent/#pynapple.process.perievent.compute_perievent","title":"<code>compute_perievent(data, tref, minmax, time_unit='s')</code>","text":"<p>Center ts/tsd/tsgroup object around the timestamps given by the tref argument. minmax indicates the start and end of the window.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Ts</code> <p>The data to align to tref. If Ts/Tsd, returns a TsGroup. If TsGroup, returns a dictionnary of TsGroup</p> required <code>tref</code> <code>Ts</code> <p>The timestamps of the event to align to</p> required <code>minmax</code> <code>tuple or int or float</code> <p>The window size. Can be unequal on each side i.e. (-500, 1000).</p> required <code>time_unit</code> <code>str, optional</code> <p>Time units of the minmax ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A TsGroup if data is a Ts/Tsd or a dictionnary of TsGroup if data is a TsGroup.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if tref is not a Ts/Tsd object or if data is not a Ts/Tsd or TsGroup</p> Source code in <code>pynapple/process/perievent.py</code> <pre><code>def compute_perievent(data, tref, minmax, time_unit=\"s\"):\n\"\"\"\n    Center ts/tsd/tsgroup object around the timestamps given by the tref argument.\n    minmax indicates the start and end of the window.\n    Parameters\n    ----------\n    data : Ts/Tsd/TsGroup\n        The data to align to tref.\n        If Ts/Tsd, returns a TsGroup.\n        If TsGroup, returns a dictionnary of TsGroup\n    tref : Ts/Tsd\n        The timestamps of the event to align to\n    minmax : tuple or int or float\n        The window size. Can be unequal on each side i.e. (-500, 1000).\n    time_unit : str, optional\n        Time units of the minmax ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    dict\n        A TsGroup if data is a Ts/Tsd or\n        a dictionnary of TsGroup if data is a TsGroup.\n    Raises\n    ------\n    RuntimeError\n        if tref is not a Ts/Tsd object or if data is not a Ts/Tsd or TsGroup\n    \"\"\"\nif not isinstance(tref, (nap.Ts, nap.Tsd)):\nraise RuntimeError(\"tref should be a Tsd object.\")\nif isinstance(minmax, float) or isinstance(minmax, int):\nminmax = np.array([minmax, minmax], dtype=np.float64)\nwindow = np.abs(nap.format_timestamps(np.array(minmax), time_unit))\ntime_support = nap.IntervalSet(start=-window[0], end=window[1])\nif isinstance(data, nap.TsGroup):\ntoreturn = {}\nfor n in data.index:\ntoreturn[n] = _align_tsd(data[n], tref, window, time_support)\nreturn toreturn\nelif isinstance(data, (nap.Ts, nap.Tsd)):\nreturn _align_tsd(data, tref, window, time_support)\nelse:\nraise RuntimeError(\"Unknown format for data\")\n</code></pre>"},{"location":"reference/process/perievent/#pynapple.process.perievent.compute_event_trigger_average","title":"<code>compute_event_trigger_average(group, feature, binsize, windowsize, ep, time_units='s')</code>","text":"<p>Bin the spike train in binsize and compute the Spike Trigger Average (STA) within windowsize. If C is the spike count matrix and feature is a Tsd array, the function computes the Hankel matrix H from windowsize=(-t1,+t2) by offseting the Tsd array.</p> <p>The STA is then defined as the dot product between H and C divided by the number of spikes.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd objects that hold the trigger time.</p> required <code>feature</code> <code>Tsd</code> <p>The 1-dimensional feature to average</p> required <code>binsize</code> <code>float</code> <p>The bin size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>windowsize</code> <code>float</code> <p>The window size. Default is second. If different, specify with the parameter time_units ('s' [default], 'ms', 'us').</p> required <code>ep</code> <code>IntervalSet</code> <p>The epoch on which STA are computed</p> required <code>time_units</code> <code>str, optional</code> <p>The time units of the parameters. They have to be consistent for binsize and windowsize. ('s' [default], 'ms', 'us').</p> <code>'s'</code> <p>Returns:</p> Type Description <code>TsdFrame</code> <p>A TsdFrame of Spike-Trigger Average. Each column is an element from the group.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if group is not a Ts/Tsd or TsGroup</p> Source code in <code>pynapple/process/perievent.py</code> <pre><code>def compute_event_trigger_average(\ngroup, feature, binsize, windowsize, ep, time_units=\"s\"\n):\n\"\"\"\n    Bin the spike train in binsize and compute the Spike Trigger Average (STA) within windowsize.\n    If C is the spike count matrix and feature is a Tsd array, the function computes\n    the Hankel matrix H from windowsize=(-t1,+t2) by offseting the Tsd array.\n    The STA is then defined as the dot product between H and C divided by the number of spikes.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd objects that hold the trigger time.\n    feature : Tsd\n        The 1-dimensional feature to average\n    binsize : float\n        The bin size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    windowsize : float\n        The window size. Default is second.\n        If different, specify with the parameter time_units ('s' [default], 'ms', 'us').\n    ep : IntervalSet\n        The epoch on which STA are computed\n    time_units : str, optional\n        The time units of the parameters. They have to be consistent for binsize and windowsize.\n        ('s' [default], 'ms', 'us').\n    Returns\n    -------\n    TsdFrame\n        A TsdFrame of Spike-Trigger Average. Each column is an element from the group.\n    Raises\n    ------\n    RuntimeError\n        if group is not a Ts/Tsd or TsGroup\n    \"\"\"\nif type(group) is not nap.TsGroup:\nraise RuntimeError(\"Unknown format for group\")\nbinsize = nap.format_timestamps(np.array([binsize], dtype=np.float64), time_units)[\n0\n]\nstart = np.abs(\nnap.format_timestamps(np.array([windowsize[0]], dtype=np.float64), time_units)[\n0\n]\n)\nend = np.abs(\nnap.format_timestamps(np.array([windowsize[1]], dtype=np.float64), time_units)[\n0\n]\n)\nidx1 = -np.arange(0, start + binsize, binsize)[::-1][:-1]\nidx2 = np.arange(0, end + binsize, binsize)[1:]\ntime_idx = np.hstack((idx1, np.zeros(1), idx2))\ncount = group.count(binsize, ep)\ntmp = feature.bin_average(binsize, ep)\n# Build the Hankel matrix\nn_p = len(idx1)\nn_f = len(idx2)\npad_tmp = np.pad(tmp, (n_p, n_f))\noffset_tmp = hankel(pad_tmp, pad_tmp[-(n_p + n_f + 1) :])[0 : len(tmp)]\nsta = np.dot(offset_tmp.T, count.values)\nsta = sta / count.sum(0).values\nsta = nap.TsdFrame(t=time_idx, d=sta, columns=group.index)\nreturn sta\n</code></pre>"},{"location":"reference/process/randomize/","title":"Randomize","text":""},{"location":"reference/process/randomize/#pynapple.process.randomize.shift_timestamps","title":"<code>shift_timestamps(ts, min_shift=0.0, max_shift=None)</code>","text":"<p>Shifts all the time stamps of a random amount between min_shift and max_shift, wrapping the end of the time support to the beginning.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to shift. If TsGroup, shifts all Ts in the group independently.</p> required <code>min_shift</code> <code>float, optional</code> <p>minimum shift (default: 0 )</p> <code>0.0</code> <code>max_shift</code> <code>float, optional</code> <p>maximum shift, (default: length of time support)</p> <code>None</code> <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The randomly shifted timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def shift_timestamps(ts, min_shift=0.0, max_shift=None):\n\"\"\"\n    Shifts all the time stamps of a random amount between min_shift and max_shift, wrapping the\n    end of the time support to the beginning.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to shift. If TsGroup, shifts all Ts in the group independently.\n    min_shift : float, optional\n        minimum shift (default: 0 )\n    max_shift : float, optional\n        maximum shift, (default: length of time support)\n    Returns\n    -------\n    Ts or TsGroup\n        The randomly shifted timestamps\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _shift_ts,\nnap.ts_group.TsGroup: _shift_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nstrategy = strategies[type(ts)]\nreturn strategy(ts, min_shift, max_shift)\n</code></pre>"},{"location":"reference/process/randomize/#pynapple.process.randomize.shuffle_ts_intervals","title":"<code>shuffle_ts_intervals(ts, min_shift=0.0, max_shift=None)</code>","text":"<p>Randomizes the timestamps by shuffling the intervals between them.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to randomize. If TsGroup, randomizes all Ts in the group independently.</p> required <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The randomized timestamps, with shuffled intervals</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def shuffle_ts_intervals(ts, min_shift=0.0, max_shift=None):\n\"\"\"\n    Randomizes the timestamps by shuffling the intervals between them.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to randomize. If TsGroup, randomizes all Ts in the group independently.\n    Returns\n    -------\n    Ts or TsGroup\n        The randomized timestamps, with shuffled intervals\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _shuffle_intervals_ts,\nnap.ts_group.TsGroup: _shuffle_intervals_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nstrategy = strategies[type(ts)]\nreturn strategy(ts)\n</code></pre>"},{"location":"reference/process/randomize/#pynapple.process.randomize.jitter_timestamps","title":"<code>jitter_timestamps(ts, max_jitter=None, keep_tsupport=False)</code>","text":"<p>Jitters each time stamp independently of random amounts uniformly drawn between -max_jitter and max_jitter.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to jitter. If TsGroup, jitter is applied to each element of the group.</p> required <code>max_jitter</code> <code>float</code> <p>maximum jitter</p> <code>None</code> <code>keep_tsupport</code> <p>If True, keep time support of the input. The number of timestamps will not be conserved. If False, the time support is inferred from the jittered timestamps. The number of tmestamps is conserved. (default: False)</p> <code>False</code> <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The jittered timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def jitter_timestamps(ts, max_jitter=None, keep_tsupport=False):\n\"\"\"\n    Jitters each time stamp independently of random amounts uniformly drawn between -max_jitter and max_jitter.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to jitter. If TsGroup, jitter is applied to each element of the group.\n    max_jitter : float\n        maximum jitter\n    keep_tsupport: bool, optional\n        If True, keep time support of the input. The number of timestamps will not be conserved.\n        If False, the time support is inferred from the jittered timestamps. The number of tmestamps\n        is conserved. (default: False)\n    Returns\n    -------\n    Ts or TsGroup\n        The jittered timestamps\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _jitter_ts,\nnap.ts_group.TsGroup: _jitter_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nif max_jitter is None:\nraise TypeError(\"missing required argument: max_jitter \")\nstrategy = strategies[type(ts)]\nreturn strategy(ts, max_jitter, keep_tsupport)\n</code></pre>"},{"location":"reference/process/randomize/#pynapple.process.randomize.resample_timestamps","title":"<code>resample_timestamps(ts)</code>","text":"<p>Resamples the timestamps in the time support, with uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Ts or TsGroup</code> <p>The timestamps to resample. If TsGroup, each Ts object in the group is independently resampled, in the time support of the whole group.</p> required <p>Returns:</p> Type Description <code>Ts or TsGroup</code> <p>The resampled timestamps</p> Source code in <code>pynapple/process/randomize.py</code> <pre><code>def resample_timestamps(ts):\n\"\"\"\n    Resamples the timestamps in the time support, with uniform distribution.\n    Parameters\n    ----------\n    ts : Ts or TsGroup\n        The timestamps to resample. If TsGroup, each Ts object in the group is independently\n        resampled, in the time support of the whole group.\n    Returns\n    -------\n    Ts or TsGroup\n        The resampled timestamps\n    \"\"\"\nstrategies = {\nnap.time_series.Ts: _resample_ts,\nnap.ts_group.TsGroup: _resample_tsgroup,\n}\n# checks input type\nif type(ts) not in strategies.keys():\nraise TypeError(\"Invalid input type, should be Ts or TsGroup\")\nstrategy = strategies[type(ts)]\nreturn strategy(ts)\n</code></pre>"},{"location":"reference/process/tuning_curves/","title":"Tuning curves","text":"<p>Summary</p>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_discrete_tuning_curves","title":"<code>compute_discrete_tuning_curves(group, dict_ep)</code>","text":"<pre><code>Compute discrete tuning curves of a TsGroup using a dictionnary of epochs.\n</code></pre> <p>The function returns a pandas DataFrame with each row being a key of the dictionnary of epochs and each column being a neurons.</p> <p>This function can typically being used for a set of stimulus being presented for multiple epochs. An example of the dictionnary is :</p> <pre><code>&gt;&gt;&gt; dict_ep =  {\n        \"stim0\": nap.IntervalSet(start=0, end=1),\n        \"stim1\":nap.IntervalSet(start=2, end=3)\n    }\n</code></pre> <p>In this case, the function will return a pandas DataFrame :</p> <pre><code>&gt;&gt;&gt; tc\n           neuron0    neuron1    neuron2\nstim0        0 Hz       1 Hz       2 Hz\nstim1        3 Hz       4 Hz       5 Hz\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>nap.TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>dict_ep</code> <code>dict</code> <p>Dictionary of IntervalSets</p> required <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>Table of firing rate for each neuron and each IntervalSet</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_discrete_tuning_curves(group, dict_ep):\n\"\"\"\n        Compute discrete tuning curves of a TsGroup using a dictionnary of epochs.\n    The function returns a pandas DataFrame with each row being a key of the dictionnary of epochs\n    and each column being a neurons.\n       This function can typically being used for a set of stimulus being presented for multiple epochs.\n    An example of the dictionnary is :\n        &gt;&gt;&gt; dict_ep =  {\n                \"stim0\": nap.IntervalSet(start=0, end=1),\n                \"stim1\":nap.IntervalSet(start=2, end=3)\n            }\n    In this case, the function will return a pandas DataFrame :\n        &gt;&gt;&gt; tc\n                   neuron0    neuron1    neuron2\n        stim0        0 Hz       1 Hz       2 Hz\n        stim1        3 Hz       4 Hz       5 Hz\n    Parameters\n    ----------\n    group : nap.TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    dict_ep : dict\n        Dictionary of IntervalSets\n    Returns\n    -------\n    pandas.DataFrame\n        Table of firing rate for each neuron and each IntervalSet\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object.\n    \"\"\"\nif not isinstance(group, nap.TsGroup):\nraise RuntimeError(\"Unknown format for group\")\nidx = np.sort(list(dict_ep.keys()))\ntuning_curves = pd.DataFrame(index=idx, columns=list(group.keys()), data=0)\nfor k in dict_ep.keys():\nif not isinstance(dict_ep[k], nap.IntervalSet):\nraise RuntimeError(\"Key {} in dict_ep is not an IntervalSet\".format(k))\nfor n in group.keys():\ntuning_curves.loc[k, n] = float(len(group[n].restrict(dict_ep[k])))\ntuning_curves.loc[k] = tuning_curves.loc[k] / dict_ep[k].tot_length(\"s\")\nreturn tuning_curves\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_1d_tuning_curves","title":"<code>compute_1d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None)</code>","text":"<p>Computes 1-dimensional tuning curves relative to a 1d feature.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>feature</code> <code>Tsd</code> <p>The 1-dimensional target feature (e.g. head-direction)</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curve</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>DataFrame to hold the tuning curves</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None):\n\"\"\"\n    Computes 1-dimensional tuning curves relative to a 1d feature.\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    feature : Tsd\n        The 1-dimensional target feature (e.g. head-direction)\n    nb_bins : int\n        Number of bins in the tuning curve\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame to hold the tuning curves\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object.\n    \"\"\"\nif not isinstance(group, nap.TsGroup):\nraise RuntimeError(\"Unknown format for group\")\nif minmax is None:\nbins = np.linspace(np.min(feature), np.max(feature), nb_bins + 1)\nelse:\nbins = np.linspace(minmax[0], minmax[1], nb_bins + 1)\nidx = bins[0:-1] + np.diff(bins) / 2\ntuning_curves = pd.DataFrame(index=idx, columns=list(group.keys()))\nif isinstance(ep, nap.IntervalSet):\ngroup_value = group.value_from(feature, ep)\noccupancy, _ = np.histogram(feature.restrict(ep).values, bins)\nelse:\ngroup_value = group.value_from(feature)\noccupancy, _ = np.histogram(feature.values, bins)\nfor k in group_value:\ncount, _ = np.histogram(group_value[k].values, bins)\ncount = count / occupancy\ncount[np.isnan(count)] = 0.0\ntuning_curves[k] = count\ntuning_curves[k] = count * feature.rate\nreturn tuning_curves\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_2d_tuning_curves","title":"<code>compute_2d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None)</code>","text":"<p>Computes 2-dimensional tuning curves relative to a 2d feature</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>The group of Ts/Tsd for which the tuning curves will be computed</p> required <code>feature</code> <code>TsdFrame</code> <p>The 2d feature (i.e. 2 columns features).</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curves</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves given as: (minx, maxx, miny, maxy) If None, the boundaries are inferred from the target variable</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: </p> <p>tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).</p> <p>xy (list): List of bins center in the two dimensions</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If group is not a TsGroup object or if feature is not 2 columns only.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_tuning_curves(group, feature, nb_bins, ep=None, minmax=None):\n\"\"\"\n    Computes 2-dimensional tuning curves relative to a 2d feature\n    Parameters\n    ----------\n    group : TsGroup\n        The group of Ts/Tsd for which the tuning curves will be computed\n    feature : TsdFrame\n        The 2d feature (i.e. 2 columns features).\n    nb_bins : int\n        Number of bins in the tuning curves\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves given as:\n        (minx, maxx, miny, maxy)\n        If None, the boundaries are inferred from the target variable\n    Returns\n    -------\n    tuple\n        A tuple containing: \\n\n        tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).\\n\n        xy (list): List of bins center in the two dimensions\n    Raises\n    ------\n    RuntimeError\n        If group is not a TsGroup object or if feature is not 2 columns only.\n    \"\"\"\nif feature.shape[1] != 2:\nraise RuntimeError(\"feature should have 2 columns only.\")\nif type(group) is not nap.TsGroup:\nraise RuntimeError(\"Unknown format for group\")\nif isinstance(ep, nap.IntervalSet):\nfeature = feature.restrict(ep)\nelse:\nep = feature.time_support\ncols = list(feature.columns)\ngroups_value = {}\nbinsxy = {}\nfor i, c in enumerate(cols):\ngroups_value[c] = group.value_from(feature[c], ep)\nif minmax is None:\nbins = np.linspace(np.min(feature[c]), np.max(feature[c]), nb_bins + 1)\nelse:\nbins = np.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins + 1)\nbinsxy[c] = bins\noccupancy, _, _ = np.histogram2d(\nfeature[cols[0]].values,\nfeature[cols[1]].values,\n[binsxy[cols[0]], binsxy[cols[1]]],\n)\ntc = {}\nfor n in group.keys():\ncount, _, _ = np.histogram2d(\ngroups_value[cols[0]][n].values,\ngroups_value[cols[1]][n].values,\n[binsxy[cols[0]], binsxy[cols[1]]],\n)\ncount = count / occupancy\n# count[np.isnan(count)] = 0.0\ntc[n] = count * feature.rate\nxy = [binsxy[c][0:-1] + np.diff(binsxy[c]) / 2 for c in binsxy.keys()]\nreturn tc, xy\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_1d_mutual_info","title":"<code>compute_1d_mutual_info(tc, feature, ep=None, minmax=None, bitssec=False)</code>","text":"<p>Mutual information as defined in</p> <p>Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993). An information-theoretic approach to deciphering the hippocampal code. In Advances in neural information processing systems (pp. 1030-1037).</p> <p>Parameters:</p> Name Type Description Default <code>tc</code> <code>pandas.DataFrame or numpy.ndarray</code> <p>Tuning curves in columns</p> required <code>feature</code> <code>Tsd</code> <p>The feature that was used to compute the tuning curves</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch over which the tuning curves were computed If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <code>bitssec</code> <code>bool, optional</code> <p>By default, the function return bits per spikes. Set to true for bits per seconds</p> <code>False</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>Spatial Information (default is bits/spikes)</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_mutual_info(tc, feature, ep=None, minmax=None, bitssec=False):\n\"\"\"\n    Mutual information as defined in\n    Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993).\n    An information-theoretic approach to deciphering the hippocampal code.\n    In Advances in neural information processing systems (pp. 1030-1037).\n    Parameters\n    ----------\n    tc : pandas.DataFrame or numpy.ndarray\n        Tuning curves in columns\n    feature : Tsd\n        The feature that was used to compute the tuning curves\n    ep : IntervalSet, optional\n        The epoch over which the tuning curves were computed\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n    bitssec : bool, optional\n        By default, the function return bits per spikes.\n        Set to true for bits per seconds\n    Returns\n    -------\n    pandas.DataFrame\n        Spatial Information (default is bits/spikes)\n    \"\"\"\nif isinstance(tc, pd.DataFrame):\ncolumns = tc.columns.values\nfx = np.atleast_2d(tc.values)\nelif isinstance(tc, np.ndarray):\nfx = np.atleast_2d(tc)\ncolumns = np.arange(tc.shape[1])\nnb_bins = tc.shape[0] + 1\nif minmax is None:\nbins = np.linspace(np.min(feature), np.max(feature), nb_bins)\nelse:\nbins = np.linspace(minmax[0], minmax[1], nb_bins)\nif isinstance(ep, nap.IntervalSet):\noccupancy, _ = np.histogram(feature.restrict(ep).values, bins)\nelse:\noccupancy, _ = np.histogram(feature.values, bins)\noccupancy = occupancy / occupancy.sum()\noccupancy = occupancy[:, np.newaxis]\nfr = np.sum(fx * occupancy, 0)\nfxfr = fx / fr\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nlogfx = np.log2(fxfr)\nlogfx[np.isinf(logfx)] = 0.0\nSI = np.sum(occupancy * fx * logfx, 0)\nif bitssec:\nSI = pd.DataFrame(index=columns, columns=[\"SI\"], data=SI)\nreturn SI\nelse:\nSI = SI / fr\nSI = pd.DataFrame(index=columns, columns=[\"SI\"], data=SI)\nreturn SI\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_2d_mutual_info","title":"<code>compute_2d_mutual_info(tc, features, ep=None, minmax=None, bitssec=False)</code>","text":"<p>Mutual information as defined in</p> <p>Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993). An information-theoretic approach to deciphering the hippocampal code. In Advances in neural information processing systems (pp. 1030-1037).</p> <p>Parameters:</p> Name Type Description Default <code>tc</code> <code>dict or numpy.ndarray</code> <p>If array, first dimension should be the neuron</p> required <code>features</code> <code>TsdFrame</code> <p>The 2 columns features that were used to compute the tuning curves</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch over which the tuning curves were computed If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target features</p> <code>None</code> <code>bitssec</code> <code>bool, optional</code> <p>By default, the function return bits per spikes. Set to true for bits per seconds</p> <code>False</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>Spatial Information (default is bits/spikes)</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_mutual_info(tc, features, ep=None, minmax=None, bitssec=False):\n\"\"\"\n    Mutual information as defined in\n    Skaggs, W. E., McNaughton, B. L., &amp; Gothard, K. M. (1993).\n    An information-theoretic approach to deciphering the hippocampal code.\n    In Advances in neural information processing systems (pp. 1030-1037).\n    Parameters\n    ----------\n    tc : dict or numpy.ndarray\n        If array, first dimension should be the neuron\n    features : TsdFrame\n        The 2 columns features that were used to compute the tuning curves\n    ep : IntervalSet, optional\n        The epoch over which the tuning curves were computed\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target features\n    bitssec : bool, optional\n        By default, the function return bits per spikes.\n        Set to true for bits per seconds\n    Returns\n    -------\n    pandas.DataFrame\n        Spatial Information (default is bits/spikes)\n    \"\"\"\n# A bit tedious here\nif type(tc) is dict:\nfx = np.array([tc[i] for i in tc.keys()])\nidx = list(tc.keys())\nelif type(tc) is np.ndarray:\nfx = tc\nidx = np.arange(len(tc))\nnb_bins = (fx.shape[1] + 1, fx.shape[2] + 1)\ncols = features.columns\nbins = []\nfor i, c in enumerate(cols):\nif minmax is None:\nbins.append(\nnp.linspace(np.min(features[c]), np.max(features[c]), nb_bins[i])\n)\nelse:\nbins.append(\nnp.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins[i])\n)\nif isinstance(ep, nap.IntervalSet):\nfeatures = features.restrict(ep)\noccupancy, _, _ = np.histogram2d(\nfeatures[cols[0]].values, features[cols[1]].values, [bins[0], bins[1]]\n)\noccupancy = occupancy / occupancy.sum()\nfr = np.nansum(fx * occupancy, (1, 2))\nfr = fr[:, np.newaxis, np.newaxis]\nfxfr = fx / fr\nwith warnings.catch_warnings():\nwarnings.simplefilter(\"ignore\")\nlogfx = np.log2(fxfr)\nlogfx[np.isinf(logfx)] = 0.0\nSI = np.nansum(occupancy * fx * logfx, (1, 2))\nif bitssec:\nSI = pd.DataFrame(index=idx, columns=[\"SI\"], data=SI)\nreturn SI\nelse:\nSI = SI / fr[:, 0, 0]\nSI = pd.DataFrame(index=idx, columns=[\"SI\"], data=SI)\nreturn SI\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_1d_tuning_curves_continous","title":"<code>compute_1d_tuning_curves_continous(tsdframe, feature, nb_bins, ep=None, minmax=None)</code>","text":"<p>Computes 1-dimensional tuning curves relative to a feature with continous data.</p> <p>Parameters:</p> Name Type Description Default <code>tsdframe</code> <code>Tsd or TsdFrame</code> <p>Input data (e.g. continus calcium data where each column is the calcium activity of one neuron)</p> required <code>feature</code> <code>Tsd</code> <p>The feature (one column)</p> required <code>nb_bins</code> <code>int</code> <p>Number of bins in the tuning curves</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>DataFrame to hold the tuning curves</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tsdframe is not a Tsd or a TsdFrame object.</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_tuning_curves_continous(\ntsdframe, feature, nb_bins, ep=None, minmax=None\n):\n\"\"\"\n    Computes 1-dimensional tuning curves relative to a feature with continous data.\n    Parameters\n    ----------\n    tsdframe : Tsd or TsdFrame\n        Input data (e.g. continus calcium data\n        where each column is the calcium activity of one neuron)\n    feature : Tsd\n        The feature (one column)\n    nb_bins : int\n        Number of bins in the tuning curves\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        If None, the boundaries are inferred from the target feature\n    Returns\n    -------\n    pandas.DataFrame\n        DataFrame to hold the tuning curves\n    Raises\n    ------\n    RuntimeError\n        If tsdframe is not a Tsd or a TsdFrame object.\n    \"\"\"\nif not isinstance(tsdframe, (nap.Tsd, nap.TsdFrame)):\nraise RuntimeError(\"Unknown format for tsdframe.\")\nif isinstance(ep, nap.IntervalSet):\nfeature = feature.restrict(ep)\ntsdframe = tsdframe.restrict(ep)\nelse:\ntsdframe = tsdframe.restrict(feature.time_support)\nif minmax is None:\nbins = np.linspace(np.min(feature), np.max(feature), nb_bins + 1)\nelse:\nbins = np.linspace(minmax[0], minmax[1], nb_bins + 1)\nalign_times = tsdframe.value_from(feature)\nidx = np.digitize(align_times.values, bins) - 1\ntmp = pd.DataFrame(tsdframe).groupby(idx).mean()\ntmp = tmp.reindex(np.arange(0, len(bins) - 1))\ntmp.index = pd.Index(bins[0:-1] + np.diff(bins) / 2)\ntmp = tmp.fillna(0)\nreturn pd.DataFrame(tmp)\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_2d_tuning_curves_continuous","title":"<code>compute_2d_tuning_curves_continuous(tsdframe, features, nb_bins, ep=None, minmax=None)</code>","text":"<p>Computes 2-dimensional tuning curves relative to a 2d feature with continous data.</p> <p>Parameters:</p> Name Type Description Default <code>tsdframe</code> <code>Tsd or TsdFrame</code> <p>Input data (e.g. continuous calcium data where each column is the calcium activity of one neuron)</p> required <code>features</code> <code>TsdFrame</code> <p>The 2d feature (two columns)</p> required <code>nb_bins</code> <code>int or tuple</code> <p>Number of bins in the tuning curves (separate for 2 feature dimensions if tuple provided)</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>The epoch on which tuning curves are computed. If None, the epoch is the time support of the feature.</p> <code>None</code> <code>minmax</code> <code>tuple or list, optional</code> <p>The min and max boundaries of the tuning curves. Should be a tuple of minx, maxx, miny, maxy If None, the boundaries are inferred from the target feature</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing: </p> <p>tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).</p> <p>xy (list): List of bins center in the two dimensions</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If tsdframe is not a Tsd/TsdFrame or if features is not 2 columns</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_2d_tuning_curves_continuous(\ntsdframe, features, nb_bins, ep=None, minmax=None\n):\n\"\"\"\n    Computes 2-dimensional tuning curves relative to a 2d feature with continous data.\n    Parameters\n    ----------\n    tsdframe : Tsd or TsdFrame\n        Input data (e.g. continuous calcium data\n        where each column is the calcium activity of one neuron)\n    features : TsdFrame\n        The 2d feature (two columns)\n    nb_bins : int or tuple\n        Number of bins in the tuning curves (separate for 2 feature dimensions if tuple provided)\n    ep : IntervalSet, optional\n        The epoch on which tuning curves are computed.\n        If None, the epoch is the time support of the feature.\n    minmax : tuple or list, optional\n        The min and max boundaries of the tuning curves.\n        Should be a tuple of minx, maxx, miny, maxy\n        If None, the boundaries are inferred from the target feature\n    Returns\n    -------\n    tuple\n        A tuple containing: \\n\n        tc (dict): Dictionnary of the tuning curves with dimensions (nb_bins, nb_bins).\\n\n        xy (list): List of bins center in the two dimensions\n    Raises\n    ------\n    RuntimeError\n        If tsdframe is not a Tsd/TsdFrame or if features is not 2 columns\n    \"\"\"\nif not isinstance(tsdframe, (nap.Tsd, nap.TsdFrame)):\nraise RuntimeError(\"Unknown format for tsdframe.\")\nif not isinstance(features, nap.TsdFrame):\nraise RuntimeError(\"Unknown format for features.\")\nif isinstance(ep, nap.IntervalSet):\nfeatures = features.restrict(ep)\ntsdframe = tsdframe.restrict(ep)\nelse:\ntsdframe = tsdframe.restrict(features.time_support)\nif features.shape[1] != 2:\nraise RuntimeError(\"features input is not 2 columns.\")\nif isinstance(nb_bins, int):\nnb_bins = (nb_bins, nb_bins)\nelif len(nb_bins) != 2:\nraise RuntimeError(\"nb_bins should be int or tuple of 2 ints\")\ncols = list(features.columns)\nbinsxy = {}\nidxs = {}\nfor i, c in enumerate(cols):\nif minmax is None:\nbins = np.linspace(np.min(features[c]), np.max(features[c]), nb_bins[i] + 1)\nelse:\nbins = np.linspace(minmax[i + i % 2], minmax[i + 1 + i % 2], nb_bins[i] + 1)\nalign_times = tsdframe.value_from(features[c], ep)\nidxs[c] = np.digitize(align_times.values, bins) - 1\nbinsxy[c] = bins\nidxs = pd.DataFrame(idxs)\ntc_np = np.zeros((tsdframe.shape[1], nb_bins[0], nb_bins[1])) * np.nan\nfor k, tmp in idxs.groupby(cols):\nif (0 &lt;= k[0] &lt; nb_bins[0]) and (0 &lt;= k[1] &lt; nb_bins[1]):\ntc_np[:, k[0], k[1]] = tsdframe.iloc[tmp.index].mean(0).values\ntc_np[np.isnan(tc_np)] = 0.0\nxy = [binsxy[c][0:-1] + np.diff(binsxy[c]) / 2 for c in binsxy.keys()]\ntc = {c: tc_np[i] for i, c in enumerate(tsdframe.columns)}\nreturn tc, xy\n</code></pre>"},{"location":"reference/process/tuning_curves/#pynapple.process.tuning_curves.compute_1d_poisson_glm","title":"<code>compute_1d_poisson_glm(group, feature, binsize, windowsize, ep, time_units='s', niter=100, tolerance=1e-05)</code>","text":"<p>Poisson GLM</p> <p>Warning : this function is still experimental!</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>TsGroup</code> <p>Spike trains</p> required <code>feature</code> <code>Tsd</code> <p>The regressors</p> required <code>binsize</code> <code>float</code> <p>Bin size</p> required <code>windowsize</code> <code>Float</code> <p>The window for offsetting the regressors</p> required <code>ep</code> <code>IntervalSet, optional</code> <p>On which epoch to perfom the GLM</p> required <code>time_units</code> <code>str, optional</code> <p>Time units of binsize and windowsize</p> <code>'s'</code> <code>niter</code> <code>int, optional</code> <p>Number of iteration for fitting the GLM</p> <code>100</code> <code>tolerance</code> <code>float, optional</code> <p>Tolerance for stopping the IRLS</p> <code>1e-05</code> <p>Returns:</p> Type Description <code>tuple</code> <p>regressors : TsdFrame</p> <p>offset : pandas.Series</p> <p>prediction : TsdFrame</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if group is not a TsGroup</p> Source code in <code>pynapple/process/tuning_curves.py</code> <pre><code>def compute_1d_poisson_glm(\ngroup, feature, binsize, windowsize, ep, time_units=\"s\", niter=100, tolerance=1e-5\n):\n\"\"\"\n    Poisson GLM\n    Warning : this function is still experimental!\n    Parameters\n    ----------\n    group : TsGroup\n        Spike trains\n    feature : Tsd\n        The regressors\n    binsize : float\n        Bin size\n    windowsize : Float\n        The window for offsetting the regressors\n    ep : IntervalSet, optional\n        On which epoch to perfom the GLM\n    time_units : str, optional\n        Time units of binsize and windowsize\n    niter : int, optional\n        Number of iteration for fitting the GLM\n    tolerance : float, optional\n        Tolerance for stopping the IRLS\n    Returns\n    -------\n    tuple\n        regressors : TsdFrame\\n\n        offset : pandas.Series\\n\n        prediction : TsdFrame\\n\n    Raises\n    ------\n    RuntimeError\n        if group is not a TsGroup\n    \"\"\"\nif type(group) is nap.TsGroup:\nnewgroup = group.restrict(ep)\nelse:\nraise RuntimeError(\"Unknown format for group\")\nbinsize = nap.format_timestamps(binsize, time_units)[0]\nwindowsize = nap.format_timestamps(windowsize, time_units)[0]\n# Bin the spike train\ncount = newgroup.count(binsize)\n# Downsample the feature to binsize\ntidx = []\ndfeat = []\nfor i in ep.index:\nbins = np.arange(ep.start[i], ep.end[i] + binsize, binsize)\nidx = np.digitize(feature.index.values, bins) - 1\ntmp = feature.groupby(idx).mean()\ntidx.append(bins[0:-1] + np.diff(bins) / 2)\ndfeat.append(tmp)\ndfeat = nap.Tsd(t=np.hstack(tidx), d=np.hstack(dfeat), time_support=ep)\n# Build the Hankel matrix\nnt = np.abs(windowsize // binsize).astype(\"int\") + 1\nX = hankel(\nnp.hstack((np.zeros(nt - 1), dfeat.values))[: -nt + 1], dfeat.values[-nt:]\n)\nX = np.hstack((np.ones((len(dfeat), 1)), X))\n# Fitting GLM for each neuron\nregressors = []\nfor i, n in enumerate(group.keys()):\nprint(\"Fitting Poisson GLM for unit %i\" % n)\nb = nap.jitted_functions.jit_poisson_IRLS(\nX, count[n].values, niter=niter, tolerance=tolerance\n)\nregressors.append(b)\nregressors = np.array(regressors).T\noffset = regressors[0]\nregressors = regressors[1:]\nregressors = nap.TsdFrame(\nt=np.arange(-nt + 1, 1) * binsize, d=regressors, columns=list(group.keys())\n)\noffset = pd.Series(index=group.keys(), data=offset)\nprediction = nap.TsdFrame(\nt=dfeat.index.values,\nd=np.exp(np.dot(X[:, 1:], regressors.values) + offset.values) * binsize,\n)\nreturn (regressors, offset, prediction)\n</code></pre>"}]}